[
  {
    "url": "https://docs.atlan.com/",
    "content": "Discover, trust, and govern your data & AI ecosystem\nEverything you need to get started with Atlan.\nSet up Snowflake\nSet up Databricks\nSet up PowerÂ BI\nAtlanÂ Architecture\nBrowserÂ Extension\nGet started\nð\nQuick-start guide\nStep-by-step onboarding\nð§\nSecure agent\nEnterprise-grade deployment options\nð\nPlaybooks automation\nRule-based metadata updates at scale\nCore features\nð\nFind & understand data\nSearch, discover, and profile assets\nð¡ï¸\nGovern & manage\nCreate data contracts & policies\nð\nIntegrate\nAutomation, collaboration & other integrations\nDeveloper hub\nâï¸\nIntroductory walkthrough\nPlay with APIs in minutes\nð»\nClient SDKs\nJava, Python & more\nð¦\nPackages\nDeveloper-built utilities and integrations\nAtlan University\nGet started with Atlan by building the right strategy and setting a strong foundation.\nAtlan Security\nA comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures\nHelp and support\nFind answers or contact our team for personalized assistance"
  },
  {
    "url": "https://docs.atlan.com/support/submit-request",
    "content": "Submit request\nAim to include as much information and detail in your request as possible to reduce delays between replies.\nName\n*\nEmail\n*\nSubject\n*\nAtlan URL\n*\nThe URL of your Atlan tenant\nSeverity\nSEV0 (S0) - System down or critical issue\nSEV1 (S1) - Major functionality affected\nSEV2 (S2) - General question or minor issue\nSEV3 (S3) - Feature request or enhancement\nRefer to severity and response SLA guidelines as outlined\nhere\nHow is this impacting you?\nI'm unable to use the product\nA major feature stopped working\nAn issue is slowing me down\nI have a non-urgent question\nI have a suggestion that will help me with my use-case\nI need help with something\nDescription\n*\nAttachments\nð\nAdd file\nor drop files here\nSubmit"
  },
  {
    "url": "https://docs.atlan.com/get-started/what-is-atlan",
    "content": "Get Started\nWhat is Atlan?\nOn this page\nWhat is Atlan?\nWe are a modern data workspace that makes collaboration among diverse users like business, analysts, and engineers easier, increasing efficiency and agility in data projects.\nWe started out as a data team, solving social good problems using data science. We built Atlan for ourselves over the course of 200 data projects, which included India's national data platform used by the prime minister and monitoring the Sustainable Development Goals with the United Nations.\nAtlan helped us build India's national data platform with an 8-member team, making it the fastest project of its kind to go live in just 12 months  - instead of the projected three years.\nWhy we built Atlan\nâ\nData teams can be diverse: analysts, scientists, engineers, and business users. Diverse people with diverse tools and skillsets mean diverse DNAs. All of it led to chaos, which made our Slack channels look like this...\nWe call this \"collaboration overheard\"\nâ\nWe knew we couldn't scale like this, there had to be a better way. We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams. We then experimented for two years and across 200 data projects to create our own idea of what makes data teams successful. We call this\nDataOps\n.\nHow Atlan helps data teams\nâ\nWith Atlan, analyst teams at Unilever have shipped 100+ additional data projects per quarter, while data science teams at Samsung have saved 50% of their time.\nCreate self-service ecosystems by reducing dependencies\nâ\nAtlan makes all your data assets easily discoverable. No more Slack messages like \"Where's that dataset?\" or long email threads for approvals. With Atlan, you can simply Cmd+K your way to the right data asset.\nKey capabilities\n:\ndiscovery and search\n,\nVisual Query Builder\n,\nsaved queries\n,\nREADMEs\nImprove the agility of your data team\nâ\nData practitioners spend 30-50% of their time finding and understanding data. Atlan cuts that time by 95%. Your data team will be shipping 2-3 times more projects in no time.\nKey capabilities\n: visibility of\ndata quality\ntests and observability alerts,\nautomated lineage\n,\nAtlan AI\nPromote governance and a sustainable data culture\nâ\nDon't lose sleep trying to figure out if your sensitive data is secure. Build ecosystems of trust, make your team happy, and let Atlan manage governance and security behind the scenes.\nKey capabilities\n:\ntag sensitive data\n,\ngranular access control\n,\ndata products\nTags:\natlan\ndocumentation\nNext\nAdministrators\nWhy we built Atlan\nHow Atlan helps data teams"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nGet Started\nSet up Snowflake\nOn this page\nSet up Snowflake\nWho can do this?\nYou need your Snowflake administrator to run these commands   -  you may not have access yourself.\nCreate user and role in Snowflake\nâ\nCreate a role and user in Snowflake using the following commands:\nCreate role\nâ\nCreate a role in Snowflake using the following commands:\nCREATE\nOR\nREPLACE\nROLE atlan_user_role\n;\nGRANT\nOPERATE\n,\nUSAGE\nON\nWAREHOUSE\n\"<warehouse-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<warehouse-name>\nwith the default warehouse to use when\nrunning the Snowflake crawler\n.\nAtlan requires the following privileges to:\nOPERATE\nenables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped.\nUSAGE\nenables Atlan to show or list metadata from Snowflake. This in turn enables the\nSnowflake crawler\nto run the\nSHOW\nquery.\nCreate a user\nâ\nCreate a separate user to integrate into Atlan, using one of the following 3 options:\nWith a public key in Snowflake\nâ\nSee\nSnowflake's official guide for details on generating an RSA key-pair\n. To create a user with a key-pair, replace the value for\nrsa_public_key\nwith the public key and run the following:\nCREATE\nUSER\natlan_user rsa_public_key\n=\n'MIIBIjANBgkqh...'\ndefault_role\n=\natlan_user_role default_warehouse\n=\n'<warehouse-name>'\ndisplay_name\n=\n'Atlan'\nTYPE\n=\n'SERVICE'\nLearn more about the\nSERVICE\ntype property in\nSnowflake documentation\n.\nDid you know?\nAtlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the\n-nocrypt\noption. Refer to\nSnowflake documentation\nto learn more.\nWith a password in Snowflake\nâ\nDid you know?\nSnowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can\nmodify the crawler configuration\nto update the authentication method.\nTo create a user with a password, replace\n<password>\nand run the following:\nCREATE\nUSER\natlan_user password\n=\n'<password>'\ndefault_role\n=\natlan_user_role default_warehouse\n=\n'<warehouse-name>'\ndisplay_name\n=\n'Atlan'\nTYPE\n=\n'LEGACY_SERVICE'\nLearn more about the\nLEGACY_SERVICE\ntype property in\nSnowflake documentation\n.\nManaged through your identity provider (IdP)\nPrivate preview\nâ\nThis method is currently only available if Okta is your IdP (Snowflake supports)\nauthenticating natively through Okta\n:\nCreate a user in your identity provider (IdP) and\nuse federated authentication in Snowflake\n.\nThe password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled.\nGrant role to user\nâ\nTo grant the\natlan_user_role\nto the new user:\nGRANT\nROLE atlan_user_role\nTO\nUSER\natlan_user\n;\nConfigure OAuth (client credentials flow) with Microsoft Entra ID\nâ\nTo configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow:\nFollow\nSnowflake's documentation\nto:\nRegister a new application in Microsoft Entra ID\nCollect the\nclient ID\n,\ntenant ID\n, and\nclient secret\nAdd the required API permissions\nIn Snowflake, create a security integration using the following:\nCREATE\nSECURITY INTEGRATION external_oauth_azure_ad\nTYPE\n=\nexternal_oauth\nENABLED\n=\ntrue\nEXTERNAL_OAUTH_TYPE\n=\nazure\nEXTERNAL_OAUTH_ISSUER\n=\n'\\<AZURE_AD_ISSUER\\>'\nEXTERNAL_OAUTH_JWS_KEYS_URL\n=\n'\\<AZURE_AD_JWS_KEY_ENDPOINT\\>'\nEXTERNAL_OAUTH_AUDIENCE_LIST\n=\n(\n'\\<SNOWFLAKE_APPLICATION_ID_URI\\>'\n)\nEXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM\n=\n'sub'\nEXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE\n=\n'login_name'\n;\nReplace the placeholders with actual values from your Azure AD app:\n<AZURE_AD_ISSUER>\nâ Your tenant's OAuth 2.0 issuer URL\n<AZURE_AD_JWS_KEY_ENDPOINT>\nâ Azure JWKs URI\n<SNOWFLAKE_APPLICATION_ID_URI>\nâ Application ID URI of the Azure app\nCreate a Snowflake user with a login name that exactly matches the Azure AD client object ID:\nCREATE\nUSER\noauth_svc_user\nWITH\nLOGIN_NAME\n=\n'\\<AZURE_AD_CLIENT_OBJECT_ID\\>'\n-- Use Azure client OBJECT ID\nDEFAULT_ROLE\n=\n\\\n<\nROLE\\\n>\nDEFAULT_WAREHOUSE\n=\n\\\n<\nWAREHOUSE\\\n>\n;\nGrant the configured role to this user:\nGRANT\nROLE \\\n<\nROLE\\\n>\nTO\nUSER\noauth_svc_user\n;\nChoose metadata fetching method\nâ\nAtlan supports two methods for fetching metadata from Snowflake   -  account usage and information schema. You should choose one of these two methods to set up Snowflake:\nÂ\nAccount usage\nInformation schema\nOverview\nSimplified grants but some limitations in functionality\nMost comprehensive approach, more grant management required\nMethod\nViews in the\nSNOWFLAKE\ndatabase that display object metadata and usage metrics for your account\nSystem-defined views and table functions that provide extensive metadata for objects created in your account\nPermissions\nUser role and account, single grant for\nSNOWFLAKE\ndatabase\nUser role and account, multiple grants per database\nData latency\n45 minutes to 3 hours (varies by view)\nNone\nHistorical data retention\n1 year\n7 days to 6 months (varies by view or table function)\nAsset extraction\nACCOUNT_USAGE\nschema\nINFORMATION_SCHEMA\nschema\nView lineage\nACCOUNT_USAGE\nschema\nINFORMATION_SCHEMA\nschema\nTable lineage\nACCOUNT_USAGE\nschema\nACCOUNT_USAGE\nschema\nTag import\nACCOUNT_USAGE\nschema\nACCOUNT_USAGE\nschema\nUsage and popularity\nACCOUNT_USAGE\nschema\nACCOUNT_USAGE\nschema\nMetadata extraction time\nVaries by warehouse size. For example, 8 minutes for 10 million assets (recommended for extracting a large number of assets)\nVaries by warehouse size. For example, 2+ hours for 10 million assets\nExtraction limitations\nExternal table location data, procedures, and primary and foreign keys\nNone\nGrant permissions for account usage method\nâ\ndanger\nIf you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time.\nThis method uses the views in\nSNOWFLAKE.ACCOUNT_USAGE\n(or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are\nlimitations with this approach\n.\nTo crawl assets, generate lineage, and import tags\nâ\nIf you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead.\nSnowflake\nstores all tag objects\nin the\nACCOUNT_USAGE\nschema. If you're using the\naccount usage method\nto crawl metadata in Atlan or you have\nconfigured the Snowflake miner\n, you need to grant the same permissions to\nimport tags\nas required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires\nEnterprise Edition or higher\n.\nTo use the default\nSNOWFLAKE\ndatabase and\nACCOUNT_USAGE\nschema and also mine Snowflake's query history (for lineage), grant these permissions:\nUSE\nROLE ACCOUNTADMIN\n;\nGRANT\nIMPORTED\nPRIVILEGES\nON\nDATABASE\nSNOWFLAKE\nTO\nROLE atlan_user_role\n;\nThe\nACCOUNTADMIN\nrole is required to grant privileges on the\nSNOWFLAKE\ndatabase due to the following reasons:\nBy default, only the\nACCOUNTADMIN\nrole can access the\nSNOWFLAKE\ndatabase.\nTo enable other roles to access the database and schemas and query the views, a user with the\nACCOUNTADMIN\nrole needs to grant\nIMPORTED PRIVILEGES\non the\nSNOWFLAKE\ndatabase to the desired roles.\nTo use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions:\nGRANT\nUSAGE\nON\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\n\"<copied-schema>\"\nIN\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<copied-database>\nwith the copied Snowflake database name.\nReplace\n<copied-schema>\nwith the copied Snowflake\nACCOUNT_USAGE\nschema name.\nThe grants for the copied version can't be used on the original\nSNOWFLAKE\ndatabase. This is because Snowflake produces an error that granular grants can't be given to imported databases.\nWhen using a cloned or copied version, verify that the table or view definition remains unchanged as in your\nSNOWFLAKE\ndatabase. If the format is different. For example, a column is missing and it no longer qualifies as a clone.\nTo crawl streams\nâ\nTo crawl streams, provide the following permissions:\nTo crawl current streams:\nGRANT\nUSAGE\nON\nALL\nSCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nSTREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the Snowflake database name.\nTo crawl future streams:\nGRANT\nUSAGE\nON\nFUTURE SCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE STREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the Snowflake database name.\n(Optional) To preview and query existing assets\nâ\nTo query and preview data within assets that already exist in Snowflake, add these permissions:\nGRANT\nUSAGE\nON\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nALL\nSCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nEXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nMATERIALIZED VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nSTREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nPIPE\n\"<pipe-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.)\n(Optional) To preview and query future assets\nâ\nTo query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead.\nGRANT\nUSAGE\nON\nFUTURE SCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE EXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE MATERIALIZED VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE STREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nFUTURE PIPES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.)\ndanger\nVerify that all the assets you'd like to crawl are present in these grants by\nchecking the grants\non the user role defined for the crawler.\nGrant permissions for information schema method\nâ\nThis method uses views in the\nINFORMATION_SCHEMA\nschema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method.\nTo crawl existing assets\nâ\nGrant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead.\nGrant permissions to crawl existing assets:\nGRANT\nUSAGE\nON\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nALL\nSCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nEXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nMATERIALIZED VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nSTREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nPIPE\n\"<pipe-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.)\nGrant permissions to crawl functions:\nGRANT\nUSAGE\nON\nALL\nFUNCTIONS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.)\nFor secure user-defined functions (UDFs), grant\nOWNERSHIP\npermissions to retrieve metadata:\nGRANT\nOWNERSHIP\nON\nFUNCTION\n<\nschema_name\n>\n.\n<\nudf_name\n>\nTO\nROLE\n<\nrole_name\n>\n;\nReplace the placeholders with the appropriate values:\n<schema_name>\n: The name of the schema that contains the user-defined function (UDF).\n<udf_name>\n: The name of the secure UDF that requires ownership permissions.\n<role_name>\n: The role that gets assigned ownership of the secure UDF.\nDid you know?\nThe statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well.\nTo crawl future assets\nâ\nTo crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead.\nTo grant permissions at a database level:\nGRANT\nUSAGE\nON\nFUTURE SCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE EXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE MATERIALIZED VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE STREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nFUTURE PIPES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nFUTURE FUNCTIONS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.)\ndanger\nFor any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to\nSnowflake documentation\n.\nTo grant permissions at a schema level:\nGRANT\nREFERENCES\nON\nFUTURE\nTABLES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE EXTERNAL\nTABLES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE VIEWS\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE MATERIALIZED VIEWS\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE STREAMS\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nFUTURE PIPES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database and\n<schema-name>\nwith the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.)\nTo mine query history for lineage\nâ\nTo also mine Snowflake's query history (for lineage), add these permissions. You can use either option:\nTo mine query history direct from Snowflake's internal tables:\nUSE\nROLE ACCOUNTADMIN\n;\nGRANT\nIMPORTED\nPRIVILEGES\nON\nDATABASE\nsnowflake\nTO\nROLE atlan_user_role\n;\nTo mine query history from a cloned or copied set of tables, where you can also remove any sensitive data:\nGRANT\nUSAGE\nON\nDATABASE\n\"<cloned-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\n\"<cloned-database>\"\n.\n\"<cloned-account-usage-schema>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nTABLES\nIN\nSCHEMA\n\"<cloned-database>\"\n.\n\"<cloned-account-usage-schema>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nVIEWS\nIN\nSCHEMA\n\"<cloned-database>\"\n.\n\"<cloned-account-usage-schema>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<cloned-database>\nwith the name of the cloned database, and\n<cloned-account-usage-schema>\nwith the name of the cloned schema containing account usage details.\nWhen using a cloned or copied version, verify that the table or view definition remains unchanged as in your\nSNOWFLAKE\ndatabase. If the format is different. For example, a column is missing and it no longer qualifies as a clone.\n(Optional) To preview and query existing assets\nâ\nTo query and preview data within assets that already exist in Snowflake, add these permissions:\nGRANT\nUSAGE\nON\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nALL\nSCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nEXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nMATERIALIZED VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nALL\nSTREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nPIPE\n\"<pipe-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.)\n(Optional) To preview and query future assets\nâ\nTo query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead.\nGRANT\nUSAGE\nON\nFUTURE SCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE EXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE MATERIALIZED VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE STREAMS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nFUTURE PIPES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.)\ndanger\nFor any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to\nSnowflake documentation\n.\nTo grant permissions at a schema level:\nGRANT\nSELECT\nON\nFUTURE\nTABLES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE EXTERNAL\nTABLES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE VIEWS\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE MATERIALIZED VIEWS\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nSELECT\nON\nFUTURE STREAMS\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nMONITOR\nON\nFUTURE PIPES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database and\n<schema-name>\nwith the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.)\ndanger\nVerify that all the assets you'd like to crawl are present in these grants by\nchecking the grants\non the user role defined for the crawler.\n(Optional) To import Snowflake tags\nâ\nSnowflake\nstores all tag objects\nin the\nACCOUNT_USAGE\nschema. Note that object tagging in Snowflake currently requires\nEnterprise Edition or higher\n.\nTo\nimport tags from Snowflake\n, grant these permissions:\nTo use the default\nSNOWFLAKE\ndatabase and\nACCOUNT_USAGE\nschema and also mine Snowflake's query history (for lineage), grant these permissions:\nUSE\nROLE ACCOUNTADMIN\n;\nGRANT\nIMPORTED\nPRIVILEGES\nON\nDATABASE\nSNOWFLAKE\nTO\nROLE atlan_user_role\n;\nThe\nACCOUNTADMIN\nrole is required to grant privileges on the\nSNOWFLAKE\ndatabase due to the following reasons:\nBy default, only the\nACCOUNTADMIN\nrole can access the\nSNOWFLAKE\ndatabase.\nTo enable other roles to access the database and schemas and query the views, a user with the\nACCOUNTADMIN\nrole needs to grant\nIMPORTED PRIVILEGES\non the\nSNOWFLAKE\ndatabase to the desired roles.\nTo use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions:\nGRANT\nUSAGE\nON\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\n\"<copied-schema>\"\nIN\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<copied-database>\nwith the copied Snowflake database name.\nReplace\n<copied-schema>\nwith the copied Snowflake\nACCOUNT_USAGE\nschema name.\nThe grants for the copied version can't be used on the original\nSNOWFLAKE\ndatabase. This is because Snowflake produces an error that granular grants can't be given to imported databases.\n(Optional) To push updated tags to Snowflake\nâ\nTo\npush tags updated for assets in Atlan to Snowflake\n, grant these permissions:\nGRANT\nAPPLY\nTAG\nON\nACCOUNT\nTO\nROLE\n<\nrole\n-\nname\n>\n;\nYou can learn more about tag privileges from\nSnowflake documentation\n.\n(Optional) To crawl dynamic tables\nâ\nAtlan currently supports fetching metadata for dynamic tables using the\nMONITOR\nprivilege. Refer to\nSnowflake documentation\nto learn more.\nTo crawl existing dynamic tables from Snowflake:\nGrant permissions at a database level:\nGRANT\nMONITOR\nON\nALL\nDYNAMIC\nTABLES\nIN\nDATABASE\n\"<DATABASE_NAME>\"\nTO\nROLE atlan_user_role\n;\nGrant permissions at a schema level:\nGRANT\nMONITOR\nON\nALL\nDYNAMIC\nTABLES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nTo crawl future dynamic tables from Snowflake:\nGrant permissions at a database level:\nGRANT\nMONITOR\nON\nFUTURE DYNAMIC\nTABLES\nIN\nDATABASE\n\"<DATABASE_NAME>\"\nTO\nROLE atlan_user_role\n;\nGrant permissions at a schema level:\nGRANT\nMONITOR\nON\nFUTURE DYNAMIC\nTABLES\nIN\nSCHEMA\n\"<database-name>.<schema-name>\"\nTO\nROLE atlan_user_role\n;\nReplace\n<database-name>\nwith the database and\n<schema-name>\nwith the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.)\n(Optional) To crawl Iceberg tables\nâ\nAtlan currently supports fetching metadata for\nIceberg tables\nonly for the information schema extraction method.\nTo crawl Iceberg tables from Snowflake, grant the following permissions:\nTo crawl existing Iceberg tables in Snowflake:\nGRANT\nREFERENCES\nON\nALL\nICEBERG\nTABLES\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_user_role\n;\nTo crawl future Iceberg tables in Snowflake:\nGRANT\nREFERENCES\nON\nFUTURE ICEBERG\nTABLES\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_user_role\n;\nTo crawl Iceberg catalog metadata for Iceberg tables in Snowflake:\nGRANT\nUSAGE\nON\nINTEGRATION\n<\nintegration\n-\nname\n>\nTO\nROLE atlan_user_role\n;\ndanger\nYou must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually.\n(Optional) To crawl Snowflake stages\nâ\nAtlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES.\nTo crawl stages from Snowflake:\nGrant\nUSAGE\nand\nREAD\nprivileges on all existing stages at the database level:\nGRANT\nUSAGE\nON\nALL\nSTAGES\nIN\nDATABASE\n<\ndatabase_name\n>\nTO\nROLE atlan_user_role\n;\nGRANT\nREAD\nON\nALL\nSTAGES\nIN\nDATABASE\n<\ndatabase_name\n>\nTO\nROLE atlan_user_role\n;\nReplace\n<database_name>\nwith the name of your Snowflake database\nReplace\n<atlan_user_role>\nwith the role you've granted Atlan to use for crawling.\nGrant\nUSAGE\nand\nREAD\nprivileges on all future stages at the database level:\nGRANT\nUSAGE\nON\nFUTURE STAGES\nIN\nDATABASE\n<\ndatabase_name\n>\nTO\nROLE atlan_user_role\n;\nGRANT\nREAD\nON\nFUTURE STAGES\nIN\nDATABASE\n<\ndatabase_name\n>\nTO\nROLE atlan_user_role\n;\nReplace\n<database_name>\nwith the name of your Snowflake database\nReplace\n<atlan_user_role>\nwith the role you've granted Atlan to use for crawling.\nAllowlist the Atlan IP\nâ\nIf you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or\nsubmit a request\n.\n(If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.)\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSnowflake\nNext\nSet up an AWS private network link to Snowflake\nCreate user and role in Snowflake\nChoose metadata fetching method\nGrant permissions for account usage method\nGrant permissions for information schema method\nAllowlist the Atlan IP"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nGet Started\nSet up Databricks\nOn this page\nSet up Databricks\nAtlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods:\nPersonal access token authentication\nAWS service principal authentication\nAzure service principal authentication\nPersonal access token authentication\nâ\nWho can do this?\nCheck that you have\nAdmin\nand\nDatabricks SQL access\nfor the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator.\nGrant user access to workspace\nâ\nTo grant workspace access to the user creating a personal access token:\nFrom the left menu of the account console, click\nWorkspaces\nand then select a workspace to which you want to add the user.\nFrom the tabs along the top of your workspace page, click the\nPermissions\ntab.\nIn the upper right of the\nPermissions\npage, click\nAdd permissions\n.\nIn the\nAdd permissions\ndialog, enter the following details:\nFor\nUser, group, or service principal\n, select the user to grant access.\nFor\nPermission\n, click the dropdown and select workspace\nUser.\nGenerate a personal access token\nâ\nYou can\ngenerate a personal access token\nin your Databricks workspace to the authenticate the\nintegration in Atlan\n.\nTo generate a personal access token:\nFrom the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click\nUser\nSettings\n.\nUnder the\nSettings\nmenu, click\nDeveloper\n.\nOn the\nDeveloper\npage, next to\nAccess tokens\n, click\nManage\n.\nOn the\nAccess tokens\npage, click the\nGenerate new token\nbutton.\nIn the\nGenerate new token\ndialog:\nFor\nComment\n, enter a description of the token's intended use - for example,\nAtlan crawler\n.\nFor\nLifetime (days)\n, consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed.\nImportant!\nIf you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time.\nAt the bottom of the dialog, click\nGenerate\n.\nCopy and save the generated token in a secure location, and then click\nDone\n.\nSelect a cluster\nâ\nDid you know?\nAtlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to\nDatabricks documentation\nfor AWS Databricks workspaces or\nMicrosoft documentation\nfor Azure Databricks workspaces.\nYou can set up personal access token authentication for your Databricks instance using one of the following cluster options:\nInteractive cluster\nSQL warehouse (formerly SQL endpoint)\nInteractive cluster\nâ\nTo confirm an\nall-purpose interactive cluster\nis configured:\nFrom the left menu of any page of your Databricks instance, click\nCompute\n.\nUnder the\nAll-purpose clusters\ntab, verify you have a cluster defined.\nClick the link under the\nName\ncolumn of the table to open your cluster.\nUnder the\nConfiguration\ntab, verify the\nAutopilot options\nto\nTerminate after ... minutes\nis enabled.\nAt the bottom of the\nConfiguration\ntab, expand the\nAdvanced options\nexpandable.\nUnder the\nAdvanced options\nexpandable, open the\nJDBC/ODBC\ntab.\nConfirm that all of the fields in this tab are populated, and copy them for use in crawling:\nServer Hostname\n,\nPort\n, and\nHTTP Path\n.\nSQL warehouse (formerly SQL endpoint)\nâ\nTo confirm a\nSQL warehouse\nis configured:\nFrom the left menu of any page of your Databricks instance, open the dropdown just below the\ndatabricks\nlogo and change to\nSQL\n.\nFrom the refreshed left menu, click\nSQL Warehouses\n.\nClick the link under the\nName\ncolumn of the table to open your SQL warehouse.\nUnder the\nConnection details\ntab, confirm that all of the fields are populated and copy them for use in crawling:\nServer hostname\n,\nPort\n, and\nHTTP path\n.\nAWS service principal authentication\nâ\nWho can do this?\nYou need your\nAWS Databricks account admin\nto create a service principal and manage OAuth credentials for the service principal and your\nAWS Databricks workspace admin\nto add the service principal to your AWS Databricks workspace - you may not have access yourself.\nYou need the following to authenticate the connection in Atlan:\nClient ID\nClient secret\nCreate a service principal\nâ\nYou can create a service principal directly in your Databricks account or from a Databricks workspace.\nIdentity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces.\nIdentity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace.\nIdentity federation enabled\nâ\nTo create a service principal from your Databricks account, with identify federation enabled:\nLog in to your Databricks\naccount console\nas an account admin.\nFrom the left menu of the account console, click\nUser management\n.\nFrom the tabs along the top of the\nUser management\npage, click the\nService principals\ntab.\nIn the upper right of the\nService principals\npage, click\nAdd service principal\n.\nOn the\nAdd service principal\npage, enter a name for the service principal and then click\nAdd\n.\nOnce the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click\nWorkspaces\nand then select a workspace to which you want to add the service principal.\nFrom the tabs along the top of your workspace page, click the\nPermissions\ntab.\nIn the upper right of the\nPermissions\npage, click\nAdd permissions\n.\nIn the\nAdd permissions\ndialog, enter the following details:\nFor\nUser, group, or service principal\n, select the service principal you created.\nFor\nPermission\n, click the dropdown and select workspace\nUser.\nIdentity federation disabled\nâ\nTo create a service principal from a Databricks workspace, with identity federation disabled:\nLog in to your AWS Databricks workspace as a workspace admin.\nFrom the top right of your workspace, click your username, and then from the dropdown, click\nAdmin Settings\n.\nIn the left menu of the\nSettings\npage, under the\nWorkspace admin\nsubheading, click\nIdentity and access\n.\nOn the\nIdentity and access\npage, under\nManagement and permissions\n, next to\nService principals\n, click\nManage\n.\nIn the upper right of the\nService principals\npage, click\nAdd service principal\n.\nIn the\nAdd service principal\ndialog, click the\nAdd new\nbutton.\nFor\nNew service principal display name\n, enter a name for the service principal and then click\nAdd\n.\nCreate an OAuth secret for the service principal\nâ\nYou need to create an OAuth secret to authenticate to Databricks REST APIs.\nTo create an OAuth secret for the\nservice principal\n:\nLog in to your Databricks\naccount console\nas an account admin.\nFrom the left menu of the account console, click\nUser management\n.\nFrom the tabs along the top of the\nUser management\npage, click the\nService principals\ntab.\nIn the upper right of the\nService principals\npage, select the\nservice principal you created\n.\nOn the service principal page, under\nOAuth secrets\n, click\nGenerate secret\n.\nFrom the\nGenerate secret\ndialog, copy the\nSecret\nand\nClient ID\nand store it in a secure location.\ndanger\nNote that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal.\nOnce you've copied the client ID and secret, click\nDone\n.\nAzure service principal authentication\nâ\nWho can do this?\nYou need your\nAzure Databricks account admin\nto create a service principal and your\nAzure Databricks workspace admin\nto add the service principal to your Azure Databricks workspace - you may not have access yourself.\nYou need the following to authenticate the connection in Atlan:\nClient ID (application ID)\nClient secret\nTenant ID (directory ID)\nCreate a service principal\nâ\nTo\nuse service principals on Azure Databricks\n, an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal.\nTo create a service principal:\nSign in to the\nAzure portal\n.\nIf you have access to multiple tenants, subscriptions, or directories, click the\nDirectories + subscriptions\n(directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal.\nIn_Search resources, services, and docs_, search for and select\nMicrosoft Entra ID\n.\nClick**+ Add\nand select\nApp registration**.\nFor_Name_, enter a name for the application.\nIn the_Supported account types_section, select\nAccounts in this organizational directory only (Single tenant)\nand then click\nRegister\n.\nOn the application page's_Overview_page, in the_Essentials_section, copy and store the following values in a secure location:\nApplication (client) ID\nDirectory (tenant) ID\nTo generate a client secret, within_Manage_, click\nCertificates & secrets\n.\nOn the_Client secrets_tab, click\nNew client secret\n.\nIn the_Add a client secret_dialog, enter the following details:\nFor\nDescription\n, enter a description for the client secret.\nFor_Expires_, select an expiry time period for the client secret and then click\nAdd\n.\nCopy and store the client secret's_Value_in a secure place.\nAdd a service principal to your account\nâ\nTo add a service principal to your Azure Databricks account:\nLog in to your\nAzure Databricks account console\nas an account admin.\nFrom the left menu of the account console, click\nUser management\n.\nFrom the tabs along the top of the\nUser management\npage, click the\nService principals\ntab.\nIn the upper right of the\nService principals\npage, click\nAdd service principal\n.\nOn the\nAdd service principal\npage, enter a name for the service principal.\nUnder\nUUID\n, paste the\nApplication (client) ID\nfor the service principal.\nClick\nAdd\n.\nAssign a service principal to a workspace\nâ\nTo add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page.\nIdentity federation enabled\nâ\nTo assign a service principal to your Azure Databricks account:\nLog in to your Databricks\naccount console\nas an account admin.\nFrom the left menu of the account console, click\nWorkspaces\nand then select a workspace to which you want to add the service principal.\nFrom the tabs along the top of your workspace page, click the\nPermissions\ntab.\nIn the upper right of the\nPermissions\npage, click\nAdd permissions\n.\nIn the\nAdd permissions\ndialog, enter the following details:\nFor\nUser, group, or service principal\n, select the\nservice principal\nyou created.\nFor\nPermission\n, click the dropdown to select workspace\nUser\n.\nIdentity federation disabled\nâ\nTo assign a service principal to your Azure Databricks workspace:\nLog in to your Azure Databricks workspace as a workspace admin.\nFrom the top right of your workspace, click your username, and then from the dropdown, click\nAdmin Settings\n.\nIn the left menu of the\nSettings\npage, under the\nWorkspace admin\nsubheading, click\nIdentity and access\n.\nOn the\nIdentity and access\npage, under\nManagement and permissions\n, next to\nService principals\n, click\nManage\n.\nIn the upper right of the\nService principals\npage, click\nAdd service principal\n.\nIn the\nAdd service principal\ndialog, click the\nAdd new\nbutton.\nFor\nNew service principal display name\n, paste the\nApplication (client) ID\nfor the\nservice principal\n, enter a display name, and then click\nAdd\n.\nGrant permissions to crawl metadata\nâ\nYou must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan.\nTo extract metadata, you can grant the\nBROWSE privilege\n, currently in public preview. You no longer require the\nData Reader\npreset that granted the following privileges on objects in the catalog -\nUSE CATALOG\n,\nUSE SCHEMA\n,\nEXECUTE\n,\nREAD VOLUME\n, and\nSELECT\n.\nTo grant permissions to a user or service principal:\nLog in to your Databricks workspace as a workspace admin.\nFrom the left menu of your workspace, click\nCatalog\n.\nIn the left menu of the\nCatalog Explorer\npage, select the catalog you want to crawl in Atlan.\nFrom the tabs along the top of your workspace page, click the\nPermissions\ntab and then click the\nGrant\nbutton.\nIn the\nGrant on (workspace name)\ndialog, configure the following:\nUnder\nPrincipals\n, click the dropdown and then select the user or service principal.\nUnder\nPrivileges\n, check the\nBROWSE\nprivilege.\nAt the bottom of the dialog, click\nGrant\n.\n(Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan.\nSystem tables extraction method\nâ\nTo crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables:\nCreate one of the following authentication methods:\nPersonal access token\nAWS service principal\nAzure service principal\nGrant the following privileges to the identity you created:\nCAN_USE\non a SQL warehouse\nUSE CATALOG\non\nsystem\ncatalog\nUSE SCHEMA\non\nsystem.information_schema\nSELECT\non the following tables:\nsystem.information_schema.catalogs\nsystem.information_schema.schemata\nsystem.information_schema.tables\nsystem.information_schema.columns\nsystem.information_schema.key_column_usage\nsystem.information_schema.table_constraints\nCross-workspace extraction\nâ\nTo crawl metadata from all workspaces within a Databricks metastore using a single connection, see\nSet up cross-workspace extraction\nfor instructions.\n(Optional) Grant permissions to query and preview data\nâ\ndanger\nAtlan currently only supports\nquerying data\nand\nviewing sample data preview\nfor the\npersonal access token\nauthentication method.\nTo grant permissions to query data and preview example data:\nLog in to your Databricks workspace as a workspace admin.\nFrom the left menu of your workspace, click\nCatalog\n.\nIn the left menu of the\nCatalog Explorer\npage, select the catalog you want to query and preview data from in Atlan.\nFrom the tabs along the top of your workspace page, click the\nPermissions\ntab and then click the\nGrant\nbutton.\nIn the\nGrant on (workspace name)\ndialog, configure the following:\nUnder\nPrincipals\n, click the dropdown and then select the user or service principal.\nUnder\nPrivilege presets\n, click the dropdown and then click\nData Reader\nto enable read-only access to the catalog. Doing so automatically selects the following privileges -\nUSE CATALOG\n,\nUSE SCHEMA\n,\nEXECUTE\n,\nREAD VOLUME\n, and\nSELECT\n.\nAt the bottom of the dialog, click\nGrant\n.\n(Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan.\n(Optional) Grant permissions to import and update tags\nâ\nTo\nimport Databricks tags\n, you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods.\nOnce you have created a\npersonal access token\n, an\nAWS service principal\n, or an\nAzure service principal\n, you will need to grant the following privileges:\nCAN_USE\non a SQL warehouse\nUSE CATALOG\non\nsystem catalog\nUSE SCHEMA\non\nsystem.information_schema\nSELECT\non the following tables:\nsystem.information_schema.catalog_tags\nsystem.information_schema.schema_tags\nsystem.information_schema.table_tags\nsystem.information_schema.column_tags\nTo push tags updated for assets in Atlan to Databricks, you need to grant the following\nprivileges\n:\nAPPLY TAG\non the object\nUSE CATALOG\non the object's parent catalog\nUSE SCHEMA\non the object's parent schema\n(Optional) Grant permissions to extract lineage and usage from system tables\nâ\nYou must have a Unity Catalog-enabled workspace to use system tables.\nAtlan supports extracting the following for your Databricks assets using\nsystem tables\n:\nlineage\nusage and popularity metrics\nEnable system.access schema\nâ\nYou need your account admin to enable the\nsystem.access\nschema using the\nSystemSchemas API\n. This enables Atlan to extract lineage using system tables.\nIn Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage.\nTo verify that system schemas are enabled for each schema, follow the steps in\nDatabricks documentation\n:\nList system schemas\nusing the SystemSchemas API to check the status.\nIf enabled for any given schema, the\nstate\nis\nEnableCompleted\n. This confirms that the schema has been enabled for that specific metastore.\nAtlan can only extract lineage using system tables when the state is marked as\nEnableCompleted\n.\n(Optional) enable\nsystem.information_schema.table\nâ\nTo generate lineage with the target type set as\nPATH\nfor a table, Atlan uses metadata from\nsystem.information_schema.table\nto resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables.\nGrant permissions\nâ\nWho can do this?\nYou must be a metastore admin, have the\nMANAGE\nprivilege on the object, or be the owner of the catalog, schema, or table to grant these permissions.\nIn Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage.\nOpen\nCatalog Explorer\nin your Databricks workspace.\nNavigate to the catalog (for example,\nmain\n) and then to the appropriate schema (for example,\nsales\n).\nClick the\nPermissions\ntab.\nClick\nGrant\n.\nEnter the user or group name (principal).\nAssign the following permissions:\nUSAGE\non the catalog\nUSAGE\non the schema\nSELECT\non each relevant table\nClick\nGrant\nto apply the changes.\nThese privileges enable Atlan to read table definitions and other metadata from the metastore.\n(Optional) enable system.query schema\nâ\nThis is only required if you also want to extract\nusage and popularity metrics\nfrom Databricks.\nYou need your account admin to enable the\nsystem.query\nschema using the\nSystemSchemas API\n. This enables Atlan to mine query history using system tables for usage and popularity metrics.\nTo verify that system schemas is enabled for each schema, follow the steps in\nDatabricks documentation\n. If enabled for any given schema, the\nstate\nis\nEnableCompleted\n.\ninfo\nðª\nDid you know?\nCan't grant\nSELECT\npermissions on the system tables in\nsystem.access\nand\nsystem.query\n? Skip the previous steps and create cloned views in a separate catalog and schema. See\nCreate cloned views of system tables\n.\nGrant permissions\nâ\nAtlan supports extracting Databricks lineage and usage and popularity metrics using system tables for\nall three authentication methods\n.\nOnce you have created a\npersonal access token\n, an\nAWS service principal\n, or an\nAzure service principal\n, you will need to grant the following permissions:\nCAN_USE\non a SQL warehouse\nUSE_CATALOG\non\nsystem\ncatalog\nUSE SCHEMA\non\nsystem.access\nschema\nUSE SCHEMA\non\nsystem.query\nschema (tomine query history for usage and popularity metrics)\nSELECT\non the following tables:\nsystem.query.history\n(to mine query history for usage and popularity metrics)\nsystem.access.table_lineage\nsystem.access.column_lineage\nYou need to\ncreate a Databricks connection in Atlan\nfor each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the\nHost\nfor the connection.\ninfo\nðª\nDid you know?\nCan't grant\nSELECT\npermissions on the system tables in\nsystem.access\nand\nsystem.query\n? Skip the previous steps and create cloned views in a separate catalog and schema. See\nCreate cloned views of system tables\n.\n(Optional) Create cloned views of system tables\nâ\nWhen you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema.\nFollow these steps to set up cloned views:\nCreate a catalog and schema to store cloned views. Use meaningful and unique namesâfor example,\natlan_cloned_catalog\nand\natlan_cloned_schema\n.\nCreate cloned views for the following system tables:\nLineage tables\nCREATE\nOR\nREPLACE\nVIEW\n<\ncloned\n-\ncatalog\n-\nname\n>\n.\n<\ncloned\n-\nschema\n-\nname\n>\n.\ncolumn_lineage\nAS\nSELECT\n*\nFROM\nsystem\n.\naccess\n.\ncolumn_lineage\n;\nCREATE\nOR\nREPLACE\nVIEW\n<\ncloned\n-\ncatalog\n-\nname\n>\n.\n<\ncloned\n-\nschema\n-\nname\n>\n.\ntable_lineage\nAS\nSELECT\n*\nFROM\nsystem\n.\naccess\n.\ntable_lineage\n;\nReplace\n<cloned-catalog-name>\nand\n<cloned-schema-name>\nwith the catalog and schema names used in your environment.\nPopularity metrics\nCREATE\nOR\nREPLACE\nVIEW\n<\ncloned\n-\ncatalog\n-\nname\n>\n.\n<\ncloned\n-\nschema\n-\nname\n>\n.\nquery_history\nAS\nSELECT\n*\nFROM\nsystem\n.\nquery\n.\nhistory\n;\nReplace\n<cloned-catalog-name>\nand\n<cloned-schema-name>\nwith the catalog and schema names used in your environment.\nGrant permissions\nâ\nGrant the following permissions to enable access to the cloned views:\nCAN_USE\non a SQL warehouse\nUSE CATALOG\non the catalog (for example,\n<cloned-catalog-name>\n)\nUSE SCHEMA\nand\nSELECT\non the schema (for example,\n<cloned-catalog-name>.<cloned-schema-name>\n)\nYou must\ncreate a Databricks connection in Atlan\nfor each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the\nHost\nfor the connection.\nLocate warehouse ID\nâ\nTo extract lineage and usage and popularity metrics using system tables, you will also need the\nwarehouse ID of your SQL warehouse\n.\nTo locate the warehouse ID:\nLog in to your Databricks workspace as a workspace admin.\nFrom the left menu of your workspace, click\nSQL Warehouses\n.\nOn the\nCompute\npage, select the warehouse you want to use.\nFrom the\nOverview\ntab of your warehouse page, next to the\nName\nof your warehouse, copy the value for your SQL warehouse\nID\n. For example,\nexample-warehouse (ID: 123ab4c5def67890)\n, copy the value\n123ab4c5def67890\nand store it in a secure location.\n(Optional) Grant view permissions to access Databricks entities via APIs\nâ\nAtlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant\nCAN VIEW\npermission to the Databricks user or service principal configured in your integration:\nNotebook API\n(\n/api/2.0/workspace/list\n): Grant\nCAN VIEW\npermission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see\nManage Access Control Lists with Folders\n.\nQueries API\n(\n/api/2.0/sql/queries\n): Grant\nCAN VIEW\npermission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see\nView Queries\n.\nJob API\n(\n/api/2.2/jobs/list\n): Grant\nCAN VIEW\npermission on each job object directly.\nDatabricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see\nControl Access to a Job\n.\nPipeline API\n(\n/api/2.0/pipelines\n): Grant\nCAN VIEW\npermission on each Delta Live Tables (DLT) pipeline object directly. For more information, see\nConfigure Pipeline Permissions\n.\n(Optional) Grant permissions for views and materialized views\nâ\nAtlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views:\nLog in to your Databricks workspace as a workspace admin.\nFrom the left menu of your workspace, click\nCatalog\n.\nIn the\nCatalog Explorer\n, select the catalog you want to extract view definitions from and generate lineage for in Atlan.\nFrom the tabs at the top, click the\nPermissions\ntab, and then click\nGrant\n.\nIn the\nGrant on (workspace name)\ndialog, configure the following:\nSelect the\nuser\nor\nservice principal\nunder\nPrincipals\n.\nSelect the following privileges under\nPrivilege presets\n:\nUSE CATALOG\nUSE SCHEMA\nSELECT\nClick\nGrant\nto apply the permissions.\nRepeat steps 3â6 for each catalog you want to crawl in Atlan.\nDid you know?\nSELECT\npermission is required to extract the definitions of views and materialized views. If you prefer not to grant\nSELECT\nat the catalog level, you can grant it on individual views and materialized views instead.\n(Optional) Grant permissions to mine query history\nâ\nTo\nmine query history\nusing REST API, you will need to assign the\nCAN MANAGE\npermission on your SQL warehouses to the user or service principal.\nTo grant permissions to mine query history:\nLog in to your Databricks workspace as a workspace admin.\nFrom the left menu of your workspace, click\nSQL Warehouses\n.\nOn the\nCompute\npage, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click\nPermissions\n.\nIn the\nManage permissions\ndialog, configure the following:\nIn the\nType to add multiple users or groups\nfield, search for and select a user or service principal.\nExpand the\nCan use\npermissions dropdown and then select\nCan manage\n. This permission enables the service principal to\nview all queries for the warehouse\n.\nClick\nAdd\nto assign the\nCAN MANAGE\npermission to the service principal.\nTags:\ndata\nauthentication\nPrevious\nDatabricks\nNext\nSet up cross-workspace extraction\nPersonal access token authentication\nAWS service principal authentication\nAzure service principal authentication\nGrant permissions to crawl metadata\n(Optional) Grant permissions to query and preview data\n(Optional) Grant permissions to import and update tags\n(Optional) Grant permissions to extract lineage and usage from system tables\n(Optional) Grant view permissions to access Databricks entities via APIs\n(Optional) Grant permissions for views and materialized views\n(Optional) Grant permissions to mine query history"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nGet Started\nSet up Microsoft Power BI\nOn this page\nSet up Microsoft Power BI\nWho can do this?\nDepending on the authentication method you choose, you may need a combination of your\nCloud Application Administrator\nor\nApplication Administrator\nfor Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and\nFabric Administrator\n(\nformerly known as Power BI Administrator\n) for Microsoft Power BI to complete these tasks -> you may not have access yourself.\nThis guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking.\nBefore you begin\nâ\nRegister application in Microsoft Entra ID\nâ\nWho can do this?\nYou need your\nCloud Application Administrator\nor\nApplication Administrator\nto complete these stepsâ> you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization.\nTo register a new application in Microsoft Entra ID:\nLog in to the\nAzure portal\n.\nSearch for\nMicrosoft Entra ID\nand select it.\nClick\nApp registrations\nfrom the left menu.\nClick\n+ New registration\n.\nEnter a name for your client application and click\nRegister\n.\nFrom the Overview screen, copy and securely store:\nApplication (client) ID\nDirectory (tenant) ID\nClick\nCertificates & secrets\nfrom the left menu.\nUnder\nClient secrets\n, click\n+ New client secret\n.\nEnter a description, select an expiry time, and click\nAdd\n.\nCopy and securely store the client secret\nValue\n.\nCreate security group in Microsoft Entra ID\nâ\nWho can do this?\nYou need your\nCloud Application Administrator\nor\nApplication Administrator\nto complete these steps - you may not have access yourself.\nTo create a security group for your application:\nLog in to the\nAzure portal\n.\nSearch for\nMicrosoft Entra ID\nand select it.\nClick\nGroups\nunder the Manage section.\nClick\nNew group\n.\nSet the Group type to\nSecurity\n.\nEnter a Group name and optional description.\nClick\nNo members selected\n.\nAdd the appropriate member:\nFor Delegated User authentication\n: search for the user and select it.\nFor Service Principal authentication\n: search for the application registration created earlier and select it.\nClick\nSelect\nand then\nCreate\n.\nBy the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member.\nConfigure authentication options\nâ\nAtlan supports two authentication methods for fetching metadata from Microsoft Power BI:\nService principal authentication (recommended)\nâ\nWhen using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options:\nAdmin API only\nâ\nThis option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments.\nWho can do this?\nYou need your\nFabric Administrator\n(\nformerly known as Power BI Administrator\n) to complete these tasks - you may not have access yourself.\nTo configure admin API access:\nLog in to the\nPower BI admin portal\n.\nClick\nTenant settings\nunder Admin portal.\nUnder\nAdmin API settings\n:\nExpand\nEnable service principals to use read-only Power BI admin APIs\nand set to\nEnabled\nAdd your security group under\nSpecific security groups\nClick\nApply\nExpand\nEnhance admin APIs responses with detailed metadata\nand set to\nEnabled\nAdd your security group\nClick\nApply\nExpand\nEnhance admin APIs responses with DAX and mashup expressions\nand set to\nEnabled\nAdd your security group\nClick\nApply\nAdmin and non-admin APIs\nâ\nThis option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets.\nAssign security group to Power BI workspaces in PowerBI service portal\nâ\nWho can do this?\nYou need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal.\nTo assign a Microsoft Power BI workspace role to the security group:\nOpen the\nMicrosoft Power BI homepage\n.\nOpen\nWorkspaces\nand select the workspace you want to access from Atlan.\nClick\nAccess\n.\nIn the panel:\nEnter the name of your security group where it says\nEnter email addresses\nChoose one of the following roles:\nViewer\n: For workspaces without parameters\nContributor\n: For workspaces with semantic models containing parameters or to generate lineage for measures\nMember\n: To generate lineage for dataflows\nClick\nAdd\n.\nConfigure admin and non-admin API access in PowerBI Service Portal\nâ\nWho can do this?\nYou need your\nFabric Administrator\n(\nformerly known as Power BI Administrator\n) to complete these tasks - you may not have access yourself.\nTo enable both admin and non-admin API access:\nLog in to the\nPower BI admin portal\n.\nClick\nTenant settings\nunder Admin portal.\nUnder\nDeveloper settings\n:\nExpand\nService principals can use Fabric APIs\nand set to\nEnabled\nAdd your security group under\nSpecific security groups\nClick\nApply\nUnder\nAdmin API settings\n:\nExpand\nEnable service principals to use read-only Power BI admin APIs\nand set to\nEnabled\nAdd your security group\nClick\nApply\nExpand\nEnhance admin APIs responses with detailed metadata\nand set to\nEnabled\nAdd your security group\nClick\nApply\nExpand\nEnhance admin APIs responses with DAX and mashup expressions\nand set to\nEnabled\nAdd your security group\nClick\nApply\nAfter making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services.\nDelegated user authentication\nâ\ninfo\nAtlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft.\nFabric administrator role assignment\nâ\nWho can do this?\nYou need your Microsoft 365 administrator to complete these steps - you may not have access yourself.\nTo assign the delegated user to the\nFabric Administrator\nrole:\nOpen the\nMicrosoft 365 admin portal\n.\nClick\nUsers\nand then\nActive users\nfrom the left menu.\nSelect the delegated user.\nUnder\nRoles\n, click\nManage roles\n.\nExpand\nShow all by category\n.\nUnder\nCollaboration\n, select\nFabric Administrator\n.\nClick\nSave changes\n.\nAPI permissions\nâ\nWho can do this?\nYou need your\nCloud Application Administrator\nor\nApplication Administrator\nto complete these steps, you may not have access yourself.\ndanger\nThe following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principalâit's\nrecommended\nthat you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot.\nTo add permissions for the\nregistered application\n:\nIn your app registration, click\nAPI permissions\nunder the Manage section.\nClick\nAdd a permission\n.\nSearch for and select\nPower BI Service\n.\nClick\nDelegated permissions\nand select:\nCapacity.Read.All\nDataflow.Read.All\nDataset.Read.All\nReport.Read.All\nTenant.Read.All\nWorkspace.Read.All\nClick\nGrant Admin consent\n(If you only see the\nAdd permissions\nbutton, you aren't an administrator).\nAdmin API settings configuration\nâ\nWho can do this?\nYou need your\nFabric Administrator\n(\nformerly known as Power BI Administrator\n) to complete these tasks, you may not have access yourself.\nTo enable the Microsoft Power BI admin API:\nLog in to the\nPower BI admin portal\n.\nClick\nTenant settings\nunder Admin portal.\nUnder\nAdmin API settings\n:\nExpand\nEnhance admin APIs responses with detailed metadata\nand set to\nEnabled\nAdd your security group\nClick\nApply\nExpand\nEnhance admin APIs responses with DAX and mashup expressions\nand set to\nEnabled\nAdd your security group\nClick\nApply\n.\nTags:\ndata\nauthentication\nPrevious\nMicrosoft Power BI\nNext\nCrawl Microsoft Power BI\nBefore you begin\nConfigure authentication options"
  },
  {
    "url": "https://docs.atlan.com/platform/references/atlan-architecture",
    "content": "Get Started\nReferences\nAtlan architecture\nOn this page\nAtlan architecture\nAtlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms:\nAmazon Web Services (AWS)\nâ\nMicrosoft Azure\nâ\nGoogle Cloud Platform (GCP)\nâ\nThe components of Atlan are isolated, across both compute and data. For more details, see\nHow are resources isolated?\nPlatform components\nâ\nKong\nis an API gateway. It handles rate limiting and token verification on all incoming API requests.\nApache Keycloak\nis an identity and access management component. It manages everything to do with users, login, SSO and so on.\nHeracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components.\nPostgreSQL\nis a SQL database. Many services on the platform use it for storage.\nHashiCorp Vault\nis a secret manager. It stores sensitive credentials provided by the user.\nApache Ranger\nis the policy engine. It provides fine-grained access control over data in the metastore.\nArgo Workflows\nis a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion.\nAdmission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts.\nMetastore stores metadata as data in a graph store. It is based on\nApache Atlas\nand has fine-grained access control on top.\nApache Zookeeper\nmanages consensus and coordination for the metastore services.\nElasticsearch\nindexes data and drives search functionality.\nApache Cassandra\nis an object-oriented database used to store the metastore's data.\nApache Kafka\nis an event stream. It enables event-driven use cases across the platform.\nHeka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by\nApache Calcite\n.\nRedis\nis a cache layer used by Heracles.\nPlatform management components\nâ\nVelero\nperforms cluster backups.\nKibana\nexplores and filters log data stored in Elasticsearch.\nFluent Bit\nis a logging and metrics processor. It parses and pushes logs from pods to various destinations.\nElasticsearch\nstores and indexes logs.\nCentral components\nâ\nZenduty\nis used for incident response. Alerts are sent when something goes wrong in one of the clusters.\nArgo CD\nis used for continuous deployment. Changes in git repositories lead to upgrades in the clusters.\nGithub Actions\nupdate the Docker container images as part of the development process.\nSendgrid\nis used to send emails.\nThe frontend is a\nVue.js\nweb application that's hosted on S3 and delivered via\nAmazon CloudFront\ncontent delivery network (CDN) service.\nAlertmanager\nsends alerts generated by metrics stored in Prometheus.\nGrafana\nprovides observability dashboards.\nVictoriaMetrics\nis a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing.\nAtlan marketplace\n(not pictured)\nâ\nThe marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors.\nSee\nsecurity.atlan.com\nfor the latest policies and standards, reports and certifications, architecture, diagrams and more.\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nIncident response plan\nNext\nProduct release stages\nPlatform components\nPlatform management components\nCentral components\nAtlan marketplace (not pictured)"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nHow-tos\nHow to use the Atlan browser extension\nOn this page\nUse the Atlan browser extension\nThe Atlan browser extension provides metadata context directly in your\nsupported data tools\n. You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge.\nInstall the extension\nâ\nTo install the Atlan browser extension, first log into your Atlan instance. Atlan saves your Atlan domain in a cookie when you log in.\nTo install Atlan's browser extension:\nYou can either:\nFind the extension in the Chrome Web Store:\nhttps://chrome.google.com/webstore/detail/atlan/fipjfjlalpnbejlmmpfnmlkadjgaaheg\nFrom the upper right of any screen in Atlan, navigate to your name and then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnder\nApps\n, for\nBrowser extension\n, click\nInstall\n.\nTo install the Atlan browser extension:\nFor Google Chrome, in the upper right of your screen, click\nAdd to Chrome\n. When prompted for confirmation, click the\nAdd extension\nbutton.\nFor Microsoft Edge, follow the steps in\nAdd an extension to Microsoft Edge from the Chrome Web Store\n.\nCurrently, you can't install the browser extension on mobile devices or tablets.\nDid you know?\nYou can also install Atlan's browser extension at the\nworkspace level\n. To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can\nconfigure the extension for managed browsers\n.\nConfigure the extension\nâ\nOnce installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can\npreconfigure custom domains for data sources\n, if any.\nConfigure the extension as a user\nâ\nTo configure the browser extension, once installed:\nIf you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted.\nIn the\nOptions\npage, to enter the URL of your Atlan instance:\nIf your organization uses an Atlan domain (for example,\n_mycompany_.atlan.com\n), the Atlan instance URL appears preselected. Click\nGet started\n. (Optional) Switch to a different Atlan domain, if required.\nIf your organization uses a custom domain (for example,\n_atlan_.mycompany.com\n), enter the URL of your Atlan instance and then click\nGet started\n.\nAfter a successful login, the message\nUpdated successfully\nappears.\n(Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool:\nClick the\nConfigure custom domain\nlink at the bottom.\nIn the dropdown on the left, select your data tool.\nIn the text box on the right, enter the custom domain you use for that tool.\nRepeat these steps for each tool hosted on a custom domain.\nClick the\nSave\nbutton when finished.\nIf your Atlan admin has\npreconfigured custom domains for data sources\n, you won't be able to update or remove these selections. Click\n+ Add\nto configure custom domains for additional data sources as required.\nYou can now close the\nOptions\ntab.\nThe extension is now ready to use! ð\n(Optional) Configure custom domains as an admin\nâ\nWho can do this?\nYou need to be an admin user in Atlan to configure custom domains for data sources from the admin center.\nTo configure custom domains, from within Atlan:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nIntegrations\n.\nUnder\nApps\n, expand the\nBrowser extension\ntile.\nIn theÂ\nBrowser extension\ntile, for\nSet up your custom data source...\n, if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the\nConfigure\nlink to configure them for users in your organization.\nFor\nConnector\n, select a\nsupported tool\nfor the browser extension.\nIn the adjacent field, enter the URL of the custom domain for your data source.\n(Optional) Click\n+ Add\nto add more.\nClick\nSave\nto save your configuration.\ninfo\nðª\nDid you know?\nFor any\nsupported tools\nthat you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources.\n(Optional) For\nDownload Atlan extension or share with your team\n, you can either install the Atlan browser extension for your own use or share the link with your users.\nUsage\nâ\nWho can do this?\nAnyone with access to Atlanâany admin, member, or guest userâand a supported tool can use the browser extension. First, log into Atlan.\nDid you know?\nWhen using Atlan's browser extension in a\nsupported tool\n, the extension only reads the URL of your browser tabâno other data is accessed. If using Atlan's browser extension on any\nwebsite\n, it only reads the favicon, page title, and URL of your browser tab. Learn more about\nAtlan browser extension security\n.\nAccess and enrich context in-flow\nâ\nTo access context for an asset, from within a supported tool:\nLog into the supported tool.\nOpen any supported asset.\nIn the lower-right corner of the page, click the small Atlan icon.\ndanger\nThe icon to activate Atlan is\nnot\nthe extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool.\nIn the sidebar that appears:\nClick the tabs and links to view all context about the asset.\nMake changes to any of the metadata you'd like.\nNow you can understand and enrich assets without leaving your data tools themselves! ð\nThe Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing.\nDid you know?\nYour permissions in Atlan control what metadata you can see and change in the extension.\nSearch for metadata\nâ\nTo search for context for any information on\nany\nwebsite:\nSelect the text you'd like to search on the web page you're viewing.\nRight-click, and then select\nSearch in Atlan ð¡\n.\nThe extension opens a new browser tab on Atlan's discovery page, with the results for that text! ð\nAdd a resource\nâ\nYou can link any web page as a\nresource\nto your assets in Atlan using the browser extension.\nTo add a web page as a resource to an asset:\nIn the top right of the web page you're viewing, click the\nAtlan Chrome extension\n.\nIn the resource clipper menu, under\nLink this page to an asset\n, select the asset to which you'd like to add the web page as a resource.\nClick\nSave\nto confirm your selection.\n(Optional) Once the resource has been linked successfully, click the\nOpen in Atlan\nbutton to view the linked asset directly in Atlan.\nYou can now add resources to your assets in Atlan from any website! ð\nDid you know?\nThe Tableau extension offers native embeddings directly in your dashboards. See\nEnable embedded metadata in Tableau\nfor more information.\nSupported tools\nâ\nCurrently, the Atlan browser extension supports assets in the following tools:\nAmazon QuickSight\n: analyses, dashboards, and datasets\nDatabricks\n: databases, schemas, views, and tables\ndbt Cloud\n: models and sources in the model editor and dbt docs\nGoogle BigQuery\n: datasets, schemas, views, and tables\nIBM Cognos Analytics\n: folders, dashboards, packages, explorations, reports, files, data sources, and modules\nLooker\n: dashboards, explores, and folders\nMicrosoft Power BI\n: dashboards, reports, dataflows, and datasets\nMode\n: collections, reports, queries, and charts\nQlik Sense Cloud\n: apps, datasets, sheets, and spaces\nRedash\n: queries, dashboards, and visualizations\nSalesforce\n: objects\nSigma\n: datasets, pages, and data elements\nSnowflake\n(via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes\nTableau\n: dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See\nEnable embedded metadata in Tableau\nfor more information.\nThoughtSpot\n: liveboards, answers, visualizations, and tables\nMicroStrategy\n: dossiers, reports, documents\nTags:\natlan\ndocumentation\nPrevious\nConfigure the extension for managed browsers\nNext\nEnable embedded metadata in Tableau\nInstall the extension\nConfigure the extension\nUsage\nSupported tools"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins",
    "content": "Get Started\nQuick Start Guides\nAdministrators\nOn this page\nAdministrators\nUser management\nâ\nUser management is a critical part of data governance. Atlan's\nuser management capabilities\nshould be a mainstay of how you organize and control access for people in your organization.\nAdd and manage users from the admin center\nâ\nIt's super simple to\ninvite\nand\nremove\nusers from Atlan from the\nAdmin center\n. You can also\nmanage existing users\nby adding them to groups, changing their roles, or set up\nSSO\n,\nSCIM\n, and\nSMTP\nconfigurations.\nManage access control from the governance center\nâ\nThe\nGovernance center\nis where you can build access control mechanisms to\nmanage user access\n.\nPersonas\nallow you to group users into teams, such as\nFinancial Analysts\nor\nCloud Engineers\n, and set policies based on the access those personas should have.\nPurposes\nare where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance.\nOnce you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it.\nGovernance workflows\nhelp you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nAsset profile\nâ\nThe\nasset profile\nin Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data.\nGlossary\nâ\nThe Atlan\nglossary\nis a rich tool for defining and organizing your data\nterminology\nto improve transparency and share knowledge. No need to ask around for what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one searchable place.\nThe glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as:\nOwners\nof your data, so you know who to ask for clarification.\nCertificate\nstatus, to easily understand if metadata enrichment is still in progress or the asset is ready to be used.\nLinked assets\nthat are relevant to the term, so you can explore other helpful material.\nDid you know?\nThe glossary helps power\nAtlan's powerful search tool\n, so tagging and defining assets are critical to helping your team find what they need.\nDiscovery\nâ\nWe rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The\ndiscovery tool\nis Atlan's powerful in-platform search, powered by the terms and descriptions you've added to your data assets.\nHere are a few of the things that make Atlan's discovery awesome:\nEvery attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need.\nIntelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed.\nSearch assets from just about any page in Atlan using\nCmd/Ctrl+K\nor by clicking\nSearch assets across Atlan\nat the top of any page.\nControl your search by using facets about your data (such as the verification status or owner) to find what's most important to you.\nSort by popularity to quickly discover what assets your teammates are using every day.\nTags:\nget-started\nquick-start\nPrevious\nWhat is Atlan?\nNext\nData consumers\nUser management\nAsset profile\nGlossary\nDiscovery"
  },
  {
    "url": "https://docs.atlan.com/secure-agent",
    "content": "Connect data\nSecure Agent\nOn this page\nSecure Agent\nThe Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing.\nFigure 1:\nThe Secure Agent runs in the customer environment and acts as a gateway.\nKey capabilities\nâ\nThe Secure Agent is designed for secure, scalable, and efficient metadata extraction.\nSecurity-first architecture\nâ\nRuns entirely within the organization's infrastructure, preventing secrets from leaving its boundary.\nUses outbound, encrypted communication to interact with Atlan SaaS.\nSupports logging and monitoring and integrates with external monitoring systems for auditing and compliance.\nScalable metadata extraction\nâ\nA single deployment of the Agent can connect to multiple source systems.\nSupports multiple concurrent metadata extraction jobs.\nUses Kubernetes-based workloads for efficient resource management.\nFlexible deployment\nâ\nDeploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters.\nScales dynamically based on workload demands.\nAutomated operations\nâ\nContinuously monitors system health and sends heartbeats to Atlan.\nCaptures and uploads execution logs for troubleshooting and auditing.\nProvides performance insights through metrics and alerts.\nHow it works\nâ\nThe Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves:\nAtlan triggers a metadata extraction job.\nThe Secure Agent retrieves job details and extracts metadata using source-specific connectors.\nExtracted metadata is shared with Atlan either through cloud storage or direct ingestion.\nAtlan workflows process the extracted metadata and publish the assets.\nLogs and execution status are sent to Atlan for monitoring and auditing.\nSee also\nâ\nDeployment architecture\n: Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction.\nTags:\nsecurity\naccess-control\npermissions\nNext\nInstall on Virtual Machine (K3s)\nKey capabilities\nHow it works\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks",
    "content": "Configure Atlan\nPlaybooks\nOn this page\nPlaybooks\nOverview:\nAtlan's playbooks provide reusable workflows and automation for common data tasks. Create, share, and execute standardized processes to maintain consistency, reduce manual effort, and enable self-service for data consumers while following governance standards.\nGet started\nâ\nHow to set up playbooks\nGuides\nâ\nPlaybook management\nâ\nHow to manage playbooks\n: Monitor and maintain your playbook workflows.\nHow to automate data profiling\n: Set up automated data quality checks.\nTroubleshooting\nâ\nTroubleshooting playbooks\n: Solutions for common playbook issues.\nTags:\nplaybooks\nautomation\nworkflows\nmetadata\ncapabilities\nNext\nSet up playbooks\nGet started\nGuides\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery",
    "content": "Use data\nDiscovery\nOn this page\nDiscovery\nOverview:\nAtlan's discovery capabilities help users find, understand, and use data assets across your organization. With powerful search, filtering, and browsing features, users can quickly locate relevant data assets, explore their context, and access the information they need to make data-driven decisions.\nGet started\nâ\nHow to search and discover assets\nFor detailed search, filtering, and troubleshooting information, use the sidebar navigation.\nTags:\ndiscovery\nsearch\nbrowse\ncapabilities\nNext\nSearch and discover assets\nGet started"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts",
    "content": "Build governance\nContracts\nOn this page\nContracts\nOverview:\nManage data contracts and agreements in Atlan to ensure data quality and compliance. Define and track data quality expectations, service level agreements (SLAs), and data sharing agreements between teams and systems.\nGet started\nâ\nFollow these steps to implement contracts in Atlan:\nCreate data contracts\nGuides\nâ\nAdd contract impact analysis in GitHub\n: Detailed instructions on adding contracts for impact analysis in GitHub.\nTags:\ncontracts\nagreements\ndata quality\ngovernance\natlan\nNext\nCreate data contracts\nGet started\nGuides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations",
    "content": "Configure Atlan\nIntegrations\nIntegrations\nAtlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack.\nKey concepts\nâ\nIntegration categories\n: Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management.\nConnection methods\n: Most integrations use secure authentication methods like OAuth, API keys, or service accounts.\nBi-directional sync\n: Updates flow between Atlan and integrated tools, ensuring data consistency across platforms.\nCustom webhooks\n: Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks.\nCore offerings\nâ\nâï¸\nAutomation\nConnect with platforms like AWS Lambda to automate data workflows and streamline routine tasks.\nð¥\nCollaboration\nIntegrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing.\nð¬\nCommunication\nConnect with SMTP for real-time alerts.\nð\nIdentity management\nIntegrate with identity providers like Okta and Azure AD for seamless authentication and user management.\nð\nProject management\nConnect with tools like Jira and Service Now to link data assets to projects and track data-related tasks.\nGet started\nâ\n1\nSelect an integration\nChoose from Atlan's available integrations based on your team's tools and workflows.\nâ\n2\nConfigure connection\nFollow the integration-specific setup guide to establish a secure connection with your tool.\nâ\n3\nTest and activate\nVerify the integration is working correctly with a test action, then activate for your organization.\nð¡\nNeed a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack.\nTags:\nintegrations\natlan\nsetup\nNext\nAutomation Integrations"
  },
  {
    "url": "https://docs.atlan.com/support/references/customer-support",
    "content": "Support\nReferences\nCustomer support\nOn this page\nCustomer support\nOne of Atlan's core values is to help you and your team do your life's best work. ð\nThat's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data.\nAtlan's customer support is a combination of several teams in Atlan:\nProduct support personnel\nCloud support personnel\nDevOps/engineering support personnel\nVast repository of self-service resources\nService-level commitment\nâ\nAtlan's Technical Support team provides support globally with high response commitment levels. This includes 24/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following:\n99.5% uptime for Atlan\nDedicated support center, available from within the Atlan product\nCommitments for aggressive response times for business critical issues\nDesignated Customer Success Manager to assist with escalations\nWays to contact support\nâ\nâï¸ Email support at a dedicated customer support email account (\n[email protected]\n)\nð¨âð» In-product support widget to log tickets and a\nhelp desk portal\nto log and track tickets. You can sign up to\ntrack support tickets\non the help desk portal. You must use your organizational email address as the username and create a password.\nð\nSubmit a support request\nvia the online form.\nTo track your support tickets:\nNavigate to\nhttps://atlan.zendesk.com\nand log into the help desk portal with your credentials or via SSO.\nFrom the top right, click your avatar, and then from the dropdown, click\nMy activities\n.\nOn the\nMy activities\npage, you can do the following:\nMy requests\nand\nRequests I'm CC'd on\n-  view and edit the support tickets you either created or were copied on, respectively.\nOrganization requests\n-  to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets.\nHours of operation\nâ\n24x7 availability for all requests and issues\nSeverity levels\nâ\nThe AtlanÂ Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs:\nSeverity\nDescription\nBasic support\nAdvanced support\nS0\nProduction software is unavailable; all customers are blocked and productivity halted\n2 hours\n1 hour\nS1\nProduction software is available; functionality or performance is severely impaired\n4 hours\n2 hours\nS2\nProduction software is available and usable with partial, noncritical loss of functionality. Or, production software has an occasional issue that customer requests identification and resolution. Also includes requests for help with administrative tasks\n16 hours\n4 hours\nS3\nCosmetic issues or request for general information about the software, documentation, processes, or procedures\n24 hours\n14 hours\nEscalation procedure\nâ\nIf the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket.\nPlease first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood.\nYou may further escalate by contacting:\n1st level of escalation\n: Technical Support Engineer\n2nd level of escalation\n: Director, Support\n3rd level of escalation\n: Head of Customer Experience\nTags:\nsupport\nService-level commitment\nWays to contact support\nHours of operation\nSeverity levels\nEscalation procedure"
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/authentication-and-authorization",
    "content": "Get Started\nCore Concepts\nAuthentication and authorization\nOn this page\nAuthentication and authorization\nAtlan supports the following authentication methods:\nBasic authentication\nâ\nAtlan initially comes with basic or username-password authentication. Admins can\ninvite new users\nto log into Atlan. When a new user opens the invitation link, they will be able to set up their user profile, including username and password.\nHowever, Atlan does not recommend using basic authentication. Instead, admins should configure and enforce\nSSO authentication\n.\nSSO authentication\nâ\nSSO using SAML 2.0\nâ\nAtlan supports single sign-on (SSO), allowing admins to configure SSO authentication.\nAtlan currently supports the following SSO providers:\nAzure AD\nGoogle\nJumpCloud\nOkta\nOneLogin\nCustom IdP\nSSO using SCIM\nâ\nSystem for Cross-domain Identity ManagementÂ (SCIM) provisioning\nworks in combination with SSO. Atlan currently supports SCIM provisioning for the following SSO providers:\nAzure AD\nOkta\nAuthorization\nâ\nRole-based access control (RBAC)\nâ\nAtlan implements role-based access control (RBAC) to ensure that users have the minimum level of access required to perform their tasks. Access rights are assigned based on roles, and users are granted permissions according to their responsibilities. A system owner or an authorized party must approve any additional permissions.\nAtlan adheres to the principle of least privilege, ensuring that users are only granted the level of access necessary to perform their job functions.\nUser access review (UAR)\nâ\nAtlan recommends that admins performÂ access reviews of users, admins, and service accounts on a quarterly basis to ensure that appropriate access levels are maintained. Access reviews should also be documented.\nIdentity and access management\nâ\nFor centralized management of groups and users, Atlan uses granular\naccess policies\n.\nAdmins can define policies to control both which actions a user can take and against which assets. These can be as broad as entire databases down to individual columns. Organizations can even build policies based on asset classification. This opens up the ability to restrict access to sensitive data like Personally Identifiable Information (PII)   -  an essential feature in the GDPR era.\nAtlan\ndenies access by default\n, and\nexplicit denials override any grants\n. You can even deny admin users access to assets, if you want.\nRoles\nâ\nYou must assign every user in Atlan a\nuser role\n. These control basic levels of access.\nGroups\nâ\nYou can also add users to\ngroups\n. Groups provide a more maintainable mechanism for applying access controls.\nPolicies\nâ\nYou can define\naccess policies\nfor both users and groups.\nThrough these policies you can restrict which users can take which actions on which assets.\nFor example, you can set up\ntags\nsuch as PII and apply this to data assets like tables. You can also configure the tag to propagate downstream to any columns or tables created from them.\nYou can then define\naccess controls based on these tags\nto restrict access to tagged assets. If Atlan propagates tags for you to derived assets, the access control is automatically applied to those derived assets as well.\nTags:\natlan\ndocumentation\nPrevious\nAtlan's open API\nNext\nData and metadata persistence\nBasic authentication\nSSO authentication\nAuthorization\nIdentity and access management"
  },
  {
    "url": "https://docs.atlan.com/platform/references/cloud-logging-and-monitoring",
    "content": "Get Started\nAdministration\nCloud logging and monitoring\nOn this page\nCloud logging and monitoring\nAtlan exports IAM service event logs in the OpenTelemetry Protocol (OTLP) specification and securely delivers them to the Amazon S3 or Google Cloud Storage (GCS) bucket of your organization. This enables you to monitor login events and integrate logs with security information and event management (SIEM) systems for real-time security monitoring and alerts.\nKey aspects\nâ\nLog format and structure: The OTLP format ensures seamless integration with SIEM systems, and logs are organized by date and event type. Logs are stored in a compressed format in your organization's preferred object storage (S3 or GCS). Once uncompressed, the logs will be available in a JSON file format containing multiple log entries.\nEach file is saved for an hour in the following folder structure in gzip:\n/year=YYYY/month=MM/day=DD/hour=HH/logs_<rnd-9-digit-int>.json.gz\nThe JSON file structure is as follows:\n{\n\"resourceLogs\"\n:\n[\n{\n\"resource\"\n:\n{\n\"attributes\"\n:\n[\n// k8s metadata\n]\n}\n,\n\"scopeLogs\"\n:\n[\n{\n\"scope\"\n:\n{\n}\n,\n\"logRecords\"\n:\n[\n{\n\"timeUnixNano\"\n:\n\"1725861538220747913\"\n,\n\"observedTimeUnixNano\"\n:\n\"1726071786185095727\"\n,\n\"body\"\n:\n{\n\"stringValue\"\n:\n\"//redacted logline\"\n}\n,\n\"traceId\"\n:\n\"\"\n,\n\"spanId\"\n:\n\"\"\n}\n]\n}\n,\n{\n...\n}\n,\n]\n}\n,\n{\n\"resource\"\n:\n{\n...\n}\n,\n\"scopeLogs\"\n:\n[\n...\n]\n}\n]\n}\nSecure delivery- Logs are encrypted in transit and at rest, with mechanisms to validate data integrity.\nCustomer access: Logs are easily accessible through S3 or GCS, allowing for a flexible monitoring and alerting setup.\nEnabling event logs in AWS\nâ\nPrerequisites\nâ\nEnable bucket versioning. Both source and destination buckets must have versioning enabled. See\nAWS documentation\n.\nCustomer-provided bucket details: account ID, bucket name, and region.\nAtlan will use these details to create an IAM role on the Atlan side and then provide you with the bucket policy to be attached.\nOnce you have confirmed that the bucket policy has been attached, Atlan will complete the final step of setting up log replication. Atlan support will complete the configuration on the Atlan side.\nYou will need to attach the following policy to your destination bucket:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Id\"\n:\n\"\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"Set-permissions-for-objects\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<Atlan Role ARN>\"\n}\n,\n\"Action\"\n:\n[\n\"s3:ReplicateObject\"\n,\n\"s3:ReplicateDelete\"\n,\n\"s3:GetBucketVersioning\"\n,\n\"s3:PutBucketVersioning\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::<Customer S3 Bucket Name>/*\"\n,\n\"arn:aws:s3:::<Customer S3 Bucket Name>\"\n]\n}\n]\n}\nContinuous replication to S3 bucket\nâ\nApplication audit logs are streamed to Atlan's S3 bucket in near real time â within 10 seconds of being generated. This is a continuous process. Once the logs are available in Atlan's bucket, the logs will be replicated to your organization's S3 bucket within 15 minutes. The replication is ongoing and occurs without delays. This ensures that logs are continuously transferred as they are generated, with no waiting period between replications.\nEnabling event logs in GCP\nâ\nFor Google Cloud Platform (GCP), Atlan utilizes\nLogs Router\nto transfer logs from the GCS bucket of your Atlan tenant to a destination bucket of your choice. The destination must be supported by the Logs Router.\nThe organization must provide details of the destination where the logs should be synced. This destination must be supported by the Logs Router.\nAtlan will create a Log Router sink and provide you with a service account.\nDepending on the selected destination, you will need to configure the necessary permissions for the service account as outlined in\nGoogle documentation\n.\nOnce you have configured the permissions, the logs will begin syncing to your preferred destination.\nNew sinks to Cloud Storage buckets may take several hours to start routing log entries. Sinks to Cloud Storage are processed hourly while other destination types are processed in real time.\nTags:\nsecurity\nmonitoring\nlogs\ncompliance\nsiem\nopentelemetry\notlp\nPrevious\nHigh availability and disaster recovery (HA/DR)\nNext\nGenerate HAR files and console logs\nKey aspects\nEnabling event logs in AWS\nEnabling event logs in GCP"
  },
  {
    "url": "https://docs.atlan.com/platform/references/infrastructure-security",
    "content": "Get Started\nSecurity & Compliance\nInfrastructure security\nOn this page\nInfrastructure security\nSee\nsecurity.atlan.com\nfor the latest policies and standards, reports and certifications, architecture, diagrams and more.\nAtlan is deployed using Kubernetes in an Atlan-managed VPC (virtual private cloud).\nAtlan also carries out:\nVulnerability management through frequent releases\nÂ   -  Atlan makes weekly releases to minimize vulnerability at a product and operating system level.\nApplication Penetration Testing (APT)\n-  Atlan uses a third-party toolÂ to conduct industry standard APT. A penetration test is an authorized simulated cyber attack on a computer system, performed to evaluate the security of the system. The test is performed to identify both weaknesses (including the potential for unauthorized parties to gain access to the system's features and data) and strengths, enabling a full risk assessment to be completed.\nEvent logging and monitoring\nÂ   -  Atlan has many tools to support monitoring and event logging:\nPrometheus and Grafana for monitoring\nFluent Bit and Loki for event logging\nNetwork access to the control plane\nâ\nWe restrict access to the Kubernetes control plane by IP address to cluster administrators. We deny public internet access to the control plane.\nNetwork access to nodes\nâ\nNodes are configured to only accept connections (via network access control lists):\nfrom the control plane on the specified ports\nfor services in Kubernetes of type\nNodePort\nand\nLoadBalancer\nEach component of the Kubernetes cluster has security measures configured. These security measures are at the following levels:\nCluster security\nNode security\nPod security\nContainer security\nNetwork security\nCode security\nSecret management\nData encryption in transit\nTags:\nintegration\nconnectors\nsecurity\naccess-control\npermissions\nPrevious\nTenant offboarding\nNext\nHow are resources isolated?\nNetwork access to the control plane\nNetwork access to nodes"
  },
  {
    "url": "https://docs.atlan.com/faq/getting-started-and-onboarding",
    "content": "Get Started\nFAQs\nGetting Started and Onboarding\nOn this page\nGetting Started and Onboarding\nEverything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements.\nIs there a trial version of Atlan that I can use to learn more on my own?\nâ\nYou can sign up for the\nproduct tour\nwith your business email to learn more about Atlan.\nDo you have customers in my industry?\nâ\nAtlan has customers across a wide variety of industries, including highly regulated industries like financial services and healthcare. Feel free to review the\ncustomers page\n, or\nreach out\nto discuss your use case in more detail.\nWhat are the implementation and maintenance requirements?\nâ\nThere are no additional costs incurred during implementation. To support the user/process transformation within your organization, Atlan provides an optional Accelerator Package with cultural enablement services.\nIn terms of individuals and responsibilities required to run and maintain Atlan, both may vary depending on the following two factors:\nSize of your organization (10, 100, 1000 users).\nOrganizational structure (data mesh vs. data stewardship model).\nTypically, the application is owned by at least 1 data engineer persona and 1 persona responsible for data governance, data enablement, and information architecture. You'd only be in charge of managing data source integrations because Atlan is deployed as software as a service (SaaS).\nWhere's Atlan deployed?\nâ\nAtlan is a cloud-first solution.\nSingle-tenant SaaS\nis the recommended deployment model.\nAtlan currently supports hosting tenants on the following cloud platforms:\nAmazon Web Services (AWS)\nMicrosoft Azure\nGoogle Cloud Platform (GCP)\nFor more information, see\nAtlan architecture\n.\nWhat cloud providers do you support?\nâ\nAtlan currently supports hosting tenants on the following cloud platforms:\nAmazon Web Services (AWS)\nMicrosoft Azure\nGoogle Cloud Platform (GCP)\nWhat's the maintenance window for managing updates?\nâ\nIn addition to\nregular feature rollouts\n, Atlan performs monthly tenant infrastructure upgrades to enhance the performance, stability, and security of the tenant. Unless explicitly outlined otherwise in your organization's contract, these upgrades are performed for a period of 2-4 hours during a predetermined low usage window, usually over the weekend in your timezone.\nFor any upgrades requiring downtime, Atlan provides notice to organizations at least 1 week in advance. For any further questions about the maintenance window, please\nreach out to Atlan support\n.\nTags:\natlan\ndocumentation\nfaq-platform\nPrevious\nQuality assurance framework\nNext\nBasic Platform Usage"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/the-dataops-culture-code",
    "content": "Get Started\nReferences\nThe DataOps Culture Code\nOn this page\nThe DataOps Culture Code\nWe experimented for two years, across 200 data projects, to create our own viewpoint of what makes data teams successful. We've codified these learnings into what we call the \"DataOps Culture Code\".\nThe data team is the most interdisciplinary team in any organization.\nData Scientists, Analysts, Engineers, Business Users\n... These are diverse people, with diverse tools, skillsets, and DNA. All doing diverse things.\nSometimes they're asking open-ended questions to get to the bottom of âwhyâ, just like a scientist in a research lab. Sometimes they're working on scaling petabyte-sized data processing systems, like a software engineer.\nAdd to all this the living and breathing thing that is...\ndata\n. Unlike code or design, it's constantly changing.\nHow do you make a data team successful?\nâ\nThere's no easy answer.\nWe started as a data team ourselves, on a quest to make ourselves as agile as we could. We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams.\nWe then experimented for two years, across 200 data projects, to create our own idea of what makes data teams successful. These principles are the foundation of everything we build at Atlan.\nThe DataOps Culture Code\nâ\nð¤Â  Itâs a team sport, and collaboration is key\nâ\nData teams will always have a variety of roles, each with their own skills, favorite tools and DNA. Embrace the diversity, and create mechanisms for effective collaboration.\nð Treat data, code, models and dashboards as assets.\nâ\nAll data assets   -  from code and models to data and dashboards   -  are assets, and they should be treated like assets.\nAssets should be easily discoverable.\nAssets should be maintained.\nAssets should be easily reusable.\nðÂ  Optimize for agility\nâ\nIn todayâs world, as business needs evolve rapidly, data teams need to be a step ahead, not deluged with three months of backlog.\nConstantly measure your teamâs velocity, and invest in foundational initiatives to improve cycle times.\nReduce dependencies between business, analysts and engineers.\nEnable a documentation-first culture.\nAutomate whatever is repetitive.Â\nð¥Â  Create systems of trust\nâ\nWith the inherent diversity of data teams, it's all too easy to misunderstand other team members' roles. But that creates trust deficiencies   -  especially when things go wrong!\nIntentionally create systems of trust in your team.\nMake everyoneâs work accessible and discoverable to break down \"tool\" silos.\nCreate transparency in data pipelines and lineage so everyone can see and troubleshoot issues.\nSet up monitoring and alerting systems to proactively know when things break.\nðï¸ Create a plug-and-play data stack\nâ\nThe data ecosystem will rapidly evolve. The tools, technology and infrastructure you use today will (and should) be different from the tools you use two years later.\nYour data stack should allow your team to experiment and innovate as technology evolves, without creating lock-ins.\nEmbrace tools that are open and extensible.\nLeverage a strong metadata layer to tie diverse tooling together.\nâ¨Â  User experience defines adoption velocity\nâ\nEmployees at\nAirbnb\nfamously said, \"Designing the interface and user experience of a data tool should not be an afterthought.\"\nWithout good user experience, the best tools or most thoughtful processes won't be adopted in your team.\nInvest in user experience, even for internal tools. It will define adoption velocity!\nInvest in simple and intuitive tools.\nSoftware shouldn't need training programs.\nTags:\natlan\ndocumentation\nPrevious\nOur 3 pro tips for saving time with Atlan\nNext\nHow are product updates deployed?\nHow do you make a data team successful?\nThe DataOps Culture Code"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/search-and-discover-assets",
    "content": "Use data\nDiscovery\nGet Started\nSearch and discover assets\nOn this page\nSearch and discover assets\nAtlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context.\nIts Amazon-like search and\nfiltering experience\nisn't just for data tables. It also extends to a variety of data assets, like columns, databases, SQL queries, BI dashboards, and much more.\nTo ensure a high-quality search experience, Atlan recommends the following:\nCertify your assets\nLink terms to your assets\nto add business context\nEnrich your assets with descriptions\nStar your assets\nfor easy access\nDid you know?\nYou can bookmark your\nsearch results with applied filters\nor share them with other Atlan users in your organization for quick and easy access.\nSearch superpowers\nâ\nLet's find out what makes Atlan's search intuitive and super quick.\nIntelligent keyword recognition\nAtlan supports powerful, intelligent search.Â When you search using keywords, the keywords in the matching search results will be highlighted for easy recognition. Even if your keyword contains an underscore\n_\nor a period\n.\n-  for example,\ninstacart_order\n-  both keywords will be highlighted across all search results.\nFor keyword-based search:\nIf the keywords you're searching by is present in the asset name,\ndescription\n, or\nlinked term\n, only then will the asset appear in your search results.Â\nAtlan displays search results based on asset names   -  technical name and\nalias\n-  that match your keyword(s).\nIf the keyword is a\nglossary term linked to assets\nor present in\nasset descriptions\n, such assets will be boosted in search results.\nWhether your search query is incomplete (\ninsta\n) or misspelled (\ninstacrt ordr\n), Atlan's powerful search can still help you discover exactly what you need.\nSearch from anywhere\nâ\nThere are multiple ways to start your search:\nClick the\nSearch assets across Atlan\nbar on the homepage.\nClick\nAssets\nin the left-side panel.\nUse\nCmd/Ctrl + K\nto open the search page from\nanywhere\nin Atlan.\nSearch using context\nâ\nThe\nAssets\nsection offers a\nvariety of filters\nto narrow down your search. Here are the different types of filters that you can use:\nSource\n: Search by\nconnectors\n, chosen from a list of connections within Atlan.\nDomains\n: Filter assets by\ndomains\n, such as a single domain, multiple domains, or no domain.\nCertificate\n: Search based on the certificate attached to data assets, such as\nVerified\n,\nDraft\n,\nDeprecated\n, and\nNo certificate\n.\nOwners\n: Filter by selecting one or more users. You can also toggle between\nusers\nand\ngroups\nÂ to filter based on a group of users.\nTags\n: Filter by user-generated tags, such as\npublic\n,\nPII\n, and more.\nTerms\n: Filter by terms from your glossaries, such as\ncost\n,\nrevenue\n, or\nP&L\n.\nProperties\n: Filter assets by other properties, like technical name or\nalias\n,\ndescription\n, last updated, and so on.\nAtlan's search results include a quick count of all the resulting data assets grouped by type. As you apply the filters, you'll see these counts change in real time.\nYou can also enter a keyword in the search bar and filter your results by a specific type of data asset. For instance, enter the keyword\norder\nin the search bar and then click the\nColumn\ncheckbox to view column results for your searched keyword.\nSort search results\nâ\nAtlan allows you to sort your search results in different ways. This helps you quickly find the assets you're interested in. Sorting options include:\nRelevance\n: Sort by how closely the search results match your searched keywords.\nName\n: Sort by the asset name in an alphabetical or a reverse alphabetical order.\nUpdated on Atlan\n: Sort by the newest or oldest updated assets.\nStar count\n: SortÂ assets by\nmost or fewest stars\n.\nOrder\n: Sort the search results in an ascending or descending order.\nPopularity\n: Sort Snowflake and Google BigQuery assets by the\nmost or least popular assets\n.\nDid you know?\nThe sorting options may vary depending on the asset type selected. For example, if you are viewing the results while filtering by the\nTable\ntab, you'll also have the option of sorting by the most or fewest number of rows and columns.\nSearch with patterns\nâ\nYou can refine your search in Atlan with the following patterns:\nExact match search\n: Wrap the keywords within single\n''\nor double\n\"\"\nquotation marks when typing them in the search bar   -  for example,\n\"instacart_total_users\"\n. Only the asset names with case-insensitive exact match and following the order of the keywords will be boosted in the search results   -  for example,\ninstacart_total_users\nor\nInstacart_Total_Users\n. If the keywords are contained in the\nasset description\nor\nlinked terms\n, such assets will show up next. Additionally, you can use exact match to search by the\nqualifiedName\nor globally unique identifier (GUID) of an asset.\nCombined string of database, schema, and table\n: For a more data-friendly search experience, copy the combined string of\ndatabase.schema.table\n(or\nschema.table\n) from your SQL editor and paste it in the search bar   -  for example,\natlan_db.public.instacart_total_orders\n.\nMultiple phrase match\n: When you enter two or more keywords, Atlan will find assets with asset names that partially match the keywords or a combination of them to narrow down the search results.\nSee only what you want to see\nâ\nAtlan gives you the option to customize your search. Want to show or hide certain fields in your search results? Click the\n3-dot\nicon next to the search bar to set display preferences for each field:\nDescription\nTerms\nTags\nConnection\nTags:\ndata\nasset-profile\nPrevious\nDiscovery\nNext\nAccess archived assets\nSearch superpowers"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/query-data",
    "content": "Use data\nInsights\nGet Started\nHow to query data\nOn this page\nquery data\nThere are two ways to query data in Atlan:\nwriting your own SQL\nusing the Visual Query Builder\nDid you know?\nAtlan pushes all queries to the source (no data is stored in Atlan). In addition, Atlan applies access policies to the results before displaying them.\nWrite your own SQL\nâ\nWho can do this?\nAnyone with the knowledge to write SQL. Any\nAtlan user\nwith\ndata access to the asset\ncan query data.\nTo query an asset with your own SQL:\nFrom the left menu of any screen, click\nInsights\n.\nUnder the\nExplorer\ntab, find the asset you want to query:\nUse the\nSelect database\ndropdown to choose another database, if necessary.\nSearch for the asset by name in the search bar, or browse for it in the tree structure.\nHover over the table or view, and click the play icon. This writes and runs a basic preview query.\n(Optional) Click the open asset sidebar icon to view more details in the asset sidebar.\n(Optional) Click the eye icon to view a preview of the query results.\n(Optional) Click the 3-dot icon for more options:\nClick\nSet editor context\nto set the same connection, database, and schema name in the query editor as selected in the\nExplorer\ntab.\nClick\nPlace name in editor\nto view the asset name in the query editor.\nClick\nCopy path\nto copy the full path of the asset, including database and schema names.\nUnder the\nUntitled\ntab on the right, change the sample query or write your own   -  separate multiple queries with a semicolon\n;\n. Click the\nRun\nbutton in the upper right to test your query as you write it.\n(Optional) Click the downward arrow next to the\nRun\nbutton to\nexport query results via email\nor\nschedule the query\n.\n(Optional) If you have multiple tabs open in the query editor, right-click a tab to open the tabs menu. You can close a specific tab or all tabs, or duplicate the query.\n(Optional) From the top right of the query editor, click the 3-dot icon for additional query editor actions or to customize it further:\nClick or hover over\nDuplicate query\nto create a duplicate version of your query.\nClick or hover over\nOpen command palette\nto view the actions you can run inside the query editor.\nClick or hover over\nThemes\nand then select your preferred theme for the query editor.\nClick or hover over\nTab spacing\nto change the tab spacing for your queries.\nClick or hover over\nFont size\nto change the font size for your queries.\nClick or hover over\nCursor\nto change the cursor position in the query editor.\nClick or hover over\nAutosuggestions\nto turn off autosuggestions for assets in the query editor.\nThe editor supports all read-based SQL statements, including\nJOIN\n. The editor will not run any write-based statements. The following SQL statements are not supported:\nUPDATE\nDELETE\nCREATE\nALTER\nDROP\nTRUNCATE\nINSERT INTO\nDid you know?\nYou can select the context for your query to the left of the\nRun\nbutton. Then you won't need to fully qualify table names with schema and database names.\nUse the Visual Query Builder\nâ\nWho can do this?\nAny\nAtlan user\nwith\ndata access to the asset\n. No SQL knowledge required!\nTo query an asset using the Visual Query Builder:\nFrom the left menu of any screen, click\nInsights\n.\nAt the top of the screen, to the right of the\nUntitled\ntab, click the\n+\nbutton and select\nNew visual query\n.\nUnder\nSelect from\nchoose the table or view you want to query.\n(Optional) In the column selector to the right, select the column you want to query.\nThen develop your query:\nClick the\nRun\nbutton to run the query and preview its results.\nClick the blue circular\n+\nbutton to add an action to the query.\nRepeat these steps until your query is complete.\n(Optional) If there are any errors in your query, click\nAuto fix\nfor Atlan to recommend a fix.Â\n(Optional) In the query results set, click\nCopy\nto copy the query results or click\nDownload\nto export them.\nDid you know?\nYou can learn more about the query builder actions in\nthis example\n.\nTags:\natlan\ndocumentation\nPrevious\nInsights\nNext\nSave and share queries\nWrite your own SQL\nUse the Visual Query Builder"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/save-and-share-queries",
    "content": "Use data\nInsights\nQuery Management\nSave and share queries\nOn this page\nSave and share queries\nYou can save queries to re-run them later, schedule them, or share them.\nDid you know?\nYou can only save queries through a\ncollection\n. You can share collections with others, to share your queries.\nSave a query\nâ\nTo save a query:\nOpen the query in Insights.\nAt the top right of the query, click the\nSave\nbutton.\nIn the resulting\nSave query\ndialog, enter the following details:\nFor\nQuery name\n, enter a name for the query.\n(Optional) For\nDescription\n, add a description for the query.\nFor\nCollection\n, you can either:\nIf you have access to existing collections, click the\nChoose collection\ndropdown to select an existing collection.\nIf you do not have any existing collections, click the\nCreate Collection\nbutton. In the\nCreate collection\ndialog, enter the following details:\nFor\nName\n, enter a name for the collection.\n(Optional) To the left of the name, click the image icon to choose an icon for the collection.\n(Optional) For\nDescription\n, describe the collection.\n(Optional) For\nShare\n, select other users or groups that can access the collection. (See below for more details.)\nAt the bottom of the\nCreate Collection\ndialog, click\nCreate\n.\n(Optional) For\nCertificate\n, click the\nNo certification\ndropdown to assign a\ncertificate\nto the query.\n(Optional) For\nLinked terms\n, click the\nSelect terms\ndropdown to assign a\nterm\nto the query.\nAt the bottom of the dialog:\nTo only save your query, click\nSave\n.\nTo save and share your query, click\nSave and share\n.\nIn the\nQuery saved\ndialog, enter the following details:\nFor\nAdd users or groups\n, select other users or groups that can access the saved query.\n(Optional) To the right of the user or group, click the\nEditor\ndropdown to change the sharing permissions:\nViewer\nallows users to view and run all queries in the collection, but not edit them.\nEditor\nallows users to view, run, and edit all queries in the collection.\nClick\nInvite\nto invite the users or groups.\n(Optional) Click\nCopy Link\nto copy the link for the saved query to share with others in your team.\n(Optional) Click the\nSlack\nor\nTeams\nbutton to share directly on\nSlack\nor\nMicrosoft Teams\n, respectively.\nClick\nDone\nto confirm your selections.\nDid you know?\nAtlan currently supports a query length of 2 million characters for saved queries.\nShare a query collection\nâ\nA collection helps you organize saved queries in Atlan. A collection could represent a topic, department, or team with similar saved queries under one roof. Within each collection, you can have a folder that contains multiple saves queries of a similar type.\nTo share a collection of queries:\nOpen Insights.\nIn the upper left, click the papers-in-a-box icon.\nUnder the icon, click the name of the selected query collection.\nFrom the resulting list of collections, hover over the collection you want to share.\nClick the 3-dot icon to the right of the collection name and choose\nEdit collection\n.\nIn the\nEdit collection\ndialog, under\nShare\n:\nSearch for users or groups with whom to share the collection.\n(Optional) To the right of the user or group, click the\nCan edit\ndropdown to change the sharing permissions:\nCan edit\nallows users to view, run, and edit all queries in the collection.\nCan view\nallows users to view and run all queries in the collection, but not edit them.\nRepeat these steps for each user or group with whom you want to share the collection.\nAt the bottom of the\nEdit collection\ndialog, click\nUpdate\n.\nDid you know?\nUsers with only\nCan view\npermissions will still be able to change the interactive part of\ninteractive queries\n.\nMove a saved query\nâ\nTo move a saved query to another query collection:\nOpen Insights.\nIn the upper left, click the papers-in-a-box icon.\nUnder the icon, click the name of the selected query collection.\nFrom the resulting list of collections, hover over the collection from which you want to move a query.\nClick the 3-dot icon to the right of the saved query name, and then click\nMove to\n.\nIn the\nMove to\ndialog, select the query collection to which you want to move your saved query.\nClick\nMove\nto complete moving the saved query.\nTo duplicate, rename, edit, or delete your saved query, click the 3-dot icon to the right of the saved query name and select the relevant option.\nDid you know?\nIf you add a Slack channel to the\nQuery output share channels\nfield in your Slack integration\n, you will be able to share your saved query and query results directly on that Slack channel. Atlan will deliver the query results as a CSV file on the same Slack thread.\nView query sidebar\nâ\nOnce you've\nsaved a query\n, you can access the query sidebar to view additional context for your saved queries.\nTo open the query sidebar for a saved query:\nIn the left\nExplorer\npanel in\nInsights\n, hover over a saved query and click the\nOpen query sidebar\nicon.\nFrom the saved query sidebar in the right, you can:\nView details about your saved query in\nOverview\n, including the actual query. (Optional) You can add more details to your saved query:\nClick the star button to\nstar the query\nfor quick access.\nFor\nDescription\n, add a\ndescription\nto your saved query.\nClick\n+Add README\nto add a\nREADME\nto your saved query and provide more context.\nFor\nCollection\n, click the collection name to view the query collection.\nCopy the SQL query or expand the query view to fullscreen.\nFor\nTerms\n, add a\nterm\nto\nlink to your saved query\n.\nFor\nOwners\n, update the\nowner\nof the saved query or add more owners.\nFor\nTags\n,\nattach a tag\nto your saved query.\nFor\nCertificate\n, update the\ncertification status\nof your saved query.\nClick the\nRelations\ntab to view queried assets.\n(Optional) Select a related asset to open the asset sidebar. From the asset sidebar, click the\nQueries\ntab to view the saved query auto-linked to the asset.\nClick the\nActivity\ntab to view the\nactivity log\nfor the saved query.\nClick the\nSchedules\ntab to view\nassociated schedules\nfor the saved query, if any.\nClick the\nResources\ntab to view any linked\nresources\n.\nClick the\nRequests\ntab to view any\nrequests\non the saved query.\nClick the\nProperties\ntab to view query properties.\nClick the\nSlack\nor\nTeams\ntab to view\nSlack\nor\nTeams\nmessages pertaining to the query.\nLink saved queries\nâ\nOnce you have saved a query, the saved query will be auto-linked to all the assets queried or referenced in the SQL query. Linked queries are displayed in the\nasset profile\nand\nsidebar\n.\nYou might want to link it to other assets, too. This can help you provide additional context on the assets and quickly find the saved query. For such assets, you can link the saved query as a\nresource\n.\nTo link a saved query to an asset:\nOpen Insights.\nIn the upper left, click the papers-in-a-box icon.\nUnder the icon, click the name of a query collection.\nFrom the resulting list of collections, select a collection and then select the saved query you want to link as a\nresource\n.\nTo copy the link for a saved query, you can either:\nClick the 3-dot icon to the right of the saved query name and then click\nCopy link\n.\nIn the top right of the query editor, click the 3-dot icon and then click\nShare\n. From the\nShare\nmenu, click\nCopy link\n.\nFrom the left of menu, click\nAssets\nto navigate to your assets.\nFrom the\nAssets\npage, select an asset to open the asset sidebar.\nFrom the asset sidebar on the right, click the\nResources\ntab and then click\n+ Add resource\n.\nIn the\nAdd Resource\ndialog, enter the following details:\nFor\nLink\n, paste the saved query link you copied in Insights.\nFor\nTitle\n, add a title for your saved query.\nClick\nAdd\nto add the saved query as a\nresource\nto the asset.\nDid you know?\nAny user in Atlan will be able to preview saved queries for auto-linked or manually linked assets from the asset sidebar   -  unless there are\naccess policies\nprohibiting them.\nTags:\natlan\ndocumentation\nPrevious\nHow to query data\nNext\nMake a query interactive\nSave a query\nShare a query collection\nMove a saved query\nView query sidebar\nLink saved queries"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo",
    "content": "Connect data\nData Quality & Observability\nMonte Carlo\nGet Started\nSet up Monte Carlo\nOn this page\nSet up Monte Carlo\nWho can do this?\nYou will probably need your Monte Carlo\naccount owner\nto complete these steps   -  you may not have access yourself.\nAtlan supports the API authentication method for fetching metadata from Monte Carlo. This method uses an API key ID and secret to fetch metadata.\nCreate an account-service API key\nâ\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the objects in your Monte Carlo environment.\nYou will need to create an\naccount-service API key\nin Monte Carlo for integration with Atlan.\nTo create an account-service API key for\ncrawling Monte Carlo\n:\nLog in to your Monte Carlo instance.\nIn the top header of your Monte Carlo instance, click\nSettings\n.\nIn the left menu under\nSettings\n, click\nAPI Access\nand then click\nAccount Service Keys\n.\nFrom the\nAccount Service Keys\npage, click the\nCreate Key\nbutton.\nIn the\nCreate Account Service Key\ndialog, enter the following details:\nFor\nDescription\n, add a meaningful description for your API key   -  for example,\nAtlan connection\n.\nFrom the\nAuthorization Groups\ndropdown, select\nViewers (All)\nto provide\nminimum permissions\nfor crawling Monte Carlo.\n(Optional) For\nExpires After\n, keep the default selection or select a preferred option.\nClick\nCreate\nto finish creating the account-service API key.\nFrom the corresponding screen, copy the\nKey ID\nand\nSecret\nand store them in a secure location.\ndanger\nThe API secret cannot be retrieved later.\nTags:\nconnectors\ndata\nintegration\ncrawl\napi\nauthentication\nPrevious\nMonte Carlo\nNext\nCrawl Monte Carlo\nCreate an account-service API key"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-is-lineage",
    "content": "Use data\nLineage\nConcepts\nWhat is lineage?\nOn this page\nLineage\nData lineage\ncaptures how data moves across your data landscape. This information is useful to:\nTrace data's origins, to assist with root cause analysis\nTrace data's destinations, to assist with impact analysis\nAutomate the propagation of metadata to derived assets\nDid you know?\nTag propagation is disabled by default in Atlan. You can\nenable tag propagation\nto child and downstream assets.\nRoot cause analysis\nâ\nRoot cause analysis is about identifying the underlying causes of a data problem. You want to know where the data came\nfrom\nand what\nhappened\nto it before it got to you. With root cause analysis, your focus is on these\nupstream\nsources and transformations.\nImpact analysis\nâ\nImpact analysis is about identifying potential consequences of changes. You want to know where the data is\ngoing\nand what\ncould happen\nto others if you change it. With impact analysis, the primary focus is on these\ndownstream\nsystems and consumers.\nDid you know?\nWhen viewing lineage in Atlan, hover over any asset to view a metadata popover. The metadata popovers display relevant metadata for the asset, providing you with more context for your analysis. For example, database and schema names for Snowflake assets, project names for dbt models, and more.\nHow does it work?\nâ\nAtlan constructs\nlineage\nby combining assets and processes:\nAssets represent the inputs and outputs of processes   -  databases, dashboards, and so on.\nProcesses\nrepresent the activities that move or transform data between the assets. (Processes are the lines between the assets in Atlan's graphical view.)\nAtlan chains these together into a flow of data from various resources:\nSQL parsing\nâ\nAtlan parses SQL queries to determine how data stores have created or transformed assets. Examples of this include:\nAmazon Redshift\ndbt\nGeneric query logs (via S3 objects)\nGoogle BigQuery\nSnowflake\nAPI crawling\nâ\nAtlan also retrieves lineage information for assets from APIs. Examples of this include:\nDatabricks (Unity Catalog)\nLooker\nMicrosoft Power BI\nTableau\nAPI ingestion\nâ\nAtlan provides built-in lineage extraction for the tools above. But you can also extend lineage with your own information using Atlan's\nopen APIs\n. You can use these to integrate lineage from your own home-grown tools or orchestration suites like\nApache Airflow\nand\nDagster\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nfaq\ntroubleshooting\nPrevious\nWhat is column-level lineage?\nNext\nWhat are partial assets?\nHow does it work?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/concepts/what-is-atlan-ai",
    "content": "Configure Atlan\nAtlan AI\nConcepts\nWhat's Atlan AI?\nOn this page\nAtlan AI\nâ\nAvailable to customers in Enterprise and Business-Critical platform editions\nYou can use Atlan AI to supercharge the documentation of your data assets and gain meaningful insights from your data estate in Atlan.\nDid you know?\nAtlan uses\nAzure OpenAI Service\nto power Atlan AI. Atlan does\nnot\nsend any data to the AI service and only uses metadata for supported capabilities. For questions about data security, see\nAtlan AI security\n.\nEnable Atlan AI\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to enable Atlan AI.\nOnly admin users in Atlan can enable Atlan AI for their organization. Once enabled, each user's existing\npermissions\nand\naccess policies\nin Atlan will determine how they can use Atlan AI. For example, a user must have the permission to edit metadata to use Atlan AI for updating asset descriptions.\nTo enable Atlan AI for your Atlan users:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nLabs\n.\nOn the\nLabs\npage, under the\nAtlan AI\nheading:\nTurn on\nEnrich metadata\nto enable your users to use Atlan AI for\ndocumenting assets\n,\nexplaining lineage transformations\n, and\ngenerating aliases\n.\n(Optional) For\nCustomize Atlan AI\n, click the\nAdd instructions\nbutton to make Atlan AI suggestions more relevant to your organization. In the\nEnhance suggestions\ndialog, for\nGeneral instructions\n, describe your organization and add details about your product, mission, and more, and then click\nSave\n.\nYou have now unleashed the power of Atlan AI for your users! ð\nIf you'd like to disable Atlan AI from your organization's Atlan workspace, follow the steps above to turn it off.\nDid you know?\nIf\nAtlan AI is disabled\n, the feature will no longer be available in your workspace. However, any descriptions previously generated by Atlan AI and added to your assets or saved SQL queries will still be available.\nCurrent capabilities\nâ\nUse Atlan AI to document assets\nâ\nAtlan AI puts you in control of your data estate in Atlan, helping you curate meaningful context for your data assets. Accept, reject, or edit any AI-powered suggestions, the choice is\nyours\n.\nYou can use Atlan AI to:\nDocument tables and views with AI-generated descriptions\nDocument columns with AI-generated descriptions\nDocument terms and categories with AI-generated descriptions\nDocument terms with AI-generated READMEs\nAdd an Atlan AI-generated alias to supported assets\nDid you know?\nTo ensure full transparency, any changes made using Atlan AI will be marked as\nUpdated using Atlan AI\nin the\nactivity log\n.\nUse Atlan AI for lineage analysis\nâ\nAtlan AI can help you understand\nlineage transformations\nusing natural language. You can use Atlan AI to create a natural language explanation for assets with SQL attributes and help you better understand the transformation logic.\nYou can use Atlan AI to:\nExplain lineage transformations\nTags:\natlan-ai\nPrevious\nAtlan AI security\nEnable Atlan AI\nCurrent capabilities"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/attach-a-tag",
    "content": "Build governance\nTags\nTag Management\nAttach a tag\nOn this page\nAttach a tag\nAtlan allows users to add\ntags\nto assets. You can use them to identify key characteristics of assets or group them together for usage or data protection.\nAtlan also supports attaching tags imported from the following supported sources:\nDatabricks\ndbt\nGoogle BigQuery\nSnowflake\nFor tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar.\nDid you know?\nTag propagation is disabled by default in Atlan. You can\nenable tag propagation\nto child and downstream assets.\nDirectly tag an asset\nâ\nTo directly tag an asset:\nIn the left menu from any screen in Atlan, click\nAssets\n.\nOn the\nAssets\npage, click an asset to view its asset profile.\nUnder\nTags\nÂ in the right menu, click the\n+\nicon.\nIn the popup, check the boxes to select one or more tags for the asset.\nNo propagation\nis the default setting. Next to your selected tag(s) in the popup, click\nEdit\nÂ to configure the propagation of tags:\nClick\nHierarchy & lineage\nto allow propagation of tags to the child and downstream assets.Â\nClick\nHierarchy only (no lineage)\nto allow propagation of tags to the child assets only.\nClick\nNo propagation\nto disallow any propagation of tags.\n(Optional) For tags imported from supported sources, you can configure the following:\nFor\nSnowflake assets\n, you can attach a\nSnowflake tag\n. If\nreverse sync is enabled\n, any updates made in Atlan will also be synced to Snowflake. If reverse sync is disabled, updates will be restricted to Atlan. Under\nSnowflake tags\n, select a\nsynced Snowflake tag\nand then:\nClick the\nSelect tag value\ndropdown to attach an\nallowed value\nfrom a predefined list, if available.\nFor\nAdd value\n, enter a tag value of your choice, if no predefined allowed values are present. Tag values added in Atlan are case-sensitive.\nFor\ndbt Cloud\nor\ndbt Core\nassets, you can attach a\ndbt tag\n.\nFor\nGoogle BigQuery\nassets, you can attach a\nGoogle BigQuery tag\n.\nFor\nDatabricks\nassets, you can attach a\nDatabricks tag\nand tag values. If\nreverse sync is enabled\n, any updates made in Atlan will also be synced to Databricks. If reverse sync is disabled, updates will be restricted to Atlan.\ndanger\nIf there are multiple synced tags mapped to an\nAtlan tag\n, you will only be able to select one synced tag. You can also only select imported tags that belong to the same connection as the selected asset.\nClick\nUpdate\nto confirm your selections.\nClick\nSave\nto save the tag(s) to your asset.\n(Optional) Hover over the attached tag to view tag propagation details in a popover, including username of the user who applied the tag, mode of tag propagation, and when the tag was configured.\n(Optional) Filter tagged assets by\nattached tags\n, including tags imported from supported sources.\nFor reverse sync to work for tags imported from\nSnowflake\nand\nDatabricks\n, first ensure that reverse sync is enabled on the imported tag and then you must attach the imported tag to the asset (complete step 6 above).\nDid you know?\nYou can\nremove tags\nfrom your tagged assets. You can also\nadd tags to your column assets\ndirectly from Google Sheets.\nTags:\nconnectors\ndata\nPrevious\nDelete a tag\nNext\nRemove a tag\nDirectly tag an asset"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data",
    "content": "Build governance\nCustom Metadata\nAccess Management\nControl access to metadata and data?\nOn this page\nControl access to metadata and data?\nYou can customize access for users through several mechanisms.\nUser roles\nâ\nThe most general mechanism is aÂ\nuser role\n. These define the very broad permissions a user has in Atlan   -  for example, whether they can administer other users, or only discover metadata. When it comes to\nwhat\nmetadata and data a user can access, though, we need to use the additional mechanisms below.\nConnection admins\nâ\nConnection admins are users who manage connectivity to a data source. By default, these users can:\nRead and write all metadata on assets from that connection.\nPreview and query the data in all data assets from that connection.\nManage access policies to grant others access to the assets from that connection.\nYou define the connection admin when crawling a new data source for the first time. A connection admin can also extend the list of connection admins on their connection at any time.\nAccess policies\nâ\nWho can do this?\nA user must be both an\nadmin user\nand a connection admin to define access policies for the connection's assets.\nAccess policies either allow or restrict access to certain assets. These allow you to be much more creative (and granular) about access than the all-or-nothing privileges of connection admins.\nYou start by defining which assets to control with each policy. There are two complementary mechanisms to do this in Atlan   -\npersonas\nand\npurposes\n.\nOnce you have defined the subset of assets, you can then define granular access to both metadata and data:\nMetadata policies\nâ\nMetadata policies control what users can do with the assets' metadata. Through them, you can control who can:\nRead\n: view an asset's activity log, custom metadata, and SQL queries\nUpdate\n: change asset metadata, including description, certification, owners, README, and resources\nUpdate Custom Metadata Values\nfor the assets\nAdd Tags\nto the assets\nRemove Tags\nfrom the assets\nAdd Terms\nto the assets\nRemove Terms\nfrom the assets\nCreate\n: create new assets within the selected connection (via API)\nDelete\n: delete assets within the selected connection (via API)\nData policies\nâ\nData policies control what users can do with the assets' data. Through them, you can control who can:\nQuery and preview the data within the assets\nWhether to hide any data, through various masking techniques:\nShow first 4\n: replaces all the data with\nX\nexcept the first 4 characters of data. For example\n1234 5678 9012 3456\nwould become\n1234XXXX\n.\nShow last 4\n: replaces all the data with X except the last 4 characters of data. For example\n1234 5678 9012 3456\nwould become\nXXXX3456\n.\nHash\n: replaces the data with a consistent hashed value. Because the hash is consistent you can still join on it across assets. For example\n1234 5678 9012 3456\nwould become\nf43jknscakc12nk21ak\n.\nNullify\n: replaces the data with the null value. For example\n1234 5678 9012 3456\nwould become\nnull\n.\nRedact\n: replaces all alphabetic data with x and all numeric data with 0. For example\n1234 Street Name\nwould become\n0000 Xxxxxx Xxxx\n.\nGlossary policies\nâ\nGlossary policies control what users can do with glossary metadata   -  terms and categories. Through them, you can control who can do the following against each glossary:\nRead permission on terms, categories, and glossaries exists by default and cannot be modified. Glossary policies do not restrict users from viewing any glossary and its contents within the\nGlossary\nsection.\nCreate terms and categories inside the glossary\nUpdate descriptions, certification, owners, READMEs, and resources for the glossary, terms and categories\nLink terms in the glossary with all other assets\nDelete terms and categories inside the glossary\nAdd tags to the terms\nRemove tags from the terms\nUpdate custom metadata values for the terms and categories inside the glossary\nGlossary policies can only be defined through personas.\nInteractions\nâ\nAll the mechanisms above can coexist. This is powerful, but can also be a bit overwhelming to think about.Â What takes priority when a user is under the control of all these mechanisms? ðµâð«\nIt's actually not as bad as you might think   -  only these three rules:\nAccess is denied by default (implicitly)\nâ\nBy default, users will not have the permissions listed above. This remains true until you explicitly grant a user a permission.\nFor example, imagine you have not set up any access policies and a new user joins.\nThey will not have any of the permissions above against\nany\nassets in Atlan.\nDid you know?\nUsers have read permission on terms, categories, and glossaries by default in Atlan.\nExplicit grants (allows) are combined\nâ\nWhen you grant a user a permission, this is combined with all other permissions you have granted the user.\nContinuing our example, imagine you add the new user to a group defined as the connection admins for Snowflake.\nThe user will now have full read/write access to all metadata for Snowflake assets, and be able to query and preview the data in those assets.\nThen you add the user to a persona that gives read/write access to a Looker project.\nThe user will now have access to all Snowflake assets and a Looker project's assets.\nExplicit restrictions (denies) take priority\nâ\ndanger\nWhen you explicitly deny a user a permission, this takes priority over all other permissions you have granted the user.\nContinuing our example, imagine you define a purpose with a data policy that masks PII data.\nThe user will still have full read/write access to all metadata for Snowflake assets and a Looker project's assets.\nIn general, they will still be able to query and preview the data in the Snowflake assets.\nHowever, any PII data in Snowflake will now be masked.\nThen you add a metadata policy to the purpose that denies permission to remove the PII tag.\nThe user will no longer have full read/write access to all metadata for Snowflake assets and a Looker project's assets.\nThe user can no longer remove the PII tag from any of these assets.\nDid you know?\nThe combination of mechanisms in the example above shows their power. Through a small number of controls we can define wide-ranging but granular access permissions.\nTags:\natlan\ndocumentation\nPrevious\nAdd options\nNext\nDisable data access\nUser roles\nConnection admins\nAccess policies\nInteractions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/concepts/what-are-data-products",
    "content": "Configure Atlan\nData Products\nConcepts\nWhat are data products?\nOn this page\nData Products\nâ\nAvailable via the Data Marketplace package\nFrom a single data table to a collection of data assets, anything can be a data product in Atlan. Data products provide a framework for your teams to curate assets specific to a domain, business unit, region of operation, brand, and more. These curated data products then empower your data consumers to easily discover data assets, quickly get the context they need, and collaborate more efficiently.\nAs organizations shift from centralized data architectures, build a new paradigm of governance with data products in Atlan.\nEnable products module\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to enable the products module for your organization.\nTo enable data products for your Atlan users:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nLabs\n.\nOn the\nLabs\npage, turn on\nProducts module\nto enable your users to create and manage\ndata domains\nand\nproducts\n.\nIn the\nWho can access Products module\ndialog, you can configure access to the module for the following sets of users:\nClick\nOnly admins\nto enable the products module for admin users only.\nClick\nSelected personas\nto enable the products module for specific\npersonas\nwith\ndomain policies\n. Select the persona(s) to which you want to limit usage of the products module.\nIf there are no personas with domain policies, you can either create a\nnew persona with a domain policy\nor\nadd a domain policy\nto an existing persona.\nInclude all admins\nis selected by default. This allows any admin user to access and manage the module irrespective of whether they belong to the specified personas. (Optional) Uncheck the\nInclude all admins\ncheckbox to remove the default selection.\nClick\nAll users and personas\nto enable the products module for all your Atlan users and personas.\n(Optional) Click\nConfigure\nto update your user selections for access to the module.\n(Optional) To hide the\nproduct scorecard\non your data products, turn off\nProduct score\n.\n(Optional) To enable your users to search for data products from\nasset discovery\n, turn on\nShow products in asset discovery\n.\nIf you'd like to disable the\nProducts\nmodule from your organization's Atlan workspace, follow the steps above to turn it off.\nOnce enabled, you can also temporarily disable the module and turn it on again as needed. For any domains and products you may have created, this will not result in any data loss.\nOrder of operations\nâ\nTo start using data products, you will need to:\nCreate a data domain\n(Optional)\nAdd data subdomains\nCreate domain policies\nCreate data products\nwithin the data domain\nDiscover and collaborate on data products\nTrack and monitor domain usage\nStakeholders\nâ\nAtlan currently supports adding\npredefined stakeholders and creating custom ones\nfor your data domains and subdomains. These are responsibilities you can assign to your users based on their function within a specific domain or subdomain. Stakeholders do not enforce access control, but are meant to help your data consumers understand the organizational structure and responsibilities.\nAtlan provides the following options:\nDomain owner\n-  overall domain management and reporting.\nArchitect\n-  design and deployment of domains and subdomains.\nData product owner\n-  creation, management, and documentation of data products.\nData engineer\n-  creation and management of data pipelines.\nCreate new stakeholders\nthat better reflect your organizational structure and functions.\nComponents of a data product\nâ\nTo search for a data product:\nFrom the left menu of any screen in Atlan, you can either:\nClick\nProducts\nto search for data products from the products homepage:\nFrom the left navigation menu, use the search bar or select the relevant domain and then select a data product.\nIn the\nData products\nsection, select a trending or recently viewed data product. The list of\nTrending products\nis sorted by the total count of views on each product, with the most viewed product listed at the top.\nFrom the top right of any screen in Atlan, click the\nstar icon\n. From the\nStarred assets\npopup, select a starred data product.\nIf your Atlan admin has enabled the\nShow products in asset discovery\ntoggle, click\nAssets\nto search for data products from\nasset discovery\n:\nClick the\nAsset type\ndropdown and then select\nProduct\nto filter for data products.\nUse the\nFilters\nmenu\non the left to further refine your search.\nClick any data product to view the product sidebar or open the product profile. In addition to the factors documented\nhere\n, Atlan uses\nproduct score\nto determine the most relevant results for your product search.\nUse the search bar to search for products using keyword-based search.\nOverview\nâ\nThis section displays important details about the data product:\nData product status   -  current status of the data product:\nDraft\n-  data product is in draft state and only visible to product owners\nPublished\n-  data product is active for consumption\nSunset\n-  data product is planned for retirement\nArchived\n-  data product is archived and will be no longer available to users\nDomain name   -  view and navigate to the data domain that the data product belongs to\nCriticality   -  view business criticality rating:\nHigh\n-  high business impact\n_Medium   - _ moderate business impact\nLow\n-  internal or non-business impact\nSensitivity   -  view sensitivity score for data product classification:\nPublic\n-  may be freely accessible\nInternal\n-  may only be distributed within the organization\nConfidential\n-  may only be limited to a specific domain or team within an organization\nFreshness   -  timestamp for when the data product was last updated in Atlan. This only includes metadata updates made on the data product and not on any underlying assets.\nDescription of the data product\nLinked assets at a glance\nList of assets designated as output ports\nOutput ports\nâ\nThis section displays a list of assets that allow users to consume the data product across multiple domains. A data product can have multiple output ports. Click the output port asset to open the asset sidebar and view more details.\nREADME and resources\nâ\nThis section allows you to add a\nREADME\nand\nresources\nto your data product. READMEs can help you provide detailed documentation about the product to your data consumers. Resources enable you to add links to internal or external URLs for more context.\nProduct score\nâ\nBased on the principles of data as a product,\nproduct scores\ncan help you signal the accuracy and completeness of your data products, helping build trust in them. Atlan calculates and assigns a product score to your data products based on a preset criteria of metadata completeness.\nDetails sidebar\nâ\nThe sidebar to the right of the product profile allows you to view and add metadata, depending on your\ndomain permissions\n:\nVisibility\nhelps you determine who can access and monitor the data product throughout its entire lifecycle:\nPrivate to members of this domain\n-  only members of a specific domain can access the data product.\nPrivate to selected members\n-  only members of a specific domain and other selected users or groups can access the data product.\nPublic\n-  everyone in the organization can access the data product.\nUnder\nTerms\n, click\n+\nto add\nterms\nand offer contextual information for your data product.Â\nUnder\nOwners\n, click\n+\nto assign\nowners\nto the data product.\nUnder\nTags\n, click\n+\nto\nattach a tag\nand configure\ntag propagation\nfor all assets in the data product.\nUnder\nCertificate\n, click\n+\nto update the certification status. Choose from\nfour certificateÂ options\n-\nDraft\n,\nVerified\n,\nDeprecated\n, and\nNo certificate\n.\nProduct profile header\nâ\nThis section helps you perform quick actions. From the top right of the product profile:\nClick the user avatars to view a list of recently visited users, total views on your product, total number of unique visitors, and total views by user.\nUse the days filter to filter product views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your product\nand bookmark it for easy access.\nClick the\nSlack\nor\nTeams\nicon to post on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the 3-dot icon to\nadd an announcement\nto your product.\nAssets\nâ\nThe\nAssets\ntab provides a comprehensive list of assets included in the data product:\nSearch for specific assets in the sidebar\nFilter assets by input and output ports\nSelect an asset to view more details in the asset sidebar\nView queried at source information for all assets\nLineage\nâ\nThe\nLineage\ntab provides a\nvisual representation\nof the provenance of and relationships between your data products in Atlan.\nActivity log\nâ\nThe\nActivity\ntab provides a\nchangelog\nfor your data product.\nActivity\n-  view details about changes made to the data product and\nfilter for specific types of metadata changes\nViews\n-  view top and recent users of the data product\nProducers\n-  view information about when the data product was created and last updated and by whom\nContracts\nâ\nThe\nContracts\ntab displays any\nlinked contracts\nfor the output ports in your data product. You can view contract specifications, track the evolution of your contract over time, and compare and contrast multiple versions.\nTags:\natlan\ndocumentation\nPrevious\nWhat is a product score?\nEnable products module\nOrder of operations\nStakeholders\nComponents of a data product"
  },
  {
    "url": "https://docs.atlan.com/tags/atlan",
    "content": "133 docs tagged with \"atlan\"\nView all tags\nAccess Control\nLearn how to manage user permissions and access to data assets in Atlan for security and compliance.\nAdd a resource\nNeed to redirect users to important information that's outside Atlan?\nAdd an alias\nAn alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use [Atlan AI](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai) to do the same, if [Atlan AI is enabled in your Atlan workspace](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai). This can help you improve the readability of your asset names while providing useful context to your users.\nAdd certificates\nHow many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data!\nAdd custom metadata badges\nBringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges.\nAdd owners\nAtlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data.\nAdd stakeholders\nStakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams.\nAdd users to groups\nAtlan supports configuring SSO group mappings. You will first need to [create groups](/product/capabilities/governance/users-and-groups/how-tos/create-groups) in Atlan that correspond to the groups you want to map from your SSO provider to Atlan.\nAdditional connectivity to data sources\nLearn about additional connectivity to data sources.\nAdministration\nLearn about administration.\nAllow guests to request updates\nIf you'd like to disable this option for your guest users, follow the steps above and then turn it off.\nAre there any limits on concurrent queries?\nLearn about are there any limits on concurrent queries?.\nAuthenticate SSO credentials to query data\nLearn about authenticate sso credentials to query data.\nAuthenticate SSO credentials to view sample data\nLearn about authenticate sso credentials to view sample data.\nAuthentication and authorization\nLearn about authentication and authorization.\nAutomate policy compliance\nâAvailable via the Advanced Policy & Compliances package\nBasic Platform Usage\nEssential information about using Atlan's core features, from browser requirements to data querying and asset management.\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nLearn about can atlan integrate with multiple azure ad tenants within a single instance?.\nCan we restrict who can query our data warehouse?\nLearn about can we restrict who can query our data warehouse?.\nConfigure custom domains for Microsoft Excel\nIf your Atlan tenant is hosted on a custom domain   -  for example, `https://<your-tenant-name>.mycompany.com`Â   -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel.\nConfigure language settings\nHow does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level.\nConfigure the extension for managed browsers\nIf you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script.\nConnectors\nLearn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management.\nContracts\nLearn how to manage data contracts and agreements in Atlan to ensure data quality and compliance.\nControl access to metadata and data?\nLearn about control access to metadata and data?.\nCreate a new tag\nFor tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar.\nCreate data contracts\nCreate data contracts <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nCreate data domains\nData domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization.\nCreate domain policies\nDomain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more.\nCreate forms\nYou can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more.\nCreate groups\n:::warning Who can do this? You will need to be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to cr.\nCreate persona\n:::warning Who can do this? You will need to be an admin user to create personas.\nCreate policies\nâAvailable via the Advanced Policy & Compliances package\nCreate purpose\n:::warning Who can do this? You will need to be an admin user to create purposes. :::.\nCustom Metadata\nAtlan provides basic metadata for assets like certification, owners, and descriptions. But since every data team has their own unique needs, Atlan allows you to extend these with your own unique properties.\nCustom Metadata\nLearn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nData Products\nFrom a single data table to a collection of data assets, anything can be a data product in Atlan. Data products provide a framework for your teams to curate assets specific to a domain, business unit, region of operation, brand, and more. These curated data products then empower your data consumers to easily discover data assets, quickly get the context they need, and collaborate more efficiently.\nDatabricks Data Quality Studio\nSet up and configure Databricks for data quality monitoring through Atlan.\nDelete a tag\nIf a tag is attached to assets, you will need to [remove the tag](/product/capabilities/governance/tags/how-tos/remove-a-tag) from the [tagged assets](/product/capabilities/governance/tags/how-tos/attach-a-tag)Â before deleting it.\nDisable user activity\nYou can [view recently visited users](/product/capabilities/discovery/concepts/what-are-asset-profiles) and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps.\nDomains\nLearn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way.\nEnable  Azure AD for SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with System for Cross-domain Identity Management (SCIM).\nEnable  discovery of process assets\nTo create a more customizable experience for your users, you can turn on discovery and tracking of process assets.\nEnable  sample data download\nAtlan allows admin users to enable or disable downloading [sample data](/product/capabilities/discovery/concepts/what-are-asset-profiles#sample-data). This can help you enforce better governance across your organization.\nEnable  scheduled queries\nTo enable scheduled queries, follow these steps.\nEnable data quality on connection\nEnable and configure data quality for your Databricks connection in Atlan.\nEnable data quality on connection\nEnable and configure data quality for your Snowflake connection in Atlan.\nGetting Started and Onboarding\nEverything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements.\nGlossary\nLearn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization.\nGoogle Dashboard login error\nLearn about why do i get an error while logging in via google dashboard?.\nHow are resources isolated?\nLearn about how are resources isolated?.\nHow do I see views instead of materialized views in the reporting center?\nOn the _Assets_ dashboard in the [reporting center](/product/capabilities/reporting/references/how-to-report-on-assets), click **View** in the _All Asset Types_ dropdown menu toÂ only see views. You can further filter by selecting a specific connector or connection as well.\nHow do I use the filters menu?\nLearn about how do i use the filters menu?.\nHow to interpret timestamps\nLearn about how to interpret timestamps.\nHow to use parameterized queries?\nLearn about how to use parameterized queries?.\nInsights tips and tricks\nAt Atlan, we are committed to powering your user experience. Here are a few tips and tricks to help you get the most out of Insights for you and your team!\nInstall on Virtual Machine (K3s)\nThis page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying [K3s in a rootless execution mode](https://docs.k3s.io/advanced#running-rootless-servers-experimental:~:text=to%20take%20effect.-,Running%20Rootless%20Servers,-\\(Experimental\\)%E2%80%8B).\nIntegrations\nLearn how to integrate Atlan with project management, communication, collaboration, automation, and identity management tools.\nInvite new users\nNote that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users.\nIs there a dashboard to see how my metadata is populated?\nLearn about is there a dashboard to see how my metadata is populated?.\nLimit SSO automatically creating users when they log in\nOnly users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan.\nMake a query interactive\nIf you want to share a query with others, but limit how they can change the query, you can make it _interactive_.\nManage domains\nMost importantly, domains help promote shared ownership and domain-level governance in your organization.\nManage playbooks\nOnce you've [created a playbook](/product/capabilities/playbooks/how-tos/set-up-playbooks), you can monitor, modify, or delete it at any time. You can also [enable notifications](/product/capabilities/playbooks/how-tos/manage-playbooks) to monitor your playbook runs directly in Slack or Microsoft Teams.\nManage policies\nYou must be an admin user in Atlan to enable, create, manage, and approve data governance policies.\nManage system announcements\nHave you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape.\nManage users\n:::warning Who can do this? You will need to be an admin user in Atlan to manage other users.\nMonitor for runaway queries?\nLearn about monitor for runaway queries?.\nOrganize assets\nLearn about organize assets.\nOur 3 pro tips for saving time with Atlan\nLearn about our 3 pro tips for saving time with atlan.\nProduct release stages\nLearn about product release stages.\nQuality assurance framework\nLearn about quality assurance framework.\nquery data\nLearn about query data.\nquery without shared credentials\nDon't want to use a single shared service account to access data?\nRemove a tag\nAtlan allows you to remove [tags](/product/capabilities/governance/tags/concepts/what-are-tags) from a [tagged asset](/product/capabilities/discovery/asset-prof.\nSave and share queries\nYou can save queries to re-run them later, schedule them, or share them.\nSet up a private network link to Amazon MSK\n:::warning Who can do this? You will need your Amazon MSK or AWS administrator involved - you may not have access to run these tasks.\nSet up a private network link to Amazon Redshift\n:::warning Who can do this? You will need your Amazon Redshift administrator or AWS administrator involved - you may not have access to run these tasks.\nSet up a private network link to Microsoft SQL Server on Amazon EC2\n:::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself.\nSet up a private network link to Microsoft SQL Server on Amazon RDS\n:::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself.\nSet up a private network link to MySQL\n:::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself.\nSet up a private network link to PostgreSQL\nBefore you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This [method](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-setup.html#rds-proxy-secrets-arns) uses a username and password to connect to the RDS database.\nSet up a private network link to Tableau server\nAs a prerequisite for TLS configuration on Tableau Server only, ensure that the health check _Protocol_ of the target group is set to **HTTPS** or [modify the health check settings](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html#modify-health-check-settings) as required.\nSet up Amazon Athena\nLearn about set up amazon athena.\nSet up an AWS private network link to Snowflake\nAtlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you.\nSet up an Azure private network link to Snowflake\nAtlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request.\nSet up Apache Kafka\n:::warning Who can do this? You will probably need your Apache Kafka administrator to run these commands - you may not have access yourself.\nSet up AWS Lambda\nLearn about set up aws lambda.\nSet up Cloudera Impala\nLearn about set up cloudera impala.\nSet up dbt Core\nLearn about set up dbt core.\nSet up Metabase\n:::warning Who can do this? You will probably need your Metabase administrator to follow the below steps - you may not have access yourself.\nSet up playbooks\nLearn about set up playbooks.\nSnowflake Data Quality Studio\nSet up and configure Snowflake for data quality monitoring through Atlan.\nStewardship\nLearn how to implement data stewardship in Atlan through automated workflows, policies, and task management.\nSuggestions from similar assets\nLearn about suggestions from similar assets.\nTags\nFor details on tagging assets, see [How to attach a tag](/product/capabilities/governance/tags/how-tos/attach-a-tag).\nTags\nLearn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance.\nTenant access management\nLearn about tenant access management.\nTenant monitoring\nLearn about tenant monitoring.\nTenant offboarding\nLearn about tenant offboarding.\nThe DataOps Culture Code\nLearn about the dataops culture code.\nTroubleshoot permission issues\nLearn about troubleshoot permission issues.\nTroubleshooting bring your own credentials\nLearn about troubleshooting bring your own credentials.\nTroubleshooting exporting large query results\nLearn about troubleshooting exporting large query results.\nTroubleshooting Google BigQuery connectivity\nLearn about troubleshooting google bigquery connectivity.\nTroubleshooting Hive connectivity\nLearn about troubleshooting hive connectivity.\nTroubleshooting MicroStrategy connectivity\nLearn about troubleshooting microstrategy connectivity.\nTroubleshooting MySQL connectivity\nLearn about troubleshooting mysql connectivity.\nTroubleshooting on-premises database connectivity\nLearn about troubleshooting on-premises database connectivity.\nTroubleshooting playbooks\nLearn about troubleshooting playbooks.\nTroubleshooting policies\nLearn about troubleshooting policies.\nTroubleshooting PostgreSQL connectivity\nLearn about troubleshooting postgresql connectivity.\nTroubleshooting Snowflake connectivity\nLearn about troubleshooting snowflake connectivity.\nTroubleshooting Snowflake tag management\nLearn about troubleshooting snowflake tag management.\nUpdate input type for existing custom metadata\nLearn about update input type for existing custom metadata.\nUse Atlan AI for documentation\nâ Available to customers in Enterprise and Business-Critical platform editions\nUse the Atlan browser extension\nThe Atlan browser extension provides metadata context directly in your [supported data tools](#supported-tools). You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge.\nUser Management and Access Control\nComplete guide to managing users, configuring access controls, and understanding permissions in Atlan.\nUsers and groups\nLearn how to manage users and groups in Atlan to control access and organize your data team.\nWhat are asset profiles?\nLearn about what are asset profiles?.\nWhat are groups?\nLearn about what are groups?.\nWhat are partial assets?\nLearn about what are partial assets?.\nWhat are personas?\nLearn about what are personas?.\nWhat are preflight checks?\nLearn about what are preflight checks?.\nWhat are purposes?\nLearn about what are purposes?.\nWhat are the query builder actions?\nLearn about what are the query builder actions?.\nWhat are the sidebar tabs?\nLearn about what are the sidebar tabs?.\nWhat column keys does Atlan crawl?\nLearn about what column keys does atlan crawl?.\nWhat controls the frequency of queries?\nLearn about what controls the frequency of queries?.\nWhat happens when users do not have access to metadata?\nLearn about what happens when users do not have access to metadata?.\nWhat is a product score?\nLearn about what is a product score?.\nWhat is Atlan?\nLearn about what is atlan?."
  },
  {
    "url": "https://docs.atlan.com/tags/documentation",
    "content": "117 docs tagged with \"documentation\"\nView all tags\nAdd a resource\nNeed to redirect users to important information that's outside Atlan?\nAdd an alias\nAn alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use [Atlan AI](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai) to do the same, if [Atlan AI is enabled in your Atlan workspace](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai). This can help you improve the readability of your asset names while providing useful context to your users.\nAdd certificates\nHow many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data!\nAdd custom metadata badges\nBringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges.\nAdd owners\nAtlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data.\nAdd stakeholders\nStakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams.\nAdd users to groups\nAtlan supports configuring SSO group mappings. You will first need to [create groups](/product/capabilities/governance/users-and-groups/how-tos/create-groups) in Atlan that correspond to the groups you want to map from your SSO provider to Atlan.\nAdditional connectivity to data sources\nLearn about additional connectivity to data sources.\nAdministration\nLearn about administration.\nAllow guests to request updates\nIf you'd like to disable this option for your guest users, follow the steps above and then turn it off.\nAre there any limits on concurrent queries?\nLearn about are there any limits on concurrent queries?.\nAuthenticate SSO credentials to query data\nLearn about authenticate sso credentials to query data.\nAuthenticate SSO credentials to view sample data\nLearn about authenticate sso credentials to view sample data.\nAuthentication and authorization\nLearn about authentication and authorization.\nAutomate policy compliance\nâAvailable via the Advanced Policy & Compliances package\nBasic Platform Usage\nEssential information about using Atlan's core features, from browser requirements to data querying and asset management.\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nLearn about can atlan integrate with multiple azure ad tenants within a single instance?.\nCan we restrict who can query our data warehouse?\nLearn about can we restrict who can query our data warehouse?.\nConfigure custom domains for Microsoft Excel\nIf your Atlan tenant is hosted on a custom domain   -  for example, `https://<your-tenant-name>.mycompany.com`Â   -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel.\nConfigure language settings\nHow does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level.\nConfigure the extension for managed browsers\nIf you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script.\nControl access to metadata and data?\nLearn about control access to metadata and data?.\nCreate a new tag\nFor tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar.\nCreate data contracts\nCreate data contracts <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nCreate data domains\nData domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization.\nCreate domain policies\nDomain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more.\nCreate forms\nYou can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more.\nCreate groups\n:::warning Who can do this? You will need to be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to cr.\nCreate persona\n:::warning Who can do this? You will need to be an admin user to create personas.\nCreate policies\nâAvailable via the Advanced Policy & Compliances package\nCreate purpose\n:::warning Who can do this? You will need to be an admin user to create purposes. :::.\nCustom Metadata\nAtlan provides basic metadata for assets like certification, owners, and descriptions. But since every data team has their own unique needs, Atlan allows you to extend these with your own unique properties.\nData Products\nFrom a single data table to a collection of data assets, anything can be a data product in Atlan. Data products provide a framework for your teams to curate assets specific to a domain, business unit, region of operation, brand, and more. These curated data products then empower your data consumers to easily discover data assets, quickly get the context they need, and collaborate more efficiently.\nDelete a tag\nIf a tag is attached to assets, you will need to [remove the tag](/product/capabilities/governance/tags/how-tos/remove-a-tag) from the [tagged assets](/product/capabilities/governance/tags/how-tos/attach-a-tag)Â before deleting it.\nDisable user activity\nYou can [view recently visited users](/product/capabilities/discovery/concepts/what-are-asset-profiles) and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps.\nEnable  Azure AD for SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with System for Cross-domain Identity Management (SCIM).\nEnable  discovery of process assets\nTo create a more customizable experience for your users, you can turn on discovery and tracking of process assets.\nEnable  sample data download\nAtlan allows admin users to enable or disable downloading [sample data](/product/capabilities/discovery/concepts/what-are-asset-profiles#sample-data). This can help you enforce better governance across your organization.\nEnable  scheduled queries\nTo enable scheduled queries, follow these steps.\nGetting Started and Onboarding\nEverything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements.\nGoogle Dashboard login error\nLearn about why do i get an error while logging in via google dashboard?.\nHow are resources isolated?\nLearn about how are resources isolated?.\nHow do I see views instead of materialized views in the reporting center?\nOn the _Assets_ dashboard in the [reporting center](/product/capabilities/reporting/references/how-to-report-on-assets), click **View** in the _All Asset Types_ dropdown menu toÂ only see views. You can further filter by selecting a specific connector or connection as well.\nHow do I use the filters menu?\nLearn about how do i use the filters menu?.\nHow to interpret timestamps\nLearn about how to interpret timestamps.\nHow to use parameterized queries?\nLearn about how to use parameterized queries?.\nInsights tips and tricks\nAt Atlan, we are committed to powering your user experience. Here are a few tips and tricks to help you get the most out of Insights for you and your team!\nInstall on Virtual Machine (K3s)\nThis page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying [K3s in a rootless execution mode](https://docs.k3s.io/advanced#running-rootless-servers-experimental:~:text=to%20take%20effect.-,Running%20Rootless%20Servers,-\\(Experimental\\)%E2%80%8B).\nInvite new users\nNote that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users.\nIs there a dashboard to see how my metadata is populated?\nLearn about is there a dashboard to see how my metadata is populated?.\nLimit SSO automatically creating users when they log in\nOnly users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan.\nMake a query interactive\nIf you want to share a query with others, but limit how they can change the query, you can make it _interactive_.\nManage domains\nMost importantly, domains help promote shared ownership and domain-level governance in your organization.\nManage playbooks\nOnce you've [created a playbook](/product/capabilities/playbooks/how-tos/set-up-playbooks), you can monitor, modify, or delete it at any time. You can also [enable notifications](/product/capabilities/playbooks/how-tos/manage-playbooks) to monitor your playbook runs directly in Slack or Microsoft Teams.\nManage policies\nYou must be an admin user in Atlan to enable, create, manage, and approve data governance policies.\nManage system announcements\nHave you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape.\nManage users\n:::warning Who can do this? You will need to be an admin user in Atlan to manage other users.\nMonitor for runaway queries?\nLearn about monitor for runaway queries?.\nOrganize assets\nLearn about organize assets.\nOur 3 pro tips for saving time with Atlan\nLearn about our 3 pro tips for saving time with atlan.\nQuality assurance framework\nLearn about quality assurance framework.\nquery data\nLearn about query data.\nquery without shared credentials\nDon't want to use a single shared service account to access data?\nRemove a tag\nAtlan allows you to remove [tags](/product/capabilities/governance/tags/concepts/what-are-tags) from a [tagged asset](/product/capabilities/discovery/asset-prof.\nSave and share queries\nYou can save queries to re-run them later, schedule them, or share them.\nSet up a private network link to Amazon MSK\n:::warning Who can do this? You will need your Amazon MSK or AWS administrator involved - you may not have access to run these tasks.\nSet up a private network link to Amazon Redshift\n:::warning Who can do this? You will need your Amazon Redshift administrator or AWS administrator involved - you may not have access to run these tasks.\nSet up a private network link to Microsoft SQL Server on Amazon EC2\n:::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself.\nSet up a private network link to Microsoft SQL Server on Amazon RDS\n:::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself.\nSet up a private network link to MySQL\n:::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself.\nSet up a private network link to PostgreSQL\nBefore you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This [method](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-setup.html#rds-proxy-secrets-arns) uses a username and password to connect to the RDS database.\nSet up a private network link to Tableau server\nAs a prerequisite for TLS configuration on Tableau Server only, ensure that the health check _Protocol_ of the target group is set to **HTTPS** or [modify the health check settings](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html#modify-health-check-settings) as required.\nSet up Amazon Athena\nLearn about set up amazon athena.\nSet up an AWS private network link to Snowflake\nAtlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you.\nSet up an Azure private network link to Snowflake\nAtlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request.\nSet up Apache Kafka\n:::warning Who can do this? You will probably need your Apache Kafka administrator to run these commands - you may not have access yourself.\nSet up AWS Lambda\nLearn about set up aws lambda.\nSet up Cloudera Impala\nLearn about set up cloudera impala.\nSet up dbt Core\nLearn about set up dbt core.\nSet up Metabase\n:::warning Who can do this? You will probably need your Metabase administrator to follow the below steps - you may not have access yourself.\nSet up playbooks\nLearn about set up playbooks.\nSuggestions from similar assets\nLearn about suggestions from similar assets.\nTags\nFor details on tagging assets, see [How to attach a tag](/product/capabilities/governance/tags/how-tos/attach-a-tag).\nTenant access management\nLearn about tenant access management.\nTenant monitoring\nLearn about tenant monitoring.\nTenant offboarding\nLearn about tenant offboarding.\nThe DataOps Culture Code\nLearn about the dataops culture code.\nTroubleshoot permission issues\nLearn about troubleshoot permission issues.\nTroubleshooting bring your own credentials\nLearn about troubleshooting bring your own credentials.\nTroubleshooting exporting large query results\nLearn about troubleshooting exporting large query results.\nTroubleshooting Google BigQuery connectivity\nLearn about troubleshooting google bigquery connectivity.\nTroubleshooting Hive connectivity\nLearn about troubleshooting hive connectivity.\nTroubleshooting MicroStrategy connectivity\nLearn about troubleshooting microstrategy connectivity.\nTroubleshooting MySQL connectivity\nLearn about troubleshooting mysql connectivity.\nTroubleshooting on-premises database connectivity\nLearn about troubleshooting on-premises database connectivity.\nTroubleshooting playbooks\nLearn about troubleshooting playbooks.\nTroubleshooting policies\nLearn about troubleshooting policies.\nTroubleshooting PostgreSQL connectivity\nLearn about troubleshooting postgresql connectivity.\nTroubleshooting Snowflake connectivity\nLearn about troubleshooting snowflake connectivity.\nTroubleshooting Snowflake tag management\nLearn about troubleshooting snowflake tag management.\nUpdate input type for existing custom metadata\nLearn about update input type for existing custom metadata.\nUse Atlan AI for documentation\nâ Available to customers in Enterprise and Business-Critical platform editions\nUse the Atlan browser extension\nThe Atlan browser extension provides metadata context directly in your [supported data tools](#supported-tools). You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge.\nUser Management and Access Control\nComplete guide to managing users, configuring access controls, and understanding permissions in Atlan.\nWhat are asset profiles?\nLearn about what are asset profiles?.\nWhat are groups?\nLearn about what are groups?.\nWhat are partial assets?\nLearn about what are partial assets?.\nWhat are personas?\nLearn about what are personas?.\nWhat are preflight checks?\nLearn about what are preflight checks?.\nWhat are purposes?\nLearn about what are purposes?.\nWhat are the query builder actions?\nLearn about what are the query builder actions?.\nWhat are the sidebar tabs?\nLearn about what are the sidebar tabs?.\nWhat column keys does Atlan crawl?\nLearn about what column keys does atlan crawl?.\nWhat controls the frequency of queries?\nLearn about what controls the frequency of queries?.\nWhat happens when users do not have access to metadata?\nLearn about what happens when users do not have access to metadata?.\nWhat is a product score?\nLearn about what is a product score?.\nWhat is Atlan?\nLearn about what is atlan?."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nOn this page\nSnowflake\nOverview:\nCatalog Snowflake databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your cloud data warehouse assets.\nGet started\nâ\nFollow these steps to connect and catalog Snowflake assets in Atlan:\nSet up the connector\nCrawl Snowflake assets\nGuides\nâ\nAuthentication\nâ\nEnable Snowflake OAuth\n: Set up OAuth authentication for Snowflake connections.\nMetadata & lineage\nâ\nMine Snowflake\n: Learn how to mine query history and construct lineage for Snowflake assets.\nTag management\nâ\nManage Snowflake tags\n: Configure and manage tags and policy tags in Snowflake.\nAdvanced features\nâ\nConfigure Snowflake data metric functions\n: Configure and use data metric functions in Snowflake.\nPrivate networking\nâ\nSet up an AWS private network link to Snowflake\n: Establish a secure, private network connection to Snowflake on AWS.\nSet up an Azure private network link to Snowflake\n: Establish a secure, private network connection to Snowflake on Azure.\nReferences\nâ\nWhat does Atlan crawl from Snowflake\n: Learn about the Snowflake assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Snowflake\n: Verify prerequisites before setting up the Snowflake connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Snowflake connection issues and errors.\nBest practices\nâ\nSnowflake warehouse configuration\n: Recommended Snowflake warehouse configuration to enable reliable Atlan workflow execution.\nTags:\nconnectivity\nsnowflake\nNext\nSet up Snowflake\nGet started\nGuides\nReferences\nTroubleshooting\nBest practices"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-aws-private-network-link-to-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nGet Started\nSet up an AWS private network link to Snowflake\nOn this page\nSet up an AWS private network link to Snowflake\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Snowflake and Atlan, when you use our Single Tenant SaaS deployment.\nWho can do this?\nYou will need Snowflake Support, and probably your Snowflake administrator involved   -  you may not have access or the tools to run these tasks.\nPrerequisites\nâ\nSnowflake must be setup with Business Critical Edition (or higher).\nOpen a ticket with Snowflake Support to enable PrivateLink for your Snowflake account.\nSnowflake support will take 1-2 days to review and enable PrivateLink.\nIf you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please\nraise a support request\nto do so.\n(For all details, see the\nSnowflake documentation\n.)\nFetch PrivateLink information\nâ\nLog in to snowCLI using the\nACCOUNTADMIN\naccount, and run the following commands:\nuse role accountadmin;\nselect system$get_privatelink_config();\nThis will produce output like the following (formatted here for readability):\n{\n\"privatelink-account-name\":\"abc123.ap-south-1.privatelink\",\n\"privatelink-vpce-id\":\"com.amazonaws.vpce.ap-south-1.vpce-svc-257a4d536bd8e3594\",\n\"privatelink-account-url\":\"abc123.ap-south-1.privatelink.snowflakecomputing.com\",\n\"regionless-privatelink-account-url\":\"xyz789-abc123.privatelink.snowflakecomputing.com\",\n\"privatelink_ocsp-url\":\"ocsp.abc123.ap-south-1.privatelink.snowflakecomputing.com\",\n\"privatelink-connection-urls\":\"[]\"\n}\nShare details with Atlan support team\nâ\nShare the following values with the\nAtlan support\nteam:\nprivatelink-account-name\nprivatelink-vpce-id\nprivatelink-account-url\nprivatelink_ocsp-url\nAtlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you.\nWhen you use this endpoint in the configuration for\ncrawling\nand\nmining\n, Atlan will connect to Snowflake over the PrivateLink.\nTags:\natlan\ndocumentation\nPrevious\nSet up Snowflake\nNext\nSet up an Azure private network link to Snowflake\nPrerequisites\nFetch PrivateLink information\nShare details with Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-azure-private-network-link-to-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nGet Started\nSet up an Azure private network link to Snowflake\nOn this page\nSet up an Azure private network link to Snowflake\nAzure Private Link\ncreates a secure, private connection between services running in Azure. This document describes the steps to set this up between Snowflake and Atlan.\nWho can do this?\nYou will need Snowflake Support, and probably your Snowflake administrator involved   -  you may not have access or the tools to run these tasks.\nPrerequisites\nâ\nSnowflake must be set up with Business Critical Edition (or higher).\nOpen a ticket with Snowflake Support to enable Azure Private Link for your Snowflake account.\nSnowflake support will take 1-2 days to review and enable Azure Private Link.\nIf you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please\nraise a support request\nto do so.\n(For all details, see the\nSnowflake documentation\n.)\nFetch Private Link information\nâ\nLog in to snowCLI using the\nACCOUNTADMIN\naccount, and run the following commands:\nuse role accountadmin;\nselect system$get_privatelink_config();\nThis will produce an output like the following (formatted here for readability):\n{\n\"regionless-snowsight-privatelink-url\": \"abc123.privatelink.snowflakecomputing.com\",\n\"privatelink-account-name\": \"abc123.west-europe.privatelink\",\n\"snowsight-privatelink-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\",\n\"privatelink-account-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\",\n\"privatelink-connection-ocsp-urls\": \"[]\",\n\"privatelink-pls-id\": \"abc123.westeurope.azure.privatelinkservice\",\n\"regionless-privatelink-account-url\": \"abc123.privatelink.snowflakecomputing.com\",\n\"privatelink_ocsp-url\": \"ocsp.abc123.west-europe.privatelink.snowflakecomputing.com\",\n\"privatelink-connection-urls\": \"[]\"\n}\nShare details with Atlan support team\nâ\nShare the following values with the\nAtlan support team\n:\nregionless-snowsight-privatelink-url\nprivatelink-account-name\nsnowsight-privatelink-url\nprivatelink-account-url\nprivatelink-connection-ocsp-urls\nprivatelink-pls-id\nregionless-privatelink-account-url\nprivatelink_ocsp-url\nprivatelink-connection-urls\nAtlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request.\nApprove the endpoint connection request\nâ\nLog in to snowCLI using the\nACCOUNTADMIN\naccount, and run the following commands:\nuse role accountadmin;\nSELECT SYSTEM$AUTHORIZE_PRIVATELINK (\n'/subscriptions/26d.../resourcegroups/sf-1/providers/microsoft.network/privateendpoints/test-self-service',\n'eyJ...'\n);\nSnowflake will return an\nAccountÂ isÂ authorizedÂ forÂ PrivateLink.\nmessage to confirm successful authorization. The status of the private endpoint in Atlan will then change to\nApproved\n.\nWhen you use this endpoint in the configuration for\ncrawling\nand\nmining\nSnowflake, Atlan will connect to Snowflake over the Private Link.\n(Optional) Configure private endpoint for internal stages\nâ\nThis is only required if you're using Snowflake internal stages. To enable Atlan to securely access your Snowflake internal stages, Atlan will require a private endpoint to your Azure storage account. Refer to\nSnowflake documentation\nto learn more.\nTo configure an Azure private endpoint to access Snowflake internal stages:\nOpen the Azure portal and navigate to your Azure Storage account.\nOn the\nStorage accounts\npage, select the storage account to connect. From the storage account menu, click\nOverview\n. In the\nResource JSON\nform, for\nResource ID\n, click the clipboard icon to copy the value and\ncontact Atlan support to share the value\n. (Atlan support will finish the configuration on the Atlan side using the\nResource ID\nvalue and contact you to confirm endpoint creation.)\nFrom the storage account menu, click\nSecurity + networking\nand then click\nNetworking\n.\nOn the\nNetworking\npage, change to the\nPrivate endpoint connections\ntab and then approve the endpoint connection request from Atlan.\nTags:\natlan\ndocumentation\nPrevious\nSet up an AWS private network link to Snowflake\nNext\nHow to enable Snowflake OAuth\nPrerequisites\nFetch Private Link information\nShare details with Atlan support team\nApprove the endpoint connection request\n(Optional) Configure private endpoint for internal stages"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/enable-snowflake-oauth",
    "content": "Connect data\nData Warehouses\nSnowflake\nGet Started\nHow to enable Snowflake OAuth\nOn this page\nEnable  Snowflake OAuth\nAtlan supports\nSnowflake OAuth-based authentication\nÂ for\nSnowflake\nconnections. Once the integration has been completed, Atlan will generate a trusted secure token with Snowflake. This will allow Atlan to authenticate users with Snowflake on their behalf to:\nQuery data with Snowflake OAuth credentials\nView sample data with Snowflake OAuth credentials\nConfigure Snowflake OAuth in Atlan\nâ\nWho can do this?\nYou will need to be a\nconnection admin\nin Atlan to complete these steps. You will also needÂ inputs and approval from your\nSnowflake account administrator\n.\nTo configure Snowflake OAuth on a Snowflake connection, from Atlan:\nFrom the left menu of any screen, click\nAssets\n.\nFrom the\nAssets\npage, click the\nConnector\nfilter, and from the dropdown, click\nSnowflake\n.\nFrom the pills below the search bar at the top of the screen, click\nConnection\n.\nFrom the list of results, select a Snowflake connection to enable Snowflake OAuth-based authentication.\nFrom the sidebar on the right, next to\nConnection settings\n, click\nEdit\n.\nIn the\nConnection settings\ndialog:\nUnder\nAllow query\n, for\nAuthentication type\n, click\nSnowflake OAuth\nto enforce Snowflake OAuth credentials for\nquerying data\n:\nFor\nAuthentication Required\n, click\nCopy Code\nto copy a security authorization code to\nexecute it in Snowflake\n.\nUnderÂ\nDisplay sample data\n, for\nSource preview\n, click\nSnowflake OAuth\nto enforce Snowflake OAuth credentials for\nviewing sample data\n:\nIf Snowflake OAuth-based authentication is enabled for querying data, the same connection details will be reused for viewing sample data.\nIf a different authentication method is enabled for querying data, click\nCopy Code\nto copy a security authorization code to\nexecute it in Snowflake\n.\n(Optional) Toggle on\nEnable data policies created at source to apply for querying in Atlan\nto apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing\ndata policies\non the connection in Atlan will be deactivated and creation of new data policies will be disabled.\nAt the bottom right of the\nConnection settings\ndialog, click\nUpdate\n.\nDid you know?\nThe refresh token does not expire by default.\nCreate a security integration in Snowflake\nâ\nWho can do this?\nYou will need your\nSnowflake account administrator\nto run these commands. You will also need to have an\nexisting Snowflake connection\nin Atlan.\nTo create a security integration in Snowflake:\nLog in to your Snowflake instance.\nFrom the top right of your Snowflake instance, click the\n+\nbutton, and then from the dropdown, click\nSQL Worksheet\nto open a new worksheet.\nIn the query editor of your Snowflake SQL worksheet, paste the\nsecurity authorization code you copied in Atlan\n. See a representative example below:\nCREATE\nSECURITY\nINTEGRATION\n<\nname\n>\nTYPE\n=\nEXTERNAL_OAUTH\nENABLED\n=\nTRUE\nEXTERNAL_OAUTH_TYPE\n=\nOKTA\nEXTERNAL_OAUTH_ISSUER\n=\n'https://<COMPANY>.okta.com/oauth2/<ID>'\nEXTERNAL_OAUTH_JWS_KEYS_URL\n=\n'https://<COMPANY>.okta.com/oauth2/<ID>/v1/keys'\nEXTERNAL_OAUTH_AUDIENCE_LIST\n=\n(\n'<snowflake_account_url'\n)\nEXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM\n=\n'sub'\nEXTERNAL_OAUTH_ANY_ROLE_MODE\n=\n'ENABLE'\n;\nEXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE\n=\n'EMAIL_ADDRESS'\nRun the security integration in Snowflake.\n(Optional) To allow the\nACCOUNTADMIN\n,\nORGADMIN\n, or\nSECURITYADMIN\nrole to query with Snowflake OAuth-based authentication, add and run the following command to set account-level permissions:\nALTER\nACCOUNT\nSET\nEXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST\n=\nFALSE\n;\nYour users will now be able to\nrun queries\nand\nview sample data\nusing their Snowflake OAuth credentials! ð\nDid you know?\nYou can refer to\ntroubleshooting connector-specific SSO authentication\nto troubleshoot any errors.\nTags:\nconnectors\ndata\nintegration\nauthentication\nPrevious\nSet up an Azure private network link to Snowflake\nNext\nCrawl Snowflake\nConfigure Snowflake OAuth in Atlan\nCreate a security integration in Snowflake"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nCrawl Snowflake Assets\nCrawl Snowflake\nOn this page\nCrawl Snowflake\nOnce you have configured the\nSnowflake user permissions\n, you can establish a connection between Atlan and Snowflake. (If you are also using\nAWS PrivateLink\nor\nAzure Private Link\nfor Snowflake, you will need to set that up first, too.)\nTo crawl metadata from Snowflake, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Snowflake as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nSnowflake Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you will need to first\nextract metadata yourself\nand\nmake it available in S3\n. This is currently only supported when using the\ninformation schema extraction method to fetch metadata with basic authentication\n.\nIn\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Snowflake credentials:\nFor\nAccount Identifiers (Host)\n, enter the hostname,\nAWS PrivateLink endpoint\n, or\nAzure Private Link endpoint\nfor your Snowflake instance.\nFor\nAuthentication\n, choose the method you configured when\nsetting up the Snowflake user\n:\nFor\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou configured in either Snowflake or the identity provider.\ninfo\nðª\nDid you know?\nSnowflake recommends transitioning away from basic authentication using username and password. Change to\nkey-pair authentication\nfor enhanced security. For any existing Snowflake workflows, you can\nmodify the crawler configuration\nto update the authentication method.\nFor\nKeypair\nauthentication, enter the\nUsername\n,\nEncrypted Private Key\n, and\nPrivate Key\nPassword\nyou configured. Atlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to\nSnowflake documentation\n.\nFor\nOkta SSO\nauthentication,Â enter the\nUsername\n,Â\nPassword\n, and\nAuthenticator\nyou configured. The\nAuthenticator\nwill be the\nOkta URL endpoint of your Okta account\n, typically in the form of\nhttps://<okta_account_name>.okta.com\n.\nFor\nRole\n, select the Snowflake role through which the crawler should run.\nFor\nWarehouse\n, select the Snowflake warehouse in which the crawler should run.\nClick\nTest Authentication\nto confirm connectivity to Snowflake using these details.\nOnce successful, at the bottom of the screen, click\nNext\n.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from Snowflake. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank.\nForÂ\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Snowflake connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Snowflake data, change\nAllow SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any Snowflake data, change\nAllow Data Preview\nto\nNo\n.\nAt the bottom of the screen, click\nNext\nto proceed.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Snowflake. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the crawler\nâ\ndanger\nWhen\nmodifying\nan existing Snowflake connection, switching to a different\nextraction method\nwill delete and recreate all assets in the existing connection. If you'd like to change the extraction method,\ncontact Atlan support\nÂ for assistance.\nBefore running the Snowflake crawler, you can further configure it.\nYou must select the\nExtraction method\nyou configured when you\nset up Snowflake\n:\nFor\nInformation Schema\nÂ\nmethod\n, keep the default selection.\nChange to\nAccount Usage\nmethod\nÂ and specify the following:\nDatabase Name\nof the copied Snowflake database\nSchema Name\nof the copied\nACCOUNT_USAGE\nschema\nIncremental extraction\nPublic preview\n- Toggle incremental extraction for faster and more efficient metadata extraction.\nYou can override the defaults for any of the remaining options:\nFor\nAsset selection\n, select a filtering option:\nTo select the assets you want to include in crawling, click\nInclude by hierarchy\nand filter for assets down to the database or schema level. (This will default to all assets, if none are specified.)\nTo have the crawler include\nDatabases\n,\nSchemas\n, or\nTables & Views\nbased on a naming convention, click\nInclude by regex\nand specify a regular expression   -  for example, specifying\nATLAN_EXAMPLE_DB.*\nfor\nDatabases\nwill include all the matching databases and their child assets.\nTo select the assets you want to exclude from crawling, click\nExclude by hierarchy\nand filter for assets down to the database or schema level. (This will default to no assets, if none are specified.)Â\nTo have the crawler ignore\nDatabases\n,\nSchemas\n, or\nTables & Views\nbased on a naming convention, click\nExclude by regex\nand specify a regular expression   -  for example, specifying\nATLAN_EXAMPLE_TABLES.*\nfor\nTables & Views\nwill exclude all the matching tables and views.\nClick\n+\nto add more filters. If you add multiple filters, assets will be crawled based on matching\nall\nthe filtering conditions you have set.\nTo exclude lineage for views in Snowflake, change\nView Definition Lineage\nto\nNo\n.\nTo\nimport tags from Snowflake to Atlan\n, change\nImport Tags\nto\nYes\n. Note the following:\nIf using the\nAccount Usage\nextraction method,\ngrant the same permissions\nas required for crawling Snowflake assets to import tags and push updated tags to Snowflake.\nIf using the\nInformation Schema\nextraction method, note that Snowflake\nstores all tag objects\nin the\nACCOUNT_USAGE\nschema. You will need to\ngrant permissions on the account usage schema instead to import tags\nfrom Snowflake.\ndanger\nObject tagging in Snowflake currently requires\nEnterprise Edition or higher\n. If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error   -  unable to retrieve tags.\nFor\nControl Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto further configure the crawler:\nIf you have received a custom crawler configuration from Atlan support, for\nCustom Config\n, enter the value provided. You can also:\nEnter\n{\"ignore-all-case\": true}\nto enable crawling assets with case-sensitive identifiers.\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or keep\nFalse\nto disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nFor\nExclude tables with empty data\n, change to\nYes\nto exclude any tables and corresponding columns without any data.\nFor\nExclude views\n, change to\nYes\nto exclude all views from crawling.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Snowflake crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nNote that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nHow to enable Snowflake OAuth\nNext\nMine Snowflake\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags",
    "content": "Connect data\nData Warehouses\nSnowflake\nManage Snowflake in Atlan\nManage Snowflake tags\nOn this page\nManage Snowflake tags\nNote that object tagging in Snowflake currently requires\nEnterprise Edition or higher\n.\nAtlan enables you to import your\nSnowflake tags\n, update your Snowflake assets with the imported tags, and push the tag updates back to Snowflake:\nImport tags   -  crawl Snowflake tags from Snowflake to Atlan\nReverse sync   -  sync Snowflake tag updates from Atlan to Snowflake\nOnce you've imported your Snowflake tags to Atlan:\nYour Snowflake assets in Atlan are automatically enriched with their Snowflake tags.\nImported Snowflake tags are mapped to corresponding\nAtlan tags\nthrough case-insensitive name match   -  multiple Snowflake tags can be matched to a single tag in Atlan.\nYou can also\nattach Snowflake tags\n, including tag values, to your Snowflake assets in Atlan   -  allowing you to categorize your assets at a more granular level. Atlan supports:\nAllowed values\n: attach an allowed value from a predefined list of values imported from Snowflake.\nTag values: enter any value in Atlan while\nattaching or editing imported Snowflake tags\non an asset.\nYou can enable reverse sync to push any tag updates for your Snowflake assets back to Snowflake   -  including\nallowed and tag values\nadded to assets in Atlan.\nYou can\nfilter your assets\nby Snowflake tags and tag and allowed values.\nDid you know?\nEnabling reverse sync only updates existing tags in Snowflake. It neither creates nor deletes any tags in Snowflake.\nPrerequisites\nâ\nDid you know?\nAdditional privileges are only required when using the\ninformation schema method\nfor fetching metadata. This is because Snowflake\nstores all tag objects\nin the\nACCOUNT_USAGE\nschema. If you're using the\naccount usage method\nto crawl metadata in Atlan or you have\nconfigured the Snowflake miner\n, any permissions required are already set.\nAccount usage method\nâ\nBefore you can import tags from Snowflake, you need to do the following:\nCreate tags\nor have existing tags in Snowflake.\nGrant the\nsame permissions\nas required for crawling Snowflake assets to import tags and push updated tags to Snowflake.\nInformation schema method\nâ\nBefore you can import tags from Snowflake, you need to do the following:\nCreate tags\nor have existing tags in Snowflake.\nGrant additional permissions to\nimport tags\nfrom Snowflake.\nGrant additional permissions to\npush updated tags\nto Snowflake.\nImport Snowflake tags to Atlan\nâ\nWho can do this?\nYou need to be an\nadmin user\nin Atlan to import Snowflake tags to Atlan. You also need to work with your Snowflake administrator to grant\nadditional permissions to import tags\nfrom Snowflake   -  you may not have access yourself.\nYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags are matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets are enriched with their synced tags from Snowflake.\nTo import Snowflake tags to Atlan, you can either:\nCreate a new Snowflake workflow and\nconfigure the crawler\nto import tags.\nModify the crawler's configuration\nfor an existing Snowflake workflow to change\nImport Tags\nto\nYes\n. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan preserves those tags.\nOnce the crawler has completed running, tags imported from Snowflake are available to use for tagging assets! ð\nView Snowflake tags in Atlan\nâ\nOnce you've imported your Snowflake tags, you can view and manage your Snowflake tags in Atlan.\nTo view Snowflake tags:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading of the _Governance cente_r, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter tags by source type. Click\nSnowflake\nto filter for tags imported from Snowflake.\nFrom the left menu under\nTags\n, select a synced tag   -  synced tags display the Snowflake âï¸ icon next to the tag name.\nIn the\nOverview\nsection, you can view a total count of synced Snowflake tags. To the right of\nOverview\n, click\nSynced tags\nto view additional details   -  including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced.\n(Optional) Click the\nLinked assets\ntab to view linked assets for your Snowflake tag.\n(Optional) In the top right, click the pencil icon to add a description and change the\ntag icon\n. You can't rename tags synced from Snowflake.\nPush tag updates to Snowflake\nâ\nWho can do this?\nAny\nadmin or member user\nin Atlan can configure reverse sync for tag updates to Snowflake. You also need to work with your Snowflake administrator to\ngrant additional permissions to push updates\n-  you may not have access yourself.\nDid you know?\nReverse sync is currently only available for imported Snowflake tags in Atlan. The imported tags display a Snowflake âï¸ icon next to the tag name. If using the\naccount usage method\n, expect a\ndata latency of up to 3 hours\nfor reverse tag sync to be successful.\nYou can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source. Once you have enabled reverse sync, any Snowflake assets with tags updated in Atlan are also updated in Snowflake.\nTo enable reverse sync for imported Snowflake tags:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading of the _Governance cente_r, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter tags by source type. Click\nSnowflake\nto filter for tags imported from Snowflake.\nIn the left menu under\nTags\n, select a synced Snowflake tag   -  synced tags display the Snowflake âï¸ icon next to the tag name.\nOn your selected tag page, to the right of\nOverview\n, click\nSynced tags\n.\nUnder\nSynced tags\n, in the upper right, turn on\nEnable reverse sync\nto synchronize tag updates from Atlan to Snowflake.\nIn the advanced settings, you can also enable\nconcatenation\nto support multiple tag values for a single column. For detailed information about multiple tag values and concatenation, see\nMultiple tag values and concatenation\n.\nIn the corresponding confirmation dialog, click\nYes, enable it\nto enable reverse tag sync or click\nCancel\n.\nNow when you\nattach Snowflake tags\nto your Snowflake assets in Atlan, these tag updates are also pushed to Snowflake! ð\nDid you know?\nEnabling reverse sync won't trigger any updates in Snowflake until synced tags are attached to Snowflake assets in Atlan. For any questions about managing Snowflake tags, head over\nhere\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nMine Snowflake\nNext\nConfigure Snowflake data metric functions\nPrerequisites\nImport Snowflake tags to Atlan\nView Snowflake tags in Atlan\nPush tag updates to Snowflake"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/multiple-tag-values-and-concatenation",
    "content": "Connect data\nData Warehouses\nSnowflake\nReferences\nMultiple tag values and concatenation\nOn this page\nMultiple tag values and concatenation\nAtlan supports assigning multiple tag values to a single Snowflake object. When multiple tag values are assigned, Atlan concatenates them into a single string using a configurable delimiter.\nRequirements\nâ\nBoth\nreverse sync\nand\nconcatenation\nmust be enabled for multi-value synchronization to work.\nConstraints\nâ\nWhen configuring multiple tag values, keep the following constraints in mind:\nThe chosen delimiter\ncan't\nappear inside any tag value to prevent parsing errors.\nThe concatenated tag values length must not exceed\n256 characters\n.\nIf the\nallowed list\nis enabled for a tag in Snowflake, concatenated tag values that you attach to Snowflake objects must come from the tagâs predefined list. To use a new value, add it to the list.\nEach tag supports up to\n300 values\n. For more information, see\nSnowflake tag quota for objects\n.\nHow concatenation works\nâ\nImportant\nTag concatenation is an Atlan feature. Concatenated values created in Atlan are synced to Snowflake. However, if you concatenate tag values in Snowflake workflows, those concatenated references won't be synced back to Atlan.\nAtlan manages multiple tag values by concatenating them into a single string that can be synchronized back to Snowflake. The process involves sorting, concatenation, and synchronization behaviors described below.\nSingle values\nâ\nWhen only one value is assigned to a tag, no concatenation occurs. The single value is sent as-is to Snowflake.\nFor example, if you have a tag\ncost_center\nand assign only the value\nfinance\nto an object, the result in Snowflake is the single value\nfinance\nwithout any concatenation.\nMultiple values\nâ\nYou can assign multiple values to a single tag for any object in Atlan. When multiple values are assigned, they're concatenated into a single string using a delimiter character.\nFor example, if you have a tag\ncost_center\nand assign the values\nfinance\n,\nengineering\n, and\nsales\nto an object with comma as the delimiter, the result is the concatenated string\nengineering,finance,sales\n.\nSorting\nâ\nTag values are sorted alphabetically before concatenation to maintain consistent ordering.\nFor example, if you have a tag\nenvironment\nwith values\nproduction\n,\ndevelopment\n, and\nstaging\nassigned to an object and use a comma (\n,\n) as your delimiter, Atlan sorts them alphabetically (\ndevelopment\n,\nproduction\n,\nstaging\n) and concatenates them as:\ndevelopment,production,staging\n.\nReverse sync\nâ\nConcatenation and reverse sync apply at the schema level for imported Snowflake tags. When reverse sync is enabled, the concatenated tag values are synchronized back to the corresponding objects in Snowflake.\nUpdates and removals\nâ\nWhen you update or remove tag assignments in Atlan, these changes can be synchronized back to Snowflake and depend on the combination of reverse sync and concatenation settings:\nWhen\nreverse sync is OFF\n: Snowflake isn't updated\nWhen\nreverse sync is ON\nbut\nconcatenation is OFF\n: Only one tag value (typically the latest) is sent to Snowflake\nWhen\nboth reverse sync and concatenation are ON\n: All tag values are concatenated and sent as a single string\nSee also\nâ\nSnowflake object tagging introduction\n- Learn about Snowflake's tag capabilities, quotas, and supported objects\nTags:\nmultiple-concatenation\nsnowflake\nPrevious\nConfigure Snowflake data metric functions\nNext\nWhat does Atlan crawl from Snowflake?\nRequirements\nConstraints\nHow concatenation works\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity",
    "content": "Connect data\nData Warehouses\nSnowflake\nTroubleshooting\nTroubleshooting Snowflake connectivity\nOn this page\nTroubleshooting Snowflake connectivity\nHow to debug test authentication and preflight check errors?\nâ\nMissing warehouse grants\nThe user doesnât have USAGE and OPERATE grants on a warehouse.\nGrant warehouse access to the role\n:\nGRANT\nOPERATE\n,\nUSAGE\nON\nWAREHOUSE\n\"<warehouse>\"\nTO\nROLE atlan_user_role\n;\nThen, ensure that you\ngrant the role to the new user\n:\nGRANT\nROLE atlan_user_role\nTO\nUSER\natlan_user\n;\nMissing authorized access to SNOWFLAKE.ACCOUNT_USAGE schema\nThe user doesnât have authorized access to the SNOWFLAKE.ACCOUNT_USAGE database\nReach out to your account admin to\ngrant imported privileges\non the\nSnowflake\ndatabase to the role:\nUSE\nROLE ACCOUNTADMIN\n;\nGRANT\nIMPORTED\nPRIVILEGES\nON\nDATABASE\nSNOWFLAKE\nTO\nROLE atlan_user_role\n;\nIf\nusing a copied database\n, you'll need to grant the following permissions:\nGRANT\nUSAGE\nON\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\n\"<copied-schema>\"\nIN\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<copied-database>\"\nTO\nROLE atlan_user_role\n;\nMissing usage grants on databases and/or schemas\nThe user doesn't have usage grants to the databases ` $missingDatabases ` and schemas ` $missingSchemas\nGrant\nmissing permissions listed here\nfor information schema extraction method.\nAtlan IP not allowlisted\nAtlan's current location or network isn't recognized by Snowflake's security settings. This can happen if Atlan's IP address isn't on the list of allowed addresses in Snowflake's network policies.\nIf you are using the IP allowlist in your Snowflake instance, you must add the\nAtlan IP to the allowlist\n. Contact\nAtlan support\nto obtain Atlan's IP addresses.\nIncorrect credentials\nThe username or the password provided to connect to the Snowflake account is incorrect.\nSign into the Snowflake account for the specified host and verify that the username and password are correct.\nYou can also create a new user, if required, by following the steps\nhere\n.\nMissing or unauthorized role\nThe role specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this role.\nIf the role does not exist or is missing the required grants,\ncreate a role\nand then\ngrant the role to the user\n.\nUser account locked\nThe user account you're using to connect to Snowflake has been locked temporarily because of multiple incorrect login attempts.\nWait for the user account to unlock or create a different user account to continue.\nMissing or unauthorized warehouse\nThe warehouse specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this warehouse.\nEnsure that the warehouse name is configured correctly.\nUpdate the warehouse name in the configuration if your account is using a different warehouse.\nCreate a role\nand then\ngrant the role to the user\nfor the updated warehouse.\nMissing access to non-system databases or schemas\nThe configured user doesn't have usage grants to any database or schema.\nor\nThe configured user doesn't have usage grants to any non-system database or schema.\nThis pertains to the information schema method of fetching metadata. Ensure that the user has authorized access to the databases and schemas to be crawled.\nGrant the requisite permissions as outlined\nhere\n.\nWhy are some assets from a database or schema missing?\nâ\nCheck the grants on the role attached to the user defined for the crawler. Ensure the missing database or schema is present in these grants.\nSHOW\nGRANTS\nTO\nROLE atlan_user_role\n;\nWhy are new tables or views missing?\nâ\nWhen using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata.\nMake sure the role attached to the user defined for the crawler has grants for future tables and views being created in the database:\nGRANT\nUSAGE\nON\nFUTURE SCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE VIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nGRANT\nREFERENCES\nON\nFUTURE EXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nROLE atlan_user_role\n;\nMake sure you run the below commands as well so that new tables and views you've created in-between are also visible to the user:\nGRANT\nUSAGE\nON\nALL\nSCHEMAS\nIN\nDATABASE\n\"<database-name>\"\nTO\nrole atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\nrole atlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nEXTERNAL\nTABLES\nIN\nDATABASE\n\"<database-name>\"\nTO\natlan_user_role\n;\nGRANT\nREFERENCES\nON\nALL\nVIEWS\nIN\nDATABASE\n\"<database-name>\"\nTO\nrole atlan_user_role\n;\nWhy is some lineage missing?\nâ\nThe query miner only mines query history for up to the previous two weeks. The miner will not mine any queries that ran before that time window. If the queries that created your assets ran before that time window, lineage for those assets will not be present.\nTo mine more than the previous two weeks of query history, either use\nS3-based query mining\nor\ncontact Atlan support\n. Note that Snowflake itself only retains query history for so long as well, though. Once Snowflake itself no longer contains the query history we will be unable to mine it for lineage.\nLineage is unsupported for parameterized queries. Snowflake currently\ndoes not resolve values\nfor parameterized queries before logging them in query history. This limits Atlan from generating lineage in such cases.\nMissing attributes and lineage\nâ\nWhen using the account usage extraction method, there are currently some limitations. We are working with Snowflake to find workarounds for crawling the following:\nExternal table location data\nProcedures\nPrimary key designation\nFurthermore, only database-level filtering is currently possible.\nWhat views does Atlan require access to for the account usage method?\nâ\nWhen using the\naccount usage method\nfor fetching metadata, Atlan requires access to the following views in Snowflake:\nFor the crawler:\nDATABASES\n,\nSCHEMATA\n,\nTABLES\n,\nVIEWS\n,\nCOLUMNS\n, and\nPIPES\nFor the miner and\npopularity metrics\n:\nQUERY_HISTORY\n,\nACCESS_HISTORY\n, and\nSESSIONS\nWhy am I getting a destination URL mismatch error when authenticating via Okta SSO?\nâ\nThis error can occur when you're connecting to Snowflake through\nOkta SSO\nand enter the URL of your Snowflake instance in a format different from the one used in Okta.\nSnowflake follows two URL formats:\nLegacy format   - Â\n<AccountLocator>.<Region>.snowflakecomputing.com\nor\n<AccountLocator>.<Region>.<cloud>.snowflakecomputing.com\nNew URL format   -\n<Orgname>-<AccountName>.snowflakecomputing.com\nEnsure that you're using the same Snowflake URL format in Snowflake and Okta. Refer to\nSnowflake documentation\nto learn more.\nWhy am I getting a 'name or service not known' error when connecting via private link?\nâ\nIf you're getting the following error messages   -\njava.net.UnknownHostException\nand\nName or service not known\n-  this is a known error for users who have upgraded to the Snowflake JDBC driver version 3.13.25., have underscores in their account name, and connect to their Snowflake accounts over\nprivate link\n(for example,\nhttps://my_account.us-west-2.privatelink.snowflakecomputing.com\n).\nIf your Snowflake account name has an underscore   -  for example,\nmy_account\n- Â the updated JDBC driver will automatically convert underscores to dashes or hyphens\n-\n. This does not affect normal URLs because Snowflake accepts URLs with both hyphens and underscores.\nFor private link users, however, the JDBC driver will return an error if there are underscores present in the account name and the connection will fail. To troubleshoot further, refer to\nSnowflake documentation\n.\nTags:\natlan\ndocumentation\nPrevious\nPreflight checks for Snowflake\nNext\nTroubleshooting Snowflake tag management"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/best-practices/snowflake-warehouse-configuration",
    "content": "Connect data\nData Warehouses\nSnowflake\nBest Practices\nSnowflake warehouse configuration\nOn this page\nSnowflake warehouse configuration\nConfigure your Snowflake warehouses following these best practices to achieve optimal performance and reliability for Atlan data workflows. These recommendations establish predictable resource allocation and maximize workflow efficiency.\nConfigure warehouse allocation\nâ\nUse a dedicated warehouse for Atlan workflows\n: Assign a dedicated warehouse exclusively for Atlan operations. This approach separates warehouse performance from other workloads, enables precise cost tracking for Atlan operations, and provides consistent workflow performance.\nOne warehouse per connection\n: Provision one Snowflake warehouse for each Atlan connection to maintain scoped capacity and predictable resource allocation.\nConfigure statement timeout\nâ\nSet appropriate timeout values\n: If your account enforces timeouts, configure both\nSTATEMENT_TIMEOUT_IN_SECONDS\nand\nSTATEMENT_QUEUED_TIMEOUT_IN_SECONDS\nto at least 6 hours (21,600 seconds) for the Atlan user to accommodate comprehensive data cataloging workflows.\nDefault values work well\n: By default, both parameters are set to\n0\n(no limit), which is optimal for Atlan operations. Only adjust if your organization requires specific timeout enforcement.\nApply at user level\n: Configure timeouts at the user level for consistent behavior across all Atlan sessions rather than at warehouse or session level.\nTags:\nsnowflake\nwarehouse\nconfiguration\nPrevious\nTroubleshooting Snowflake tag management\nConfigure warehouse allocation\nConfigure statement timeout"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/manage-connectivity",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nHow-tos\nManage connectivity\nOn this page\nManage connectivity\nOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.\nModify connectivity\nâ\nTo modify the configuration of an existing workflow, complete the following steps.\nOn the left of any screen, navigate toÂ\nWorkflow\n.\nUnder\nMonitor\nselect an existing workflow tile. (You may need to expand the run history or filter first.)\nFrom the\nWorkflow Run History\ntable, click on the previous run of the workflow you want to modify.\nIn the upper left of the screen, change to the\nConfig\ntab.\nModify the parts of the workflow configuration you require:\nUnder\n<Connector>\nCredential\n, use the\nEdit Credentials\nbutton to change the credentials for the source.\ndanger\nIf you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically.\nUnder\nConnection settings\n, use the\nEdit\nÂ button to change the connection details:\nModify whether or not querying or data previews are allowed for the source.\nModify the query row limit to enable\nexporting large query results via email\n.\nModify the query timeout limit   -  expandable up to 60 minutes.\nUnder\nConnection Admins\n, click the pencil icon to add or remove connection admins.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nUnder\nMetadata\n, use the selectors to modify which metadata to include and exclude.\nTo check for any\npermissions or other configuration issues\nbefore running the workflow, click\nPreflight checks\n.\nOnce you've made your updates, click theÂ\nUpdate\nbutton to save the changes.\nYou can optionally run the workflow with the new configuration immediately.\nYou will need to confirm your changes by clicking theÂ\nYes\nbutton. Note that some workflow changes may take a few minutes to come into effect.\nThat's it   -  next time you run the workflow, or it runs on its schedule, it will use your changes! ð\ndanger\nIf you modify the\nMetadata\nportion, any previously crawled metadata that is now excluded will be\narchived\non the next workflow run.\nTags:\nintegration\nconnectors\nworkflow\nautomation\norchestration\nPrevious\nConnectors\nNext\nMonitor connectivity\nModify connectivity"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/mine-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nCrawl Snowflake Assets\nMine Snowflake\nOn this page\nMine Snowflake\nOnce you have\ncrawled assets from Snowflake\n, you can mine its query history to construct lineage.\nTo mine lineage from Snowflake, review the\norder of operations\nand then complete the following steps.\nSelect the miner\nâ\nTo select the Snowflake miner:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the filters along the top, click\nMiner\n.\nFrom the list of packages, select\nSnowflake Miner\nand then click\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the Snowflake miner:\nFor\nConnection\n, select the connection to mine. (To select a connection,\nthe crawler\nmust have already run.)\nFor\nMiner Extraction Method\n, select\nSource\n,\nAgent\n, or see the separate instructions for the\nS3 miner\n.\nFor\nSnowflake Database\n:\nIf the connection is configured with\naccess to the snowflake database\n, choose\nDefault\n.\nIf the connection can only\naccess a separate cloned database\n, choose\nCloned Database\n.\nIf you are using a cloned database, enter the name of the cloned database in\nDatabase Name\nand the name of the cloned schema in\nSchema Name\n.\nFor\nStart time\n, choose the earliest date from which to mine query history.\ninfo\nðª\nDid you know?\nThe miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the\nS3 miner\nfirst. After the initial load, you can\nmodify the miner's configuration\nto use query history extraction.\nTo check for any permissions or other configuration issues before running the miner, click\nPreflight checks\n.\nAt the bottom of the screen, click\nNext\nto proceed.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for mining query history from Snowflake. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the connection configuration used when\ncrawling Snowflake\n.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\ndanger\nIf running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic\nhere\n.\nConfigure the miner behavior\nâ\nTo configure the Snowflake miner behavior:\n(Optional) For\nCalculate popularity\n, keep\nTrue\nto retrieve\nusage and popularity metrics\nfor your Snowflake assets from query history.\nFor\nExcluded Users\n, type the names of users to be excluded while calculating\nusage metrics\nfor Snowflake assets. Press\nEnter\nafter each name to add more names.Â\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the miner:\nIf Atlan support has provided you with a custom control configuration,Â enter the configuration into theÂ\nCustom Config\nbox.\nYou can also enter\n{âignore-all-caseâ: true}\nto enable crawling assets with case-sensitive identifiers.\nFor\nPopularity Window (days)\n, 90 days is the maximum limit. You can set a shorter popularity window of less than 90 days.\nRun the miner\nâ\nTo run the Snowflake miner, after completing the steps above:\nTo run the miner once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the miner has completed running, you will see lineage for Snowflake assets that were created in Snowflake between the start time and when the miner ran! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nCrawl Snowflake\nNext\nManage Snowflake tags\nSelect the miner\nConfigure the miner\nConfigure the miner behavior\nRun the miner"
  },
  {
    "url": "https://docs.atlan.com/tags/connectors",
    "content": "299 docs tagged with \"connectors\"\nView all tags\nAdd impact analysis in GitHub\nLearn about add impact analysis in github.\nAdd impact analysis in GitLab\nLearn about add impact analysis in gitlab.\nAtlan browser extension security\nLearn about atlan browser extension security.\nAttach a tag\nAtlan allows users to add [tags](/product/capabilities/governance/tags/concepts/what-are-tags) to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection.\nAutomate data profiling\nâAvailable via the Data Quality Studio package\nBulk enrich metadata\nAtlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan.\nCan Atlan integrate with Airflow to generate lineage?\nAtlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage).\nCan I add Atlan's browser extension for everyone in my organization?\nRefer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nCan I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.\nCan site renaming affect the Jira integration?\nLearn about can site renaming affect the jira integration?.\nCan the Hive crawler connect to an independent Hive metastore?\nLearn about can the hive crawler connect to an independent hive metastore?.\nCan we use a Microsoft SSO login?\nLearn about can we use a microsoft sso login?.\nConfigure workflow execution\nLearn about configure workflow execution.\nConnect data sources for Azure-hosted Atlan instances\nThis document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:.\nConnect on-premises databases to Kubernetes\nYou can configure and use [Atlan's metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access) to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose.\nConnection issues\nResolve common connection and authentication issues when setting up CrateDB connector\nConnectors\nLearn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management.\nConnectors and capabilities\nLearn about connectors and capabilities.\nCrawl Aiven Kafka\nOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.\nCrawl Amazon Athena\nTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon DynamoDB\nOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.\nCrawl Amazon MSK\nTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon QuickSight\nOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.\nCrawl Amazon Redshift\nOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.\nCrawl Apache Kafka\nLearn about crawl apache kafka.\nCrawl AWS Glue\nOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.\nCrawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nCrawl Confluent Kafka\nLearn about crawl confluent kafka.\nCrawl Confluent Schema Registry\nOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.\nCrawl CrateDB\nConfigure and run the CrateDB crawler to extract metadata from your database\nCrawl Dagster assets\nCreate a crawler workflow in Atlan to capture lineage from Dagster assets\nCrawl Databricks\nTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl DataStax Enterprise\nCrawl DataStax Enterprise\nCrawl dbt\nOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.\nCrawl Domo\nOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.\nCrawl Fivetran\nLearn about crawl fivetran.\nCrawl Google BigQuery\nOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.\nCrawl Hive\nTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl IBM Cognos Analytics\nOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nCrawl Looker\nOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.\nCrawl Matillion\nOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.\nCrawl Metabase\nOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.\nCrawl Microsoft Azure Cosmos DB\nOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.\nCrawl Microsoft Azure Data Factory\nOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.\nCrawl Microsoft Azure Event Hubs\nOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.\nCrawl Microsoft Azure Synapse Analytics\nOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.\nCrawl Microsoft Power BI\nOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.\nCrawl Microsoft SQL Server\nOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.\nCrawl MicroStrategy\nOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.\nCrawl Mode\nOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.\nCrawl MongoDB\nOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.\nCrawl Monte Carlo\nOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.\nCrawl MySQL\nTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl on-premises databases\nOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.\nCrawl on-premises Databricks\nOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.\nCrawl on-premises IBM Cognos Analytics\nOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.\nCrawl on-premises Kafka\nOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.\nCrawl on-premises Looker\nOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.\nCrawl on-premises Tableau\nOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.\nCrawl on-premises ThoughtSpot\nOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.\nCrawl Oracle\nOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.\nCrawl PostgreSQL\nTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl PrestoSQL\nOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.\nCrawl Qlik Sense Cloud\nOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.\nCrawl Qlik Sense Enterprise on Windows\nOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.\nCrawl Redash\nOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.\nCrawl Redpanda Kafka\nOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.\nCrawl Salesforce\nOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.\nCrawl SAP HANA\nOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.\nCrawl Sigma\nOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.\nCrawl Sisense\nOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.\nCrawl Snowflake\nTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Soda\nOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.\nCrawl Tableau\nTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Teradata\nOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.\nCrawl ThoughtSpot\nOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.\nCrawl Trino\nTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCreate README templates\nAdmin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and [enrich their assets with READMEs](/product/integrations). They will also be able to see a rich preview of each template before adding the relevant documentation.\nCustom solutions\nLearn about custom solutions.\nDagster integration\nFrequently asked questions about Dagster integration with Atlan\nData Connections and Integration\nComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nDelete a connection\nLearn about delete a connection.\nDeployment architecture\nThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.\nDoes Atlan require an admin user in Salesforce?\nNo. However, it is recommended that a Salesforce administrator establishes a [connection between Atlan and Salesforce](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce). To learn more, see [here](/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity).\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nEnable  Azure AD for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  Google for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  JumpCloud for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  Okta for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  OneLogin for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  SAML 2.0 for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  Snowflake OAuth\nAtlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware.\nEnable  SSO for Amazon Redshift\nYou will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift).\nEnable  SSO for Google BigQuery\nCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.\nEnable Okta for SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM).\nEnrich Atlan through dbt\nBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.\nETL tools connectors\nOverview and entry point for all ETL tools connectors in Atlan.\nextract lineage and usage from Databricks\nOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.\nextract on-premises Databricks lineage\nOnce you have [set up the databricks-extractor tool](/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction), you can extract lineage from your on-premises Databricks instances by completing the following steps.\nFind assets by usage\nData teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance.\nHow are product updates deployed?\nLearn about how are product updates deployed?.\nHow can I identify an Insights query in my database access log?\nAtlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nImplement OpenLineage in Airflow operators\nIf you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage.\nInfrastructure security\nLearn about infrastructure security.\nIntegrate Amazon MWAA/OpenLineage\nTo learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Anomalo\nOnce you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo.\nIntegrate Apache Airflow/OpenLineage\nTo integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Astronomer/OpenLineage\nTo integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/.\nIntegrate Atlan with Google Sheets\nThe Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.\nIntegrate Jira Cloud\nYou must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work.\nIntegrate Microsoft Teams\nOnce you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams.\nInterpret usage metrics\nAtlan currently supports usage and popularity metrics for the following connectors:\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nLink your Jira account\nTo create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that [set up the Jira integration](/product/integrations/project-management/jira/how-tos/integrate-jira-cloud), but not for other users.\nLink your Microsoft Teams account\nTo get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users.\nLink your Slack account\nTo see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that [set up the Slack integration](/product/integrations/collaboration/slack/how-tos/integrate-slack), but not for other users.\nManage connectivity\nOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.\nManage Databricks tags\nYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.\nManage dbt tags\nAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.\nManage Google BigQuery tags\nAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).\nManage requests\nIf your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected.\nManage Snowflake tags\nYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.\nMigrate from dbt to Atlan action\nThe dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/.\nMine Amazon Redshift\nOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).\nMine Google BigQuery\nOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.\nMine Microsoft Azure Synapse Analytics\nLearn about mine microsoft azure synapse analytics.\nMine Microsoft Power BI\nOnce you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics.\nMine queries through S3\nOnce you have crawled assets from a supported connector, you can mine query history.\nMine Snowflake\nOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.\nMine Teradata\nOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.\nMonitor connectivity\nAtlan runs its crawlers through an orchestrated set of automated tasks.\nOkta first-time login authentication error\nLearn about why do i get an authentication error when logging in via okta for the first time?.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\norder workflows\nThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.\nPermissions and limitations\nFrequently asked questions about CrateDB connector setup, permissions, and limitations\nPreflight checks for Aiven Kafka\nBefore [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne.\nPreflight checks for Amazon MSK\nBefore [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti.\nPreflight checks for Amazon QuickSight\nThe [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission.\nPreflight checks for Amazon Redshift\nBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.\nPreflight checks for Anomalo\nThis check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid.\nPreflight checks for Apache Kafka\nBefore [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection.\nPreflight checks for Confluent Schema Registry\nBefore [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca.\nPreflight checks for CrateDB\nTechnical validations performed before running the CrateDB crawler to verify connectivity and permissions\nPreflight checks for Databricks\nBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.\nPreflight checks for DataStax Enterprise\nPreflight checks for DataStax Enterprise\nPreflight checks for dbt\nThis checks if manifest files are present in the provided bucket and prefix.\nPreflight checks for Domo\nAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.\nPreflight checks for Fivetran\nLearn about preflight checks for fivetran.\nPreflight checks for Google BigQuery\nEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).\nPreflight checks for Hive\nBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.\nPreflight checks for Looker\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project#get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management).\nPreflight checks for Metabase\nBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.\nPreflight checks for Microsoft Azure Data Factory\nBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.\nPreflight checks for Microsoft Azure Synapse Analytics\nThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.\nPreflight checks for Microsoft Power BI\nBefore [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run.\nPreflight checks for Microsoft SQL Server\nBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.\nPreflight checks for MicroStrategy\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html#/Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions.\nPreflight checks for Mode\nBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.\nPreflight checks for Monte Carlo\nBefore [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c.\nPreflight checks for MySQL\nBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.\nPreflight checks for Oracle\nBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.\nPreflight checks for PostgreSQL\nBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.\nPreflight checks for PrestoSQL\nBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.\nPreflight checks for Qlik Sense Cloud\nThis check tests for access to datasets and other Qlik objects.\nPreflight checks for Redash\nBefore [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti.\nPreflight checks for Redpanda Kafka\nBefore [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod.\nPreflight checks for Salesforce\nBefore [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co.\nPreflight checks for SAP S/4HANA\nPreflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nPreflight checks for Sigma\nFirst, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission.\nPreflight checks for Sisense\nAtlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.\nPreflight checks for Snowflake\nBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.\nPreflight checks for Tableau\nThe [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm#server_info) REST API is used to fetch the `restApiVersion` value.\nPreflight checks for Teradata\nBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.\nPreflight checks for Trino\nBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.\nProvide credentials to query data\nLearn about provide credentials to query data.\nProvide credentials to view sample data\nLearn about provide credentials to view sample data.\nprovide SSL certificates\nSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nReport on assets\nLearn about report on assets.\nReport on automations\nYou can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions.\nS3 Inventory Report Structure\nExpected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion.\nSet default user roles for SSO\n:::warning Who can do this? You will need to be an admin user and [configure SSO](/product/integrations/identity-management/sso) with a provider first.\nSet up a private network link to Amazon Athena\n:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.\nSet up a private network link to Hive\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nSet up a private network link to Trino\n:::warning Who can do this? You will need your AWS administrator involved - you may not have access to run these tasks yourself.\nSet up Aiven Kafka\nAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nSet up Alteryx\nSet up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run.\nSet up Amazon DynamoDB\nLearn about set up amazon dynamodb.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.\nSet up AWS Glue\nLearn about set up aws glue.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nSet up client credentials flow\nConfigure Salesforce for OAuth 2.0 client credentials authentication in Atlan.\nSet up Confluent Schema Registry\n:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.\nSet up CrateDB\nConfigure authentication and connection settings for CrateDB connector\nSet up Dagster\nConfigure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assets\nSet up dbt Cloud\n:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.\nSet up Domo\n:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.\nSet up Google BigQuery\nYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan.\nSet up IBM Cognos Analytics\n:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.\nSet up Informatica CDI\nConfigure authentication and user permissions for Informatica Cloud Data Integration connector\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.\nSet up JWT bearer flow\nConfigure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan.\nSet up Matillion\nConfigure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance.\nSet up Microsoft Azure Event Hubs\nAtlan supports the following authentication methods for Microsoft Azure Event Hubs:.\nSet up Microsoft Azure Synapse Analytics\nAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.\nSet up MicroStrategy\nAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.\nSet up Mode\nIf you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email.\nSet up MongoDB\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Microsoft Azure Synapse Analytics miner access\nIn some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to [mine query history from the Query Store](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics). For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Teradata miner access\nIn some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up PrestoSQL\nLearn about set up prestosql.\nSet up Redpanda Kafka\nAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nSet up Salesforce\nLearn about setting up Salesforce authentication for Atlan.\nSet up Sisense\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nSet up Snowflake\n:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.\nSet up Teradata\n:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.\nSet up ThoughtSpot\n:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.\nSet up username-password flow\nConfigure Salesforce username-password flow for Atlan integration.\nSupported connections for on-premises databases\nThe metadata-extractor tool supports the following connection types.\nSupported sources\nLearn about supported sources.\nTask and crawl issues\nTroubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance.\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector\nTransformations\nUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan\nTroubleshooting AWS Glue connectivity\nLearn about troubleshooting aws glue connectivity.\nTroubleshooting connector-specific SSO authentication\nLearn about troubleshooting connector-specific sso authentication.\nTroubleshooting Metabase connectivity\nLearn about troubleshooting metabase connectivity.\nTroubleshooting Microsoft Teams\nWhy do I get an error while adding Atlan to Microsoft Teams?\nTroubleshooting Mode connectivity\nLearn about troubleshooting mode connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity.\nTroubleshooting SCIM provisioning\nLearn about troubleshooting scim provisioning.\nTroubleshooting ServiceNow\nWhy is the security\\_admin role required to complete the ServiceNow integration?\nTroubleshooting Sisense connectivity\nLearn about troubleshooting sisense connectivity.\nTroubleshooting Slack\nWhat do the colors in Slack notifications for modified assets mean?\nTroubleshooting spreadsheets\nWhy do I need admin consent for exporting assets to Microsoft Excel?\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nupdate column metadata in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.\nUpdate column metadata in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.\nView event logs\nEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.\nWhat are user roles?\nLearn about what are user roles?.\nWhat does Atlan crawl from Aiven Kafka?\nAtlan crawls and maps the following assets and properties from Aiven Kafka.\nWhat does Atlan crawl from Amazon MSK?\nAtlan crawls and maps the following assets and properties from Amazon MSK.\nWhat does Atlan crawl from Amazon MWAA/OpenLineage?\nOnce you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [.\nWhat does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.\nWhat does Atlan crawl from Anomalo?\nOnce you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nOnce you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),.\nWhat does Atlan crawl from Apache Kafka?\nAtlan crawls and maps the following assets and properties from Apache Kafka.\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nAtlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.\nWhat does Atlan crawl from Astronomer/OpenLineage?\nAtlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).\nWhat does Atlan crawl from BigID?\nReference guide for BigID metadata crawled by Atlan.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Confluent Kafka?\nAtlan crawls and maps the following assets and properties from Confluent Kafka.\nWhat does Atlan crawl from Confluent Schema Registry?\nAtlan crawls and maps the following assets and properties from Confluent Schema Registry.\nWhat does Atlan crawl from CrateDB?\nComplete list of CrateDB assets and metadata properties extracted by Atlan during crawling\nWhat does Atlan crawl from Dagster\nLearn about the Dagster metadata that Atlan captures and visualizes\nWhat does Atlan crawl from Fivetran?\nLearn about what does atlan crawl from fivetran?.\nWhat does Atlan crawl from Google BigQuery?\nAtlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).\nWhat does Atlan crawl from Google Cloud Composer/OpenLineage?\nAtlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).\nWhat does Atlan crawl from IBM Cognos Analytics?\nAtlan crawls and maps the following assets and properties from IBM Cognos Analytics.\nWhat does Atlan crawl from Informatica CDI\nUnderstand the metadata and assets discovered during crawling from Informatica Cloud Data Integration\nWhat does Atlan crawl from Matillion?\nAtlan crawls and maps the following assets and properties from Matillion.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase.\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nOnce you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.\nWhat does Atlan crawl from Microsoft Azure Event Hubs?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs.\nWhat does Atlan crawl from Microsoft Power BI?\nAtlan crawls and maps the following assets and properties from Microsoft Power BI.\nWhat does Atlan crawl from MicroStrategy?\nAtlan crawls and maps the following assets and properties from MicroStrategy.\nWhat does Atlan crawl from Mode?\nAtlan crawls and maps the following assets and properties from Mode.\nWhat does Atlan crawl from MongoDB?\nAtlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.\nWhat does Atlan crawl from Monte Carlo?\nWhat does Atlan crawl from Monte Carlo? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from MySQL?\nAtlan crawls and maps the following assets and properties from MySQL.\nWhat does Atlan crawl from PostgreSQL?\nAtlan crawls and maps the following assets and properties from PostgreSQL.\nWhat does Atlan crawl from Qlik Sense Cloud?\nAtlan crawls and maps the following assets and properties from Qlik Sense Cloud.\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\nAtlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows.\nWhat does Atlan crawl from Redash?\nAtlan crawls and maps the following assets and properties from Redash.\nWhat does Atlan crawl from Redpanda Kafka?\nAtlan crawls and maps the following assets and properties from Redpanda Kafka.\nWhat does Atlan crawl from Sisense?\nAtlan crawls and maps the following assets and properties from Sisense.\nWhat does Atlan crawl from Snowflake?\nAtlan crawls and maps the following assets and properties from Snowflake.\nWhat does Atlan crawl from Soda?\nAtlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.\nWhat does Atlan crawl from Tableau?\nAtlan crawls and maps the following assets and properties from Tableau.\nWhat does Atlan crawl from ThoughtSpot?\nOnce you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters].\nWhat is the crawler logic for a deprecated asset?\nLearn about what is the crawler logic for a deprecated asset?.\nWhat lineage does Atlan extract from Matillion?\nAtlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nAtlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\nLearn about what lineage does atlan extract from microsoft azure synapse analytics?.\nWhat lineage does Atlan extract from Microsoft Power BI?\nThis document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.\nWhat type of user provisioning does Atlan support for SSO integrations?\nAtlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:.\nWhat's the difference between connecting to Athena and Glue?\nLearn about what's the difference between connecting to athena and glue?.\nWhy did my users not receive an invite email from Atlan?\nIf you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:.\nWhy do I get an error message when I click on Atlan's browser extension?\nRefer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).\nWhy does the description from Salesforce not show up in Atlan?\nAtlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce).\nWhy is Atlan's browser extension not loading?\nRefer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension)."
  },
  {
    "url": "https://docs.atlan.com/tags/data",
    "content": "255 docs tagged with \"data\"\nView all tags\nAccess archived assets\nLearn about access archived assets.\nAdd contract impact analysis in GitHub\nAdd contract impact analysis in GitHub <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nAdd custom metadata\n<div style={{position: \"relative\", paddingBottom: \"calc(66.33333333333333% + 41px)\", height: 0}}> <iframe src=\"https://demo.arcade.software/1dT1bPneM5fp1O71lb.\nAdd descriptions\nYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.\nAdd impact analysis in GitLab\nLearn about add impact analysis in gitlab.\nAdd options\n:::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties.\nAI and Automation Features\nGuide to Atlan's AI capabilities and automation features for enhanced data governance and productivity.\nAtlan AI security\nAtlan uses [Azure OpenAI Service](https://azure.microsoft.com/en-in/products/cognitive-services/openai-service) to power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model.\nAttach a tag\nAtlan allows users to add [tags](/product/capabilities/governance/tags/concepts/what-are-tags) to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection.\nAutomate data profiling\nâAvailable via the Data Quality Studio package\nCan Atlan read a dump of SQL statements to create lineage?\nAtlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/.\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nYou can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events.\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nCan I query any DW/DL?\nYou can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources#data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature.\nCan I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.\nConfigure Snowflake data metric functions\nConfigure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nConnect data sources for Azure-hosted Atlan instances\nThis document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:.\nConnect on-premises databases to Kubernetes\nYou can configure and use [Atlan's metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access) to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose.\nCrawl Aiven Kafka\nOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.\nCrawl Amazon Athena\nTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon DynamoDB\nOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.\nCrawl Amazon MSK\nTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon QuickSight\nOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.\nCrawl Amazon Redshift\nOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.\nCrawl Apache Kafka\nLearn about crawl apache kafka.\nCrawl AWS Glue\nOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.\nCrawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nCrawl Confluent Kafka\nLearn about crawl confluent kafka.\nCrawl Confluent Schema Registry\nOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.\nCrawl CrateDB\nConfigure and run the CrateDB crawler to extract metadata from your database\nCrawl Databricks\nTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl DataStax Enterprise\nCrawl DataStax Enterprise\nCrawl dbt\nOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.\nCrawl Domo\nOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.\nCrawl Fivetran\nLearn about crawl fivetran.\nCrawl Google BigQuery\nOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.\nCrawl Hive\nTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl IBM Cognos Analytics\nOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.\nCrawl Looker\nOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.\nCrawl Matillion\nOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.\nCrawl Metabase\nOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.\nCrawl Microsoft Azure Cosmos DB\nOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.\nCrawl Microsoft Azure Data Factory\nOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.\nCrawl Microsoft Azure Event Hubs\nOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.\nCrawl Microsoft Azure Synapse Analytics\nOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.\nCrawl Microsoft Power BI\nOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.\nCrawl Microsoft SQL Server\nOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.\nCrawl MicroStrategy\nOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.\nCrawl Mode\nOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.\nCrawl MongoDB\nOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.\nCrawl Monte Carlo\nOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.\nCrawl MySQL\nTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl on-premises databases\nOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.\nCrawl on-premises Databricks\nOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.\nCrawl on-premises IBM Cognos Analytics\nOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.\nCrawl on-premises Kafka\nOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.\nCrawl on-premises Looker\nOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.\nCrawl on-premises Tableau\nOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.\nCrawl on-premises ThoughtSpot\nOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.\nCrawl Oracle\nOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.\nCrawl PostgreSQL\nTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl PrestoSQL\nOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.\nCrawl Qlik Sense Cloud\nOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.\nCrawl Qlik Sense Enterprise on Windows\nOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.\nCrawl Redash\nOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.\nCrawl Redpanda Kafka\nOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.\nCrawl Salesforce\nOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.\nCrawl SAP HANA\nOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.\nCrawl Sigma\nOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.\nCrawl Sisense\nOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.\nCrawl Snowflake\nTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Soda\nOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.\nCrawl Tableau\nTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Teradata\nOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.\nCrawl ThoughtSpot\nOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.\nCrawl Trino\nTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCreate announcements\nAdding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions.\nData Connections and Integration\nComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.\nData Models\nData models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nDisable data access\n:::warning Who can do this? You will need to be an admin user in Atlan to configure these options.\nDiscovery FAQs\nFrequently asked questions about Atlan's Discovery capabilities.\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.\nDownload impacted assets in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nEnable  Snowflake OAuth\nAtlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware.\nEnable  SSO for Amazon Redshift\nYou will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift).\nEnable  SSO for Google BigQuery\nCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.\nEnrich Atlan through dbt\nBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.\nextract lineage and usage from Databricks\nOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.\nextract on-premises Databricks lineage\nOnce you have [set up the databricks-extractor tool](/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction), you can extract lineage from your on-premises Databricks instances by completing the following steps.\nFind assets by usage\nData teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance.\nHow can I identify an Insights query in my database access log?\nAtlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs.\nHow can I use personas to update a term in a glossary?\nBy default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section.\nImplement OpenLineage in Airflow operators\nIf you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage.\nIntegrate Amazon MWAA/OpenLineage\nTo learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Apache Airflow/OpenLineage\nTo integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Apache Spark/OpenLineage\nAtlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Atlan with Microsoft Excel\nThe Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel.\nIntegrate Jira Data Center\nYou will need to [configure an incoming link](https://confluence.atlassian.com/adminjiraserver/configure-an-incoming-link-1115659067.html) with an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider.\nIntegrate ServiceNow\nIf your Atlan admin has [enabled the governance workflows and inbox module](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) in your Atlan workspace, you can create a ServiceNow integration to allow your users to [grant or revoke data access](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) for governed assets in Atlan or any other data source.\nInterpret usage metrics\nAtlan currently supports usage and popularity metrics for the following connectors:\nLink your account\nTo [export assets to and bulk enrich metadata from](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) a supported spreadsheet tool,.\nLink your ServiceNow account\nTo request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that [set up the ServiceNow integration](/product/integrations/project-management/servicenow/how-tos/integrate-servicenow), but not for other users.\nManage custom metadata structures\n:::warning Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones.\nManage Databricks tags\nYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.\nManage Google BigQuery tags\nAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).\nManage Snowflake tags\nYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.\nMigrate from dbt to Atlan action\nThe dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/.\nMine Amazon Redshift\nOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).\nMine Google BigQuery\nOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.\nMine Microsoft Azure Synapse Analytics\nLearn about mine microsoft azure synapse analytics.\nMine queries through S3\nOnce you have crawled assets from a supported connector, you can mine query history.\nMine Snowflake\nOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.\nMine Teradata\nOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.\norder workflows\nThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.\nPreflight checks for Amazon Redshift\nBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.\nPreflight checks for Anomalo\nThis check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid.\nPreflight checks for Databricks\nBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.\nPreflight checks for DataStax Enterprise\nPreflight checks for DataStax Enterprise\nPreflight checks for Domo\nAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.\nPreflight checks for Fivetran\nLearn about preflight checks for fivetran.\nPreflight checks for Google BigQuery\nEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).\nPreflight checks for Hive\nBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.\nPreflight checks for Metabase\nBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.\nPreflight checks for Microsoft Azure Data Factory\nBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.\nPreflight checks for Microsoft Azure Synapse Analytics\nThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.\nPreflight checks for Microsoft SQL Server\nBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.\nPreflight checks for Mode\nBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.\nPreflight checks for MySQL\nBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.\nPreflight checks for Oracle\nBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.\nPreflight checks for PostgreSQL\nBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.\nPreflight checks for PrestoSQL\nBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.\nPreflight checks for Qlik Sense Cloud\nThis check tests for access to datasets and other Qlik objects.\nPreflight checks for SAP S/4HANA\nPreflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nPreflight checks for Snowflake\nBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.\nPreflight checks for Soda\nLearn about preflight checks for soda\nPreflight checks for Teradata\nBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.\nPreflight checks for Trino\nBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.\nprovide SSL certificates\nSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).\nSchedule a query\nYou must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients.\nSearch and discover assets\nAtlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context.\nSecurity\nThe Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring.\nSecurity and Compliance\nComplete guide to Atlan's security features, compliance certifications, and data protection capabilities.\nSet up a private network link to Amazon Athena\n:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.\nSet up Aiven Kafka\nAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nSet up Amazon DynamoDB\nLearn about set up amazon dynamodb.\nSet up Amazon MSK\nLearn about set up amazon msk.\nSet up Amazon QuickSight\nLearn about set up amazon quicksight.\nSet up Amazon Redshift\n:::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up an Azure private network link to Databricks\nFor all details, see [Databricks documentation](https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/private-link-simplified?source=recommendations#create-the-workspace-and-private-endpoints-in-the-azure-portal-ui).\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.\nSet up AWS Glue\nLearn about set up aws glue.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nSet up Confluent Kafka\nAtlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata.\nSet up Confluent Schema Registry\n:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.\nSet up Databricks\nAtlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods:.\nSet up DataStax Enterprise\nSet up DataStax Enterprise\nSet up dbt Cloud\n:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.\nSet up Domo\n:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.\nSet up Fivetran\nLearn about set up fivetran.\nSet up Google BigQuery\nYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan.\nSet up Hive\n:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.\nSet up IBM Cognos Analytics\n:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.\nSet up Microsoft Azure Cosmos DB\nIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db).\nSet up Microsoft Azure Data Factory\nAtlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata.\nSet up Microsoft Azure Event Hubs\nAtlan supports the following authentication methods for Microsoft Azure Event Hubs:.\nSet up Microsoft Azure Synapse Analytics\nAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.\nSet up Microsoft Power BI\nThis guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking.\nSet up Microsoft SQL Server\n:::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself.\nSet up MicroStrategy\nAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.\nSet up MongoDB\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up MySQL\n:::warning Who can do this? You will probably need your MySQL administrator to run these commands - you may not have access yourself.\nSet up on-premises database access\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nSet up on-premises Databricks access\nIn some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises IBM Cognos Analytics access\n:::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,.\nSet up on-premises Kafka access\nIn some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Looker access\nIn some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Microsoft Azure Synapse Analytics miner access\nIn some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to [mine query history from the Query Store](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics). For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Tableau access\nIn some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Teradata miner access\nIn some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises ThoughtSpot access\nIn some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up Oracle\n:::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself.\nSet up PostgreSQL\n:::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself.\nSet up PrestoSQL\nLearn about set up prestosql.\nSet up Redash\n:::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself.\nSet up Redpanda Kafka\nAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nSet up SAP HANA\n:::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself.\nSet up Sisense\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nSet up Snowflake\n:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.\nSet up Soda\n:::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -.\nSet up Tableau\n:::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself.\nSet up Teradata\n:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.\nSSO integration with PingFederate using SAML\nTo use both IdP- and SP-initiated SSO, add both the URLs mentioned above.\nStar assets\n:::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can star assets.\nSupported connections for on-premises databases\nThe metadata-extractor tool supports the following connection types.\nTags and Metadata Management\nComplete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization.\nTroubleshooting data models\nWhat are the known limitations of data models in Atlan?\nTroubleshooting Jira\nWhat fields are supported when creating tickets or requesting access?\nTroubleshooting lineage\nSo you've crawled your source, and mined the queries, but lineage is missing. Why?\nupdate column metadata in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.\nUpdate column metadata in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.\nUse the filters menu\nYou can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you.\nview data models\nOnce you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:.\nView query logs\nYou can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization.\nWhat are Power BI processes on the lineage graph?\nNote that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets.\nWhat does Atlan crawl from Amazon Athena?\nAtlan crawls and maps the following assets and properties from Amazon Athena.\nWhat does Atlan crawl from Amazon DynamoDB?\nAtlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran.\nWhat does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.\nWhat does Atlan crawl from Amazon Redshift?\nAtlan crawls and maps the following assets and properties from Amazon Redshift.\nWhat does Atlan crawl from Anomalo?\nOnce you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nAtlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.\nWhat does Atlan crawl from AWS Glue?\nAtlan crawls and maps the following assets and properties from AWS Glue.\nWhat does Atlan crawl from BigID?\nReference guide for BigID metadata crawled by Atlan.\nWhat does Atlan crawl from Databricks?\nAtlan crawls and maps the following assets and properties from Databricks.\nWhat does Atlan crawl from DataStax Enterprise?\nWhat does Atlan crawl from DataStax Enterprise?\nWhat does Atlan crawl from Domo?\nAtlan supports lineage for the following asset types:.\nWhat does Atlan crawl from Fivetran?\nLearn about what does atlan crawl from fivetran?.\nWhat does Atlan crawl from Google BigQuery?\nAtlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).\nWhat does Atlan crawl from Hive?\nAtlan crawls and maps the following assets and properties from Hive.\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nOnce you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.\nWhat does Atlan crawl from Microsoft Azure Data Factory?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Data Factory.\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources.\nWhat does Atlan crawl from Microsoft SQL Server?\nAtlan crawls and maps the following assets and properties from Microsoft SQL Server.\nWhat does Atlan crawl from MongoDB?\nAtlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.\nWhat does Atlan crawl from MySQL?\nAtlan crawls and maps the following assets and properties from MySQL.\nWhat does Atlan crawl from Oracle?\nAtlan crawls and maps the following assets and properties from Oracle.\nWhat does Atlan crawl from PostgreSQL?\nAtlan crawls and maps the following assets and properties from PostgreSQL.\nWhat does Atlan crawl from PrestoSQL?\nAtlan crawls and maps the following assets and properties from PrestoSQL.\nWhat does Atlan crawl from Qlik Sense Cloud?\nAtlan crawls and maps the following assets and properties from Qlik Sense Cloud.\nWhat does Atlan crawl from SAP ECC?\nWhat does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from SAP S/4HANA?\nWhat does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from Sisense?\nAtlan crawls and maps the following assets and properties from Sisense.\nWhat does Atlan crawl from Snowflake?\nAtlan crawls and maps the following assets and properties from Snowflake.\nWhat does Atlan crawl from Soda?\nAtlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.\nWhat does Atlan crawl from Tableau?\nAtlan crawls and maps the following assets and properties from Tableau.\nWhat does Atlan crawl from Teradata?\nAtlan crawls and maps the following assets and properties from Teradata.\nWhat does Atlan crawl from Trino?\nAtlan crawls and maps the following assets and properties from Trino.\nWhat is included in the Jira integration?\nWith two of your most important workspaces connected, you can save time and improve the way you track issues for your data.\nWhat is included in the Microsoft Teams integration?\nWith two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.\nWhat is included in the Slack integration?\nLearn about the features and capabilities of the Slack integration with Atlan.\nWhat is the default permission for a glossary?\nBy default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data#glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access.\nWhat is the difference between a Power BI data source and dataflow?\nLearn about what is the difference between a power bi data source and dataflow?.\nWhat lineage does Atlan extract from Matillion?\nAtlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nAtlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\nLearn about what lineage does atlan extract from microsoft azure synapse analytics?.\nWhat lineage does Atlan extract from Microsoft Power BI?\nThis document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.\nWhen does Atlan become a personal data processor or subprocessor?\nAtlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.\nWhy do I only see tables from the same schema to join from in a visual query?\nWhen [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder.\nWhy does the description from Salesforce not show up in Atlan?\nAtlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce).\nWhy is lineage available for table level but not column level?\nThe home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset.\nWorkflows and Data Processing\nEverything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/crawl",
    "content": "227 docs tagged with \"crawl\"\nView all tags\nAdd descriptions\nYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.\nAdd options\n:::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties.\nAutomate data profiling\nâAvailable via the Data Quality Studio package\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nCan I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.\nCrawl Aiven Kafka\nOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.\nCrawl Amazon Athena\nTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon DynamoDB\nOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.\nCrawl Amazon MSK\nTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon QuickSight\nOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.\nCrawl Amazon Redshift\nOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.\nCrawl Apache Kafka\nLearn about crawl apache kafka.\nCrawl AWS Glue\nOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.\nCrawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nCrawl Confluent Kafka\nLearn about crawl confluent kafka.\nCrawl Confluent Schema Registry\nOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.\nCrawl CrateDB\nConfigure and run the CrateDB crawler to extract metadata from your database\nCrawl Databricks\nTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl DataStax Enterprise\nCrawl DataStax Enterprise\nCrawl dbt\nOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.\nCrawl Domo\nOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.\nCrawl Fivetran\nLearn about crawl fivetran.\nCrawl GCS assets\nConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.\nCrawl Google BigQuery\nOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.\nCrawl Hive\nTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl IBM Cognos Analytics\nOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nCrawl Looker\nOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.\nCrawl Matillion\nOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.\nCrawl Metabase\nOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.\nCrawl Microsoft Azure Cosmos DB\nOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.\nCrawl Microsoft Azure Data Factory\nOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.\nCrawl Microsoft Azure Event Hubs\nOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.\nCrawl Microsoft Azure Synapse Analytics\nOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.\nCrawl Microsoft Power BI\nOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.\nCrawl Microsoft SQL Server\nOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.\nCrawl MicroStrategy\nOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.\nCrawl Mode\nOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.\nCrawl MongoDB\nOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.\nCrawl Monte Carlo\nOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.\nCrawl MySQL\nTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl on-premises databases\nOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.\nCrawl on-premises Databricks\nOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.\nCrawl on-premises IBM Cognos Analytics\nOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.\nCrawl on-premises Kafka\nOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.\nCrawl on-premises Looker\nOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.\nCrawl on-premises Tableau\nOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.\nCrawl on-premises ThoughtSpot\nOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.\nCrawl Oracle\nOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.\nCrawl PostgreSQL\nTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl PrestoSQL\nOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.\nCrawl Qlik Sense Cloud\nOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.\nCrawl Qlik Sense Enterprise on Windows\nOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.\nCrawl Redash\nOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.\nCrawl Redpanda Kafka\nOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.\nCrawl S3 assets\nConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.\nCrawl Salesforce\nOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.\nCrawl SAP ECC\nTo crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl SAP HANA\nOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.\nCrawl SAP S/4HANA\nTo crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Sigma\nOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.\nCrawl Sisense\nOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.\nCrawl Snowflake\nTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Soda\nOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.\nCrawl Tableau\nTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Teradata\nOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.\nCrawl ThoughtSpot\nOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.\nCrawl Trino\nTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nDisable data access\n:::warning Who can do this? You will need to be an admin user in Atlan to configure these options.\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.\nEnrich Atlan through dbt\nBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.\nextract lineage and usage from Databricks\nOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.\nManage Databricks tags\nYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.\nManage dbt tags\nAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.\nManage Google BigQuery tags\nAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).\nManage Snowflake tags\nYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.\nMine Amazon Redshift\nOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).\nMine Google BigQuery\nOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.\nMine Microsoft Azure Synapse Analytics\nLearn about mine microsoft azure synapse analytics.\nMine Microsoft Power BI\nOnce you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics.\nMine queries through S3\nOnce you have crawled assets from a supported connector, you can mine query history.\nMine Snowflake\nOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.\nMine Teradata\nOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.\norder workflows\nThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.\nPreflight checks for Aiven Kafka\nBefore [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne.\nPreflight checks for Amazon MSK\nBefore [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti.\nPreflight checks for Amazon QuickSight\nThe [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission.\nPreflight checks for Amazon Redshift\nBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.\nPreflight checks for Apache Kafka\nBefore [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection.\nPreflight checks for Confluent Schema Registry\nBefore [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca.\nPreflight checks for Databricks\nBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.\nPreflight checks for DataStax Enterprise\nPreflight checks for DataStax Enterprise\nPreflight checks for dbt\nThis checks if manifest files are present in the provided bucket and prefix.\nPreflight checks for Domo\nAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.\nPreflight checks for Fivetran\nLearn about preflight checks for fivetran.\nPreflight checks for Google BigQuery\nEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).\nPreflight checks for Hive\nBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.\nPreflight checks for Looker\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project#get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management).\nPreflight checks for Metabase\nBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.\nPreflight checks for Microsoft Azure Data Factory\nBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.\nPreflight checks for Microsoft Azure Synapse Analytics\nThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.\nPreflight checks for Microsoft Power BI\nBefore [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run.\nPreflight checks for Microsoft SQL Server\nBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.\nPreflight checks for MicroStrategy\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html#/Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions.\nPreflight checks for Mode\nBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.\nPreflight checks for Monte Carlo\nBefore [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c.\nPreflight checks for MySQL\nBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.\nPreflight checks for Oracle\nBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.\nPreflight checks for PostgreSQL\nBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.\nPreflight checks for PrestoSQL\nBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.\nPreflight checks for Qlik Sense Cloud\nThis check tests for access to datasets and other Qlik objects.\nPreflight checks for Redash\nBefore [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti.\nPreflight checks for Redpanda Kafka\nBefore [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod.\nPreflight checks for Salesforce\nBefore [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co.\nPreflight checks for SAP S/4HANA\nPreflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nPreflight checks for Sigma\nFirst, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission.\nPreflight checks for Sisense\nAtlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.\nPreflight checks for Snowflake\nBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.\nPreflight checks for Soda\nLearn about preflight checks for soda\nPreflight checks for Tableau\nThe [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm#server_info) REST API is used to fetch the `restApiVersion` value.\nPreflight checks for Teradata\nBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.\nPreflight checks for Trino\nBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.\nprovide SSL certificates\nSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).\nSet up a private network link to Amazon Athena\n:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.\nSet up Amazon Redshift\n:::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up AWS Glue\nLearn about set up aws glue.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nSet up Confluent Schema Registry\n:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.\nSet up DataStax Enterprise\nSet up DataStax Enterprise\nSet up dbt Cloud\n:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.\nSet up Domo\n:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.\nSet up Fivetran\nLearn about set up fivetran.\nSet up Google BigQuery\nYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan.\nSet up Hive\n:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.\nSet up IBM Cognos Analytics\n:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.\nSet up Looker\n:::warning Who can do this? You will probably need your Looker administrator to run these commands - you may not have access yourself.\nSet up Microsoft Azure Cosmos DB\nIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db).\nSet up Microsoft Azure Synapse Analytics\nAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.\nSet up Microsoft SQL Server\n:::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself.\nSet up MicroStrategy\nAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.\nSet up Mode\nIf you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email.\nSet up MongoDB\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up on-premises database access\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nSet up on-premises Databricks access\nIn some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises IBM Cognos Analytics access\n:::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,.\nSet up on-premises Kafka access\nIn some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Looker access\nIn some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Tableau access\nIn some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises ThoughtSpot access\nIn some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up Oracle\n:::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself.\nSet up PostgreSQL\n:::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself.\nSet up SAP HANA\n:::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself.\nSet up Sisense\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nSet up Snowflake\n:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.\nSet up Tableau\n:::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself.\nSet up Teradata\n:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.\nSet up ThoughtSpot\n:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.\nSet up Trino\n:::warning Who can do this? You will probably need your Trino administrator to run these commands - you may not have access yourself.\nTroubleshooting data models\nWhat are the known limitations of data models in Atlan?\nTroubleshooting lineage\nSo you've crawled your source, and mined the queries, but lineage is missing. Why?\nupdate column metadata in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.\nUpdate column metadata in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.\nview data models\nOnce you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:.\nWhat does Atlan crawl from Aiven Kafka?\nAtlan crawls and maps the following assets and properties from Aiven Kafka.\nWhat does Atlan crawl from Amazon Athena?\nAtlan crawls and maps the following assets and properties from Amazon Athena.\nWhat does Atlan crawl from Amazon DynamoDB?\nAtlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran.\nWhat does Atlan crawl from Amazon MSK?\nAtlan crawls and maps the following assets and properties from Amazon MSK.\nWhat does Atlan crawl from Amazon MWAA/OpenLineage?\nOnce you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [.\nWhat does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.\nWhat does Atlan crawl from Amazon Redshift?\nAtlan crawls and maps the following assets and properties from Amazon Redshift.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.\nWhat does Atlan crawl from Anomalo?\nOnce you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nOnce you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),.\nWhat does Atlan crawl from Apache Kafka?\nAtlan crawls and maps the following assets and properties from Apache Kafka.\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nAtlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.\nWhat does Atlan crawl from Astronomer/OpenLineage?\nAtlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).\nWhat does Atlan crawl from AWS Glue?\nAtlan crawls and maps the following assets and properties from AWS Glue.\nWhat does Atlan crawl from BigID?\nReference guide for BigID metadata crawled by Atlan.\nWhat does Atlan crawl from Confluent Kafka?\nAtlan crawls and maps the following assets and properties from Confluent Kafka.\nWhat does Atlan crawl from CrateDB?\nComplete list of CrateDB assets and metadata properties extracted by Atlan during crawling\nWhat does Atlan crawl from Databricks?\nAtlan crawls and maps the following assets and properties from Databricks.\nWhat does Atlan crawl from DataStax Enterprise?\nWhat does Atlan crawl from DataStax Enterprise?\nWhat does Atlan crawl from Domo?\nAtlan supports lineage for the following asset types:.\nWhat does Atlan crawl from Fivetran?\nLearn about what does atlan crawl from fivetran?.\nWhat does Atlan crawl from Google BigQuery?\nAtlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).\nWhat does Atlan crawl from Google Cloud Composer/OpenLineage?\nAtlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging.\nWhat does Atlan crawl from Hive?\nAtlan crawls and maps the following assets and properties from Hive.\nWhat does Atlan crawl from IBM Cognos Analytics?\nAtlan crawls and maps the following assets and properties from IBM Cognos Analytics.\nWhat does Atlan crawl from Looker?\nAtlan crawls and maps the following assets and properties from Looker.\nWhat does Atlan crawl from Matillion?\nAtlan crawls and maps the following assets and properties from Matillion.\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nOnce you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.\nWhat does Atlan crawl from Microsoft Azure Data Factory?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Data Factory.\nWhat does Atlan crawl from Microsoft Azure Event Hubs?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs.\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources.\nWhat does Atlan crawl from Microsoft Power BI?\nAtlan crawls and maps the following assets and properties from Microsoft Power BI.\nWhat does Atlan crawl from Microsoft SQL Server?\nAtlan crawls and maps the following assets and properties from Microsoft SQL Server.\nWhat does Atlan crawl from MicroStrategy?\nAtlan crawls and maps the following assets and properties from MicroStrategy.\nWhat does Atlan crawl from MongoDB?\nAtlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.\nWhat does Atlan crawl from Monte Carlo?\nWhat does Atlan crawl from Monte Carlo? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from MySQL?\nAtlan crawls and maps the following assets and properties from MySQL.\nWhat does Atlan crawl from Oracle?\nAtlan crawls and maps the following assets and properties from Oracle.\nWhat does Atlan crawl from PostgreSQL?\nAtlan crawls and maps the following assets and properties from PostgreSQL.\nWhat does Atlan crawl from PrestoSQL?\nAtlan crawls and maps the following assets and properties from PrestoSQL.\nWhat does Atlan crawl from Qlik Sense Cloud?\nAtlan crawls and maps the following assets and properties from Qlik Sense Cloud.\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\nAtlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows.\nWhat does Atlan crawl from Redash?\nAtlan crawls and maps the following assets and properties from Redash.\nWhat does Atlan crawl from Redpanda Kafka?\nAtlan crawls and maps the following assets and properties from Redpanda Kafka.\nWhat does Atlan crawl from Salesforce?\nAtlan only performs GET requests on these five endpoints:.\nWhat does Atlan crawl from SAP ECC?\nWhat does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from SAP S/4HANA?\nWhat does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from Sisense?\nAtlan crawls and maps the following assets and properties from Sisense.\nWhat does Atlan crawl from Snowflake?\nAtlan crawls and maps the following assets and properties from Snowflake.\nWhat does Atlan crawl from Soda?\nAtlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.\nWhat does Atlan crawl from Tableau?\nAtlan crawls and maps the following assets and properties from Tableau.\nWhat does Atlan crawl from Teradata?\nAtlan crawls and maps the following assets and properties from Teradata.\nWhat does Atlan crawl from ThoughtSpot?\nOnce you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters].\nWhat does Atlan crawl from Trino?\nAtlan crawls and maps the following assets and properties from Trino.\nWhat lineage does Atlan extract from Matillion?\nAtlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nAtlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).\nWhat lineage does Atlan extract from Microsoft Power BI?\nThis document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.\nWhen does Atlan become a personal data processor or subprocessor?\nAtlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.\nWhy does the description from Salesforce not show up in Atlan?\nAtlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce)."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nOn this page\nDatabricks\nOverview:\nCatalog Databricks workspaces, databases, schemas, and tables in Atlan. Gain visibility into lineage, usage, and governance for your Databricks assets.\nGet started\nâ\nFollow these steps to connect and catalog Databricks assets in Atlan:\nSet up the connector\nCrawl Databricks assets\nGuides\nâ\nCross-workspace setup\nâ\nSet up cross-workspace extraction\n: Configure a single service principal to crawl metadata from all workspaces within a Databricks metastore.\nLineage & usage\nâ\nExtract lineage and usage from Databricks\n: Extract lineage and usage metrics from your Databricks assets.\nTag management\nâ\nManage Databricks tags\n: Configure and manage tags in Databricks.\nOn-premises\nâ\nSet up on-premises Databricks access\n: Configure Atlan to access on-premises Databricks environments.\nSet up on-premises Databricks lineage extraction\n: Prepare for offline lineage extraction from on-premises Databricks.\nExtract on-premises Databricks lineage\n: Step-by-step instructions for extracting lineage from on-premises Databricks.\nCrawl on-premises Databricks\n: Crawl metadata from on-premises Databricks environments.\nPrivate networking\nâ\nSet up an AWS private network link to Databricks\n: Establish a secure, private network connection to Databricks on AWS.\nSet up an Azure private network link to Databricks\n: Establish a secure, private network connection to Databricks on Azure.\nReferences\nâ\nWhat does Atlan crawl from Databricks\n: Learn about the Databricks assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Databricks\n: Verify prerequisites before setting up the Databricks connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Databricks connection issues and errors.\nTags:\ndatabricks\nconnector\ndata warehouse\nconnectivity\nNext\nSet up Databricks\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-cross-workspace-extraction",
    "content": "Connect data\nData Warehouses\nDatabricks\nCross-workspace Setup\nSet up cross-workspace extraction\nOn this page\nSet up cross-workspace extraction\nEliminate the need for separate crawler configurations by using a single service principal to crawl metadata from all workspaces within a Databricks metastore. This guide walks you through configuring the necessary permissions to enable cross-workspace extraction.\nImportant!\nCross-workspace extraction isn't supported for REST API or JDBC extraction methods.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nA Unity Catalog-enabled Databricks workspace\nAccount admin access to create and manage service principals\nWorkspace admin access to grant permissions across all target workspaces\nAt least one active SQL warehouse in each workspace you intend to crawl\nSet up Databricks authentication\ncompleted with one of the supported authentication methods\nSystem table extraction enabled\nfor lineage and usage extraction\nPermissions required\nâ\nThe service principal needs the following permissions to enable cross-workspace extraction:\nCAN_USE\non SQL warehouses in each workspace\nSELECT\non\nsystem.access.workspace_latest\ntable\nUSE CATALOG\n,\nBROWSE\n, and\nSELECT\non all catalogs you want to crawl\nAdd service principal to all workspaces\nâ\nYou must use a\nsingle, common service principal\nthat has been granted access to\nall\nDatabricks workspaces you intend to crawl within the metastore.\nLog in to your Databricks account console as an account admin\nFrom the left menu, click\nWorkspaces\nand select a workspace\nFrom the tabs along the top, click the\nPermissions\ntab\nIn the upper right, click\nAdd permissions\nIn the\nAdd permissions\ndialog:\nFor\nUser, group, or service principal\n, select your service principal\nFor\nPermission\n, select workspace\nUser\nClick\nAdd\nRepeat steps 2-5 for each workspace you intend to crawl\nGrant permissions\nâ\nConfigure the necessary permissions for the service principal to access and extract metadata from all workspaces within the metastore.\nSQL workspace permissions:\nThe service principal must have usage permissions on\nat least one active SQL warehouse within each workspace\n. The extractor uses the smallest available warehouse to run its discovery queries.\nVia SQL\nVia UI\nConnect to your Databricks workspace using a SQL client or the SQL editor\nRun the following command for each workspace, replacing the placeholders:\nGRANT\nCAN_USE\nON\nWAREHOUSE\n<\nwarehouse_name\n>\nTO\n`\n<service_principal_id>\n`\n;\nReplace\n<warehouse_name>\nwith your actual warehouse name\nReplace\n<service_principal_id>\nwith your service principal's application ID\nExample\nGRANT\nCAN_USE\nON\nWAREHOUSE production\n-\nwarehouse\nTO\n`\n12345678-1234-1234-1234-123456789012\n`\n;\nLog in to your Databricks workspace as a workspace admin\nFrom the left menu, click\nSQL Warehouses\nOn the\nCompute\npage, for each SQL warehouse, click the 3-dot icon and then click\nPermissions\nIn the\nManage permissions\ndialog:\nIn the\nType to add multiple users or groups\nfield, search for and select your service principal\nSelect\nCan use\npermission\nClick\nAdd\nto assign the permission\nSystem table permissions:\nAccess to the system schema is essential for workspace and lineage discovery.\nVia SQL\nVia UI\nConnect to your Databricks workspace using a SQL client or the SQL editor\nRun the following command, replacing the placeholder:\nGRANT\nSELECT\nON\nTABLE\nsystem\n.\naccess\n.\nworkspace_latest\nTO\n`\n<service_principal_id>\n`\n;\nReplace\n<service_principal_id>\nwith your service principal's application ID\nExample\nGRANT\nSELECT\nON\nTABLE\nsystem\n.\naccess\n.\nworkspace_latest\nTO\n`\n12345678-1234-1234-1234-123456789012\n`\n;\nLog in to your Databricks workspace as a workspace admin\nFrom the left menu, click\nCatalog\nIn the\nCatalog Explorer\n, navigate to\nsystem\n>\naccess\nClick on the\nworkspace_latest\ntable\nClick the\nPermissions\ntab and then click\nGrant\nIn the\nGrant permissions\ndialog:\nUnder\nPrincipals\n, select your service principal\nUnder\nPrivileges\n, check\nSELECT\nClick\nGrant\nto apply the permissions\nAsset permissions:\nThe service principal requires permissions to \"see\" and \"read\" the metadata for all data assets you wish to extract. These grants must be applied to all private, public, and shared catalogs that are in scope for crawling.\nImportant!\nFor private catalogs, grant permissions from each workspace. For public catalogs, grant from any workspace.\nVia SQL\nVia UI\nConnect to your Databricks workspace using a SQL client or the SQL editor\nGrant catalog-level permissions (required even when using BROWSE - BROWSE automatically grants access to all schemas and tables):\nGRANT\nUSE\nCATALOG\nON\nCATALOG\n<\ncatalog_name\n>\nTO\n`\n<service_principal_id>\n`\n;\nGRANT\nBROWSE\nON\nCATALOG\n<\ncatalog_name\n>\nTO\n`\n<service_principal_id>\n`\n;\nReplace\n<catalog_name>\nwith your actual catalog name\nReplace\n<service_principal_id>\nwith your service principal's application ID\nIf not using BROWSE, along with catalog permissions, grant additional permissions:\nGrant schema-level permissions:\nGRANT\nUSE\nSCHEMA\nON\nSCHEMA\n<\ncatalog_name\n>\n.\n<\nschema_name\n>\nTO\n`\n<service_principal_id>\n`\n;\nReplace\n<catalog_name>\nand\n<schema_name>\nwith your actual values\nReplace\n<service_principal_id>\nwith your service principal's application ID\nExample\nGRANT\nUSE\nCATALOG\nON\nCATALOG main\nTO\n`\n12345678-1234-1234-1234-123456789012\n`\n;\nGRANT\nBROWSE\nON\nCATALOG main\nTO\n`\n12345678-1234-1234-1234-123456789012\n`\n;\nLog in to your Databricks workspace as a workspace admin\nFrom the left menu, click\nCatalog\nIn the\nCatalog Explorer\n, navigate to the catalog you want to grant permissions on (for example,\nmain\n)\nClick the\nPermissions\ntab and then click\nGrant\nIn the\nGrant permissions\ndialog:\nUnder\nPrincipals\n, select your service principal\nUnder\nPrivileges\n, check the following permissions:\nUSE CATALOG\nUSE SCHEMA\nBROWSE\nSELECT\nClick\nGrant\nto apply the permissions\nRepeat steps 3-5 for each catalog you want to crawl in Atlan\nNeed help?\nâ\nCheck\nTroubleshooting Databricks connectivity\nfor common issues\nContact Atlan support\nfor help with setup or integration\nNext steps\nâ\nCrawl Databricks\nTags:\ndatabricks\nsetup\ncross-workspace-extraction\nPrevious\nSet up Databricks\nNext\nCrawl Databricks\nPrerequisites\nPermissions required\nAdd service principal to all workspaces\nGrant permissions\nNeed help?\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nCrawl Databricks Assets\nCrawl Databricks\nOn this page\nCrawl Databricks\nOnce you have configured the\nDatabricks access permissions\n, you can establish a connection between Atlan and your Databricks instance. (If you are also using\nAWS PrivateLink\nor\nAzure Private Link\nfor Databricks, you will need to set that up first, too.)\nTo crawl metadata from your Databricks instance, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Databricks as your source:\nIn the top right corner of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nDatabricks Assets\n, and click\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly. Next, select an authentication method:\nIn\nJDBC\n, you will need a\npersonal access token and HTTP path for authentication\n.\nIn\nAWS Service\n, you will need a\nclient ID and client secret for AWS service principal authentication\n.\nIn\nAzure Service\n, you will need a\ntenant ID, client ID, and client secret for Azure service principal authentication\n.\nIn\nOffline\nextraction, you will need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nJDBC\nâ\nTo enter your Databricks credentials:\nFor\nHost\n, enter the hostname,\nAWS PrivateLink endpoint\n, or\nAzure Private Link endpoint\nfor your Databricks instance.\nFor\nPort\n, enter the port number of your Databricks instance.\nFor\nPersonal Access Token\n, enter the access token you generated when\nsetting up access\n.\nFor\nHTTP Path\n, enter one of the following:\nA path starting with\n/sql/1.0/warehouses\nto use the\nDatabricks SQL warehouse\n.\nA path starting with\nsql/protocolv1/o\nto use the\nDatabricks interactive cluster\n.\nClick\nTest Authentication\nto confirm connectivity to Databricks using these details.\nOnce successful, at the bottom of the screen click\nNext\n.\ndanger\nMake sure your Databricks instance (SQL warehouse or interactive cluster) is up and running, otherwise the\nTest Authentication\nstep times out.\nAWS service principal\nâ\nTo enter your Databricks credentials:\nFor\nHost\n, enter the hostname or\nAWS PrivateLink endpoint\nfor your Databricks instance.\nFor\nPort\n, enter the port number of your Databricks instance.\nFor\nClient ID\n, enter the\nclient ID for your AWS service principal\n.\nFor\nClient Secret\n, enter the\nclient secret for your AWS service principal\n.\nClick\nTest Authentication\nto confirm connectivity to Databricks using these details.\nOnce successful, at the bottom of the screen click\nNext\n.\nAzure service principal\nâ\nTo enter your Databricks credentials:\nFor\nHost\n, enter the hostname or\nAzure Private Link endpoint\nfor your Databricks instance.\nFor\nPort\n, enter the port number of your Databricks instance.\nFor\nClient ID\n, enter the\napplication (client) ID for your Azure service principal\n.\nFor\nClient Secret\n, enter the\nclient secret for your Azure service principal\n.\nFor\nTenant ID\n, enter the\ndirectory (tenant) ID for your Azure service principal\n.\nClick\nTest Authentication\nto confirm connectivity to Databricks using these details.\nOnce successful, at the bottom of the screen click\nNext\n.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from Databricks. This method uses Atlan's databricks-extractor tool to fetch metadata. You need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\noutput/databricks-example/catalogs/success/result-0.json\n,\noutput/databricks-example/schemas/{{catalog_name}}/success/result-0.json\n,\noutput/databricks-example/tables/{{catalog_name}}/success/result-0.json\n, and similar files.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Databricks. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Databricks data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the Databricks connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you don't specify any user or group, nobody can manage the connection - not even admins.\n(Optional) To prevent users from querying any Databricks data, change\nEnable SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any Databricks data, change\nEnable Data Preview\nto\nNo\n.\n(Optional) To prevent users from running large queries, change\nMax Row Limit\nor keep the default selection.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the Databricks crawler, you can further configure it.\nSystem tables extraction method\nâ\nThe system metadata extraction method is only available for\nUnity Catalog-enabled workspaces\n. It provides access to detailed metadata from system tables and supports all three authentication types. You can extract metadata from your Databricks workspace using this method. Follow these steps:\nSet up authentication using one of the following:\nPersonal access token\nAWS service principal\nAzure service principal\nThe default options can work as is. You may choose to override the defaults for any of the remaining options:\nFor\nAsset selection\n, select a filtering option:\nFor\nSQL warehouse\n, click the dropdown to select the SQL warehouse you want to configure.\nTo select the assets you want to include in crawling, click\nInclude by hierarchy\nand filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.)\nTo have the crawler include\nDatabases\n,\nSchemas\n, or\nTables & Views\nbased on a naming convention, click\nInclude by regex\nand specify a regular expression - for example, specifying\nATLAN_EXAMPLE_DB.*\nfor\nDatabases\nincludes all the matching databases and their child assets.\nTo select the assets you want to exclude from crawling, click\nExclude by hierarchy\nand filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.)\nTo have the crawler ignore\nDatabases\n,\nSchemas\n, or\nTables & Views\nbased on a naming convention, click\nExclude by regex\nand specify a regular expression - for example, specifying\nATLAN_EXAMPLE_TABLES.*\nfor\nTables & Views\nexcludes all the matching tables and views.\nClick\n+\nto add more filters. If you add multiple filters, assets are crawled based on matching\nall\nthe filtering conditions you have set.\nTo\nimport tags from Databricks to Atlan\n, change\nImport Tags\nto\nYes\n. Note that you must have a\nUnity Catalog-enabled workspace\nto import Databricks tags in Atlan.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nIncremental extraction\nPublic preview\nâ\nToggle incremental extraction, for a faster and more efficient metadata extraction.\nJDBC extraction method\nâ\nThe JDBC extraction method uses JDBC queries to extract metadata from your Databricks instance. This was the original extraction method provided by Databricks. This extraction method is only supported for\npersonal access token authentication\n.\nYou can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in the\nExclude regex for tables & views\nfield.\nFor\nView Definition Lineage\n, keep the default\nYes\nto generate upstream lineage for views based on the tables referenced in the views or click\nNo\nto exclude from crawling.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nAdvanced\nto further configure the crawler:\nTo enable or disable schema-level filtering at source, click\nEnable Source Level Filtering\nand select\nTrue\nto enable it or\nFalse\nto disable it.\nREST API extraction method\nâ\nThe REST API extraction method uses\nUnity Catalog\nto extract metadata from your Databricks instance. This extraction method is supported for all three authentication options:\npersonal access token\n,\nAWS service principal\n, and\nAzure service principal\n.\nThis method is only supported by\nUnity Catalog-enabled\nworkspaces.\nIf you enable an existing workspace, you also need to\nupgrade your tables and views to Unity Catalog\n.\nWhile REST APIs are used to extract metadata, JDBC queries are still used for querying purposes.\nYou can override the defaults for any of these options:\nChange the extraction method under\nExtraction method\nto\nREST API\n.\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo\nimport tags from Databricks to Atlan\n, change\nImport Tags\nto\nYes\n. Note that you must have a\nUnity Catalog-enabled workspace\nto import Databricks tags in Atlan.\nFor\nSQL warehouse\n, click the dropdown to select the SQL warehouse you have configured.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nFollow these steps to run the Databricks crawler:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up cross-workspace extraction\nNext\nSet up on-premises Databricks access\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-access",
    "content": "Connect data\nData Warehouses\nDatabricks\nOn-premises Setup\nSet up on-premises Databricks access\nOn this page\nSet up on-premises Databricks access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Databricks instance details, including credentials.\nIn some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your on-premises Databricks instance, you will need to use Atlan's databricks-extractor tool.\nDid you know?\nAtlan uses exactly the same databricks-extractor behind the scenes when it connects to Databricks in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the databricks-extractor tool\nâ\nTo get the databricks-extractor tool:\nRaise a support ticket\nto get the link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to crawl Databricks:\nsudo docker load -i /path/to/databricks-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a\nDocker compose file\nfor the databricks-extractor tool.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises Databricks instance.\nThe file is\ndocker-compose.yaml\n.\nDefine Databricks connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your Databricks connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises Databricks instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nconnection-name:\n<<: *extract\nenvironment:\n<<: *databricks-defaults\nINCLUDE_FILTER: '{\"DB_1\": [], \"DB_2\": [\"SCHEMA_1\", \"SCHEMA_2\"]}'\nEXCLUDE_FILTER: '{\"DB_1\": [\"SCHEMA_1\", \"SCHEMA_2\"]}'\nTEMP_TABLE_REGEX: '.*temp.*|.*tmp.*|.*TEMP.*|.*TMP.*'\nSYSTEM_SCHEMA_REGEX: '^information_schema$'\nvolumes:\n- ./output/connection-name:/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\ntells the databricks-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nINCLUDE_FILTER\n-  specify the databases and schemas from which you want to extract metadata. Remove this line if you want to extract metadata from all databases and schemas.\nEXCLUDE_FILTER\n-  specify the databases and schemas you want to exclude from metadata extraction. This will take precedence over\nINCLUDE_FILTER\n. Remove this line if you do not want to exclude any databases or schemas.\nTEMP_TABLE_REGEX\n-  specify a regular expression for excluding temporary tables. Remove this line if you do not want to exclude any temporary tables.\nSYSTEM_SCHEMA_REGEX\n-  specify a regular expression for excluding system schemas. If unspecified,\nINFORMATION_SCHEMA\nwill be excluded from the extracted metadata by default.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Databricks connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your Databricks connections, you will need to provide a Databricks configuration file.\nThe Databricks configuration is a\n.ini\nfile with the following format:\n[DatabricksConfig]\nhost = <host>\nport = <port>\n# seconds to wait for a response from the server\ntimeout = 300\n# Databricks authentication type. Options: personal_access_token, aws_service_principal\nauth_type = personal_access_token\n# Required only if auth_type is personal_access_token.\n[PersonalAccessTokenAuth]\npersonal_access_token = <personal_access_token>\n# Required only if auth_type is aws_service_principal.\n[AWSServicePrincipalAuth]\nclient_id = <client_id>\nclient_secret = <client_secret>\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Databricks credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets:\ndatabricks_config:\nfile: ./databricks.ini\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a sub-section of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the Databricks configuration file:\nsudo docker secret create databricks_config path/to/databricks.ini\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\ndatabricks_config:\nexternal: true\nname: databricks_config\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local Databricks configuration file.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\ndatabricks_config:\nexternal: true\nname: databricks_config\nx-templates:\n# ...\nservices:\ndatabricks-example:\n<<: *extract\nenvironment:\n<<: *databricks-defaults\nINCLUDE_FILTER: '{\"DB_1\": [], \"DB_2\": [\"SCHEMA_1\", \"SCHEMA_2\"]}'\nEXCLUDE_FILTER: '{\"DB_1\": [\"SCHEMA_1\", \"SCHEMA_2\"]}'\nTEMP_TABLE_REGEX: '.*temp.*|.*tmp.*|.*TEMP.*|.*TMP.*'\nSYSTEM_SCHEMA_REGEX: '^information_schema$'\nvolumes:\n- ./output/databricks-example:/output\nsecrets:\n- databricks_config\nIn this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The\ndatabricks_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\ndatabricks-example\n. You can use any meaningful name you want.\nThe\n<<: *databricks-defaults\nsets the connection type to Databricks.\nThe\n./output/databricks-example:/output\nÂ line tells the extractor where to store results. In this example, the extractor will store results in theÂ\n./output/databricks-example\ndirectory on the local file system. We recommend you output the extracted metadata for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\ndata\ncrawl\nPrevious\nCrawl Databricks\nNext\nCrawl on-premises Databricks\nPrerequisites\nGet the compose file\nDefine Databricks connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-an-aws-private-network-link-to-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nPrivate Network Setup\nSet up an AWS private network link to Databricks\nOn this page\nSet up an AWS private network link to Databricks\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Databricks and Atlan.\nWho can do this?\nYou will need Databricks support, and probably your Databricks administrator involved   -  you may not have access or the tools to run these tasks.\nPrerequisites\nâ\nDatabricks must be set up on the E2 version of the platform and Enterprise pricing tier.\nYour Databricks workspace must be in an AWS region that supports the E2 version of the platform, and not the\nus-west-1\nregion. Your Databricks workspace must also be hosted in the same region as Atlan.\nYour Databricks workspace must use customer-managed VPC. (Note that you cannot update an existing Databricks-managed VPC to a customer-managed VPC.)\nFor all details, see\nDatabricks documentation\n.\nNotify Atlan support\nâ\nOnce setup is completed,\nprovide Atlan support\nwith the following information:\nThe AWS region of your Databricks instance.\nThere are additional steps that Atlan will need to complete:\nCreating a security group\nCreating an endpoint\nOnce the Atlan team has confirmed that the configuration is ready, please continue with the remaining steps.\nAccept the endpoint connection request\nâ\nYou can either:\nAccept the endpoint connection request from Atlan via\nAPI\n.\nAccept the endpoint connection request from Atlan from the\nDatabricks console\n.\nOnce the endpoint connection is accepted, Atlan support will finish the configuration on the Atlan side.\nWhen you use this endpoint in the configuration for\ncrawling Databricks\n, Atlan will connect to Databricks over AWS PrivateLink.\nTags:\napi\nrest-api\ngraphql\nPrevious\nSet up on-premises Databricks lineage extraction\nNext\nSet up an Azure private network link to Databricks\nPrerequisites\nNotify Atlan support\nAccept the endpoint connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/extract-lineage-and-usage-from-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nLineage and Usage\nHow to extract lineage and usage from Databricks\nOn this page\nextract lineage and usage from Databricks\nOnce you have\ncrawled assets from Databricks\n, you can retrieve lineage from\nUnity Catalog\nand\nusage and popularity metrics\nfrom\nquery history\nor system tables. This is supported for all\nthree authentication methods\n: personal access token, AWS service principal, and Azure service principal.\nBoth Atlan and Databricks strongly recommend using the system tables method to extract\nlineage\nand\nusage and popularity metrics\nfrom Databricks.\ndanger\nUsage and popularity metrics\ncan be retrieved for all Databricks users. However, your Databricks workspace must be\nUnity Catalog-enabled\nfor the retrieval of lineage and usage and popularity metrics to succeed. You may also need to\nupgrade existing tables and views to Unity Catalog\n, as well as reach out to your Databricks account executive to enable lineage in Unity Catalog. (As of publishing, the feature is still in preview from Databricks on AWS and Azure.)\nTo retrieve lineage and usage from Databricks, rev\niew the\norder of operations\nand then complete the following steps.\nSelect the extractor\nâ\nTo select the Databricks lineage and usage extractor:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the filters along the top, click\nMiner\n.\nFrom the list of packages, select\nDatabricks Miner\nand click on\nSetup Workflow\n.\nConfigure the lineage extractor\nâ\nChoose your lineage extraction method:\nIn\nREST API\n, Atlan connects to your database and extracts lineage directly.\nIn\nOffline\n, you will need to first\nextract lineage yourself\nand\nmake it available in S3\n.\nIn\nSystem Table\n, Atlan connects to your database and\nqueries system tables\nto extract lineage directly.\nREST API\nâ\nTo configure the Databricks lineage extractor:\nFor\nConnection\n, select the connection to extract. (To select a connection,\nthe crawler\nmust have already run.)\nClick\nNext\nto proceed.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor extracting lineage from Databricks This method uses Atlan's databricks-extractor tool to extract lineage. You will need to first\nextract lineage yourself\nand\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nConnection\n, select the connection to extract. (To select a connection,\nthe crawler\nmust have already run.)\nFor\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\nextracted-lineage/result-0.json\n,\nextracted-query-history/result-0.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nSystem table\nâ\nTo configure the Databricks lineage extractor:\nFor\nConnection\n, select the connection to extract. (To select a connection,\nthe crawler\nmust have already run.)\n*\nExtraction Catalog Type\n:\nDefault\n: Select to fetch lineage from the system catalog and\naccess\nschema.\nCloned_catalog\n: Select to fetch lineage from a cloned catalog and schema.\nBefore proceeding, make sure the following prerequisites are met:\nYou have already created cloned views named\ncolumn_lineage\nand\ntable_lineage\nin your schema.\nIf not, follow the steps in\nCreate cloned views of system tables\n.\nThe\natlan-user\nmust have\nSELECT\npermissions on both views to access lineage data.\nThen, provide values for the following fields:\nCloned Catalog Name\nâ Catalog containing the cloned views.\nCloned Schema Name\nâ Schema containing the cloned views.\nFor\nSQL Warehouse ID\n, enter the\nID you copied from your SQL warehouse\n.\nClick\nNext\nto proceed.\n(Optional) Configure the usage extractor\nâ\nAtlan extracts\nusage and popularity metrics\nfrom:\nQuery history\nSystem tables\nThis feature is currently limited to queries on SQL warehouses   -  queries on interactive clusters are not supported. Additionally, expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the\nDatabricks APIs\n.\nTo configure the Databricks usage and popularity extractor:\nFor\nFetch Query History and Calculate Popularity\n, click\nYes\nto retrieve\nusage and popularity metrics\nfor your Databricks assets.\nFor\nPopularity Extraction Method\n: Choose one of the following methods to extract usage and popularity metrics::\nClick\nREST API\nto extract usage and popularity metrics from query history.\nClick\nSystem table\nto extract metrics directly from system tables:\nExtraction catalog type for popularity\n: Choose where to fetch popularity data from:\nDefault\n: Uses the system catalog and\nquery\nschema to fetch popularity metrics.\nCloned_catalog\n: Select to fetch popularity from cloned views in a separate catalog and schema.\nBefore proceeding:\nThe\nquery_history\nview must exist in the provided schema.\nThe\natlan-user\nmust have\nSELECT\npermission on the view.\nThen provide:\nCloned Catalog Name\nâ The catalog that contains the\nquery_history\nview.\nCloned Schema Name\nâ The schema that contains the\nquery_history\nview.\nFor more information, see\nCreate cloned views of system tables\n.\nFor\nSQL Warehouse ID\n, enter the\nID you copied from your SQL warehouse\n.\nConfigure the usage extractor: Â Â\nFor\nPopularity Window (days)\n, 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days.\nFor\nStart time\n, choose the earliest date from which to mine query history. If you're using the\noffline extraction method\nto extract query history from Databricks, skip to the next step.\nFor\nExcluded Users\n, type the names of users to be excluded while calculating\nusage metrics\nfor Databricks assets. Press\nenter\nafter each name to add more names.Â\ndanger\nIf running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic\nhere\n.\nRun the extractor\nâ\nTo run the Databricks lineage and popularity extractor, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n. This isÂ currently only supported when using REST API and offline extraction methods. If you're using system tables, skip to step 2.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the extractor has completed running, you will see lineage for Databricks assets! ð\nTags:\nconnectors\ndata\ncrawl\napi\nauthentication\nPrevious\nSet up an Azure private network link to Databricks\nNext\nHow to extract on-premises Databricks lineage\nSelect the extractor\nConfigure the lineage extractor\n(Optional) Configure the usage extractor\nRun the extractor"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/manage-databricks-tags",
    "content": "Connect data\nData Warehouses\nDatabricks\nTag management\nManage Databricks tags\nOn this page\nManage Databricks tags\nYou must have a\nUnity Catalog-enabled workspace\nto import Databricks tags in Atlan.\nAtlan enables you to import your\nDatabricks tags\n, update your Databricks assets with the imported tags, and push the tag updates back to Databricks:\nImport tags\n-  crawl Databricks tags from Databricks to Atlan\nReverse sync\n-  sync Databricks tag updates from Atlan to Databricks\nOnce you've\nimported your Databricks tags\nto Atlan:\nYour Databricks assets in Atlan will be automatically enriched with their Databricks tags.\nImported Databricks tags\nwill be mapped to corresponding\nAtlan tags\nthrough case-insensitive name match   -  multiple Databricks tags can be matched to a single tag in Atlan.\nYou can also\nattach Databricks tags\n, including tag values, to your Databricks assets in Atlan   -  allowing you to categorize your assets at a more granular level.\nYou can\nfilter your assets\nby Databricks tags and tag values.\nYou can\nenable reverse sync\nto push any tag updates for your Databricks assets back to Databricks   -  including tag values added to assets in Atlan.\nDid you know?\nEnabling reverse sync will only update existing tags in Databricks. It will neither create nor delete any tags in Databricks.\nPrerequisites\nâ\nYou must have a\nUnity Catalog-enabled workspace\nand SQL warehouse configured to import Databricks tags in Atlan.\nBefore you can import tags from andÂ push tag updates to Databricks using\npersonal access token\n,\nAWS service principal\n, or\nAzure service principal\nauthentication, you will need to do the following:\nEnsure that you have a\nUnity Catalog-enabled workspace\nand a SQL warehouse configured.\nCreate tags\nor have existing tags in Databricks.\nGrant permissions\nto import tags from and push tag updates to Databricks.\nImport Databricks tags to Atlan\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to import Databricks tags to Atlan. You will also need to work with your Databricks administrator to grantÂ permissions to import tags from Databricks   -  you may not have access yourself.\nYou can import your Databricks tags to Atlan through one-way tag sync. The synced Databricks tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Databricks assets will be enriched with their synced tags from Databricks.\nTo import Databricks tags to Atlan, you can either:\nCreate a new Databricks workflow and\nconfigure the crawler\nto import tags.\nModify the crawler's configuration\nfor an existing Databricks workflow to change\nImport Tags\nto\nYes\n. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags.\nOnce the crawler has completed running, tags imported from Databricks will be available to use for\ntagging assets\n! ð\nView Databricks tags in Atlan\nâ\nOnce you've imported your Databricks tags, you will be able to view and manage your Databricks tags in Atlan.\nTo view Databricks tags:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading of the _Governance cente_r, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter tags by source type. Click\nDatabricks\nto filter for tags imported from Databricks.\nFrom the left menu under\nTags\n, select a synced tag.\nIn the\nOverview\nsection, you can view a total count of synced Databricks tags. To the right of\nOverview\n, click\nSynced tags\nto view additional details   -  including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced.\n(Optional) Click the\nLinked assets\ntab to view linked assets for your Databricks tag.\n(Optional) In the top right, click the pencil icon to add a description and change the\ntag icon\n. You cannot rename tags synced from Databricks.\nPush tag updates to Databricks\nâ\nWho can do this?\nAny\nadmin or member user\nin Atlan can configure reverse sync for tag updates to Databricks. You will also need to work with your Databricks administrator to grant additional permissions to push updates   -  you may not have access yourself.\nYou can enable reverse sync for your imported Databricks tags in Atlan and push all tag updates for your Databricks assets back to source. Once you have enabled reverse sync, any Databricks assets with tags updated in Atlan will also be updated in Databricks.\nTo enable reverse sync for imported Databricks tags:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading of the _Governance cente_r, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter tags by source type. Click\nDatabricks\nto filter for tags imported from Databricks.\nIn the left menu under\nTags\n, select a synced Databricks tag   -  synced tags will display the Databricks icon next to the tag name.Â\nOn your selected tag page, to the right of\nOverview\n, click\nSynced tags\n.\nUnder\nSynced tags\n, in the upper right, turn on\nEnable reverse sync\nto synchronize tag updates from Atlan to Databricks.\nIn the corresponding confirmation dialog, click\nYes, enable it\nto enable reverse tag sync or click\nCancel\n.\nNow when you\nattach Databricks tags\nto your Databricks assets in Atlan, these tag updates will also be pushed to Databricks! ð\nDid you know?\nEnabling reverse sync will\nnot\ntrigger any updates in Databricks until synced tags are attached to Databricks assets in Atlan.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nHow to extract on-premises Databricks lineage\nNext\nWhat does Atlan crawl from Databricks?\nPrerequisites\nImport Databricks tags to Atlan\nView Databricks tags in Atlan\nPush tag updates to Databricks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/references/what-does-atlan-crawl-from-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nReferences\nWhat does Atlan crawl from Databricks?\nOn this page\nWhat does Atlan crawl from Databricks?\nAtlan crawls and maps the following assets and properties from Databricks.\ninfo\nThe following properties aren't crawled by the\nSystem tables\nextraction method:\nTable properties\n:\npartitionList\n,\npartitionCount\nColumn properties\n:\nmaxLength\n,\nprecision\nDatabases\nâ\nAtlan maps databases from Databricks to its\nDatabase\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_CATALOG\nname\nAsset profile and overview sidebar\nSCHEMA_COUNT\nschemaCount\nAPI only\nSchemas\nâ\nAtlan maps schemas from Databricks to its\nSchema\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_SCHEMA\nname\nAsset profile and overview sidebar\nTABLE_COUNT\ntableCount\nAsset preview and profile\nVIEW_COUNT\nviewsCount\nAsset preview and profile\nTABLE_CATALOG\ndatabaseName\nAsset preview and profile\nTables\nâ\nAtlan maps tables from Databricks to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nAsset profile and overview sidebar\nREMARKS, DESCRIPTION\ndescription\nAsset profile and overview sidebar\nCOLUMN_COUNT\ncolumnCount\nAsset profile and overview sidebar\nLOCATION\nexternalLocation\nOverview sidebar\nFORMAT\nexternalLocationFormat\nOverview sidebar\nOWNER\nCreated (in Databricks)\nProperties sidebar\nCREATEDAT\nsourceCreatedAt\nProperties sidebar\nUPDATED_BY\nLast updated\nProperties sidebar\nLASTMODIFIED\nsourceUpdatedAt\nProperties sidebar\nPARTITIONS\nisPartitioned\n,\npartitionCount\n,\npartitionList\nAPI only\nViews\nâ\nAtlan maps views from Databricks to its\nView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nAsset profile and overview sidebar\nREMARKS\ndescription\nAsset profile and overview sidebar\nCOLUMN_COUNT\ncolumnCount\nAsset profile and overview sidebar\nCREATETAB_STMT\ndefinition\nAsset profile and overview sidebar\nOWNER\nCreated (in Databricks)\nProperties sidebar\nCREATEDAT\nsourceCreatedAt\nProperties sidebar\nUPDATED_BY\nLast updated\nProperties sidebar\nLASTMODIFIED\nsourceUpdatedAt\nProperties sidebar\nMaterialized views\nâ\nAtlan maps materialized views from Databricks to its\nMaterialisedView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nAsset profile and overview sidebar\nREMARKS\ndescription\nAsset profile and overview sidebar\nCOLUMN_COUNT\ncolumnCount\nAsset profile and overview sidebar\nCREATETAB_STMT\ndefinition\nAsset profile and overview sidebar\nOWNER\nCreated (in Databricks)\nProperties sidebar\nCREATEDAT\nsourceCreatedAt\nProperties sidebar\nUPDATED_BY\nLast updated\nProperties sidebar\nLASTMODIFIED\nsourceUpdatedAt\nProperties sidebar\nColumns\nâ\nDid you know?\nTo help you work seamlessly with STRUCT data types, Atlan supports\nnested columns up to level 30\nin Databricks. You can view these columns in the Tree view or the asset sidebar of your table assets, and also explore child columns of STRUCTs nested within MAPs or ARRAYs. However,\nlineage\nfor nested columns\nisn't\nsupported.\nAtlan maps columns from Databricks to its\nColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nPRIMARY KEY\nisPrimary\nAsset preview and filter, overview sidebar\nFOREIGN KEY\nisForeign\nAsset preview and filter, overview sidebar\nCOLUMN_NAME\nname\nAsset profile and overview sidebar\nREMARKS\ndescription\nAsset profile and overview sidebar\nORDINAL_POSITION\norder\nAsset profile\nTYPE_NAME\ndataType\nAsset profile and overview sidebar\nPARTITION_INDEX\nisPartition\nAsset preview and profile\nNULLABLE\nisNullable\nAPI only\nCHAR_OCTET_LENGTH\nmaxLength\nAPI only\nDECIMAL_DIGITS\nprecision\nAPI only\nTags:\ndata\ncrawl\napi\nPrevious\nManage Databricks tags\nNext\nPreflight checks for Databricks\nDatabases\nSchemas\nTables\nViews\nMaterialized views\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/troubleshooting/troubleshooting-databricks-connectivity",
    "content": "Connect data\nData Warehouses\nDatabricks\nTroubleshooting\nTroubleshooting Databricks connectivity\nOn this page\nTroubleshooting Databricks connectivity\nDid you know?\nThe documentation refers to both\nSQL endpoint\nand\ninteractive cluster\nas\ncompute engine\nbelow.\nDoes Atlan consider expensive queries and compute costs?\nâ\nNo, Atlan doesn't factor in expensive queries or compute costs due to limitations in the Databricks APIs, which don't expose this information.\nHow does Atlan calculate popularity for Databricks assets?\nâ\nAtlan calculates popularity for\ntables\n,\nviews\n, and\ncolumns\nin Databricks by analyzing query execution data. It retrieves query history from the\nsystem.query.history\ntable and specifically filters for\nexecution_status = 'FINISHED'\nand\nstatement_type = 'SELECT'\nto determine how frequently assets are accessed.\nHow to debug test authentication and preflight check errors?\nâ\nHostname resolution error\nProvided Host name cannot be resolved via DNS, please check and try again.\nThe hostname you have provided can't be resolved through DNS. Check that the hostname is correct.\nVerify that the DNS settings have been configured properly.\nInvalid client ID or secret\nProvided Client ID is invalid, please check and try again.\nThe client ID or secret you have provided is either invalid or no longer working. Follow the steps for\nAWS\nor\nAzure\nsetup to generate new credentials.\nInvalid tenant ID\nProvided tenant ID is invalid, please check and try again.\nThe tenant ID you have provided is incorrect.\nEnsure that the tenant ID you have provided corresponds to the one in your\nMicrosoft Entra ID application\n.\nUnity Catalog not linked\nConfigured Databricks instance doesn't have Unity Catalog linked. Please choose JDBC extraction instead of REST API in Atlan.\nIf you have not set up Unity Catalog in your Databricks workspace, you can\nchange the extraction method to JDBC\ninstead of REST API to crawl your Databricks assets in Atlan.\nConnection timeout\nFailed to connect to Databricks (connection timed out). Please check your host and port and try again.\nThe connection to the Databricks instance has timed out.\nVerify that the host and port are correct.\nCheck that no firewall rules or network issues are blocking the connection.\nInvalid HTTP path\nProvided HTTP path is invalid, please check and try again.\nThe\nHTTP path\nyou have provided is invalid.\nEnsure that the endpoint is properly configured and accessible, and the warehouse ID in the HTTP path is correct.\nInvalid personal access token\nPAT token is invalid, please check and try again.\nThe personal access token used for authentication is invalid.\nEnsure that the token is valid and neither deleted nor expired.\nYou can also generate a new\npersonal access token\n, if needed.\nInsufficient permisions for crawling metadata\nUser doesn't have access to any schemas / dbs, please check the accesses provided to the atlan user and try again.\nCheck that the service principal or the user who's PAT token is being used has the necessary permissions provided. Refer to the\nsetup doc\nto understand permissions required for different auth types.\nInsufficient permisions for some of the included crawling metadata\nWarning, user doesn't have access to the following objects anymore, or the objects no longer exist on the source!, check failed for ...\nuser doesn't have access to one or more db objects from the include filter, (such as catalogs / schemas).\nYou can either remove these objects from the include filter if they no longer exist on the source.\nOr check that the service principal or the user who's PAT token is being used has the necessary permissions provided. Refer to the\nsetup doc\nto understand permissions required for different auth types.\nInsufficient permisions to crawl tags\nUser doesn't have access to the following system tables\nCheck that you have sufficient permissions provided for the\ntags extraction\n.\nUser doesn't have permission to access warehouses\nplease check your credentials and warehouse access\nCheck that the configured user / service principal has\nCAN_USE\non the configured SQL warehouse.\nUnable to access query history from the source, user doesn't have the access\nCheck the\npermissions required\nfor the system tables based lineage extraction are provided.\nSystem table extraction checks failing with\nUser doesn't have access to the following system tables\nCheck the\npermissions required\nfor the system tables based extraction.\nGeneral connection failure\nUnable to connect to the configured Databricks instance, please check your credentials and configs and then try again. If the problem persists, contact\n[email protected]\n.\nCheck that you have entered the host and port correctly.\nVerify that the credentials for the connection are correct.\nCheck that your Databricks instance is properly configured and available.\nIf the problem still persists after verifying all of the previous steps,\ncontact Atlan support\n.\nWhy does the workflow take longer than usual in the extraction step?\nâ\nCertain Databricks runtime versions don't have an easy way to extract some metadata (for example partitioning, table_type, and format). Extra operations must be performed to retrieve these, resulting in slower performance.\nIf you aren't already, you may want to try the\nUnity Catalog extraction method\n.\nWhy is some metadata missing?\nâ\nWhen using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata.\nCurrently, some metadata can't be extracted from Databricks:\nMetadata\nJDBC\nREST API\nSystem Tables\nViewCount\nand\nTableCount\n(on schemas)\nâ\nâ\nâ\nRowCount\n(on tables and views)\nâ\nâ\nâ\nTABLE_KIND\n(on tables and views)\nâ\nâ\nâ\nPARTITION_STRATEGY\n(on tables and views)\nâ\nâ\nâ\nCONSTRAINT_TYPE\n(on columns)\nâ\nâ\nâ\nPartition key (on columns)\nâ\nâ\nâ\nTable partitioning information\nâ\nâ\nâ\nBYTES\n,\nSIZEINBYTES\n(table size)\nâ\nâ\nâ\nThe team is exploring ways to bring this metadata into Atlan if Databricks supports extraction of the metadata.\nWhy doesn't my SQL work when querying Databricks?\nâ\nAtlan currently supports\nSparkSQL on Databricks runtime 7.x and above\n.\nCan I use Atlan when the Databricks compute engine isn't running?\nâ\nAtlan needs the Databricks compute engine to be running for two activities:\nCrawling assets (normal and scheduled run)\nQuerying assets (including data previews)\nIf you don't need to perform the activities listed, your experience shouldn't be affected.\nIn any other case, you'll get a downgraded experience on Atlan if the compute engine isn't running. Queries won't work as expected and a scheduled workflow might fail after a couple of retries.\nThe team recommends turning off the\nTerminate after x minutes of inactivity\noption in your cluster to avoid these problems. If you have this turned on, any of the listed activities triggers the cluster to come back online within about 30 seconds.\nWhy can't I see all the assets on Atlan that are available in Databricks?\nâ\nHave you\nexcluded the database or schema when crawling\n?\nDoes the\nDatabricks user you configured for crawling\nhave access to these other assets?\nWhy is the test authentication taking so long?\nâ\nPlease check the state of the compute engine. It must be in a\nrunning\nstate for all operations, including authentication.\nWhat limitations are there with the REST API (Unity Catalog) extraction method?\nâ\nCurrently, schema-level filtering and retrieving table partitioning information aren't supported.\nWhy has my workflow started to fail when it worked before?\nâ\nThis can happen if\nthe PAT you configured\nthe workflow with has since expired.\nYou will need to create a new PAT in Databricks, and then\nmodify the workflow configuration in Atlan\nwith this new PAT.\nIf you are unable to update the PAT, pause the workflow and\nreach out to us\n.\nHow do I migrate to Unity Catalog?\nâ\nCurrently Unity Catalog is in a public preview state.\nThe Databricks team is working on an automated migration to Unity Catalog.\nCurrently you must migrate individual tables manually.\nWhy are some notebooks missing from metadata extraction?\nâ\nNotebooks stored inside hidden directories (names starting with \".\" such as\n.hidden_dir/\n) are generally not returned by the\n/api/2.0/workspace/list\nAPI endpoint. This may cause missing notebook details in Atlan.\nWhy is metadata missing for some Databricks entities?\nâ\nThe Databricks APIs used provide data only within a single configured workspace. If an entity used in lineage creation exists outside this workspace, its details won't be available via these APIs.\nDoes Atlan support nested columns beyond level 30?\nâ\nAtlan doesn't support nested columns beyond 30 levels for complex types such as Struct, Array, and Map. Columns exceeding this nesting depth aren't parsed. Instead, the deepest column level gets assigned the data type string, and its value contains a string representation of the remaining nested structure.\nFor example,\nLEVEL_31\nhas the data type\n<LEVEL_32:STRUCT<LEVEL_33:STRUCT<...>>>\n.\nWhat happens if the service principal loses access to one workspace?\nâ\nThe crawler is resilient to this scenario. During the discovery phase, it fails to connect to that specific workspace, logs it as inaccessible, and simply skips it. The process continues for all other available workspaces without failing the entire run.\nWhy are assets from a specific catalog not appearing in Atlan?\nâ\nThis is almost always a permission issue. Verify that the service principal has been granted\nUSE CATALOG\n,\nBROWSE\n, and\nSELECT\npermissions on the catalog and its contents (schemas, tables).\nWhy is my lineage view incomplete?\nâ\nCheck the source and target tables of the missing lineage link. The service principal must have\nSELECT\npermissions on\nboth\ntables for lineage to be captured.\nFor more information on cross-workspace extraction setup, see\nSet up cross-workspace extraction\n.\nTags:\napi\nrest-api\ngraphql\nPrevious\nPreflight checks for Databricks"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/references/provide-credentials-to-view-sample-data",
    "content": "Use data\nDiscovery\nReferences\nProvide credentials to view sample data\nOn this page\nProvide credentials to view sample data\nOnce your connection admins have\nconfigured\nbring your own credentials\n(BYOC)\nin Atlan, users will need to provide their own credentials before they can view the sample data in the asset profile. This will help you enforce better governance across your organization.\nWho can do this?\nAny\nAtlan user\nwith\ndata access to the asset\nand their own credentials for the data store. Atlan will display a\n100-row sample of the data\n.\nUse your own credentials to view sample data\nâ\nAtlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports\nSSO authentication\n.\nTo set up your own credentials for viewing sample data:\nOn the\nAssets\npage, click on an asset to view its asset profile.\nIn the asset profile, click\nSample Data\n.\nTo set up your credentials for viewing the sample data, click\nGet Started\n.\nIn the popup window, click\nGet Started\nonce again to proceed.\nIn the\nUser credential setup\ndialog box,\nBasic\nis selected as the default authentication option. Enter the following:\nFor\nUsername\n, enter the username for the connection.\nFor\nPassword\n, enter the password for that connection.\nFor\nRole\n, enter your role for that connection.\nFor\nWarehouse\n, enter the name of the warehouse.\nClick the\nTest Authentication\nbutton to confirm your credentials.\nOnce authentication is successful, click\nDone\n.\nYou can now view sample data using your own credentials! ð\nWhen using the key pair method, you'll need to enter your encrypted private key and the private key password to complete the authentication process.\nDid you know?\nOnce you've set up your credentials for viewing sample data, you can also\nmanage your credentials\n. If your admin has enabled\nsample data download\n, you can export sample data in a CSV file.\nTags:\nintegration\nconnectors\nPrevious\nWhat are asset profiles?\nNext\nDiscovery FAQs\nUse your own credentials to view sample data"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics",
    "content": "Use data\nUsage & Popularity\nAnalysis\nHow to interpret usage metrics\nOn this page\nInterpret usage metrics\nAtlan currently supports usage and popularity metrics for the following connectors:\nAmazon Redshift\n-  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source.\nDatabricks\n-  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the\nDatabricks APIs\n.\nGoogle BigQuery\n-  tables, views, and columns\nMicrosoft Power BI\n-  reports and dashboards\nSnowflake\n-  tables, views, and columns\nPowered by Atlan's enhanced query-mining capabilities, you can view popularity metrics for supported assets:\nThe\npopularity score\nof an asset is computed using both the number of queries and the number of users who have queried that asset in the last 30 days. The popularity score of an asset helps determine its relative popularity. All assets with a popularity score are then slotted into one of four percentile groups   -\nLeast popular\n,\nLess popular\n,\nPopular\n, and\nMost popular\n.\nPopularity score is calculated using the following formula:\nnumber of distinct users * log (total number of read queries)\nTime period = 30 days\nThe\npopularity indicator\nis displayed for all supported assets that have been queried in the last 30 days. This indicator visualizes the relative popularity of an asset on a scale of 1 to 4 blue bars   -  1 being the lowest score and 4 being the highest.\nA\npopularity popover\nwill appear when hovering over the popularity indicator. It displays additional information pertaining to an asset, such as a graph for trends in the data, last queried and by whom, and when the data was last updated.\nView popularity metrics\nâ\nTo view popularity metrics for your assets, complete these steps.\nIdentify popular assets\nâ\nBeing able to identify your most relevant and trusted data assets can help you increase their adoption and drive usage within your organization.\nTo view popularity metrics for an asset:\nFrom the left menu in Atlan, click\nAssets\n.\nFor\nConnector\non the\nAssets\npage, select a supported connector   -  for this example, we'll select\nSnowflake\n.\nNext to the search bar on the\nAssets\npage, click the sort button.\nFrom the\nPopularity\nsorting menu, click\nMost popular\nto view most used assets or\nLeast popular\nto view least used assets.\nYour assets will now have a popularity indicator. To view the popularity popover for an asset, click or hover over the\npopularity\nindicator\n.Â\nYou'll now be able to see all the relevant popularity metrics for your asset! ð\nView usage metrics in the asset sidebar\nâ\nThe new\nUsage\ntab in the asset sidebar helps you view usage metadata for your assets.\nFor example, if you'd like to appoint a data steward for your data assets, you'll be able to determine the right candidate based on the top users for that asset. You'll also be able to review popular queries or users for a particular table while checking for data compliance.\nTo view usage details for an asset:\nFrom the left menu in Atlan, click\nAssets\n.\nFor\nConnector\non the\nAssets\npage, select a supported connector   -  for this example, we'll select\nSnowflake\n.\nNext to the search bar on the\nAssets\npage, click the sort button.\nFrom the\nPopularity\nsorting menu, click\nMost popular\nto view most used assets or\nLeast popular\nto view least used assets.\nIn the bottom right of any asset card, click or hover over the\npopularity\nindicator\nto open the popularity popover.Â\nIn the popularity popover, click\nView usage details\nto view the following:\nFor\nUsage\n, view top and recent users in the last 30 days.\nFor\nQueries\n, view top five queries by context   -\nPopular\n,\nSlow\n, and\nExpensive\n. Only read queries or\nSELECT\nstatements are shown for these queries.\nFor\nCompute\n, view the total\ncompute cost\nfor an asset. The compute cost is split between read and write queries, allowing you to better understand the cost breakdown for individual assets:\nRead queries   -\nSELECT\nstatements.\nWrite queries   -  all non-\nSELECT\nstatements, for example,\nUPDATE\n,\nINSERT\n,\nCREATE\n, and more.\nThe usage details for the asset will now appear in the asset sidebar! ð\nView and sort columns by popularity\nâ\nFor any Snowflake, Databricks, or Google BigQuery table or view sorted by popularity, you'll also be able to view and sort the columns by popularity in the asset profile.\nTo view column assets by popularity:\nFrom the left menu in Atlan, click\nAssets\n.\nFor\nConnector\non the\nAssets\npage, select a supported connector   -  for this example, we'll select\nSnowflake\n.\nNext to the search bar on the\nAssets\npage, click the sort button.\nFrom the\nPopularity\nsorting menu, click\nMost popular\nto view most used assets or\nLeast popular\nto view least used assets.\nClick any asset to open to its asset profile.\nIn the\nColumn preview\ntab of the asset profile, hover over the popularity indicator to view the popularity popover for your columns.\n(Optional) In the search bar under\nColumn preview\n, click the\nsort icon\nand then click\nMost popular\nor\nLeast popular\nto sort columns by popularity.\nYou'll now be able to view the popularity score, number of queries and users, and timestamp for last queried for your columns! ð\nView queries by context\nâ\nGet the context you need before querying an asset to help you optimize your queries. Query popular, slow, or expensive queries from the\nUsage\ntab directly in Insights.\nTo view and work with queries by context:\nFrom the left menu in Atlan, click\nAssets\n.\nFor\nConnector\non the\nAssets\npage, select a supported connector   -  for this example, we'll select\nSnowflake\n.\nNext to the search bar on the\nAssets\npage, click the sort button.\nFrom the\nPopularity\nsorting menu, click\nMost popular\nto view most used assets or\nLeast popular\nto view least used assets.\nIn the bottom right of any asset card, click or hover over the\npopularity\nindicator\nto open the popularity popover.Â\nIn the popularity popover, click\nView usage details\n.\nIn the\nUsage\ntab in the asset sidebar, navigate to\nQueries\nand depending on the type of query you'd like to see:\nClick\nPopular\nÂ to see the top five most popular queries.Â\nClick\nSlow\nto see queries sorted by average duration and last run.\nClick\nExpensive\nto see the top five most expensive queries.Â\nOnce you've selected the relevant query type, hover over a query card to:\nClick the\nexpand icon\nto see the query details.\nClick the\ncopy icon\nto copy the query and use it as a template for writing your own queries.\nClick the\ncode icon\nto open the query directly in\nInsights\nand run it.\nDid you know?\nIf you have any questions about usage and popularity metrics, head over\nhere\n.\nTags:\nconnectors\ndata\napi\nPrevious\nHow to find assets by usage\nNext\nTroubleshooting usage and popularity metrics\nView popularity metrics\nView queries by context"
  },
  {
    "url": "https://docs.atlan.com/tags/authentication",
    "content": "53 docs tagged with \"authentication\"\nView all tags\nAdd impact analysis in GitHub\nLearn about add impact analysis in github.\nAdd impact analysis in GitLab\nLearn about add impact analysis in gitlab.\nAuthentication\nUnderstand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure.\nCreate an AWS Lambda trigger\nOnce you have configured the [AWS Lambda permissions](/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda), you can run an AWS Lambda function.\nEnable  Snowflake OAuth\nAtlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware.\nEnable  SSO for Amazon Redshift\nYou will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift).\nEnable  SSO for Google BigQuery\nCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.\nextract lineage and usage from Databricks\nOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.\nIntegrate Anomalo\nOnce you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo.\nIntegrate Apache Spark/OpenLineage\nAtlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nPingFederate SSO 404 error\nIf you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion.\nPreflight checks for Google BigQuery\nEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).\nPreflight checks for Microsoft Azure Synapse Analytics\nThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.\nSecurity\nThe Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring.\nSecurity and Compliance\nComplete guide to Atlan's security features, compliance certifications, and data protection capabilities.\nSet up Amazon DynamoDB\nLearn about set up amazon dynamodb.\nSet up Amazon MSK\nLearn about set up amazon msk.\nSet up Amazon QuickSight\nLearn about set up amazon quicksight.\nSet up Amazon Redshift\n:::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself.\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.\nSet up client credentials flow\nConfigure Salesforce for OAuth 2.0 client credentials authentication in Atlan.\nSet up Confluent Kafka\nAtlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata.\nSet up Confluent Schema Registry\n:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.\nSet up Databricks\nAtlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods:.\nSet up Domo\n:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.\nSet up Hive\n:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.\nSet up IBM Cognos Analytics\n:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.\nSet up Informatica CDI\nConfigure authentication and user permissions for Informatica Cloud Data Integration connector\nSet up JWT bearer flow\nConfigure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan.\nSet up Microsoft Azure Cosmos DB\nIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db).\nSet up Microsoft Azure Data Factory\nAtlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata.\nSet up Microsoft Azure Event Hubs\nAtlan supports the following authentication methods for Microsoft Azure Event Hubs:.\nSet up Microsoft Azure Synapse Analytics\nAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.\nSet up Microsoft Power BI\nThis guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking.\nSet up Microsoft SQL Server\n:::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself.\nSet up MicroStrategy\nAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.\nSet up MongoDB\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up MySQL\n:::warning Who can do this? You will probably need your MySQL administrator to run these commands - you may not have access yourself.\nSet up Oracle\n:::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself.\nSet up PrestoSQL\nLearn about set up prestosql.\nSet up Redash\n:::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself.\nSet up Salesforce\nLearn about setting up Salesforce authentication for Atlan.\nSet up SAP HANA\n:::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself.\nSet up Sisense\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nSet up Soda\n:::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -.\nSet up Teradata\n:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.\nSet up ThoughtSpot\n:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.\nSet up Trino\n:::warning Who can do this? You will probably need your Trino administrator to run these commands - you may not have access yourself.\nSet up username-password flow\nConfigure Salesforce username-password flow for Atlan integration.\nSupported connections for on-premises databases\nThe metadata-extractor tool supports the following connection types.\nUsers and groups\nLearn how to manage users and groups in Atlan to control access and organize your data team.\nWhat does Atlan crawl from Domo?\nAtlan supports lineage for the following asset types:."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nOn this page\nMicrosoft Power BI\nOverview:\nCatalog workspaces, reports, dashboards, and datasets from Microsoft Power BI in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Power BI assets in Atlan:\nSet up the connector\nCrawl Microsoft Power BI assets\nGuides\nâ\nMine Power BI usage\n: Extract usage metrics and activity events from Power BI.\nReferences\nâ\nWhat does Atlan crawl from Microsoft Power BI\n: Learn about the Power BI assets and metadata that Atlan discovers and catalogs.\nWhat lineage does Atlan extract from Microsoft Power BI\n: Learn about supported lineage extraction for Power BI assets.\nPreflight checks for Microsoft Power BI\n: Verify prerequisites before setting up the Microsoft Power BI connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Power BI connection issues and errors.\nTags:\npower bi\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Microsoft Power BI\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nCrawl Power BI Assets\nCrawl Microsoft Power BI\nOn this page\nCrawl Microsoft Power BI\nOnce you have configured the\nMicrosoft Power BI user permissions\n, you can establish a connection between Atlan and Microsoft Power BI.\nTo crawl metadata from Microsoft Power BI, review\nthe\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Microsoft Power BI as your source:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the list of packages, selectÂ\nPower BI Assets\nand click onÂ\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Microsoft Power BI credentials:\nFor\nAuthentication,\nchoose the method you want to use to access Microsoft Power BI:\nFor\nService Principal\nauthentication, enter the\nTenant Id\n,\nClient Id\n, and\nClient Secret\nyou configured when\nsetting up Microsoft Power BI\n. Use the\nEnable Only Admin API Access\noption to control how metadata is extracted. When enabled, the crawler uses only admin APIs. If disabled, both admin and non-admin APIs are used.\nFor\nDelegated User\nauthentication, enter the\nUsername\n,\nPassword\n,\nTenant Id\n,\nClient Id\n, and\nClient Secret\nyou configured when\nsetting up Microsoft Power BI\n.\nAt the bottom of the form, click the\nTest Authentication\nbutton to confirm connectivity to Microsoft Power BI using these details.\nOnce successful, at the bottom of the screen click the\nNext\nbutton.\nConfigure connection\nâ\nTo complete the Microsoft Power BI connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you don't specify any user or group, nobody can manage the connection   -  not even admins.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the Microsoft Power BI crawler, configure metadata extraction and advanced options. You can override the default settings for the following fields.\nConfigure metadata\nâ\nInclude Workspaces\n: Select Microsoft Power BI workspaces to include. Defaults to all workspaces when left blank. Use\nAdvanced Search\nto filter workspaces using the following options:\nContains\n: Matches workspaces that contain the given substring.\nStarts with\n: Matches workspaces that begin with the specified text.\nEnds with\n: Matches workspaces that end with the specified text.\nRegex pattern\n: Matches workspaces based on a regular expression.\nAll selected filters apply using an\nAND\ncondition.\nExclude Workspaces\n: Select workspaces to exclude. No workspaces are excluded by default.\nAdvanced Search\nis also available for exclusion, with the same filtering options as mentioned previously.\nInclude Dashboard and Reports Regex\n: Use a regular expression to include dashboards and reports based on naming patterns. Includes all by default.\nExclude Dashboard and Reports Regex\n: Use a regular expression to exclude dashboards and reports based on naming patterns. Excludes none by default.\nAttach Endorsements from Power BI\n: Automatically certify assets endorsed in Power BI. To manually review before applying, change this setting to\nSend a Request\n. For more details, see\nWhat does Atlan crawl from Microsoft Power BI?\nConfigure advanced settings\nâ\nSource Connections\n: When your tenant has multiple connections available for the same source system that share the similar metadata, confirm the advanced options and choose the correct connections from the\nSource Connections\nlist drop down to avoid creating duplicate lineage to such connections.\nEnable ODBC DSN Connectivity Mapping\n: Power BI provides multiple ways of connecting to a SQL source, including ODBC connectivity for building Reports and Dashboards. When datasets are populated using ODBC, provide a mapping of the DSN ( Data Source Name ) names to their appropriate database qualified names after enabling this toggle.\nDid you know?\nIf a workspace appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Microsoft Power BI crawler:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you can see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Microsoft Power BI\nNext\nMine Microsoft Power BI\nSelect the source\nProvide credentials\nConfigure connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-does-atlan-crawl-from-microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nReferences\nWhat does Atlan crawl from Microsoft Power BI?\nOn this page\nWhat does Atlan crawl from Microsoft Power BI?\nAtlan crawls and maps the following assets and properties from Microsoft Power BI.\nOnce you've\ncrawled Microsoft Power BI\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nMeasures\n-  External measure filter\ndanger\nCurrently Atlan only represents the assets marked with ð in lineage.\nFor your Microsoft Power BI\nreports\n, Atlan also provides asset previews to help with quick discovery and give you the context you need.\nApps\nâ\nAtlan maps\nApps\nfrom Microsoft Power BI to its\nPowerBIApp\nasset type.\nSource Property\nAtlan Property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nid\npowerBIAppId\nproperties sidebar\ndescription\ndescription\nasset profile and overview sidebar\npublishedBy\nsourceOwners\noverview sidebar\nusers   ( displayName, appUserAccessRight )\npowerBIAppUsers\nasset profile\ngroups   ( displayName, appUserAccessRight )\npowerBIAppGroups\nasset profile\nWorkspaces\nâ\nAtlan maps workspaces from Microsoft Power BI to its\nPowerBIWorkspace\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nreportCount\nreportCount\nasset preview and profile\ndashboardCount\ndashboardCount\nasset profile\ndatasetCount\ndatasetCount\nasset profile\ndataflowCount\ndataflowCount\nasset profile\nwebUrl\nsourceURL\noverview sidebar\nDashboards ð\nâ\nAtlan maps dashboards from Microsoft Power BI to its\nPowerBIDashboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndisplayName\nname\nasset profile and overview sidebar\nworkspace_name\nworkspaceName\nAPI only\ndescription\ndescription\nasset profile and overview sidebar\ntileCount\ntileCount\nasset profile\nwebUrl\nsourceURL\noverview sidebar\nData sources\nâ\nAtlan maps data sources from Microsoft Power BI to its\nPowerBIDatasource\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nconnectionDetails\nconnectionDetails\noverview sidebar\nDatasets ð\nâ\nAtlan maps datasets from Microsoft Power BI to its\nPowerBIDataset\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nworkspace_name\nworkspaceName\nAPI only\ndescription\ndescription\nasset profile and overview sidebar\nwebUrl\nsourceURL\noverview sidebar\nconfiguredBy\nsourceOwners\nasset profile and properties sidebar\ncreatedDate\nsourceCreatedAt\nasset profile and properties sidebar\nendorsementDetails\ncertificateStatus (VERIFIED)\nasset profile and overview sidebar\nendorsementDetails (endorsement)\ncertificateStatusMessage\n,\npowerBIEndorsement\nasset profile and overview sidebar\nendorsementDetails (certifiedBy)\ncertificateUpdatedBy\nasset profile and overview sidebar\nDataflows ð\nâ\nAtlan maps dataflows from Microsoft Power BI to its\nPowerBIDataflow\nasset type. Atlan currently only supports dataflow lineage for the following SQL sources:\nMicrosoft SQL Server\nOracle\nSAP HANA\nSnowflake\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nworkspace_name\nworkspaceName\nAPI only\ndescription\ndescription\nasset profile and overview sidebar\nwebUrl\nsourceURL\noverview sidebar\nconfiguredBy\nsourceOwners\nasset profile and properties sidebar\nmodifiedDateTime\nsourceUpdatedAt\nasset profile and properties sidebar\nendorsementDetails\ncertificateStatus (VERIFIED)\nasset profile and overview sidebar\nendorsementDetails (endorsement)\ncertificateStatusMessage\n,\npowerBIEndorsement\nasset profile and overview sidebar\nendorsementDetails (certifiedBy)\ncertificateUpdatedBy\nasset profile and overview sidebar\nmodifiedBy\nsourceUpdatedBy\nasset profile and properties sidebar\ndays\npowerBIDataflowRefreshScheduleFrequency\nproperties sidebar\ntimes\npowerBIDataflowRefreshScheduleTimes\nproperties sidebar\nlocalTimeZoneId\npowerBIDataflowRefreshScheduleTimeZone\nproperties sidebar\nDataflow entity columns ð\nâ\nAtlan maps attributes of dataflow entities from Microsoft Power BI to its\nPowerBIDataflowEntityColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nattrbutes.name\nname\nasset profile and overview sidebar\nentities.name\npowerBIDataflowEntityName\noverview sidebar\nattributes.$type\npowerBIDataflowEntityColumnDataType\nasset profile and overview sidebar\nReports ð\nâ\nAtlan maps reports from Microsoft Power BI to its\nPowerBIReport\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nworkspace_name\nworkspaceName\nAPI only\ndescription\ndescription\nasset profile and overview sidebar\npageCount\npageCount\nasset profile\nwebUrl\nsourceURL\noverview sidebar\ncreatedDateTime\nsourceCreatedAt\nasset profile and properties sidebar\nmodifiedDateTime\nsourceUpdatedAt\nasset profile and properties sidebar\ncreatedBy\nsourceCreatedBy\n,\nsourceOwners\nasset profile and properties sidebar\nmodifiedBy\nsourceUpdatedBy\nasset profile and properties sidebar\nendorsementDetails\ncertificateStatus (VERIFIED)\nasset profile and overview sidebar\nendorsementDetails (endorsement)\ncertificateStatusMessage\n,\npowerBIEndorsement\nasset profile and overview sidebar\nendorsementDetails (certifiedBy)\ncertificateUpdatedBy\nasset profile and overview sidebar\nPages ð\nâ\nAtlan maps pages from Microsoft Power BI to its\nPowerBIPage\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndisplayName\nname\nasset profile and overview sidebar\nworkspace_name\nworkspaceName\nAPI only\nreport_name\nreportName\nAPI only\nTiles ð\nâ\nAtlan maps tiles from Microsoft Power BI to its\nPowerBITile\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\nsubTitle\ndescription\nasset profile and overview sidebar\nworkspace_name\nworkspaceName\nAPI only\ndashboard_name\ndashboardName\nAPI only\nTables ð\nâ\nAtlan maps tables from Microsoft Power BI to its\nPowerBITable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nisHidden\nisHidden\nAPI only\nsourceExpressions\npowerBITableSourceExpressions\nAPI only\ncolumnCount\npowerBITableColumnCount\nasset profile and preview\nmeasureCount\npowerBITableMeasureCount\nasset profile and preview\nColumns ð\nâ\nAtlan maps columns from Microsoft Power BI to its\nPowerBIColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nisHidden\npowerBIIsHidden\nAPI only\ndataCategory\npowerBIColumnDataCategory\nAPI only\ndataType\npowerBIColumnDataType\nasset preview and overview sidebar\nformatString\npowerBIFormatString\nAPI only\nsortByColumn\npowerBISortByColumn\nAPI only\nsummarizeBy\npowerBIColumnSummarizeBy\nAPI only\nMeasures ð\nâ\nAtlan maps measures from Microsoft Power BI to its\nPowerBIMeasure\nasset type. Atlan supports PowerBI Measures for downstream lineage to a PowerBI Page.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nisHidden\npowerBIIsHidden\nAPI only\nexpression\npowerBIMeasureExpression\noverview sidebar\nisExternalMeasure\npowerBIIsExternalMeasure\nasset filter\nformatString\npowerBIFormatString\nAPI only\nTags:\nconnectors\ncrawl\nPrevious\nMine Microsoft Power BI\nNext\nPreflight checks for Microsoft Power BI\nApps\nWorkspaces\nDashboards ð\nData sources\nDatasets ð\nDataflows ð\nDataflow entity columns ð\nReports ð\nPages ð\nTiles ð\nTables ð\nColumns ð\nMeasures ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/troubleshooting/troubleshooting-microsoft-power-bi-connectivity",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nTroubleshooting\nTroubleshooting Microsoft Power BI connectivity\nOn this page\nTroubleshooting Microsoft Power BI connectivity\nWhat are the known limitations of the Microsoft Power BI connector?\nâ\nAtlan currently doesn't support the following:\nFiltering hidden pages in\nreports\n-  known limitation of\nPowerBI reports API\nCrawling reports developed and published in\nPower BI Report Server\nWhat are the limitations of the Microsoft PowerBI Connector, when only admin APIs are used?\nâ\nWhen the Microsoft PowerBI Connector is configured to use only admin APIs, it results in reduced metadata extraction and limited lineage capabilities compared to using a combination of\nadmin & non-admin APIs\n.\nThe following limitations apply as mentioned below:\nPowerBI Pages (which are a part of PowerBI Report) won't be catalogued.\nDownstream Lineage from PowerBI Table Column / Measure -> PowerBI Page won't be available.\nCan users who don't have access to a report still see the preview?\nâ\nUsers can only see asset previews if the following conditions are met:\nThey have the necessary permissions in both Microsoft Power BI and Atlan.\nThey're logged into Atlan and Microsoft Power BI on the same browser.\nTherefore, if a user lacks the permission to view a report in Microsoft Power BI, they won't be able to see the report preview in Atlan. Even if they do have the necessary permissions, they need to be logged into Microsoft Power BI on the same browser as their Atlan instance for asset previews to work.\nWhy can I not see previews for my Microsoft Power BI assets?\nâ\nYour Microsoft Power BI assets are updated with previews during the next run of your Microsoft Power BI workflow. If you have run the workflow and still don't see the previews, rerun the workflow. Once you've rerun the workflow, the previews are visible to all eligible users.\nHow does Atlan calculate views for Microsoft Power BI reports and dashboards?\nâ\nAtlan calculates\nUsage\nviews based on the number of times users open a report or dashboard in Microsoft Power BI.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nupstream-dependencies\ndata-sources\nPrevious\nWhat lineage does Atlan extract from Microsoft Power BI?\nNext\nWhat is the difference between a Power BI data source and dataflow?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/faq/power-bi-dataflow-datasource",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nFAQ\nWhat is the difference between a Power BI data source and dataflow?\nWhat is the difference between a Power BI data source and dataflow?\nIn Microsoft Power BI:\nData source   -  a data source is any source that fuels a dataset. You can connect to different types of data sources, such as databases, files, Microsoft Fabric, and more, to fetch data directly into Microsoft Power BI for creating reports and dashboards. For a full list, refer to\nMicrosoft Power BI documentation\n.\nDataflows   -  data flows help you encode the process of extracting and transforming data from a data source to produce a dataset. These have a refresh frequency that keeps the dataset in sync with the data source. To learn more, refer to\nMicrosoft Power BI documentation\n.\nTags:\ndata\nmodel\nPrevious\nTroubleshooting Microsoft Power BI connectivity"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/product-release-stages",
    "content": "Get Started\nReferences\nProduct release stages\nOn this page\nProduct release stages\nThe following release stages are part of the lifecycle of an Atlan feature:\nStage\nAccess\nStatus\nAlpha\nBy invitation only to few data teams\nExperimental\nPrivate preview\nBy invitation only to a larger cohort of data teams\nNear production grade\nPublic preview\nOpen to all data teams\nProduction grade with known limitations\nGeneral availability\nOpen to all data teams\nProduction grade\nAtlan release stages\nâ\nWhile features are stable, Atlan may on occasion deprecate legacy functionality to support product updates.\nAlpha\nâ\nEnabled for select customers on request to test, validate, and provide feedback.\nExperimental in nature, user feedback critical in preparing for a wider release.\nFormal support and documentation unavailable.\nStandard SLAs, terms, and warranties don't apply.\nPrivate preview\nâ\nEnabled for a larger cohort of customers on request to test, validate, and provide feedback.\nPartially production ready   -  breaking changes may occur.\nInformal support and documentation from product and engineering teams.\nStandard SLAs, terms, and warranties don't apply.\nTo get access to private preview features,\nraise a support request\n.\nPublic preview\nâ\nEnabled for all customers, usually includes a\nPreview\nlabel in the product or requires admins to enable the feature from\nLabs\nin the admin center.\nProduction grade is high, but with known limitations.\nFormal support and documentation provided on best available basis.\nFeature release may be announced on the\nShipped channel\n.\nStandard SLAs, terms, and warranties do not apply.\nGeneral availability\nâ\nEnabled for all users.\nProduction grade is of the highest quality to ensure the best possible user experience.\nFormal support and documentation available.\nFeature release announced on the\nShipped channel\n.\nStandard SLAs, terms, and warranties apply.\nTags:\natlan\nreleases\nPrevious\nAtlan architecture\nNext\nOur 3 pro tips for saving time with Atlan\nAtlan release stages\nAlpha\nPrivate preview\nPublic preview\nGeneral availability"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/our-3-pro-tips-for-saving-time-with-atlan",
    "content": "Get Started\nReferences\nOur 3 pro tips for saving time with Atlan\nOn this page\nOur 3 pro tips for saving time with Atlan\nThere are a lot of incredibly time-saving functionalities built into Atlan. Here are three pro tips that you can use to save time for you and your team!\nSearch from anywhere in Atlan\nâ\nAtlan supports powerful, intelligent search. Start your search from anywhere in Atlan to find the data asset you're looking for.\nYou can also enter a keyword in the search bar and filter your results by a specific type of asset. For instance, enter the keyword\nproduct\nin the search bar and then click the\nTable\nfilter to view table results for your searched keyword.\nDid you know?\nYou can use\nCmd/Ctrl + K\nto open the search page from\nanywhere\nin Atlan.\nCollaborate with your team\nâ\nUse Atlan's seamless\nSlack\nor\nMicrosoft Teams\nintegration to share updates with your team. Don't waste time adding the correct link   -  just click the\nSlack\nor\nTeams\nicon in Atlan to post directly on a Slack or Microsoft Teams channel. You can also add your message as a resource to the asset.\nRefine your search using filters\nâ\nUse the filters in the left panel of the\nAssets\nworkspace to refine your search. Check out our three best pro tips for using filters!\nSearch by certification status\nâ\nCertificates\nin Atlan are a useful tool for letting your team know the status of an asset. You can search by certification status to only find the assets you need.\nSearch by owners or data contributors\nâ\nWant to find out what your teammate is working on? Looking for a dashboard that your manager is delegating to you? Use the\nOwners\nfilter to search by asset\nowner\nin Atlan.\nYou can also search by other user roles. To do this, you first need to\nset up custom metadata\nproperties to represent those other roles. Then you can filter by those custom metadata properties to find assets with any Atlan user who has that role for the asset.\nSearch by lineage\nâ\nWant to only find assets with a completed\nlineage\n? You can filter your search by assets that have data lineage using the\nHas lineage\nfilter listed under\nProperties\n.Â\nTry using these 3 pro tips to make Atlan a powerful part of your team's tool kit.\nTags:\natlan\ndocumentation\nPrevious\nProduct release stages\nNext\nThe DataOps Culture Code\nSearch from anywhere in Atlan\nCollaborate with your team\nRefine your search using filters"
  },
  {
    "url": "https://docs.atlan.com/get-started/faqs/how-are-product-updates-deployed",
    "content": "Get Started\nReferences\nHow are product updates deployed?\nOn this page\nHow are product updates deployed?\nAs the control plane for your entire data stack, Atlan's product release strategy is centered on being fast, iterative, and adaptive to the needs of our partners. All the while ensuring that product releases are mature and stable to truly empower users.\nAtlan follows a standard CI/CD model   -  continuous integration (CI) and continuous delivery (CD)   -  for\ndeploying product updates\n. This includes bug fixes and making improvements regularly.Â\nTo minimize risks and enhance security, each component in Atlan is tracked and released individually. When developers make changes, a series of automated validations are initiated, including building component images, running tests, and verifying that no vulnerabilities have been introduced. Peer reviews further ensure high-quality standards.\nOnly after the changes have been approved, the new release will go through the following stages:\nStaging   -  for pre-production testing\nBeta   -  for limited user testing\nProduction   -  live environment release\nAtlan's engineering team actively reviews any issues with new releases to extract valuable insights and learning opportunities toward offering a better service. Accordingly, Atlan recommends that any connections or integrations you would like to make must be approved by Atlan unless already tested and documented. Note that any unauthorized integration may lead to breakdown of service and impact end user experience.\nSignificant feature updates are always highlighted within the application. You can also subscribe to our\nShipped channel\nfor updates on product rollouts.\nView product updates\nâ\nTo view product updates in Atlan:\nFrom the top right of any screen in Atlan, click the\ngift box\nicon.Â\nIn the\nProduct Updates\nsidebar, view the latest product releases. You can also:\n(Optional) Click\nSubscribe\nto subscribe to our\nShipped channel\n.\n(Optional) Type a keyword in the search bar to search for specific product releases.\nTags:\nintegration\nconnectors\nfaq-integrations\nPrevious\nThe DataOps Culture Code\nNext\nQuality assurance framework\nView product updates"
  },
  {
    "url": "https://docs.atlan.com/platform/references/quality-assurance-framework",
    "content": "Get Started\nReferences\nQuality assurance framework\nOn this page\nQuality assurance framework\nAtlan has a robust quality assurance (QA) framework for delivering reliable, high-quality products that exceed user expectations and build trust. The QA strategies and processes implemented throughout the product development lifecycle have the following benefits to offer:\nRigorous testing and quality checks ensure that the product is reliable, functional, and user-friendly.\nThorough testing helps identify and mitigate potential defects and risks early in the development process   -  minimizing rework and reducing the time and effort for bug fixes.\nClear communication, sharing test results, and involving all stakeholders throughout the process fosters effective collaboration and alignment on quality goals.\nManual testing process\nâ\nAtlan uses\nTestmo\nfor the manual testing process and\nJira\nÂ for project management:\nTest planning\nâ\nThe objectives, scope, and test approach are defined for the manual testing phase. Testable requirements are identified and test cases prioritized based on risk analysis.\nTest case design\nâ\nOnce a feature is ready for testing, it is handed over to the QA team. Test scenarios are created based on user requirements. The product owner reviews these test scenarios and approves them for testing.\nManual testing execution\nâ\nManual testing is performed using the approved test scenarios. Testing is conducted in different environments, including\nbeta\n,\nstaging\n, and\nproduction\n. Comprehensive testing takes place in the staging environment. Any issues identified during testing are documented using a bug reporting tool. Reported issues are then assigned to the development team for resolution.\nManual testing covers the following areas:\nFunctional testing\n-  to verify that the feature functions according to specified requirements.\nSecurity testing\n-  to identify vulnerabilities and ensure the security of user data and system resources.\nIntegration testing\n-  to validate the seamless integration of the feature with other components or systems.\nUsability testing\n-  to evaluate the user-friendliness and intuitiveness of the feature.\nDefect reporting\nâ\nAny deviations or defects encountered during testing are recorded. Steps to reproduce the issue are documented with screenshots or logs, if applicable, and a severity level is assigned to each issue.\nIssue retesting and sanity testing\nâ\nAfter the development team has fixed the reported issues, the QA team retests the scenarios. A round of sanity tests is conducted to ensure the overall stability of the feature.\nTest cycle closure\nâ\nTest results are evaluated, including defect metrics, test coverage, and overall quality. A test summary report is prepared that highlights the testing activities, challenges faced, and recommendations for improvement.\nAutomation testing process\nâ\nAtlan uses\nmabl\nfor the test automation process:\nIncreased test coverage\nâ\nAutomation testing allows Atlan to execute a large number of test cases across different scenarios, configurations, and environments. By automating repetitive and time-consuming tasks, Atlan is able to achieve a broader test coverage, ensuring that critical functionalities and edge cases are thoroughly validated.\nFaster time-to-market\nâ\nAutomation testing significantly reduces the time required to execute test cases compared to manual testing. Atlan can accelerate the testing process and obtain quicker feedback on the quality of the product. This helps in meeting tight deadlines and release features faster.\nImproved accuracy and consistency\nâ\nAutomated tests are executed with precision and consistency, minimizing human errors and ensuring accurate results. By eliminating manual intervention, Atlan reduces the risk of human-induced mistakes, resulting in reliable and repeatable test outcomes.\nRegression testing\nâ\nAutomation testing is particularly effective for regression testing, which involves retesting previously validated functionalities to ensure that new changes or fixes do not introduce unintended issues. By automating regression tests, Atlan can quickly and accurately verify that existing features are working as expected after modifications.\nContinuous integration and deployment\nâ\nAutomation testing seamlessly integrates with Atlan's continuous integration and continuous delivery (CI/CD) pipeline. This enables Atlan to automate the execution of tests at various stages of the development process, such as after each code commit or prior to a deployment. By automating tests as part of the CI/CD process, Atlan ensures that software updates are thoroughly validated before being released.\nMaintenance and reusability\nâ\nAutomation testing scripts can be maintained and reused across multiple test cycles, saving time and effort. As the product evolves, the scripts can be updated to accommodate changes. This ensures that the automation suite is up-to-date and aligned with the latest functionalities.\nLabel-based testing\nâ\nLabel-based testing incorporates mabl's automated testing capabilities into the CI/CD pipeline using labels. Here's how it works:\nTriggering tests based on labels\n-  tests can be triggered based on specific labels in the CI/CD pipeline. For example, a particular set of mabl tests labeled as\nRegression\nor\nSmoke\ncan be executed during different stages of the pipeline, such as pre-production or post-deployment.\nTest result reporting\n-  after executing the mabl tests, results are reported back to the CI/CD pipeline. This information includes the test outcomes, such as pass, fail, or blocked, along with any associated logs, screenshots, or other artifacts for further analysis.\nDeployment process\nâ\nOnce a feature has successfully passed manual and automation testing, it is ready for deployment:\nEnvironment setup\nâ\nTarget environments required for testing are set up and a base user is created for the environment to perform the test cases.Â Each environment closely resembles the intended production environment to ensure accurate testing and validation.\nBuild generation\nâ\nOnce the code changes are ready for deployment, a build or package is generated that encapsulates the necessary files and resources for the software update. The build ensures that all components are packaged correctly and ready for deployment.\nAutomation testing\nâ\nBefore deploying the software update to the target environment, a suite of automated tests is executed. These tests validate the functionality, performance, and stability of the software, ensuring that it meets Atlan's quality standards.\nMonitoring and rollback\nâ\nOnce the deployment is completed, the application is closely monitored in the target environment. Monitoring tools and logs are used to identify any issues or anomalies. In case of critical failures or any unexpected issues, there is a rollback plan in place to revert to the previous version.\nContinuous improvement\nâ\nThe deployment process is continuously evaluated to identify areas for improvement. This includes capturing feedback, analyzing deployment metrics, and implementing changes to enhance efficiency, reliability, and quality.\nDaily test reports\nâ\nTests are executed on a daily schedule to ensure the proper functioning of the product and promptly address any issues. The daily test reports include the following:\nTest case execution status\nâ\nOverall status of the test case execution for the day   -  including the number of cases that were executed, passed, failed, or blocked.\nTest coverage\nâ\nInsights into the coverage of test cases across different functional areas, features, or modules. These help stakeholders understand the parts of the system that were tested and to what extent.\nDefects or issues\nâ\nInformation on any defects or issues discovered during testing. This can encompass newly identified defects, severity and priority levels, steps to reproduce the issues, and any relevant supporting details.\nMetrics and statistics\nâ\nMetrics and statistics related to test case execution, such as the pass/fail ratio, defect density, test coverage percentage, or any other relevant metrics agreed upon by the QA team.\nSummary and recommendations\nâ\nKey findings, observations, and recommendations for further actions or improvements to help with decision making and prioritizing testing efforts.\nProtocol for test failures\nâ\nWhen a test fails, Atlan adheres to a specific protocol to address and resolve the failure:\nFailure identification\nâ\nOnce a test fails, the specific test case or test suite that encountered the failure is promptly identified. This involves reviewing test logs, error messages, and any available diagnostic information to pinpoint the cause of the failure.\nRoot cause analysis\nâ\nA thorough root cause analysis is conducted to determine why the test failed. This may involve examining the test environment, reviewing test data, analyzing code changes, or investigating any external factors that could have influenced the failure.\nBug reporting\nâ\nIf the test failure is determined to be due to a software defect, the bug is reported following a standard bug reporting process. The QA team provides detailed information regarding the failure, including steps to reproduce, relevant test data, and any supporting evidence or screenshots.\nBug resolution\nâ\nOnce the bug is reported, the development team takes appropriate action to address and resolve the defect. This includes analyzing the bug report, reproducing the issue, and implementing the necessary fixes. The bug resolution process follows Atlan's established software development lifecycle and bug prioritization guidelines.\nRetesting\nâ\nAfter the bug fixes are implemented, the QA team retests to verify that the issue has been resolved and the test case has passed successfully. This is to ensure that the fix does not introduce any new issues or regressions.\nTags:\natlan\ndocumentation\nPrevious\nHow are product updates deployed?\nNext\nGetting Started and Onboarding\nManual testing process\nAutomation testing process\nDeployment process\nDaily test reports\nProtocol for test failures"
  },
  {
    "url": "https://docs.atlan.com/platform/references/how-are-resources-isolated",
    "content": "Get Started\nSecurity & Compliance\nHow are resources isolated?\nHow are resources isolated?\nEach Atlan customer has their own isolated set of nodes within Kubernetes.\nDeployments isolate all running services, including the metastore and its persistence in Cassandra and Elasticsearch.\nThe underlying Kubernetes control plane and networking layer (coredns) are shared between tenants. To achieve logical isolation, Loftâs\nvirtual clusters\nare implemented.\nThe compute resources (nodes, nodegroups) and storage are physically isolated between tenants.\nOnly Atlan's cloud team is able to manage the\nAWS\n,\nAzure\n, and\nGCP\nresources across these levels of isolation.\nSee\nsecurity.atlan.com\nfor the latest policies and standards, reports and certifications, architecture, diagrams and more.\nTags:\natlan\ndocumentation\nPrevious\nInfrastructure security\nNext\nSecurity monitoring"
  },
  {
    "url": "https://docs.atlan.com/tags/security",
    "content": "24 docs tagged with \"security\"\nView all tags\nAccess Control\nLearn how to manage user permissions and access to data assets in Atlan for security and compliance.\nAtlan architecture\nLearn about atlan architecture.\nAtlan browser extension security\nLearn about atlan browser extension security.\nBigID\nIntegrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata.\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring.\nCompliance standards and assessments\nLearn about compliance standards and assessments.\nConfigure network security\nConfigure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services\nCustomer environment security\nCustomer environment security best practices for deploying and operating Secure Agent 2.0\nDeployment and security\nFrequently asked questions about Secure Agent 2.0 deployment and security\nDeployment architecture\nThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.\nEncryption and key management\nLearn about encryption and key management.\nIncident response plan\nLearn about incident response plan.\nInfrastructure security\nLearn about infrastructure security.\nInstall on AWS EKS\nThis guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster.\nManage user authentication\nWhen users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication.\nMicrosoft Defender SSO error\nLearn about unable to log into atlan via sso due to an \"internal error\" from microsoft defender.\nRoles and permissions\nExplanation of Snowflake's security model and role requirements for data quality operations.\nSecure Agent\nThe Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing.\nSecurity\nSecurity overview and controls for Secure Agent 2.0\nSecurity monitoring\nLearn about security monitoring.\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nTroubleshooting Salesforce connectivity\nLearn about troubleshooting salesforce connectivity.\nTroubleshooting ServiceNow\nWhy is the security\\_admin role required to complete the ServiceNow integration?\nVerify container images\nVerify the authenticity and integrity of Secure Agent container images with Cosign"
  },
  {
    "url": "https://docs.atlan.com/tags/access-control",
    "content": "19 docs tagged with \"access-control\"\nView all tags\nAccess Control\nLearn how to manage user permissions and access to data assets in Atlan for security and compliance.\nAtlan architecture\nLearn about atlan architecture.\nAtlan browser extension security\nLearn about atlan browser extension security.\nAutomatically assign roles based on group names\nLearn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app.\nCompliance standards and assessments\nLearn about compliance standards and assessments.\nDeployment architecture\nThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.\nEncryption and key management\nLearn about encryption and key management.\nIncident response plan\nLearn about incident response plan.\nInfrastructure security\nLearn about infrastructure security.\nInstall on AWS EKS\nThis guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster.\nManage user authentication\nWhen users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication.\nMicrosoft Defender SSO error\nLearn about unable to log into atlan via sso due to an \"internal error\" from microsoft defender.\nRequests\nRequest and manage changes to assets that you don't have direct edit access to.\nSecure Agent\nThe Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing.\nSecurity monitoring\nLearn about security monitoring.\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nTroubleshooting Salesforce connectivity\nLearn about troubleshooting salesforce connectivity.\nTroubleshooting ServiceNow\nWhy is the security\\_admin role required to complete the ServiceNow integration?\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/permissions",
    "content": "20 docs tagged with \"permissions\"\nView all tags\nAccess Control\nLearn how to manage user permissions and access to data assets in Atlan for security and compliance.\nAtlan architecture\nLearn about atlan architecture.\nAtlan browser extension security\nLearn about atlan browser extension security.\nCompliance standards and assessments\nLearn about compliance standards and assessments.\nData quality permissions\nReference for data quality permission scopes and configuration in Atlan.\nDeployment architecture\nThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.\nEncryption and key management\nLearn about encryption and key management.\nIncident response plan\nLearn about incident response plan.\nInfrastructure security\nLearn about infrastructure security.\nInstall on AWS EKS\nThis guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster.\nManage user authentication\nWhen users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication.\nMicrosoft Defender SSO error\nLearn about unable to log into atlan via sso due to an \"internal error\" from microsoft defender.\nRoles and permissions\nExplanation of Snowflake's security model and role requirements for data quality operations.\nSecure Agent\nThe Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing.\nSecurity monitoring\nLearn about security monitoring.\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up SAP ECC\nSet up user accounts and permissions required for SAP ECC metadata extraction in Atlan.\nSet up SAP S/4HANA\nSet up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan.\nTroubleshooting Salesforce connectivity\nLearn about troubleshooting salesforce connectivity.\nTroubleshooting ServiceNow\nWhy is the security\\_admin role required to complete the ServiceNow integration?"
  },
  {
    "url": "https://docs.atlan.com/platform/references/incident-response-plan",
    "content": "Get Started\nSecurity & Compliance\nIncident response plan\nOn this page\nIncident response plan\nAtlan's incident response plan for any potential network security incidents is as follows:\nIncident response process\nâ\nFor any critical issues, the Incident Response Team at Atlan will follow a structured process designed to investigate, contain, and remediate the threat as well as recover systems and services. The process includes:\nEvent reported   -  initial notification of the incident.\nTriage and analysisÂ   -  assessment of the incident's severity and potential impact.\nInvestigationÂ   -  detailed examination to understand the cause and scope.\nContainment and neutralizationÂ   -  actions to limit the impact and prevent further exploitation.\nRecovery and vulnerability remediationÂ   -  restoration of systems and addressing vulnerabilities.\nHardening and detection improvementsÂ   -  enhancing security measures and detection capabilities to prevent future incidents.\nKey details about this process are as follows:\nIncident managerÂ   -  oversees incident response efforts.\nWar roomÂ   -  a central location, either physical or virtual (for example, Slack), dedicated to managing the incident.\nRecurring meetingsÂ   -  regular meetings to review the incident status until resolution.\nNotificationÂ   -  legal and executive staff will be informed as required.\nIncident severity levels\nâ\nSeverity\nCategory\nDescription\nP0\nCritical\nActively exploited risk involves the engagement of a malicious actor. Identifying such active exploitation is essential. Major data breach, widespread system outage, critical vulnerability being actively exploited.\nP1\nHigh\nActive exploitation is not yet confirmed but is highly probable. The vulnerability presents a high risk, potentially causing severe performance degradation or unauthorized access to sensitive data.\nP2/P3\nMedium/Low\nSuspicious or unusual behavior that has not yet been verified and requires further investigation. This includes moderate performance issues, non-critical vulnerabilities, and isolated incidents affecting a small group of users.\nIncident reporting\nâ\nAtlan will report any breaches to customers, consumers, data subjects, and regulators without undue delay and in accordance with allÂ contractual commitments and applicable legislation.\nIf any users become aware of an information security incident, potential incident, imminent incident, unauthorized access, policy violation, security weakness, or suspicious activity, please\nnotify Atlan support\nimmediately.\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nCompliance standards and assessments\nNext\nAtlan architecture\nIncident response process\nIncident severity levels\nIncident reporting"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAutomation Integrations\nIntegrate Atlan with automation tools such as AWS Lambda, Connections, Webhooks, Browser Extension, and Always On to automate workflows and extend Atlan's capabilities.\nTags:\nintegrations\nautomation\nlambda\nwebhooks\nbrowser extension\nPrevious\nIntegrations\nNext\nAWS Lambda"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAWS Lambda\nOn this page\nAWS Lambda\nOverview:\nConnect Atlan with AWS Lambda to automate workflows, create triggers, and extend Atlan's automation capabilities.\nGet started\nâ\nFollow these steps to connect and integrate AWS Lambda with Atlan:\nSet up AWS Lambda\nCreate an AWS Lambda trigger\nGuides\nâ\nSet up AWS Lambda\n: Step-by-step instructions to set up AWS Lambda integration with Atlan.\nCreate an AWS Lambda trigger\n: How to create triggers for automated workflows using AWS Lambda.\nTags:\naws lambda\nintegration\nautomation\nPrevious\nAutomation Integrations\nNext\nSet up AWS Lambda\nGet started\nGuides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/always-on",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAlways On\nOn this page\nAlways On\nOverview:\nConnect Atlan with Always On to enable continuous automation, tag propagation, and receive suggestions from similar assets.\nReferences\nâ\nSuggestions from similar assets\n: Learn how Atlan provides suggestions based on similar assets.\nTag propagation\n: Understand how tags are propagated automatically in Atlan.\nTags:\nalways on\nintegration\nautomation\nPrevious\nCreate an AWS Lambda trigger\nNext\nSuggestions from similar assets\nReferences"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nOn this page\nBrowser Extension\nOverview:\nConnect Atlan with the Browser Extension to enhance your data catalog experience, streamline workflows, and access Atlan features directly from your browser.\nGuides\nâ\nConfigure the extension for managed browsers\n: How to configure the Atlan browser extension for managed environments.\nUse the Atlan browser extension\n: Step-by-step instructions to use the Atlan browser extension.\nConcepts\nâ\nAtlan browser extension security\n: Learn about the security features of the Atlan browser extension.\nTroubleshooting\nâ\nTroubleshooting Atlan browser extension\n: Solutions for common issues encountered when using the Atlan browser extension.\nFAQ\nâ\nAdd the browser extension for everyone in my organization\nBrowser extension shows an error message\nBrowser extension not loading\nTags:\nbrowser extension\nintegration\nautomation\nPrevious\nTag propagation\nNext\nConfigure the extension for managed browsers\nGuides\nConcepts\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/configure-the-extension-for-managed-browsers",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nHow-tos\nConfigure the extension for managed browsers\nOn this page\nConfigure the extension for managed browsers\nIf you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script.\nAtlan supports managing the Atlan browser extension for the following:\nOperating systems: macOS and Microsoft Windows\nBrowsers: Google Chrome and Microsoft Edge\nThe deployment scripts   -  .mobileconfig file for macOS and PowerShell script for Microsoft Windows   -  are designed to make only the most necessary modifications required for the Atlan browser extension to function properly. Both deployment methods adhere to the principle of least privilege:\nThe .mobileconfig file for macOS only includes the configuration settings required to install and operate the Atlan browser extension.\nThe PowerShell script creates essential registry keys required for the Atlan browser extension to operate on Microsoft Windows systems.\nTo configure the Atlan browser extension for a managed browser, you must complete these steps in the following order:\nConfigure the browser extension\nBulk install the browser extension\nDeploy the configuration script\n(Optional)\nVerify and monitor the installation\nConfigure the browser extension\nâ\nWho can do this?\nYou will need to be an\nadmin\nin Atlan to configure the browser extension for users in your organization. You will also need inputs and approval from the IT administrator of your organization.\nYou can configure the browser extension and then download a configuration script to bulk install and deploy it for everyone in your organization.\nTo configure the browser extension, from within Atlan:\nFrom the left menu on any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nIntegrations\n.\nUnder\nApps\n, expand the\nBrowser extension\ntile.\nIn theÂ\nBrowser extension\ntile, for\nBulk install the browser extension\n, click the\nSet up now\nbutton.\nIn the\nSet up browser extension\nform, enter the following details:\nFor\nChoose browser\n, the browser and operating system values will be prefilled based on what you're currently using   -  you can modify the fields, if required.\nFor\nYour\nAtlan domain\n, enter the URL of your Atlan instance   -  for example,\nhttps://(instance_name).atlan.com\n.\ninfo\nðª\nDid you know?\nIf you enable multiple Atlan domains, your users will be able to select the most relevant Atlan domain from a dropdown menu while using the browser extension. The default value in the dropdown will be the Atlan instance entered as\nYour Atlan domain\n. If your organization does not have multiple Atlan domains, only the default selection will be displayed.\n(Optional) For\nAdvanced settings\n, you can configure the following:\nIf you have multiple Atlan instances, toggle on\nMultiple Atlan domains\nand then enter the URLs of your additional Atlan instances. Click\n+\nAdd\nto add more Atlan domains.\nIf your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on\nCustom data source\ndomain\n. Click\n+ Add\nÂ to add more custom domains for data sources.\nFor\nConnector\n, select a\nsupported tool\nfor the browser extension.\nIn the adjacent field, enter the URL of your custom data source domain.\ninfo\nðª\nDid you know?\nFor any\nsupported tools\nconfigured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources.\nClick the\nDownload Script\nbutton to download the corresponding configuration script. The IT administrator(s) in your organization will need to install this configuration file in your organization's devices using a mobile device management (MDM) software. Administrative permissions to the MDM platform are required to complete the setup. Based on your operating system, the downloaded file can be one of the following two types:\n.mobileconfig\n-  use this file to configure profiles with specific settings in\nmacOS devices\n.\n.ps1\n-  use this PowerShell script to create registry keys in\nMicrosoft Windows devices\n.\nBulk install the browser extension\nâ\nWho can do this?\nYou will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin.\nYou will need to configure the\nExtensionInstallForcelist\nbrowser policy for either Google Chrome or Microsoft Edge to force-install the extension for everyone in your organization.\nThe\nExtensionInstallForcelist\nbrowser policy:\nGoverns extensions that can be silently installed and automatically enabled for all users.\nProvides extension IDs that the browser will automatically install and enable when a user logs in.\nGoogle Chrome\nâ\nTo bulk install the Atlan browser extension in Google Chrome, follow the steps in Google documentation:\nForce install apps and extensions\n.\nMicrosoft Edge\nâ\nTo bulk install the Atlan browser extension in Microsoft Edge, follow the steps in Microsoft documentation:\nForce-install an extension\n.\nFor the\nExtension/App IDs and update URLs to be silently installed (Device)\nfield, copy and paste the following value:\nfipjfjlalpnbejlmmpfnmlkadjgaaheg;https://clients2.google.com/service/update2/crx\nfipjfjlalpnbejlmmpfnmlkadjgaaheg\nis the\nextension-id\nfor the Atlan browser extension.\nhttps://clients2.google.com/service/update2/crx\nindicates that it needs to be installed from the Chrome Web Store.\nDeploy the configuration script\nâ\nWho can do this?\nYou will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin.\nThe browser extension relies on managed storage for configuring domains in the Atlan extension. The values for managed storage can be configured through:\nA configuration profile in macOS\nRegistry keys in Microsoft Windows\nAlthough Atlan's solution is platform-agnostic, the following example pertains to\nMicrosoft Intune\n.\nmacOS\nâ\nYou will need to create a custom managed profile to configure the domains for the Atlan browser extension.\nTo deploy the\n.mobileconfig\nfile for your organization, you can use any MDM platform.\nFor example:\nMicrosoft Intune   -  follow the steps in\nCustom configuration profile settings\n.\nMicrosoft Windows\nâ\nYou will need to create registry keys to deploy the extension. You can create the required registry keys with a PowerShell script, which can then be deployed to your usersâ devices using an MDM software.\nTo deploy the PowerShell configuration script for your organization, you can use any MDM platform.\nFor example:\nMicrosoft Intune   -  follow the steps in\nCreate a script policy and assign it\n. For\nScript settings\n, enter the following details:\nScript location\n-  upload the\n.ps1\nconfiguration script downloaded from Atlan.\nRun this script using the logged on credentials\n-  change to\nNo\n.\nEnforce script signature check\nÂ   -  change to\nNo\n.\nRun script in 64 bit PowerShell Host\nÂ   -  change to\nYes\n.\nVerify and monitor the installation\nâ\nTo ensure that the Atlan browser extension has been successfully deployed across all selected devices in your organization, you can:\nVerify the installation   -  after you have deployed the policies, check a few target devices to ensure that the extension was installed and configured correctly.\nMonitor compliance   -  monitor the compliance status of the policy and troubleshoot any issues.\nYour users will now be able to use the Atlan browser extension in a managed browser! ð\nOnce the managed browser has synced with the latest configuration changes for your organization, the Atlan browser extension will be automatically installed and a new tab will open to indicate that the Atlan browser extension is now active.\nTags:\natlan\ndocumentation\nPrevious\nBrowser Extension\nNext\nHow to use the Atlan browser extension\nConfigure the browser extension\nBulk install the browser extension\nDeploy the configuration script\nVerify and monitor the installation"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/enable-embedded-metadata-in-tableau",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nHow-tos\nEnable embedded metadata in Tableau\nOn this page\nEnable embedded metadata in Tableau\nAtlan metadata layers Atlan context directly onto the source application, rather than in a sidebar. This embedded experience provides users with immediate access to data lineage, quality metrics, and governance information without leaving their Tableau workflow.\nPrerequisites\nâ\nBefore enabling embedded metadata in Tableau:\nYou must have the Atlan browser extension installed. If not, see the\nHow to use the Atlan browser extension\nguide for instructions.\nTableau dashboards must be available through the browser\nPermissions required\nâ\nTo enable this feature, you need:\nAdmin\nrole in Atlan\nAccess to the\nLabs\nsettings\nEnable embedded metadata in Tableau\nâ\nTo enable the embedded Atlan experience in Tableau for your users:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAccess Control\nheading of the\nLabs\npage, turn on\nView embedded metadata in Tableau\nOnce enabled, users with the extension installed can view Atlan metadata in Tableau dashboards.\nSee also\nâ\nTroubleshoot the Atlan browser extension\nUse the Atlan browser extension\nTableau connector documentation\nTags:\nintegrations\ntableau\nbrowser-extension\nmetadata\nembedded\nautomation\nhow-to\nPrevious\nHow to use the Atlan browser extension\nNext\nAtlan browser extension security\nPrerequisites\nPermissions required\nEnable embedded metadata in Tableau\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/concepts/atlan-browser-extension-security",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nConcepts\nAtlan browser extension security\nOn this page\nAtlan browser extension security\nAtlan adheres to strict security standards for the\nbrowser extension\n. Atlan mandates security throughout the extension coding lifecycle:\nHardening configurations through content security policies,\nValidating all inputs,\nRequiring least privileges,\nEmploying defense-in-depth techniques like code obfuscation to frustrate reverse engineering,\nAccessing customer resources over secure HTTPS channels after SSL certificate verification to prevent tampering.\nAtlan follows proven CI/CD methodologies used for our SaaS application, enabling rapid and frequent updates to Atlan's Chrome extension. This allows:\nPatching identified vulnerabilities faster through new releases while simultaneously upholding stability.\nMandatory code reviews specifically focused on analyzing security to help with identifying issues before these can impact customers.\nOnce ready, both static and dynamic scanning tools rigorously test the extension codebase for any weaknesses before distribution. Atlan is committed to transparency. If any post-deployment points of concern arise, Atlan will notify impacted customers promptly and address their concerns responsibly.\nBy incorporating security into each phase   -  secure architecture, peer reviews, robust testing, and responsible disclosure   -  Atlan strives to build browser extensions with both user needs and enterprise risks top of mind.\nReach out to Atlan support\nfor any questions.\nPermissions\nâ\nWhen using Atlan's browser extension in a\nsupported tool\n, Atlan will read:\nthe URL of your browser tab\nDocument Object Model (DOM) elements such as asset title, hierarchy information, text,\ndata-test-id\nattributes, and more to locate an asset in a\nsupported source tool\n. This list may vary depending on the source tool.\nIf you're using Atlan's browser extension on any\nwebsite\n, it will only read the favicon, page title, and URL of your browser tab.\nAtlan uses the following permissions for the browser extension to work in a supported tool:\nactiveTab\n-  the\nactiveTab permission\nallows the browser extension to temporarily access the content of the active tab as you interact with the extension. This enables Atlan to display the Atlan badge and read the URL and DOM elements to locate the asset for displaying asset metadata in the sidebar.\nstorage\n-  the\nstorage permission\nallows Atlan to store information about the locally domains configured. This enables the browser extension to remember the sites that you want to use it on, even when you close and reopen your browser.\ncookies\n-  the\ncookies permission\nallows Atlan to manage cookies for maintaining session states or user preferences for a supported tool. These login cookies are only shared between your Atlan tenant and the browser extension.\ncontextMenus\n-  the\ncontextMenus permission\nallows Atlan to add context menu options (for example, right-click menus) to help you interact with the extension's features, namely\nsearch in Atlan\n, directly from any website.\nhost_permissions\n-  the\nhost permissions\nallow the browser extension to work specifically with Atlan tenants, which is the host in this case. For example,\nhttps://atlan.com/*\n,\nhttps://atlan.dev/*\n.\n\"content_scripts\": [ { \"matches\": [\"http://*/*\", \"https://*/*\"]\n-  the\ncontent_scripts key\nallows Atlan to inject Atlan's content script to any website you visit. Although this content script will be injected into all webpages, it will neither be executed nor any DOM elements captured if the webpage is not a supported tool.\nTags:\nintegration\nconnectors\nsecurity\naccess-control\npermissions\nPrevious\nEnable embedded metadata in Tableau\nNext\nTroubleshooting Atlan browser extension\nPermissions"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nTroubleshooting\nTroubleshooting Atlan browser extension\nOn this page\nTroubleshooting Atlan browser extension\nCan I add the browser extension for everyone in my organization?\nâ\nYes! To install the Atlan browser extension at the workspace level, follow the instructions in\nthis guide\n. You will need to be an administrator or have access to the admin console of your organization's Google account for this setup. Once installed, users in your organization can start\nusing Atlan's browser extension\n.\nIf your organization uses managed browsers, refer to\nHow to configure a managed browser extension\n.\nOnce you have installed the browser extension for your organization, anyone with access to Atlan and a supported tool can use the browser extension. You will first need to log into Atlan.\nFor users in your organization who do not have access to Atlan and attempt to use the extension, they will receive an error message notifying them that they do not have an Atlan account.\nWhy is the extension not loading?\nâ\nIf a blank page appears while loading the Atlan browser extension or the page is continuously loading,\nupdate your browser's security settings\nto\nBlock third party cookies in Incognito\n.\nAs long as you have access to the data you are trying to view within Atlan, you should be able to\nuse the browser extension\n.\nWhy do I get an error message when I click on the extension in Brave?\nâ\nUsers of the Brave browser may receive an error message showing that the content is blocked when they click on the Atlan browser extension. In that case, turning off the Brave Shields feature can help you access the content.\nWhy can't I see the Atlan logo in my data tool?\nâ\nIf your data tool is hosted on a custom domain and you're not seeing the Atlan logo on the bottom right of your BI tool, such as Tableau, you'll need to\nmap your data tool's custom URL in the extension\n.\nTags:\nintegration\nsetup\nPrevious\nAtlan browser extension security\nNext\nCan I add Atlan's browser extension for everyone in my organization?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/faq/add-browser-extension",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nFAQ\nCan I add Atlan's browser extension for everyone in my organization?\nCan I add Atlan's browser extension for everyone in my organization?\nRefer to\nTroubleshooting the Atlan browser extension\n.\nTags:\nintegration\nconnectors\nPrevious\nTroubleshooting Atlan browser extension\nNext\nWhy do I get an error message when I click on Atlan's browser extension?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/connections",
    "content": "Configure Atlan\nIntegrations\nAutomation\nConnections\nOn this page\nConnections Integration\nOverview:\nConnect Atlan with Connections to create webhooks, automate notifications, and streamline your data workflows.\nGuides\nâ\nHow to create webhooks\n: Step-by-step instructions to create webhooks and automate notifications in Atlan.\nTags:\nintegrations\nautomation\nconnections\nwebhooks\nPrevious\nWhy is Atlan's browser extension not loading?\nNext\nDelete a connection\nGuides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/webhooks",
    "content": "Configure Atlan\nIntegrations\nAutomation\nWebhooks\nOn this page\nWebhooks Integration\nOverview:\nConnect Atlan with Webhooks to automate actions, trigger notifications, and integrate with external systems.\nGuides\nâ\nHow to delete a connection\n: Step-by-step instructions to delete a connection and its assets using Webhooks in Atlan.\nTags:\nintegrations\nautomation\nwebhooks\nPrevious\nDelete a connection\nNext\nCreate webhooks\nGuides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nCollaboration Integrations\nIntegrate Atlan with collaboration tools such as Microsoft Teams and Slack to streamline communication and enhance teamwork around your data assets.\nTags:\nintegrations\ncollaboration\nteams\nslack\nPrevious\nCreate webhooks\nNext\nMicrosoft Teams"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication",
    "content": "Configure Atlan\nIntegrations\nCommunication\nCommunication Integrations\nIntegrate Atlan with communication tools such as SMTP and Announcements to automate notifications and keep your teams informed.\nTags:\nintegrations\ncommunication\nsmtp\nannouncements\nPrevious\nSend alerts for workflow events\nNext\nSMTP and Announcements Integration"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nIdentity Management Integrations\nIntegrate Atlan with identity management tools such as SCIM and SSO to automate user provisioning, authentication, and access control.\nTags:\nintegrations\nidentity management\nscim\nsso\nPrevious\nManage system announcements\nNext\nSCIM Integration"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management",
    "content": "Configure Atlan\nIntegrations\nProject Management\nProject Management Integrations\nIntegrate Atlan with popular project management tools such as Jira and ServiceNow. Automate ticket creation, link your accounts, and streamline your data operations with these integrations.\nTags:\nintegrations\nproject management\njira\nservicenow\nPrevious\nWhy did my users not receive an invite email from Atlan?\nNext\nJira"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-a-resource",
    "content": "Use data\nDiscovery\nAsset Management\nAdd a resource\nAdd a resource\nNeed to redirect users to important information that's outside Atlan?\nYou can add links to internal or external URLs wit\nhin an asset profile. These can help your team better understand the contextual information for your asset.\nTo add resources to your assets, follow these steps:\nOn the Atlan homepage, click\nAssets\nin the left menu.\nClick on an asset to open the asset profile.\nIn the navigation bar to the right of the asset profile, click\nResources\n.\nClick\n+Add Resource\nand paste your URL.\nFor\nTitle\n, type a title for your resource.\nYour resource is now ready to use! ð\nOnce you've added a resource, click\n+Add\nin the\nResource\nmenu to add more resources for your asset. You can also edit or delete them to curate your list of resources.Â\nDid you know?\nYou can also add any web page as a resource to your assets\nusing Atlan's Chrome extension\n.\nTags:\natlan\ndocumentation\nPrevious\nAdd certificates\nNext\nConfigure language settings"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/references/what-does-atlan-crawl-from-amazon-quicksight",
    "content": "Connect data\nBI Tools\nCloud-based BI\nAmazon QuickSight\nReferences\nWhat does Atlan crawl from Amazon QuickSight?\nOn this page\nWhat does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:\nAmazon Athena\nAmazon Redshift\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nSalesforce\nSnowflake\nAtlan crawls and maps the following assets and properties from Amazon QuickSight.\nAnalyses\nâ\nAtlan maps analyses from Amazon QuickSight to its\nQuickSightAnalysis\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nstatus\nquickSightAnalysisStatus\ncalculatedFields\nquickSightAnalysisCalculatedFields\nparameterDeclarations\nquickSightAnalysisParameterDeclarations\nfilterGroups\nquickSightAnalysisFilterGroups\nDashboards\nâ\nAtlan maps dashboards from Amazon QuickSight to its\nQuickSightDashboard\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\npublishedVersionNumber\nquickSightDashboardPublishedVersionNumber\nlastPublishedTime\nquickSightDashboardLastPublishedTime\nanalysisName\nquickSightAnalysisQualifiedName\nDatasets\nâ\nAtlan maps datasets from Amazon QuickSight to its\nQuickSightDataset\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nImportMode\nquickSightDatasetImportMode\noutputColumnCount\nquickSightDatasetColumnCount\nFolders\nâ\nAtlan maps folders from Amazon QuickSight to its\nQuickSightFolder\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nFolderType\nquickSightFolderType\nSharingModel\nquickSightFolderSharingModel\nfolderHierarchy\nquickSightFolderHierarchy\nDataset fields\nâ\nAtlan maps dataset fields from Amazon QuickSight to its\nQuickSightDatasetField\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\ntype\nquickSightDatasetFieldType\ndatasetQualifiedName\nquickSightDatasetQualifiedName\nAnalysis visuals\nâ\nAtlan maps analysis visuals from Amazon QuickSight to its\nQuickSightAnalysisVisual\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nId\nquickSightId\nanalysisQualifiedName\nquickSightAnalysisQualifiedName\nsheetId\nquickSightSheetId\nsheetName\nquickSightSheetName\nDashboard visuals\nâ\nAtlan maps dashboard visuals from Amazon QuickSight to its\nQuickSightDashboardVisual\nasset type.\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nId\nquickSightId\ndashboardQualifiedName\nquickSightDashboardQualifiedName\nsheetId\nquickSightSheetId\nsheetName\nquickSightSheetName\nTags:\nconnectors\ndata\ncrawl\nsalesforce\nPrevious\nCrawl Amazon QuickSight\nNext\nPreflight checks for Amazon QuickSight\nAnalyses\nDashboards\nDatasets\nFolders\nDataset fields\nAnalysis visuals\nDashboard visuals"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud",
    "content": "Connect data\nETL Tools\ndbt\nReferences\nWhat does Atlan crawl from dbt Cloud?\nOn this page\nWhat does Atlan crawl from dbt Cloud?\nAtlan crawls and maps the following assets and properties from dbt Cloud. Atlan also supports lineage between the following:\ndbt models\ndbt seeds\ndbt sources\nSQL tables and views materialized by dbt models, dbt seeds, dbt sources\nColumn-level lineage for these entities\nwarning\nAtlan only crawls dbt assets that are in the âappliedâ (built) state in dbt Cloud. Models must be part of a successful run to be picked up during crawling; models that are only defined in your project files but havenât been executed wonât be included. For more information about project state, see\nProject states in dbt Cloud.\nOnce you've\ncrawled dbt\n, you can\nuse dbt-specific filters\nfor quick asset discovery:\nTest status\n-  filter\ndbt tests\nthat passed, failed, or have a warning or error\nAlias\n-  filter by the name of a dbt model's identifier in the dbt project\nUnique id\n-  filter by the unique node identifier of a dbt model\nProject name\n-  filter by dbt project name, only supported for\ndbt Core version 1.6+\nEnvironment name\n-  filter by dbt environment name\nJob status\n-  filter by dbt job status\nLast job run\n-  filter by the last run of the dbt job\nAtlan's dbt connectivity also populates custom metadata to further enrich the assets in Atlan. The\nAtlan dbt-specific property\ncolumn in the tables below gives the name of the mapped custom metadata property in Atlan.\nDid you know?\nAtlan enables you to\nsync your dbt tags\nand update your dbt assets with the synced tags. It's also possible to\nmap other metadata on Atlan's assets through your dbt models\n.\nTables\nâ\nAtlan maps tables from dbt Cloud to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndescription\ndescription\nasset profile and overview sidebar\n[collected via REST API]\nassetDbtTestStatus\nAPI only\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\naccountName\nassetDbtAccountName\nasset filter\nprojectName\nassetDbtProjectName\nasset filter and overview sidebar\npackageName\nassetDbtPackageName\nasset filter and properties sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\nenvironment.name (collected via REST API)\nassetDbtEnvironmentName\nAPI only\nColumns\nâ\nAtlan maps columns from dbt Cloud to its\nColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndescription\ndescription\nasset profile and overview sidebar\n[collected via REST API]\nassetDbtTestStatus\nAPI only\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\naccountName\nassetDbtAccountName\nasset filter\nprojectName\nassetDbtProjectName\nasset filter and overview sidebar\npackageName\nassetDbtPackageName\nasset filter and properties sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\nModels\nâ\nAtlan maps models from dbt Cloud to its\nModel\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nowner\nsourceCreatedBy\nasset profile and properties sidebar\n[dynamically generated using accountId, projectId, and uniqueId]\nsourceURL\noverview sidebar\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\naccountName\nassetDbtAccountName\nasset filter\nprojectName\nassetDbtProjectName\nasset filter and overview sidebar\npackageName\nassetDbtPackageName\nasset filter and properties sidebar\nrawCode (available via REST API)\ndbtRawSQL\noverview sidebar\ncompiledCode (available via REST API)\ndbtCompiledSQL\noverview sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\nmaterializedType\ndbtMaterializationType\nAPI only\nstats\ndbtStats\nAPI only\nexecutionInfo.lastRunStatus\ndbtJobRuns.dbtModelRunStatus\noverview sidebar\njob.status (available via REST API)\ndbtJobRuns.dbtJobRunStatus\noverview sidebar\njob.name (available via REST API)\ndbtJobRuns.dbtJobName\noverview sidebar\nexecutionInfo.lastJobDefinitionId\ndbtJobRuns.dbtJobId\noverview sidebar\nexecutionInfo.lastRunId\ndbtJobRuns.dbtJobRunId\nAPI only\nexecutionInfo.lastRunGeneratedAt\ndbtJobRuns.dbtJobRunCompletedAt\noverview sidebar\nenvironmentId\ndbtJobRuns.dbtEnvironmentId\nAPI only\nenvironment.name (available via REST API)\ndbtJobRuns.dbtEnvironmentName\noverview sidebar\ncompiledCode\ndbtJobRuns.dbtCompiledCode\nAPI only\nSources\nâ\nAtlan maps sources from dbt Cloud to its\nDbtSource\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nAsset profile and overview sidebar\ndescription\ndescription\nAsset profile and overview sidebar\nowner\nsourceCreatedBy\nAsset profile and properties sidebar\n[dynamically generated using accountId, projectId, and uniqueId]\nsourceURL\nOverview sidebar\nalias\nassetDbtAlias\nAsset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nAsset filter and overview sidebar\naccountName\nassetDbtAccountName\nAsset filter\nprojectName\nassetDbtProjectName\nAsset filter and overview sidebar\npackageName\nassetDbtPackageName\nAsset filter and properties sidebar\ntags\nassetDbtTags\nAsset filter and overview sidebar\nstats\ndbtStats\nAPI only\nfreshness\nassetDbtSourceFreshnessCriteria\nOverview sidebar\nenvironmentId\ndbtJobRuns.dbtEnvironmentId\nAPI only\nenvironment.name\n(available via REST API)\ndbtJobRuns.dbtEnvironmentName\nOverview sidebar\nTests\nâ\nAtlan maps tests from dbt Cloud to its\nTest\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\n[dynamically generated using accountId, projectId, and uniqueId]\nsourceURL\noverview sidebar\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\naccount (name)\nassetDbtAccountName\nasset filter\nproject (name)\nassetDbtProjectName\nasset filter, overview and properties sidebar\npackageName\nassetDbtPackageName\nasset filter and properties sidebar\nrawCode (available via REST API)\ndbtTestRawCode\noverview sidebar\ncompiledCode (available via REST API)\ndbtTestCompiledCode\noverview sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\nstats\ndbtStats\nAPI only\nexecutionInfo.lastRunError\ndbtTestError\noverview sidebar\nexecutionInfo.lastRunStatus\ndbtJobRuns.dbtTestRunStatus\noverview sidebar\njob.status (available via REST API)\ndbtJobRuns.dbtJobRunStatus\noverview sidebar\njob.name (available via REST API)\ndbtJobRuns.dbtJobName\noverview sidebar\nexecutionInfo.lastJobDefinitionId\ndbtJobRuns.dbtJobId\noverview sidebar\nexecutionInfo.lastRunId\ndbtJobRuns.dbtJobRunId\nAPI only\nexecutionInfo.lastRunGeneratedAt\ndbtJobRuns.dbtJobRunCompletedAt\noverview sidebar\nenvironmentId\ndbtJobRuns.dbtEnvironmentId\nAPI only\nenvironment.name (available via REST API)\ndbtJobRuns.dbtEnvironmentName\noverview sidebar\ncompiledCode\ndbtJobRuns.dbtCompiledCode\nAPI only\nSeeds\nâ\nAtlan maps models from dbt Core to its\nSeed\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nexecuteCompletedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nowner\nsourceCreatedBy\nasset profile and properties sidebar\nstatus\ndbtJobRuns.dbtModelRunStatus\noverview sidebar\nSource property\nAtlan dbt-specific property\nWhere in Atlan\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\nstats\ndbtSeedStats\nAPI only\nfilePath\ndbtSeedfilePath\nasset profile and overview sidebar\nPrevious\nAdd impact analysis in GitLab\nNext\nWhat does Atlan crawl from dbt Core?\nTables\nColumns\nModels\nSources\nTests\nSeeds"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nReferences\nWhat does Atlan crawl from Google BigQuery?\nOn this page\nWhat does Atlan crawl from Google BigQuery?\nOnce you have\ncrawled Google BigQuery\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nTables\n-  BigQuery labels and\nIs sharded\nfilters\nAtlan doesn't run any table scans. Atlan leverages the table preview options from\nGoogle BigQuery\nÂ that enable you to view data for free and without affecting any quotas using the\ntabledata.list\nAPI. Hence,\ntable\nasset previews in Atlan are already cost-optimized. However, this doesn't apply to\nviews\nand\nmaterialized views\n.\nFor Google BigQuery\nviews\nand\nmaterialized views\n, Atlan sends you a cost nudge before viewing a sample data preview. This informs you about the precise bytes that are spent during the execution of the query, helping you decide if you still want to run the preview.\nDid you know?\nYou also receive a cost nudge before\nquerying your Google BigQuery assets\n.\nAtlan crawls and maps the following assets and properties from Google BigQuery.\nDatabases\nâ\nAtlan maps projects from Google BigQuery to its\nDatabase\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nProject ID\nname\nasset preview, profile, and filter, overview sidebar\nSchemas\nâ\nAtlan maps datasets from Google BigQuery to its\nSchema\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_SCHEMA\nname\nasset preview and profile, overview sidebar\nTABLE_COUNT\ntableCount\nasset preview and profile\nVIEW_COUNT\nviewsCount\nasset preview and profile, overview sidebar\nTABLE_CATALOG\ndatabaseName\nasset preview\nREMARKS\ndescription\nasset preview and profile, overview sidebar\nCREATED\nsourceCreatedAt\nasset profile and properties sidebar\nMODIFIED\nsourceUpdatedAt\nasset profile and properties sidebar\nTables\nâ\nDid you know?\nTable\nasset previews are already cost-optimized.\nGoogle BigQuery\nenables you to use the table preview options to view data for free and without affecting any quotas. Note that this isn't currently supported for\nGoogle BigQuery views and materialized views\nin Atlan.\nAtlan maps tables from Google BigQuery to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview and profile, overview sidebar\nREMARKS\ndescription\nasset preview and profile, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview and profile, overview sidebar\nROW_COUNT\nrowCount\nasset preview, profile, and filter, overview sidebar\nSIZE_BYTES\nsizeBytes\nasset filter and overview sidebar\nTABLE_TYPE\nsubType\nasset preview and profile\nLABELS\nassetTags\noverview sidebar\nCREATED\nsourceCreatedAt\nasset profile and properties sidebar\nMODIFIED\nsourceUpdatedAt\nasset profile and properties sidebar\nOPTION_NAMES (require_partition_filter)\nisPartitioned\nAPI only\nViews\nâ\nAtlan maps views from Google BigQuery to its\nView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview and profile, overview sidebar\nREMARKS\ndescription\nasset preview and profile, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview and profile, overview sidebar\nTABLE_TYPE\nsubType\nasset preview and profile\nCREATED\nsourceCreatedAt\nasset profile and properties sidebar\nMODIFIED\nsourceUpdatedAt\nasset profile and properties sidebar\nOPTION_NAMES (require_partition_filter)\nisPartitioned\nAPI only\nDDL\ndefinition\nasset profile and overview sidebar\nMaterialized views\nâ\nAtlan maps materialized views from Google BigQuery to its\nMaterialisedView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview and profile, overview sidebar\nREMARKS\ndescription\nasset preview and profile, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview and profile, overview sidebar\nROW_COUNT\nrowCount\nasset preview, profile, and filter, overview sidebar\nSIZE_BYTES\nsizeBytes\nasset filter and overview sidebar\nTABLE_TYPE\nsubType\nasset preview and profile\nCREATED\nsourceCreatedAt\nasset profile and properties sidebar\nMODIFIED\nsourceUpdatedAt\nasset profile and properties sidebar\nOPTION_NAMES (require_partition_filter)\nisPartitioned\nAPI only\nDDL\ndefinition\nasset profile and overview sidebar\nColumns\nâ\nAtlan supports\nnested columns up to level 1\nfor Google BigQuery to help you enrich your semi-structured data types. You can view nested columns in the asset sidebar for your table assets. Atlan maps columns from Google BigQuery to its\nColumn\nasset type.\nImportant\nAtlan doesn't crawl primary key (PK) and foreign key (FK) information from Google BigQuery.\nSource property\nAtlan property\nWhere in Atlan\nCOLUMN_NAME\nname\nasset preview and profile, overview sidebar\nREMARKS, DESCRIPTION\ndescription\nasset preview and profile, overview sidebar\nORDINAL_POSITION\norder\nasset profile\nTYPE_NAME\ndataType\nasset preview, profile, and filter, overview sidebar\nIS_NULLABLE\nisNullable\nAPI only\nIS_PARTITIONING_COLUMN\nisPartition\nasset preview, profile, and filter\nCLUSTERING_COLUMN_LIST\nisClustered\nasset preview, profile, and filter\nStored procedures\nâ\nAtlan maps stored procedures from Google BigQuery to its\nProcedure\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nPROCEDURE_NAME\nname\nAPI only\nREMARKS\ndescription\nAPI only\nPROCEDURE_TYPE\nsubType\nAPI only\nROUTINE_DEFINITION\ndefinition\nAPI only\nCREATED\nsourceCreatedAt\nAPI only\nMODIFIED\nsourceUpdatedAt\nAPI only\nTags:\nconnectors\ndata\ncrawl\nPrevious\nManage Google BigQuery tags\nNext\nPreflight checks for Google BigQuery\nDatabases\nSchemas\nTables\nViews\nMaterialized views\nColumns\nStored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/references/what-does-atlan-crawl-from-ibm-cognos-analytics",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nReferences\nWhat does Atlan crawl from IBM Cognos Analytics?\nOn this page\nWhat does Atlan crawl from IBM Cognos Analytics?\nAtlan crawls and maps the following assets and properties from IBM Cognos Analytics.\nAtlan also supports lineage:\nFor packages, files, reports, and modules.\nTo upstream sources   -  Microsoft SQL Server and Snowflake.\nField-level lineage is currently not supported.\nAtlan generates the\nsourceURL\nproperty for IBM Cognos Analytics assets using a combination of the host, port, and\nid\nof the asset. This allows Atlan to help you view your assets directly in IBM Cognos Analytics from the asset profile.\nDirect extraction\n-  in addition to\nid\n, Atlan obtains the host and port values from the credentials you provided while setting up a crawler workflow.\nOffline extraction\n-  in addition to\nid\n, Atlan obtains the host and port values from the parameters with which the offline extractor is executed.\nImportant\nAssets marked with ð includes lineage and column-level lineage.\nAssets marked with ð display column information.\nFolders\nâ\nAtlan maps folders from IBM Cognos Analytics to its\nCognosFolder\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nDashboards ð\nâ\nAtlan maps dashboards from IBM Cognos Analytics to its\nCognosDashboard\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nPackages ð\nâ\nAtlan maps packages from IBM Cognos Analytics to its\nCognosPackage\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nExplorations ð\nâ\nAtlan maps explorations from IBM Cognos Analytics to its\nCognosExploration\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nReports ð\nâ\nAtlan maps reports from IBM Cognos Analytics to its\nCognosReport\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nFiles ð\nâ\nAtlan maps files from IBM Cognos Analytics to its\nCognosFile\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nData sources\nâ\nAtlan maps data sources from IBM Cognos Analytics to its\nCognosDatasource\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nhidden\ncognosIsHidden\ncreationTime\nsourceCreatedAt\nmodificationTime\nsourceUpdatedAt\nowner\nsourceOwners\nconnectionString\ncognosDatasourceConnectionString\nModules ð\nâ\nAtlan maps modules from IBM Cognos Analytics to its\nCognosModule\nasset type.\nSource property\nAtlan property\ndefaultName\nname\ndefaultDescription\ndescription\nid\ncognosId\nsearchPath\ncognosPath\ntype\ncognosType\nversion\ncognosVersion\nmodificationTime\nsourceUpdatedAt\nColumns\nâ\nAtlan maps fields from IBM Cognos Analytics to its CognosColumns asset type. Based on the asset type, some attributes may not be extracted:\ncognosColumnRegularAggregate\nappears only for reports and datasets.\ncognosColumnDatatype\nappears only for modules.\nSource property\nAtlan property\nlabel\nname\ndatatype\ncognosDatatype\nregularAggregate\ncognosRegularAggregate\nTags:\nconnectors\ncrawl\nPrevious\nCrawl on-premises IBM Cognos Analytics\nNext\nTroubleshooting IBM Cognos Analytics connectivity\nFolders\nDashboards ð\nPackages ð\nExplorations ð\nReports ð\nFiles ð\nData sources\nModules ð\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/references/what-does-atlan-crawl-from-looker",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nReferences\nWhat does Atlan crawl from Looker?\nOn this page\nWhat does Atlan crawl from Looker?\nAtlan crawls and maps the following assets and properties from Looker.\nAtlan also supports the following lineage:\nAsset-level lineage for views, models, looks, dashboards, tiles, and explores.\nField-level lineage for views, explores, looks, tiles, and dashboards.\nLineage between explore fields and dashboards. This allows you to view all the fields used in a given dashboard and trace their upstream lineage to SQL columns.\nCross-project lineage for Looker assets. For example, if an explore includes a view from an imported project, Atlan will parse\nproject manifest files\nto generate lineage.\nLooker refinements\nfor views and explores. Atlan will parse\nproject manifest files\nto generate lineage. Refined fields for views and explores are displayed with a\nRefinement\nlabel in Atlan.\ndanger\nCurrently Atlan only represents the assets marked with ð in lineage.\nConnections\nâ\nAtlan maps connections from Looker to its\nConnection\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nhost\nhost\nAPI only\nport\nport\nAPI only\ndatabase\ndatabase\nAPI only\nschema\nschema\nAPI only\ndialect_name\ndialect_name\nAPI only\ncreated_at\nsourceCreatedAt\nasset profile and properties sidebar\nProjects\nâ\nAtlan maps projects from Looker to its\nLookerProject\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nViews ð\nâ\nAtlan maps views from Looker to its\nLookerView\nasset type. To trace the upstream lineage of these views, Atlan currently supports\nSQL-based derived tables\n. Persistent derived tables (PDTs) and Liquid parameterized tables are currently not supported. However, Atlan will always catalog the associated views.\nAtlan also supports\nview refinements\n. Atlan includes the fields from refinements in the parent view asset, and marks the fields with a\nRefinement\nlabel. You can hover over the label to view the file path and line number where the refinement is defined.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nproject_name\nprojectName\nAPI only\nModels ð\nâ\nAtlan maps models from Looker to its\nLookerModel\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nlabel\nname\nasset profile and overview sidebar\nproject_name\nprojectName\nAPI only\nProject not found\ncertificateStatus (DEPRECATED)\nasset profile and overview sidebar\nProject not found\ncertificateStatusMessage (\"Project attached to this model was not found by the workflow in Looker.\")\nasset profile and overview sidebar\nFolders\nâ\nAtlan maps folders from Looker to its\nLookerFolder\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncontent_metadata_id\nsourceContentMetadataId\nproperties sidebar\ncreator_id\nsourceCreatorId\nAPI only\nchild_count\nsourceChildCount\nasset preview and profile\nparent_id\nsourceParentID\nAPI only\ncreated_at\nsourceCreatedAt\nasset profile and properties sidebar\nFields ð\nâ\nFor explores\nâ\nAtlan maps fields for explores from Looker to its\nLookerField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nlabel\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncategory\nsubType\nasset preview and profile\nproject_name\nprojectName\nAPI only\nmodel_name\nmodelName\nAPI only\nparsed from LookML files\nlookerFieldIsRefined\nasset preview and profile, overview sidebar\nparsed from LookML files\nlookerFieldRefinementFilePath\nasset preview and profile, overview sidebar\nparsed from LookML files\nlookerFieldRefinementLineNumber\nasset preview and profile, overview sidebar\nFor views\nâ\nAtlan maps fields for views from Looker to its\nLookerField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncategory\nsubType\nasset preview and profile\nproject_name\nprojectName\nAPI only\nparsed from LookML files\nlookerFieldIsRefined\nasset preview and profile, overview sidebar\nparsed from LookML files\nlookerFieldRefinementFilePath\nasset preview and profile, overview sidebar\nparsed from LookML files\nlookerFieldRefinementLineNumber\nasset preview and profile, overview sidebar\nFor looks\nâ\nAtlan maps fields for looks from Looker to its\nLookerField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset preview, profile, and filter, and overview sidebar\nlook\nlook\nAPI only\nFor tiles\nâ\nAtlan maps fields for tiles from Looker to its\nLookerField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset preview, profile, and filter, and overview sidebar\ntile\ntile\nAPI only\nFor dashboards\nâ\nAtlan maps fields for dashboards from Looker to its\nLookerField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset preview, profile, and filter, and overview sidebar\ndashboard\ndashboard\nAPI only\nLooks ð\nâ\nAtlan maps looks from Looker to its\nLookerLook\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nfolder_name\nfolderName\nAPI only\nuser_id\nsourceUserId\nAPI only\nview_count\nsourceViewCount\nasset preview and profile, overview sidebar\nlast_updater_id\nsourceLastUpdaterId\nAPI only\nlast_updater_name\nsourceUpdatedBy\nasset profile and properties sidebar\nuser_name\nsourceOwners\nasset profile and properties sidebar\ncontent_metadata_id\nsourceContentMetadataId\nproperties sidebar\nquery_id\nlookerSourceQueryId\nAPI only\nlast_accessed_at\nsourceLastAccessedAt\nAPI only\nlast_viewed_at\nsourceLastViewedAt\nAPI only\ncreated_at\nsourceCreatedAt\nasset profile and properties sidebar\nupdated_at\nsourceUpdatedAt\nasset profile and properties sidebar\nDashboards ð\nâ\nAtlan maps dashboards from Looker to its\nLookerDashboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nslug\nlookerSlug\nAPI only\nfolder_name\nfolderName\nAPI only\nuser_id\nsourceUserId\nAPI only\nview_count\nsourceViewCount\nasset preview and profile, overview sidebar\nlast_updater_id\nsourceLastUpdaterId\nAPI only\nlast_updater_name\nsourceUpdatedBy\nasset profile and properties sidebar\nuser_name\nsourceOwners\nasset profile and properties sidebar\ncontent_metadata_id\nsourceMetadataId\nproperties sidebar\nlast_accessed_at\nsourceLastAccessedAt\nAPI only\nlast_viewed_at\nsourceLastViewedAt\nAPI only\ncreated_at\nsourceCreatedAt\nasset profile and properties sidebar\nupdated_at\nsourceUpdatedAt\nasset profile and properties sidebar\nTiles ð\nâ\nAtlan maps tiles from Looker to its\nLookerTile\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\ntitle\nof\nLook Deleted\ncertificateStatus (DEPRECATED)\nasset profile and overview sidebar\ntitle\nof\nLook Deleted\ncertificateStatusMessage (\"Look attached to this tile has been deleted. This tile might be unusable.\")\nasset profile and overview sidebar\nbody_text\ndescription\nasset profile and overview sidebar\nlookml_link_id\nlookml_link_id\nAPI only\nmerge_result_id\nmerge_result_id\nAPI only\nnote_text\nnoteText\noverview sidebar\nquery_id\nlookerQueryID\nAPI only\nresult_maker_id\nresultMakerID\nproperties sidebar\nlook_id\nlookId\nAPI only\nsubtitle_text\nsubtitleText\noverview sidebar\ntype\nsubType\nasset preview and profile\nExplores ð\nâ\nAtlan maps explores from Looker to its\nLookerExplore\nasset type.\nAtlan also supports\nexplore refinements\n:\nFor explores defined in the same model, Atlan includes the fields from refinements in the parent explore asset.\nFor explores with the same name that are defined in a different model, Atlan will create a new explore asset.\nIn both cases, Atlan marks the fields with a\nRefinement\nlabel. You can hover over the label to view the file path and line number where the refinement is defined.\nSource property\nAtlan property\nWhere in Atlan\ntitle\n,\nname\n, or\nid\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nmodel_name\nmodelName\nAPI only\nconnection_name\nsourceConnectionName\nAPI only\nuser_name\nsourceOwners\nasset profile and properties sidebar\nview_name\nviewName\nasset profile and properties sidebar\nsql_table_name\nsqlTableName\nAPI only\nproject_name\nprojectName\nAPI only\nTags:\ncrawl\nmodel\nPrevious\nCrawl on-premises Looker\nNext\nPreflight checks for Looker\nConnections\nProjects\nViews ð\nModels ð\nFolders\nFields ð\nLooks ð\nDashboards ð\nTiles ð\nExplores ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/references/what-does-atlan-crawl-from-mode",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMode\nReferences\nWhat does Atlan crawl from Mode?\nOn this page\nWhat does Atlan crawl from Mode?\nAtlan crawls and maps the following assets and properties from Mode.\nWorkspaces\nâ\nAtlan maps workspaces from Mode to its\nModeWorkspace\nasset type.\nSource property\nAtlan property\nname\nname\nid\nmodeId\ntoken\nmodeToken\nusername\nmodeWorkspaceUsername\nspace_count\nmodeCollectionCount\n_links.web.href\nsourceURL\ncreated_at\nsourceCreatedAt\nCollections\nâ\nAtlan maps collections from Mode to its\nModeCollection\nasset type.\nSource property\nAtlan property\nname\nname\ndescription\ndescription\nid\nmodeId\ntoken\nmodeToken\nspace_type\nmodeCollectionType\nstate\nmodeCollectionState\n_links.creator\nsourceCreatedBy\n_links.web\nsourceURL\nextras.workspace.name\nmodeWorkspaceName\nArray of reports\nmodeReports\nReports\nâ\nAtlan maps reports from Mode to its\nModeReport\nasset type.\nSource property\nAtlan property\nname\nname\ndescription\ndescription\nid\nmodeId\ntoken\nmodeToken\ncreated_at\nsourceCreatedAt\nupdated_at\nsourceUpdatedAt\n_links.creator\nsourceCreatedBy\n_links.web\nsourceURL\nspace_token\nmodeCollectionToken\naccount_username\nmodeWorkspaceUsername\npublished_at\nmodeReportPublishedAt\nquery_count\nmodeQueryCount\nchart_count\nmodeChartCount\nquery_preview\nmodeQueryPreview\npublic\nmodeIsPublic\nshared\nmodeIsShared\nview_count\npopularityScore\narchived\nmodeIsArchived\nArray of collections\nmodeCollections\nQueries\nâ\nAtlan maps queries from Mode to its\nModeQuery\nasset type.\nSource property\nAtlan property\nname\nname\nid\nmodeId\ntoken\nmodeToken\ncreated_at\nsourceCreatedAt\nupdated_at\nsourceUpdatedAt\n_links.creator.href\nsourceCreatedBy\nraw_query\nmodeRawQuery\nexplorations_count\nmodeExplorationCount\nreport_imports_count\nmodeReportImportCount\ndata_source_id\nmodeDatasourceId\ndatasource.name\nor\nextras.datasource.name\nmodeDatasourceName\nextras.report.name\nmodeReportName\nCharts\nâ\nAtlan maps charts from Mode to its\nModeChart\nasset type.\nSource property\nAtlan property\nname\nname\ndescription\ndescription\ntoken\nmodeToken\nview_vegas.chartType\nmodeChartType\ncreated_at\nsourceCreatedAt\nupdated_at\nsourceUpdatedAt\n_links.creator.href\nsourceCreatedBy\n_links.report_viz_web.href\nsourceURL\nextras.query.name\nmodeQueryName\nTags:\nintegration\nconnectors\nPrevious\nCrawl Mode\nNext\nPreflight checks for Mode\nWorkspaces\nCollections\nReports\nQueries\nCharts"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/references/what-does-atlan-crawl-from-qlik-sense-cloud",
    "content": "Connect data\nBI Tools\nCloud-based BI\nQlik Sense Cloud\nReferences\nWhat does Atlan crawl from Qlik Sense Cloud?\nOn this page\nWhat does Atlan crawl from Qlik Sense Cloud?\nAtlan crawls and maps the following assets and properties from Qlik Sense Cloud.\nOnce you've\ncrawled Qlik Sense Cloud\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nApps\nSheets\nDatasets\nLineage\nâ\nAtlan only supports asset-level lineage for the following asset types:\nDatasets --> Charts --> Sheets --> Apps\nSpaces\nâ\nAtlan maps spaces from Qlik Sense Cloud to its\nQlikSpace\nasset type.\nSource property\nAtlan property\ntype\nqlikSpaceType\nownerId\nqlikOwnerId\nid\nqlikId\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nApps\nâ\nAtlan maps apps from Qlik Sense Cloud to its\nQlikApp\nasset type.\n!important\nOnly the\napp\nresource type is retrieved. Other types, such as\nqvapp\nor\nqlikview\n, are not crawled.\nSource property\nAtlan property\nattributes.name\nname\nattributes.description\ndescription\nattributes.resourceId\nqlikId\nstatic_byte_size\nqlikAppStaticByteSize\nattributes.spaceId\nqlikSpaceId\nattributes.resourceCreatedAt\nsourceCreatedAt\nattributes.resourceUpdatedAt\nsourceUpdatedAt\nattributes.ownerId\nqlikOwnerId\nattributes.resourceAttributes.originAppId\nqlikOriginAppId\nattributes.resourceAttributes.hasSectionAccess\nqlikHasSectionAccess\nattributes.resourceAttributes.directQueryMode\nqlikIsDirectQueryMode\nattributes.resourceAttributes.published\nqlikIsPublished\nSheets\nâ\nAtlan maps sheets from Qlik Sense Cloud to its\nQlikSheet\nasset type.\nSource property\nAtlan property\nqProperty.qMetaDef.title\nname\nqProperty.qMetaDef.description\ndescription\nqProperty.qInfo.qId\nqlikId\nspaceId\nqlikSpaceId\nappId\nqlikAppId\napproved\nqlikIsApproved\npublished\nqlikIsPublished\nCharts\nâ\nAtlan maps charts from Qlik Sense Cloud to the\nQlikChart\nasset type and catalogs only those linked to dataset fields. For example, table charts are crawled because their columns represent dataset dimensions or measures. UI elements that do not reference dataset fields  - such as filters, buttons, and text elements  - are ignored.\ndanger\nThese elements are not considered charts and are not crawled:\nfilterpane\n,\nqlik-button-for-navigation\n,\nVizlibAdvancedTextObject\n,\nlistbox\n,\naction-button\n,\nVizlibFilter\n,\nvariable\n,\ntext-image\n,\nVizlibLineObject\n.\nSource property\nAtlan property\nqProperty.qInfo.qId\nqlikId\nqProperty.subtitle\nqlikChartSubtitle\nqProperty.footnote\nqlikChartFootnote\nqProperty.qInfo.qType\nqlikChartType\nqProperty.options.dimensionsOrientation\nqlikChartOrientation\nDatasets\nâ\nAtlan maps datasets from Qlik Sense Cloud to the\nQlikDataset\nasset type. Datasets loaded through the Data Load Editor are called\nimplicit datasets\nin Atlan and appear under this type.\nSource property\nAtlan property\nid\nqlikId\nresourceAttributes.technicalName\nqlikDatasetTechnicalName\nresourceAttributes.dataStoreType\nqlikDatasetType\nresourceAttributes.uri\nqlikDatasetUri\nresourceAttributes.secureQri\nqlikQRI\nresourceSubType\nqlikDatasetSubtype\nownerId\nqlikOwnerId\nresourceCreatedAt\nsourceCreatedAt\nresourceUpdatedAt\nsourceUpdatedAt\nspaceId\nqlikSpaceId\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl Qlik Sense Cloud\nNext\nPreflight checks for Qlik Sense Cloud\nLineage\nSpaces\nApps\nSheets\nCharts\nDatasets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/references/what-does-atlan-crawl-from-redash",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nRedash\nReferences\nWhat does Atlan crawl from Redash?\nOn this page\nWhat does Atlan crawl from Redash?\nAtlan crawls and maps the following assets and properties from Redash.\nOnce you've\ncrawled Redash\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nQueries\n-  Schedule, Is Published, and Redash tags filters\nVisualizations\n-  Type filter\nDashboards\n-  Redash tags filter\ndanger\nCurrently, Atlan only represents the assets marked with ð in lineage.\nQueries ð\nâ\nAtlan maps queries from Redash to its\nRedashQuery\nasset type.\nSource property\nAtlan property\nname\nname\ncreated_at\nsourceCreatedAt\nupdated_at\nsourceUpdatedAt\nquery\nquery\nDashboards ð\nâ\nAtlan maps dashboards from Redash to its\nRedashDashboard\nasset type.\nSource property\nAtlan property\nwidgets\nwidget_count\nVisualizations ð\nâ\nAtlan maps visualization elements from Redash to its\nRedashVisualization\nasset type.\nSource property\nAtlan property\nname\nname\ntype\ntype\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Redash\nNext\nPreflight checks for Redash\nQueries ð\nDashboards ð\nVisualizations ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce",
    "content": "Connect data\nCRM\nSalesforce\nReferences\nWhat does Atlan crawl from Salesforce?\nOn this page\nWhat does Atlan crawl from Salesforce?\nAtlan only performs\nGET\nrequests on these five endpoints:\nsObject Basic Information\nQuery\nReports\nDashboards\nFolders\nDid you know?\nEach endpoint will be set in its own OAuth client session. For every API request, it will hit the Salesforce login endpoint, which means there will be at least five (same as the number of endpoints above) login entries in your Salesforce account's login history within the duration of the scheduled workflow run.\nAtlan crawls and maps the following assets and properties from Salesforce.\nOnce you've\ncrawled Salesforce\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nFields\n-  Is encrypted and Is required filters\nOrganizations\nâ\nAtlan maps organizations from Salesforce to its\nSalesforceOrganization\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nid\nsourceId\noverview sidebar\nname\nname\nasset preview and profile, overview sidebar\ndescription\ndescription\nasset preview and profile, overview sidebar\nwebUrl\nsourceURL\noverview sidebar\ncreatedDate\nsourceCreatedAt\nasset profile and properties sidebar\nlastModifiedDate\nsourceUpdatedAt\nasset profile and properties sidebar\ncreatedBy\nsourceCreatedBy\nasset profile and properties sidebar\nlastModifiedBy\nsourceUpdatedBy\nasset profile and properties sidebar\nObjects\nâ\nAtlan maps objects from Salesforce to its\nSalesforceObject\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nlabel\nname\nasset preview and profile, overview sidebar\nname\napiName\noverview sidebar\ndescription\ndescription\nasset preview and profile, overview sidebar\ncustom\nisCustom\nasset preview and profile, overview sidebar\nmergable\nisMergable\nAPI only\nqueryable\nisQueryable\nAPI only\nfieldCount\nfieldCount\nasset preview and overview sidebar\nwebUrl\nsourceURL\noverview sidebar\nlastModifiedDate\nsourceUpdatedAt\nasset profile and properties sidebar\nlastModifiedBy\nsourceUpdatedBy\nasset profile and properties sidebar\nFields\nâ\nAtlan maps fields from Salesforce to its\nSalesforceField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nlabel\nname\nasset preview and profile, overview sidebar\nname\napiName\noverview sidebar\ntype\ndataType\nasset preview, profile, and filter, overview sidebar\ndescription\ndescription\nasset preview and profile, overview sidebar\nlastModifiedDate\nsourceUpdatedAt\nasset profile and properties sidebar\nlastModifiedBy\nsourceUpdatedBy\nasset profile and properties sidebar\ncalculated\nisCalculated\nAPI only\ncalculatedFormula\nformula\noverview sidebar\ndefaultValue\ndefaultValue\nAPI only\ncaseSensitive\nisCaseSensitive\nAPI only\ncustom\nisCustom\nasset preview and profile, overview sidebar\nencrypted\nisEncrypted\nAPI only\nnillable\nisNullable\noverview sidebar (\nEmpty values allowed\nfield)\npolymorphicForeignKey\nisPolymorphicForeignKey\nAPI only\norder\norder\nAPI only\nlength\nmaxLength\nproperties sidebar\nprecision\nprecision\nproperties sidebar\nscale\nnumericScale\nAPI only\nunique\nisUnique\nAPI only\ninlineHelpText\ninlineHelpText\noverview sidebar\npicklistValues\npicklistValues\noverview sidebar\nReports\nâ\nAtlan maps reports from Salesforce to its\nSalesforceReport\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nid\nsourceId\noverview sidebar\nname\nname\nasset preview and profile, overview sidebar\nreportType\nreportType\noverview sidebar\ndetailColumns\ndetailColumns\noverview sidebar\ndescription\ndescription\nasset preview and profile, overview sidebar\nwebUrl\nsourceURL\noverview sidebar\ncreatedDate\nsourceCreatedAt\nasset profile and properties sidebar\nlastModifiedDate\nsourceUpdatedAt\nasset profile and properties sidebar\ncreatedBy\nsourceCreatedBy\nasset profile and properties sidebar\nlastModifiedBy\nsourceUpdatedBy\nasset profile and properties sidebar\nDashboards\nâ\nAtlan maps dashboards from Salesforce to its\nSalesforceDashboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nid\nsourceId\noverview sidebar\nname\nname\nasset preview and profile, overview sidebar\ndashboardType\ndashboardType\noverview sidebar\nreportCount\nreportCount\nasset preview and profile\ndescription\ndescription\nasset preview and profile, overview sidebar\nwebUrl\nsourceURL\noverview sidebar\nTags:\ncrawl\nsalesforce\napi\nPrevious\nCrawl Salesforce\nNext\nPreflight checks for Salesforce\nOrganizations\nObjects\nFields\nReports\nDashboards"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/references/what-does-atlan-crawl-from-sigma",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSigma\nReferences\nWhat does Atlan crawl from Sigma?\nOn this page\nWhat does Atlan crawl from Sigma?\nAtlan crawls and maps the following assets and properties from Sigma.\ndanger\nCurrently, Atlan only represents the assets marked with ð in lineage.\nFor your Sigma\nworkbooks\n, Atlan also provides asset previews to help with quick discovery and give you the context you need.\nWorkbooks ð\nâ\nAtlan maps workbooks from Sigma to its\nSigmaWorkbook\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nurl\nsourceURL\noverview sidebar\nPages ð\nâ\nAtlan maps pages from Sigma to its\nSigmaPage\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\nData elements ð\nâ\nAtlan maps table, pivot table, and visualization elements from Sigma to its\nSigmaDataElement\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndisplayName\nname\nasset profile and overview sidebar\ntype\ntype\noverview sidebar\nData element fields ð\nâ\nAtlan maps table, pivot table, and visualization element fields from Sigma to its\nSigmaDataElementField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndisplayName\nname\nasset profile and overview sidebar\nDatasets\nâ\nAtlan maps datasets from Sigma to its\nSigmaDataset\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nurl\nsourceURL\noverview sidebar\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nCrawl Sigma\nNext\nPreflight checks for Sigma\nWorkbooks ð\nPages ð\nData elements ð\nData element fields ð\nDatasets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/what-does-atlan-crawl-from-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nReferences\nWhat does Atlan crawl from Snowflake?\nOn this page\nWhat does Atlan crawl from Snowflake?\nAtlan crawls and maps the following assets and properties from Snowflake.\nOnce you've\ncrawled Snowflake\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for Snowflake assets:\nStreams\n-  Source type and Stale filters\nFunctions\n-  Language, Function type, Is secure, and Is external filters\nSnowflake tags and tag values\nLineage\nâ\nAtlan supports lineage for the following asset types:\nExternal Named Stages\nâ\nTable\nPipe â Table\nExternal Table\nIceberg Table\nInternal Named Stages\nâ\nTable\nPipe â Table (auto-ingest not recommended)\nNot supported for External or Iceberg Tables\nDatabases\nâ\nAtlan maps databases from Snowflake to its\nDatabase\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nDATABASES.DATABASE_NAME\nname\nasset profile and overview sidebar\nDATABASE.COMMENT\ndescription\nasset profile and overview sidebar\nSCHEMATA\n(count)\nschemaCount\nasset preview and profile\nDATABASES.DATABASE_OWNER\nCreated (in Snowflake)\nproperties sidebar\nDATABASES.CREATED\nsourceCreatedAt\nproperties sidebar\nDATABASES.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nSchemas\nâ\nAtlan maps schemas from Snowflake to its\nSchema\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nSCHEMATA.SCHEMA_NAME\nname\nasset profile and overview sidebar\nSCHEMA.COMMENT\ndescription\nasset profile and overview sidebar\nTABLES\nof type %TABLE% (count)\ntableCount\nasset preview and profile\nTABLES\nof type %VIEW% (count)\nviewsCount\nasset preview and profile\nSCHEMATA.CATALOG_NAME\ndatabaseName\nasset preview and profile\nSCHEMATA.SCHEMA_OWNER\nCreated (in Snowflake)\nproperties sidebar\nSCHEMATA.CREATED\nsourceCreatedAt\nproperties sidebar\nSCHEMATA.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nTables\nâ\nAtlan maps tables from Snowflake to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLES.TABLE_NAME\nname\nasset profile and overview sidebar\nTABLES.COMMENT\ndescription\nasset profile and overview sidebar\nCOLUMNS\n(count)\ncolumnCount\nasset preview and profile, overview sidebar\nTABLES.ROW_COUNT\nrowCount\nasset preview and profile, overview sidebar\nTABLES.BYTES\nsizeBytes\nasset filter and overview sidebar\nTABLES.TABLE_OWNER\nCreated (in Snowflake)\nproperties sidebar\nTABLES.CREATED\nsourceCreatedAt\nproperties sidebar\nTABLES.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nFor Iceberg tables\nâ\nIn addition to the table metadata above, Atlan supports additional metadata for Iceberg tables crawled from Snowflake. Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method.\nSource property\nAtlan property\nWhere in Atlan\nTABLES.IS_ICEBERG\ntableType\nasset preview, profile, and filter, and overview sidebar\nICEBERG_TABLES.CATALOG_NAME\nicebergCatalogName\noverview sidebar\nICEBERG_TABLES.ICEBERG_TABLE_TYPE\nicebergTableType\noverview sidebar\nCATALOG_INTEGRATION.CATALOG_SOURCE\nicebergCatalogSource\noverview sidebar\nICEBERG_TABLES.CATALOG_TABLE_NAME\nicebergCatalogTableName\noverview sidebar\nICEBERG_TABLES.CATALOG_NAMESPACE\nicebergCatalogTableNamespace\noverview sidebar\nICEBERG_TABLES.EXTERNAL_VOLUME_NAME\ntableExternalVolumeName\noverview sidebar\nICEBERG_TABLES.BASE_LOCATION\nicebergTableBaseLocation\noverview sidebar\nTABLES.RETENTION_TIME\ntableRetentionTime\noverview sidebar\nViews\nâ\nAtlan maps views from Snowflake to its\nView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLES.TABLE_NAME\nname\nasset profile and overview sidebar\nTABLES.COMMENT\ndescription\nasset profile and overview sidebar\nCOLUMNS\n(count)\ncolumnCount\nasset preview and profile, overview sidebar\nVIEWS.VIEW_DEFINITION\ndefinition\nasset profile and overview sidebar\nTABLES.TABLE_OWNER\nCreated (in Snowflake)\nproperties sidebar\nTABLES.CREATED\nsourceCreatedAt\nproperties sidebar\nTABLES.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nMaterialized views\nâ\nAtlan maps materialized views from Snowflake to its\nMaterialisedView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLES.TABLE_NAME\nname\nasset profile and overview sidebar\nTABLES.COMMENT\ndescription\nasset profile and overview sidebar\nCOLUMNS\n(count)\ncolumnCount\nasset preview and profile, overview sidebar\nTABLES.ROW_COUNT\nrowCount\nasset preview and profile, overview sidebar\nTABLES.BYTES\nsizeBytes\nasset filter and overview sidebar\nVIEWS.VIEW_DEFINITION\ndefinition\nasset profile and overview sidebar\nTABLES.TABLE_OWNER\nCreated (in Snowflake)\nproperties sidebar\nTABLES.CREATED\nsourceCreatedAt\nproperties sidebar\nTABLES.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nExternal tables\nâ\nAtlan maps external tables from Snowflake to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLES.TABLE_NAME\nname\nasset profile and overview sidebar\nTABLES.COMMENT\ndescription\nasset profile and overview sidebar\nCOLUMNS\n(count)\ncolumnCount\nasset preview and profile, overview sidebar\nTABLES.ROW_COUNT\nrowCount\nasset preview and profile, overview sidebar\nTABLES.BYTES\nsizeBytes\nasset filter and overview sidebar\nEXTERNAL_TABLES.LOCATION\nexternalLocation\noverview sidebar\nSTAGES.STAGE_REGION\nexternalLocationRegion\nAPI only\nEXTERNAL_TABLES.FILE_FORMAT_TYPE\nexternalLocationFormat\noverview sidebar\nTABLES.TABLE_OWNER\nCreated (in Snowflake)\nproperties sidebar\nTABLES.CREATED\nsourceCreatedAt\nproperties sidebar\nTABLES.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nColumns\nâ\nAtlan maps columns from Snowflake to its\nColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nCOLUMNS.COLUMN_NAME\nname\nasset profile and overview sidebar\nCOLUMNS.COMMENT\ndescription\nasset profile and overview sidebar\nCOLUMNS.ORDINAL_POSITION\norder\nAPI only\nCOLUMNS.DATA_TYPE\ndataType\nasset filter, preview, and profile, overview sidebar\nPRIMARY KEY\nisPrimary\nasset preview and filter, overview sidebar\nCOLUMNS.IS_NULLABLE\nisNullable\noverview sidebar\nCOLUMNS.CHARACTER_MAXIMUM_LENGTH\nmaxLength\nproperties sidebar\nCOLUMNS.NUMERIC_PRECISION\nprecision\nproperties sidebar\nStages\nâ\nAtlan maps stages from Snowflake to its\nStage\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nStages.STAGE_NAME\nname\nasset profile and overview sidebar\nStages.COMMENT\ndescription\nasset profile and overview sidebar\nStages.STAGE_SCHEMA\nschemaName\nasset profile and overview sidebar\nStages.STAGE_CATALOG\ndatabaseName\nasset profile and overview sidebar\nStages.STAGE_URL\nsnowflakeStageExternalLocation\nasset profile and overview sidebar\nStages.STAGE_REGION\nsnowflakeStageExternalLocationRegion\nasset profile and overview sidebar\nStages.STAGE_TYPE\nsnowflakeStageType\nasset profile and overview sidebar\nStages.STAGE_OWNER\nsourceOwners\noverview sidebar\nStages.CREATED\nsourceCreatedAt\nproperties sidebar\nStages.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nStages.STORAGE_INTEGRATION\nsnowflakeStageStorageIntegration\nasset profile and overview sidebar\nStreams\nâ\nAtlan maps streams from Snowflake to its\nStream\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nSTREAMS.NAME\nname\nasset profile and overview sidebar\nSTREAMS.COMMENT\ndescription\nasset profile and overview sidebar\nSTREAMS.OWNER\nsourceOwners\nasset preview and profile, properties sidebar\nSTREAMS.DATABASE_NAME\ndatabaseName\nasset preview and profile\nSTREAMS.SCHEMA_NAME\nschemaName\nasset preview and profile\nSTREAMS.SOURCE_TYPE\nsnowflakeStreamSourceType\nasset filter and overview sidebar\nSTREAMS.STALE\nsnowflakeStreamIsStale\nasset filter and overview sidebar\nSTREAMS.MODE\nsnowflakeStreamMode\noverview sidebar\nSTREAMS.STALE_AFTER\nsnowflakeStreamStaleAfter\noverview sidebar\nPipes\nâ\nAtlan maps pipes from Snowflake to its\nPipe\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nPIPES.PIPE_NAME\nname\nasset profile and overview sidebar\nPIPES.COMMENT\ndescription\nasset profile and overview sidebar\nPIPES.DEFINITION\ndefinition\nasset profile and overview sidebar\nPIPES.PIPE_OWNER\nsourceOwners\nasset preview and profile, properties sidebar\nPIPES.PIPE_CATALOG\ndatabaseName\nasset preview and profile\nPIPES.PIPE_SCHEMA\nschemaName\nasset preview and profile\nPIPES.IS_AUTOINGEST_ENABLED\nsnowflakePipeIsAutoIngestEnabled\noverview sidebar\nPIPES.NOTIFICATION_CHANNEL_NAME\nsnowflakePipeNotificationChannelName\noverview sidebar\nUser-defined functions\nâ\nAtlan maps user-defined functions (UDFs) from Snowflake to its\nFunction\nasset type. Atlan currently does not support lineage for user-defined functions from Snowflake.\nSource property\nAtlan property\nWhere in Atlan\nNAME\nname\nasset profile and overview sidebar\nFUNCTION_DEFINITION\nfunctionDefinition\noverview sidebar\nCOMMENT\ndescription\nasset profile and overview sidebar\nFUNCTION_CATALOG\ndatabaseName\nasset preview and profile\nFUNCTION_SCHEMA\nschemaName\nasset preview and profile\nFUNCTION_OWNER\nCreated (in Snowflake)\nproperties sidebar\nCREATED\nsourceCreatedAt\nproperties sidebar\nLAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nFUNCTION_LANGUAGE\nfunctionLanguage\noverview sidebar\nDATA_TYPE\nfunctionReturnType\nAPI only\nIS_SECURE\nfunctionIsSecure\nasset filter and properties sidebar\nIS_EXTERNAL\nfunctionIsExternal\nasset filter and properties sidebar\nIS_MEMOIZABLE\nfunctionIsMemoizable\nAPI only\nARGUMENT_SIGNATURE\nfunctionArguments\nAPI only\nDynamic tables\nâ\nAtlan maps dynamic tables from Snowflake to its\nDynamicTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLES.TABLE_NAME\nname\nasset profile and overview sidebar\nTABLES.COMMENT\ndescription\nasset profile and overview sidebar\nCOLUMNS\n(count)\ncolumnCount\nasset preview and profile, overview sidebar\nTABLES.DEFINITION\ndefinition\nasset profile and overview sidebar\nTABLES.ROW_COUNT\nrowCount\nasset preview and profile, overview sidebar\nTABLES.BYTES\nsizeBytes\nasset filter and overview sidebar\nTABLES.TABLE_OWNER\nCreated (in Snowflake)\nproperties sidebar\nTABLES.CREATED\nsourceCreatedAt\nproperties sidebar\nTABLES.LAST_ALTERED\nsourceUpdatedAt\nproperties sidebar\nTags:\nconnectors\ndata\ncrawl\nPrevious\nMultiple tag values and concatenation\nNext\nPreflight checks for Snowflake\nLineage\nDatabases\nSchemas\nTables\nViews\nMaterialized views\nExternal tables\nColumns\nStages\nStreams\nPipes\nUser-defined functions\nDynamic tables"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-from-tableau",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nReferences\nWhat does Atlan crawl from Tableau?\nOn this page\nWhat does Atlan crawl from Tableau?\nAtlan crawls and maps the following assets and properties from Tableau.\nOnce you've\ncrawled Tableau\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nProjects   -  filter Tableau assets by projects, including nested projects\nData sources   -  Is published filter\nFor your Tableau\nworksheets\nand\ndashboards\n, Atlan also provides asset previews to help with quick discovery and give you the context you need.\nwarning\nYou may need to\ndisable clickjack protection\nfor Tableau asset previews to load.\nLineage\nâ\ninfo\nDid you know?\nLineage to dashboards may appear incomplete or missing if worksheets are not crawled. Additionally, Tableau assets that haven't been refreshed since May 27, 2025, won't display the new column-level lineage (CLL) or updated lineage paths.\nAtlan supports lineage for the following:\nAsset Lineage\n- Datasource to Dashboard, Datasource to Worksheet, Datasource to Workbook\nColumn Level Lineage\n- Supported for Datasource to Worksheet and Worksheet to Dashboard\nSites\nâ\nAtlan maps sites from Tableau to its\nTableauSite\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nProjects\nâ\nAtlan maps projects from Tableau to its\nTableauProject\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nowner\n[sourceOwner](/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity)\nasset profile and overview sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nhierarchy\nprojectHierarchy\nasset preview and profile, overview sidebar\ntopLevelProject\nisTopLevelProject\nAPI only\nFlows\nâ\nwarning\nDue to limitations at source, Atlan won't be able to crawl Tableau flows if you use the\nJWT bearer authentication\nmethod.\nAtlan maps flows from Tableau to its\nTableauFlow\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nowner\n[sourceOwner](/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity)\nasset profile and overview sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nMetrics\nâ\nwarning\nTableau has\nretired metrics methods in API 3.22\nfor Tableau Cloud and Tableau Server version 2024.2. If you're using Tableau API version 3.22 or higher, metadata for metrics is unavailable in Atlan.\nAtlan maps metrics from Tableau to its\nTableauMetric\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nWorkbooks\nâ\nAtlan maps workbooks from Tableau to its\nTableauWorkbook\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nwebpageUrl\nsourceURL\noverview sidebar\nowner\n[sourceOwner](/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity)\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nWorksheets\nâ\nAtlan maps worksheets from Tableau to its\nTableauWorksheet\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nsource_url\nsourceURL\noverview sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nDashboards\nâ\nAtlan maps dashboards from Tableau to its\nTableauDashboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncreatedAt\nsourceCreatedAt\nasset profile and properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nsource_url\nsourceURL\noverview sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nData sources\nâ\nAtlan maps data sources (embedded and published) from Tableau to its\nTableauDatasource\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nowner\nsourceOwner\n(for published data sources only)\nasset profile and overview sidebar\nisPublished\nisPublished\nasset preview and overview sidebar\nhasExtracts\nhasExtracts\nAPI only\nupstreamTables\nupstreamTables\nAPI only\nupstreamDatabases\nupstreamDatabases\nAPI only\nisCertified\ncertificateStatus\n(\nVERIFIED\n)\nasset preview and filter, overview sidebar\ncertifier\ncertifier\nAPI only\ncertificationNote\ncertificationStatusMessage\nAPI only\ncertifierDisplayName\ncertificateUpdatedBy\nasset preview and overview sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nData source fields\nâ\nAtlan maps data source fields and column fields from Tableau to its\nTableauDatasourceField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nupstreamTables\nupstreamTables\nAPI only\nupstreamColumns\nupstreamColumns\nAPI only\ndataCategory\ntableauDatasourceFieldDataCategory\nAPI only\nrole\ntableauDatasourceFieldRole\nAPI only\ndataType\ntableauDatasourceFieldDataType\nAPI only\nformula\ntableauDatasourceFieldFormula\nAPI only\nbinSize\ntableauDatasourceFieldBinSize\nAPI only\n__typename\ndatasourceFieldType\nAPI only\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nCustom SQL\nâ\nAtlan parses custom SQL queries used in Tableau data sources to extract lineage information. This process identifies the relationships between data assets based on the SQL logic defined within Tableau.\nSource property\nAtlan property\nWhere in Atlan\ndownstreamDatasources\nTableauDatasource\nUsed to define downstream impact of lineage\nquery\nCustomSQLQuery\nUsed to form lineage from source\nCalculated fields\nâ\nAtlan maps calculated fields from Tableau to its\nTableauCalculatedField\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ndataCategory\ndataCategory\nAPI only\nrole\nrole\nAPI only\ndataType\ntableauDataType\nasset preview, filter, and profile, overview sidebar\nformula\nformula\noverview sidebar\nproject_extra (hierarchy)\nprojectHierarchy\nasset preview and profile, overview sidebar\nLineage\nâ\nAtlan calculates lineage for Tableau as follows:\nSource object\nTableau object\nTableau object (downstream)\nTable\nPublished data source\nPublished data source\nTable\nPublished data source\nEmbedded data source\nTable\nEmbedded data source\nColumn\nData source field\nCalculated field\nColumn\nData source field\nData source field\ninfo\nLineage is currently not supported for Tableau\nflows\nand\nmetrics\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl on-premises Tableau\nNext\nPreflight checks for Tableau\nLineage\nSites\nProjects\nFlows\nMetrics\nWorkbooks\nWorksheets\nDashboards\nData sources\nData source fields\nCustom SQL\nCalculated fields\nLineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/references/what-does-atlan-crawl-from-thoughtspot",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nReferences\nWhat does Atlan crawl from ThoughtSpot?\nOn this page\nWhat does Atlan crawl from ThoughtSpot?\nOnce you've\ncrawled ThoughtSpot\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for all ThoughtSpot assets:\nTags and chart type filters\nAtlan supports lineage for the following ThoughtSpot assets:\nAnswers\n-  upstream lineage to tables, views, or worksheets from multiple sources (if applicable), no downstream lineage\nVisualizations\n-  upstream lineage to tables, views, or worksheets from multiple sources (if applicable)\nLiveboards\n-  upstream lineage to visualizations\nTables\n-  upstream lineage to source tables, and column-level lineage between ThoughtSpot tables and worksheets\nViews\n-  upstream lineage to ThoughtSpot tables or worksheets, and column-level lineage between ThoughtSpot views and worksheets\nWorksheets\n-  upstream lineage to ThoughtSpot tables or views from multiple sources (if applicable)\nAtlan crawls and maps the following assets and properties from ThoughtSpot.\ndanger\nCurrently, Atlan only represents the assets marked with ð in lineage.\nAnswers ð\nâ\nAtlan maps answers from ThoughtSpot to its\nThoughtSpotAnswer\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_header.created\nsourceCreatedAt\nasset profile and properties sidebar\nmetadata_header.modified\nsourceUpdatedAt\nasset profile and properties sidebar\nmetadata_header.authorDisplayName\nsourceCreatedBy\nasset profile and properties sidebar\nquestion.text\nthoughtspotQuestionText\nproperties sidebar\nmetadata_header.tags\nassetTags\nasset filter and overview sidebar\nvisualisations.chart_type\nthoughtspotChartType\nasset filter and profile, overview sidebar\nVisualizations ð\nâ\nAtlan maps visualizations from ThoughtSpot to its\nThoughtspotDashlet\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_header.created\nsourceCreatedAt\nasset profile and properties sidebar\nmetadata_header.modified\nsourceUpdatedAt\nasset profile and properties sidebar\nmetadata_header.authorDisplayName\nsourceCreatedBy\nasset profile and properties sidebar\nquestion.text\nthoughtspotQuestionText\nproperties sidebar\nvisualisations.chart_type\nthoughtspotChartType\nasset filter and profile, overview sidebar\nmetadata_header.name\nthoughtspotLiveboardName\nAPI only\nLiveboards ð\nâ\nAtlan maps Liveboards from ThoughtSpot to its\nThoughtspotLiveboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_header.created\nsourceCreatedAt\nasset profile and properties sidebar\nmetadata_header.modified\nsourceUpdatedAt\nasset profile and properties sidebar\nmetadata_header.authorDisplayName\nsourceCreatedBy\nasset profile and properties sidebar\nmetadata_header.tags\nassetTags\nasset filter and overview sidebar\nTables ð\nâ\nAtlan maps tables from ThoughtSpot to its\nThoughtspotTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_header.created\nsourceCreatedAt\nasset profile and properties sidebar\nmetadata_header.modified\nsourceUpdatedAt\nasset profile and properties sidebar\nmetadata_header.authorDisplayName\nsourceCreatedBy\nasset profile and properties sidebar\nlength(metadata_detail.relationships[])\nthoughtspotJoinCount\nasset preview and profile\nlength(metadata_detail.columns[])\nthoughtspotColumnCount\nasset preview and profile\nmetadata_header.tags\nassetTags\nasset filter and overview sidebar\nViews ð\nâ\nAtlan maps views from ThoughtSpot to its\nThoughtspotView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_header.created\nsourceCreatedAt\nasset profile and properties sidebar\nmetadata_header.modified\nsourceUpdatedAt\nasset profile and properties sidebar\nmetadata_header.authorDisplayName\nsourceCreatedBy\nasset profile and properties sidebar\nlength(metadata_detail.relationships[])\nthoughtspotJoinCount\nasset preview and profile\nlength(metadata_detail.columns[])\nthoughtspotColumnCount\nasset preview and profile\nmetadata_header.tags\nassetTags\nasset filter and overview sidebar\nWorksheets ð\nâ\nAtlan maps worksheets from ThoughtSpot to its\nThoughtspotWorksheet\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_header.created\nsourceCreatedAt\nasset profile and properties sidebar\nmetadata_header.modified\nsourceUpdatedAt\nasset profile and properties sidebar\nmetadata_header.authorDisplayName\nsourceCreatedBy\nasset profile and properties sidebar\ncalculated using unique source and destination table pairs from join paths\nthoughtspotJoinCount\nasset preview and profile\nlength(metadata_detail.columns[])\nthoughtspotColumnCount\nasset preview and profile\nmetadata_header.tags\nassetTags\nasset filter and overview sidebar\nColumns ð\nâ\nAtlan maps columns from ThoughtSpot to its\nThoughtspotColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nmetadata_header.name\nname\nasset profile and overview sidebar\nmetadata_header.description\ndescription\nasset profile and overview sidebar\nmetadata_detail.dataType\nthoughtspotColumnDataType\nasset preview and filter, overview sidebar\nmetadata_detail.type\nthoughtspotColumnType\nasset preview and filter, overview sidebar\nTags:\nconnectors\ncrawl\nPrevious\nCrawl on-premises ThoughtSpot\nNext\nTroubleshooting ThoughtSpot connectivity\nAnswers ð\nVisualizations ð\nLiveboards ð\nTables ð\nViews ð\nWorksheets ð\nColumns ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/references/what-does-atlan-crawl-from-microstrategy",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicroStrategy\nReferences\nWhat does Atlan crawl from MicroStrategy?\nOn this page\nWhat does Atlan crawl from MicroStrategy?\nAtlan crawls and maps the following assets and properties from MicroStrategy.\ndanger\nCurrently Atlan only represents the assets marked with ð in lineage.\nOnce you've\ncrawled MicroStrategy\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nProjects\n,\nattributes\n,\nfacts\n,\nmetrics\n,\ncubes\n,\nreports\n,\ndocuments\n,\ndossiers\n, and\nvisualizations\n-  Is Certified filter\nCubes\n,\nreports\n, and\nvisualizations\n-  Type filter\nProjects\nâ\nAtlan maps projects from MicroStrategy to its\nMicroStrategyProject\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nowners\noverview sidebar\nAttributes\nâ\nAtlan maps attributes from MicroStrategy to its\nMicroStrategyAttribute\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nforms\nforms\noverview sidebar\nFacts\nâ\nAtlan maps facts from MicroStrategy to its\nMicroStrategyFact\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nexpressions\nexpressions\noverview sidebar\nMetrics\nâ\nAtlan maps metrics from MicroStrategy to its\nMicroStrategyMetric\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nexpression\nexpression\noverview sidebar\nCubes ð\nâ\nAtlan maps cubes from MicroStrategy to its\nMicroStrategyCube\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nsubtype\ntype\nasset filter and properties sidebar\nsqlStatement\nquery\nproperties sidebar\nReports ð\nâ\nAtlan maps reports from MicroStrategy to its\nMicroStrategyReport\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nsubtype\ntype\nasset filter and properties sidebar\nDocuments ð\nâ\nAtlan maps documents from MicroStrategy to its\nMicroStrategyDocument\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nDossiers ð\nâ\nAtlan maps dossiers from MicroStrategy to its\nMicroStrategyDossier\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\ncreatedAt\nproperties sidebar\nupdated_at\nupdatedAt\nproperties sidebar\nowner\nsource owner\noverview sidebar\ncertifiedInfo\nisCertified\nasset filter and properties sidebar\ncertifiedInfo\ncertifiedBy\nproperties sidebar\ncertifiedInfo\ncertifiedAt\nproperties sidebar\nancestors\nlocation\nproperties sidebar\nchapter\nchapterNames\noverview sidebar\nVisualizations\nâ\nAtlan maps dossier visualizations from MicroStrategy to its\nMicroStrategyVisualization\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\nvisualizationType\nvisualizationType\nasset filter and properties sidebar\nTags:\nconnectors\ncrawl\nPrevious\nCrawl MicroStrategy\nNext\nPreflight checks for MicroStrategy\nProjects\nAttributes\nFacts\nMetrics\nCubes ð\nReports ð\nDocuments ð\nDossiers ð\nVisualizations"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-data-consumers",
    "content": "Get Started\nQuick Start Guides\nData consumers\nOn this page\nData consumers\nDiscovery\nâ\nWe rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The\ndiscovery tool\nis Atlan's powerful in-platform search, powered by the terms, tags, and definitions you've added to your data.\nHere are a few of the things that make Atlan's discovery awesome:\nEvery attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need.\nDiscovery allows you to toggle the type of data asset you are looking for.\nIntelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed.\nSearch assets from just about any page in Atlan using\nCmd/Ctrl+K\nor by clicking\nSearch assets across Atlan\nat the top of any page.\nControl your search by using facets about your data (such as the verification status or owner) to find what's most important to you.\nSort by popularity to quickly discover what assets your teammates are using every day.\nInsights\nâ\nAtlan was built by data teams for data teams, so we know that querying your data is crucial to answering important business questions.\nSQL is the most common way that data teams query their data, so we wanted to make it easy for Atlan users to seamlessly query and share SQL work with their team.\nAtlan's\nInsights\nworkspace allows teams to find the data they need, query it, and save and manage all SQL queries in one place. You can write SQL queries from scratch or use the Visual Query Builder to write SQL scripts quicker.\nDid you know?\nThe Visual Query Builder is a great way to start building complex queries without fluency in SQL. Think of it as \"fill in the blank\" but for queries!\nSome of our favorite features about Insights:\nCustomize the SQL editor's look and feel through preferences\nExamine a data asset's lineage alongside your SQL editor, so you can effortlessly understand your data asset in relation to other assets\nSave, organize, and share your SQL queries\nTags:\nget-started\nquick-start\nPrevious\nAdministrators\nNext\nContributors\nDiscovery\nInsights"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-contributors",
    "content": "Get Started\nQuick Start Guides\nContributors\nOn this page\nContributors\nAsset profile\nâ\nThe\nasset profile\nin Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data.\nWhat's in an asset profile?\nSummary\n: The numbers of columns and rows, certification status (e.g. verified, draft, etc.), owner of the asset, and more\nColumn Preview\n: An overview of column names and definitions\nSample Data\n: A snapshot of what the data looks like (with anything sensitive hidden, according to your policies)\nReadme\n: An in-depth description of the asset that should provide all the context, knowledge, and links needed to fully understand the asset\nLineage\n: Visualization of how your data asset relates to other assets, which helps you figure out upstream and downstream dependencies\nGlossary\nâ\nThe Atlan\nglossary\nis a rich tool for defining and organizing your data\nterminology\nto improve transparency and share knowledge. No need to ask around about what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one (searchable!) place.\nThe glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as...\nOwners\nof your data, so you know who to ask for clarification\nCertificate\nstatus, to easily understand if the data is still in progress or ready to be used\nLinked Assets\nthat are relevant to the term, so you can explore other helpful material\nDid you know?\nThe glossary helps power Atlan's powerful search tool, so tagging and defining assets are critical to helping your team find what they need.\nDiscovery\nâ\nWe rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The\ndiscovery tool\nis Atlan's powerful in-platform search, powered by the terms, tags, and definitions you've added to your data.\nHere are a few of the things that make Atlan's discovery awesome:\nEvery attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need.\nDiscovery allows you to toggle the type of data asset you are looking for.\nIntelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed.\nSearch assets from just about any page in Atlan using\nCmd/Ctrl+K\nor by clicking\nSearch assets across Atlan\nat the top of any page.\nControl your search by using facets about your data (such as the verification status or owner) to find what's most important to you.\nSort by popularity to quickly discover what assets your teammates are using every day.\nInsights\nâ\nAtlan was built by data teams for data teams, so we know that querying your data is crucial to answering important business questions.\nSQL is the most common way that data teams query their data, so we wanted to make it easy for Atlan users to seamlessly query and share SQL work with their team.\nAtlan's\nInsights\nworkspace allows teams to find the data they need, query it, and save and manage all SQL queries in one place. You can write SQL queries from scratch or use the Visual Query Builder to write SQL scripts quicker.\nDid you know?\nThe Visual Query Builder is a great way to start building complex queries without fluency in SQL. Think of it as \"fill in the blank\" but for queries!\nSome of our favorite features about Insights:\nCustomize the SQL editor's look and feel through preferences\nExamine a data asset's lineage alongside your SQL editor, so you can effortlessly understand your data asset in relation to other assets\nSave, organize, and share your SQL queries\nTags:\nget-started\nquick-start\nPrevious\nData consumers\nNext\nAPI authentication\nAsset profile\nGlossary\nDiscovery\nInsights"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/api-authentication",
    "content": "Get Started\nQuick Start Guides\nDevelopers\nAPI authentication\nOn this page\nAPI authentication\nWho can do this?\nYou will need to be an\nadmin user\nto create a bearer token. However, you can share the token with anyone to give them programmatic access.\nTo create a bearer token:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nAPI tokens\n.\nIn the upper right of the\nAPI tokens\ntable, click the\nGenerate API token\nbutton and enter the following details:\nFor\nName\n, enter a name for your API token   -  for example, the system or application that will use this token.\n(Optional) For\nDescription\n, enter a description for your API token   -  for example, its intended use. You can also add or change the description later.\n(Optional) For\nPersonas\n, select any\nasset-level permissions\nyou want to give to the token. You can also add these later.\n(Optional) For\nCollections\n, select any\nquery collections\nyou want to provide access to the token. You can also add these later.\n(Optional) If you would like the token to be temporary, for\nExpiry\n, choose the time after which the token should automatically become invalid.\nAt the bottom right, click the\nSave\nbutton.\n(Optional) If and when you no longer need your API token, on an active token's row, click the trash icon to delete your API token and then click the\nDelete\nbutton to confirm deletion.\ndanger\nRemember to copy or download the token now   -  this is your only opportunity to do so. (If you've already forgotten, just delete the API token and create a new one.)\nUse the bearer token\nâ\nYou must authenticate all requests to Atlan's APIs. You can authenticate your requests by sending the following header:\nAuthorization: Bearer <token>\nSo, for example, if the API token you copied had the value\neyJhbGciOi...\n, you would use the header:\nAuthorization: Bearer eyJhbGciOi...\ndanger\nNote that the value of the\nAuthorization\nheader is the combination of the word\nBearer\n, a space, and then the token's value. The token copied from Atlan does not include this\nBearer\nprefix.\nToken permissions\nâ\nBy default, each API token will have the permissions of an\nadmin user\n,\nwithout\nconnection admin privileges. This means the token is able to:\nCall administrative API endpoints. For example, to create users and groups.\nCall governance API endpoints. For example, to create governance objects like tags, custom metadata, personas and purposes.\ndanger\nThe API token will only be able to access connections (and assets within them) that the token itself created. Even connections with\nAll Admins\nset as connection admins will not be accessible by the token, without a persona assigned to the token.\nTo provide access to any connections and assets, you need to add one or more\npersonas\nto the token that have access to that connection's assets.\nOnce personas are assigned to the token, the token will be able to:\nCreate, read, update, delete, and search glossaries (and their content) that are accessible by those assigned personas.\nCreate, read, update, delete, and search any assets that are accessible by those personas.\nTags:\napi\nrest-api\ngraphql\nPrevious\nContributors\nNext\nCustom solutions\nUse the bearer token\nToken permissions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/invite-new-users",
    "content": "Configure Atlan\nAccess control\nGet started\nInvite new users\nOn this page\nInvite new users\nWho can do this?\nYou will need to be an admin user in Atlan to invite users.\nDid you know?\nUsernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. When logging into Atlan for the first time, ensure that you configure your username as per your preference.\nWithout SSO\nâ\nTo invite new users to Atlan, without SSO:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\nclick\nUsers\n.\nClick theÂ\nInvite Users\nbutton.\nUnderÂ\nInvite users to Default\nenter one or more email addresses of the users to invite.\ndanger\nAtlan does not allow the use of disposable email addresses. You will receive an error indicating this if you attempt to send an invitation to one.\n(Optional) To the right of each email, clickÂ\nMember\nto change the\nrole of the user\n.\nClick theÂ\nSend Invite\nbutton.\nYour user(s) will now receive an email with a link to sign up on Atlan! ð\nNote that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users.\nWith SSO\nâ\nDid you know?\nWhen\nSSO is enforced\n, you will not be able to invite users in Atlan   -  they can only be invited through the SSO provider. Any users mapped to the SAML app can log into Atlan via SSO. A user profile will be generated for them automatically, if one does not already exist. Admins can also\nassign default user roles\nfor SSO to give appropriate permissions to users as soon as they log into Atlan.\nTags:\natlan\ndocumentation\nPrevious\nAccess Control\nNext\nCreate groups\nWithout SSO\nWith SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/manage-users",
    "content": "Configure Atlan\nAccess control\nManage users and groups\nManage users\nOn this page\nManage users\nWho can do this?\nYou will need to be an admin user in Atlan to manage other users.\nAs an admin user in Atlan, you can:\nChange a user's role\nTemporarily disable or permanently remove a user\nReactivate a disabled user\nChange a user's role\nâ\nTo change a user's role:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nUsers\n.\nUnder the\nRole\ncolumn, on a given user's row, click the role name.\nUnder\nChange Role\n, select the new role to give the user, and then click\nChange\n.\nThat's it, the user's role has now been changed! ð\nDeactivate a user\nâ\nTo deactivate a user, you can either disable or remove them from Atlan. For example, you can disable a user to temporarily suspend their access to Atlan and reactivate it at a future date. If a user leaves the organization, you can remove their access.\nAtlan recommends that you proceed with caution if you want remove a user. While a disabled user can be\nreactivated\n, removing a user is a permanent and irreversible action. If a removed user needs to be restored to Atlan later on, you will need to add them as a new user.\nYou can remove users irrespective of whether you're using basic authentication, SSO, or SCIM provisioning. Note that if you're using SCIM provisioning,\ndisabling or reactivating users in Atlan is not allowed\n. If a SCIM-provisioned user is unassigned from the Atlan app in the identity provider, that user will be disabled in Atlan as well. Only then will you have the option to remove that user permanently.\nTo deactivate a user:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nUsers\n.\nScroll the table all the way to the right, if necessary.\nOn an active user's row, click the\n3-dot\nmenu button and then from the dropdown:\nTo temporarily remove a user, click\nDisable user\nand then click\nDisable\n.\nTo permanently remove a user, click\nRemove user\n.\nIn the\nRemove user\ndialog, for\nTransfer ownership\n, select an existing user to\ntransfer any and all assets and workflows\nowned by the user you want to remove. Depending on the volume of assets to be transferred, the user removal process may take 15 minutes to a few hours to complete. You will be notified via email upon completion.\nCheck the\nI understand this action can't be reversed and want to permanently remove this user\nbox.\nClick\nRemove user\n.\nThat's it, the user is now deactivated and will no longer be able to access Atlan. ð¢\nDeactivated user accounts do\nnot\ncount towards the total number of Atlan licenses procured by your organization.\ndanger\nIf the user you want to disable is a\nconnection admin\n, you will need to ensure that other users can manage the connection before disabling the account. You can\nmodify a connection\nto add more connection admins.\nReactivate a user\nâ\nYou can only reactivate a disabled user's account.\nTo reactivate a user:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nUsers\n.\nScroll the table all the way to the right, if necessary.\nOn a deactivated user's row, click the 3-dot menu button.\nClick\nEnable user\n, and then\nEnable\n.\nThat's it, the user can once again access Atlan! ð\nDid you know?\nAtlan has a retention policy of 60 days for user login events. If a user has not logged into Atlan for more than 60 days, the\nLast Active\ncolumn will display\n-\nfor the user.\nTags:\natlan\ndocumentation\nPrevious\nCreate purpose\nNext\nAdd users to groups\nChange a user's role\nDeactivate a user\nReactivate a user"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nOn this page\nSSO Integration\nOverview:\nConnect Atlan with SSO to enable secure authentication, streamline user access, and enhance identity management.\nGet started\nâ\nHow to enable Azure AD for SSO\nHow to enable Google for SSO\nHow to enable JumpCloud for SSO\nHow to enable Okta for SSO\nHow to enable OneLogin for SSO\nHow to enable SAML 2.0 for SSO\nGuides\nâ\nAuthenticate SSO credentials to query data\nAuthenticate SSO credentials to view sample data\nHow to enable associated terms\nHow to limit SSO automatically creating users when they log in\nHow to set default user roles for SSO\nReferences\nâ\nSSO integration with PingFederate using SAML\nTroubleshooting\nâ\nTroubleshooting SSO\nTroubleshooting connector-specific SSO authentication\nWhy do I get a 404 error when using PingFederate SSO?\nWhy do I get an authentication error when logging in via Okta for the first time?\nWhy do I get an error while logging in via Google dashboard?\nUnable to log into Atlan via SSO due to an internal error from Microsoft Defender\nFAQ\nâ\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nCan we use a Microsoft SSO login?\nWhat type of user provisioning does Atlan support for SSO integrations?\nWhen does Atlan become a personal data processor or subprocessor?\nWhy did my users not receive an invite email from Atlan?\nTags:\nintegrations\nidentity management\nsso\nPrevious\nTroubleshooting SCIM provisioning\nNext\nHow to enable Azure AD for SSO\nGet started\nGuides\nReferences\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/configure-scim-provisioning",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSCIM\nConfigure SCIM provisioning\nConfigure SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your users and groups in Atlan with\nSystem for Cross-domain Identity Management\n(SCIM). Atlan supports\nSCIM 2.0\nfor SCIM provisioning.\nSCIM provisioning works in combination with your single sign-on (SSO) setup. Setting up SCIM enables you to manage all your users from one central location.\nAtlan currently supports SCIM provisioning for the following SSO providers:\nAzure AD\nOkta\nFor more questions about SCIM provisioning, head over to\nTroubleshooting SCIM provisioning\n.\nTags:\nintegration\nsetup\nPrevious\nSCIM Integration\nNext\nHow to enable Azure AD for SCIM provisioning"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp",
    "content": "Configure Atlan\nIntegrations\nCommunication\nSMTP and Announcements\nConfigure SMTP\nConfigure SMTP\nWho can do this?\nYou will need to be an admin user in Atlan to configure SMTP. The default SMTP setup is preconfigured by Atlan. You should only update this configuration if you want to set up custom SMTP.\nAtlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and\nscheduled queries\n. We provide an embedded SMTP server to do this, out-of-the-box.\nIf desired, you can override this embedded SMTP se\nrver with your own.\nTo override the embedded SMTP server:\nFrom the left menu of any screen, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, clickÂ\nSMTP\n.\nFill in the configuration of your SMTP server, at least:\nForÂ\nHost\nthe fully-qualified hostname of the SMTP server.\nForÂ\nFrom Email\nthe email address that should be used to send emails from the server.\nForÂ\nUsername\nthe username required by your SMTP server.\nAt the bottom of the page, click theÂ\nTest SMTP Config\nbutton to test your configuration. This will attempt to send an email to your profile's email address.\nOnce you successfully receive the test email, at the bottom of the page clickÂ\nSave\n.\nTags:\nsetup\nconfiguration\nPrevious\nSMTP and Announcements Integration\nNext\nCreate announcements"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/concepts/what-are-personas",
    "content": "Configure Atlan\nAccess control\nConcepts\nWhat are personas?\nOn this page\nWhat are personas?\nPersonas define policies to control which users can (or cannot) take certain actions on specific assets. They address a combination of two main objectives:\nCurating the assets that are relevant to a team of users\nControlling the actions users can take on those assets (querying data, updating metadata, etc)\nDid you know?\nThink of\npersonas\nas a way of curating assets for a group of users.\nTeam-based personalization\nâ\nPersonas curate the assets that are discoverable by users, and the detailed metadata shown. In this way, users can focus on only those assets (and metadata) relevant to their role in the organization. This reduces distractions and \"noise\" for users.\nFor example, a team of marketers may only work with a few data sets and dashboards. Rather than flooding the marketers with details about every asset in your company, you want to focus their attention.\nBroad-brush access control\nâ\nCombined with the personalization are broad-brush access control policies. When defining the policies in a persona, you not only select the assets but also what actions users can take on those assets.\nFor example, your team of marketers should be able to describe their dashboards. But perhaps they should only be able to see the shared data sets that feed those dashboards, not change their descriptions or tags. You can define the persona so that they can read and write the dashboards, but only read the data sets.\nTags:\natlan\ndocumentation\nPrevious\nAutomatically assign roles\nNext\nWhat are purposes?\nTeam-based personalization\nBroad-brush access control"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/concepts/what-are-purposes",
    "content": "Configure Atlan\nAccess control\nConcepts\nWhat are purposes?\nOn this page\nWhat are purposes?\nPurposes provide ways to interact with tagged assets. They address two main objectives:\nGrouping assets together in ways they may be used by many teams   -  for example, by project or domain\nControlling access to very granular, typically sensitive data\nWhen\ndefining a purpose\n, you choose its tags. Atlan then considers all assets with at least one of those tags as part of the purpose.\nDid you know?\nThink of\npurposes\nas a way of further protecting particularly sensitive data. Even if a user can see data in a table, you may not want them to see one or two sensitive columns within that table.\nAsset curation by domain\nâ\nOne way you can use purposes is to curate assets. In this approach, the purpose's tag tends to be a domain. For example, this could be a project or an area of your organization's business.\nThrough the purpose, you can grant permissions to assets with that tag.\nDid you know?\nWith purposes, anyÂ\nfuture\nassets given a tag will gain the same permissions   -  no policy changes needed.\nGranular data protection\nâ\nThe other way you can use purposes is to enforce granular data protection. In this approach, the purpose's tag tends to be some level of information sensitivity. For example, this could be personally-identifiable information (PII) or confidential internal financial metrics.\nThese sensitivity tags will tend to be against granular data assets   -  often columns.\nPersonas\ntend to control permissions at a broader level, for example entire data sources, databases or schemas. Through these more granular tags, purposes give you more fine-grained control. And you can layer this\non top of\nthe permissions granted by\npersonas\n.\nFor example, you might grant permission to preview and query a database to a group of users through a\npersona\n. But you don't want those users to be able to see any PII data   -  specific columns   -  wherever they appear in the database. There could be hundreds of these columns, scattered across thousands of tables. By tagging the columns, you can restrict access to them through a single policy in a purpose. This way you don't need to maintain many separate per-column policies through a\npersona\n.\nTags:\natlan\ndocumentation\nPrevious\nWhat are personas?\nNext\nWhat are groups?\nAsset curation by domain\nGranular data protection"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/automate-data-governance",
    "content": "Build governance\nStewardship\nGet Started\nAutomate data governance\nOn this page\nAutomate data governance\nWho can do this?\nYou must be an\nadmin user\nin Atlan to\nenable\n,\ncreate\n, and\nmanage\ngovernance workflows. Anyone with access to Atlan   -  admin, member, or guest user   -  can use the\ninbox\n.\nYou can streamline your data governance requirements in Atlan with\ngovernance workflows\nand manage alerts, approvals, and tasks using the\ninbox\n. Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nFor example, instead of allowing your users to directly query data or update the certification status of an asset, you can specify assets that require advanced controls and create governance workflows to govern them. These workflows will run in the background, ensure that all required approvals are in place, and only then approve users with appropriate permissions to perform any action.\nYou can use governance workflows to ensure:\nRisk mitigation\n-  determine how data is used and shared in your organization with automated access policies.\nData security\n-  manage requests for data access and processing to only allow access to authorized individuals or teams.\nMetadata change management\n-  monitor and audit metadata changes to align with established organizational standards.\nNew entity creation\n-  manage and audit documentation of business context such as glossaries and tags to align with established organizational standards.\nPolicy compliance\n-  set up repeatable processes and approval flows for your data assets in Atlan to adhere to regulatory requirements   -  currently only applicable if you have also\nenabled the policy center module\n.\nWorkflow properties\nâ\nA common set of properties are applicable to all governance workflows in Atlan:\nOnly an\nadmin user\ncan create, update, or delete governance workflows.\nOut-of-the-box workflow templates.\nPredefined steps based on workflow selection.\nMust be associated with an asset type or action.\nSet up auto-approval rules for users, groups, or owners based on metadata attributes and policies.\nActivity logs for all workflows available by default.\nVisibility into the transition states of a workflow.\nOverlapping workflows   -  governance workflows provide you with the flexibility of creating workflows per team or business domain on the same set of assets instead of creating one complex workflow to cover all your use cases. Atlan will handle all the complexities, only allowing approvals to go through once\nall\napproval conditions have been met.\nWorkflow templates\nâ\nYou can choose from the following workflow templates to govern your assets and manage access:\nChange management\nâ\nThis template allows you to control changes to metadata within your organization's data management and governance framework. Use cases include requests to:\nAdd, update, and remove\ndescriptions\nmanually and\nusing Atlan AI\nAdd, update, and remove\ncertificates\nAdd, update, and remove an\nalias\nLink and remove terms\nfrom asset profile\nAdd, update, and remove\nowners\nAttach, update, and remove\ntags\nAdd, update, and remove\ncustom metadata\nAdd, update, and remove\ndomains\nAdd, update, and remove\nREADMEs\nAdd, update, and remove\nannouncements\nUpdate and archive\nglossaries, categories, and terms\nMove terms and categories\nChange management workflows will override any permissions assigned through\nuser roles\nor\naccess policies\n. For example, even for users with\nedit access\n, metadata update requests will go through change management workflows.\nIf there are no change management workflows in place, then users with edit access will be able to update metadata while users without edit access will only be able to\nsuggest changes to metadata\n.\nNew entity creation\nâ\nThis template allows you to control the creation and publication of new entities in Atlan. The new entity creation workflow will override existing\nglossary policies\nand\nuser role permissions\nto create new entities.\nCreation of the following entities is currently supported for the new entity creation workflow:\nGlossaries\nCategories\nTerms\nTags\nData products\n:Â Â\nCreation of a new data product\nChange of a data product's status from\nSunset\n,\nArchived\n, or\nDraft\nto\nPublished\nWhether you are an admin or a member user in Atlan, the existence of a new entity creation workflow means you will need to submit a request for creating new entities. Guest users are neither allowed to directly create nor suggest the creation of glossaries, categories, terms, and tags.\nAccess management\nâ\nThis template allows you to automate the process of requesting, approving, and revoking access to data assets in Atlan. It includes the combination of a self-service approach as well as mandating human intervention for approval.\nYou can also\nrevoke data access\nin Atlan or other data sources. For data sources other than Atlan, you can configure additional actions to revoke data access in the data source.\nUse cases include requests to query data or view sample data for the following supported asset types   -  tables, views, and materialized views.\nGrant access in Atlan   -  allow requesters to request data access for querying data in Insights and previewing sample data within Atlan only.\nRaise\nJira\nticket to grant or revoke data access on source   -  allow requesters to request or revoke data access for any tool. Atlan will create a support ticket in Jira Cloud for your team to grant or revoke data access and display the status of your request in Atlan. You will need to:\nIntegrate Jira Cloud and Atlan\n.\nLink your individual Jira Cloud account to Atlan\n.\nInstall or register a webhook\n.\nCreate an access management workflow\nto enable or revoke access everywhere using Jira.\nAdd a Jira project and issue type and specify an issue status while creating the data access workflow.\nYour users will be granted access or their access will be revoked once the request is approved in Jira.\nRaise\nServiceNow\nrequest to grant or revoke data access on source   -  allow requesters to request or revoke data access for any tool. Atlan will create a request in the\nAtlan Data Access\ncatalog for your team in ServiceNow to grant or revoke data access and display the status of your request in Atlan. You will need to:\nIntegrate ServiceNow and Atlan\n.\nLink your individual ServiceNow account to Atlan\n.\nCreate a data access approval workflow\nto enable or revoke access everywhere using ServiceNow.\nSpecify the request state(s) for approval while creating the data access workflow.\nYour users will be granted access or their access will be revoked once the request is approved in ServiceNow.\nTrigger a webhook   -  allow requesters to request or revoke data access for any tool. Atlan will trigger a webhook to a URL of your choice for your team to grant or revoke data access.\nFor\nURL\n, enter the URL for where you want to receive events, including details on requester, approver, and asset, and then validate the URL.\ndanger\nAtlan will send a sample payload to test if the webhook URL is correct. You must respond with a\n2xx\nstatus for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure.\nCopy the\nSecret Key\nand store it in a secure location to verify data access approval or revocation requests from Atlan.\nPolicy approval\nâ\nYou must\nenable the policy center module\nto use the policy approval workflow template.\nThis template allows you to automate approvals for your\ndata governance policies\nin Atlan. Automated policy approval workflows can help you streamline the approval process, facilitate compliance with regulatory standards, and simplify data governance for your organization.\nUse cases include requests to:\nCreate new policies\nRevise existing policies\nEnable governance workflows and inbox\nâ\nWho can do this?\nYou must be an\nadmin user\nin Atlan to enable the governance workflows and inbox module for your organization.\nTo enable governance workflows and inbox for your Atlan users:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nLabs\n.\nOn the\nLabs\npage, under\nGovernance center\n, turn on\nGovernance Workflows and Inbox\nto\ngovern your assets\nand\nmanage alerts, approvals, and tasks\nin Atlan more effectively.\nIf you'd like to disable the\nGovernance Workflows and Inbox\nmodule from your organization's Atlan workspace, follow the steps above to turn it off.\nOnce enabled, you can also temporarily disable the module and turn it on again as needed. For any governance workflows you may have created or\nexisting requests\n, this will not result in any data loss.\nInteractions with existing access control mechanisms\nâ\nOnce you have turned on governance workflows and inbox, the module will interact with existing access control mechanisms in Atlan as follows:\nRequests\n: Atlan will channel requests and approvals through governance workflows and land them in the inbox.\nNew requests   -  once you have enabled governance workflows and inbox, the\nrequests widget\nwill be replaced by an inbox and your member and guest users will not be able to raise any new requests until an admin user has created at least one governance workflow. To enable your member and guest users to raise new requests in Atlan:\nCreate a change management governance workflow\n.\nSelect all connections present in your Atlan workspace\n.\nSkip auto-approval\n.\nSelect\nAnyone approves\nand list the users or groups designated as your Atlan admins\n.\nPublish your first governance workflow! Once published, this comprehensive workflow will allow your member and guest users to raise requests. Now you can focus on creating more use-case-driven workflows and consequently removing governed assets from the first workflow until you no longer need it.\nExisting requests   -  only admin users can take action on existing requests from the\nrequests center\n. Your member and guest users will only be able to raise new requests on governed assets.Â\nPersonas\nand\npurposes\n:\nMetadata policies\n-  your users must have read access to an asset for triggering governance workflows. If an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow/deny permissions in metadata policies.\nData policies\n:\nNo data policy exists   -  if the workflow connection allows querying and previewing sample data but a data policy has not been configured, your users will be able to raise a data access request on governed assets in the connection.\nData policy with explicit restrictions   -  if an existing data policy denies querying and previewing sample data and assets are governed by a governance workflow, your users will not be able toÂ raise a data access request on governed assets in the connection.\nData policy with explicit grants   -  if an existing data policy allows querying and previewing sample data and assets are governed by a governance workflow, your users will be able to raise a data access request on governed assets in the connection.\nGlossary policies\n-  if an asset (glossaries, categories, and terms) is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow/deny permissions in glossary policies.\nDomain policies\n-  governance workflows are currently not applicable to domain policies.\nUser roles\n-  if an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of their role or permissions. For any asset not governed by a governance workflow, default role permissions will apply.\nConnection admins\n-  if an asset is governed by a governance workflow, connection admins will have to go through the approval process for governed assets in the connection.\nGovernance workflows will currently not be triggered for the following actions:\nAdd associated terms\nAdd, update, and remove categories for terms from term profile\nAdd, update, and remove\nresources\nAdd a README to a term using Atlan AI\nLink and remove terms\nfrom term profile\nBulk updates through\nspreadsheet tools\nBulk updates using\nplaybooks\nBulk updates using Atlan AI\nBulk updates through API, SDK, and CLI operations\nMetadata updates in supported tools using\nAtlan browser extension\nTags:\nalerts\nmonitoring\nnotifications\nworkflow\nautomation\norchestration\nPrevious\nStewardship\nNext\nCreate governance workflows\nWorkflow properties\nWorkflow templates\nEnable governance workflows and inbox"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/concepts/what-are-asset-profiles",
    "content": "Use data\nDiscovery\nConcepts\nWhat are asset profiles?\nOn this page\nWhat are asset profiles?\nEvery asset in Atlan has its own asset profile, which consists of all the information available for that particular asset. After you've discovered an asset, click to open the asset profile. This view gives you all the context you need about the asset.\nComponents of an asset profile\nâ\nOverview\nâ\nThis section displays important details about the asset:\nTechnical name and\nalias\n, if added\nNumber of rows and columns\nConnections\nDescription\nof the asset\nCertification status\n(verified, draft, or deprecated)\nOwner\nof the asset\nLineage\nview\nRelated assets\nFor table profiles, you can also view the\nColumns\ntab. This tab allows you to update the metadata for your columns directly from the asset profile.\nLineage\nâ\nLineage\noffers a visual representation of the sources and transformations of your asset. You can also\ndownload lineage\nin a CSV file or as an image from here.\nRelated assets\nâ\nRelated assets\ndisplays all the assets that have associations with each other beyond a parent-child relationship. For example, a Microsoft Power BI workspace will generally have reports, datasets, dashboards, and dataflows, all of which are related to each other.\nRelated assets are additionally listed in the\nRelations\ntab of the asset sidebar.\nColumn preview\nâ\nColumn preview\noffers a snapshot of all the columns in a data table, including the column name, data type, and description.\nThis will change to\nField preview\nin the asset profile and\nFields\ntab in the asset sidebar for certain asset types   -  including but not limited to data source fields and calculated fields for Tableau data sources, columns for Microsoft Power BI tables, fields for Looker explores and views, and more.\nYou can also\nexport child assets\nsuch as columns or fields from the parent asset profile.\nSample data\nâ\nSample data\nshows the sample data for an asset. This helps users understand what kind of data is included in the asset and allows them to copy or export this data.\nConnection admins can also enforce users to\nvalidate their credentials\nbefore viewing sample data, helping you enforce better governance across your organization.\nLinked queries\nâ\nLinked queries\ndisplays any\nsaved queries\nÂ auto-linked to the asset when queried or referenced in the SQL query. This helps users quickly find the saved queries for additional context or\nlaunch the query in Insights\ndirectly from the asset profile.\nFrom the\nLinked queries\ntab:\nView all saved queries linked to the asset, along with a total count of such queries.\nHover over a linked query to:\nView the total number of query runs in the last 30 days in a popover.\nClick the play icon to run the query in Insights.\nClick the open asset sidebar icon to view details about the query in the sidebar.\nClick the 3-dot icon to view any additional linked queries.\nIf you do not have access to query data in Atlan, the linked queries will be displayed with a\nlock icon\n.\nREADME\nâ\nReadme\nprovides contextual information about the asset. It's a great place to crowdsource all the tribal knowledge and context that different users might have about the data asset.\nAsset profile header\nâ\nThis section helps you perform quick actions. From the top right of the asset profile:\nClick the user avatars to view a list of recently visited users, total views on your asset, total number of unique visitors, and total views by user.\nUse the days filter to filter asset views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button toÂ\nstar your asset\nand bookmark it for easy access.\nExpand the\nQuery\ndropdown to view sample data or\nquery the asset\nin Insights.\nClick the clipboard icon to copy the link for your asset.\nClick the\nSlack\nor\nTeams\nicon to post on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the 3-dot icon to\nadd an announcement\nor a\nresource\nto your asset.\nAsset sidebar\nâ\nThe sidebar to the right of the asset profile provides high-level information about the asset. Here's what you can view:\nOverview\noffers a preview of the key characteristics of the asset, including linked\ndomains\nand\npolicies\n.\nColumns\ndisplays a list of columns in an asset, along with a search bar for quick search.\nRelations\nshows a list of all the related assets.\nUsage\ndisplaysÂ\nusage metadata\nfor your Snowflake and Google BigQuery assets.\nLineage\nshows the upstream sources and downstream transformations for the asset.\nFact-Dim Relations\ndisplays foreign-key relationships between fact and dimension tables.\nFact-dimension relationships\nbetween assets can currently only be defined and published via API.\nActivity\nserves as a\nchangelog for the asset\n.\nResources\nare links to internal or external URLs that help your team understand the asset.\nQueries\nshows all the saved queries for the asset.\nRequests\nfor an asset can be filtered by their status, such as\nPending\n,\nApproved\n, and\nRejected\n.\nProperties\nshows the unique identification number of the asset and other essential properties.\nIntegrations\nshow Slack messages and Jira tickets pertaining to the asset.\nCustom metadata\ntabs display custom metadata properties of the asset, if enabled.\nTags:\natlan\ndocumentation\nPrevious\nHow to interpret timestamps\nNext\nProvide credentials to view sample data\nComponents of an asset profile"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/concepts/what-is-a-glossary",
    "content": "Build governance\nGlossary\nConcepts\nWhat is a glossary?\nOn this page\nA Glossary\nA glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like\ncost\n,\nP&L\n, and\nrevenue\ncan be used to group and search all financial data assets.\nUsing familiar terminology helps people quickly understand the data and its context. This is a crucial element of data governance since it adds business context to the data initiatives of an organization.\nIn Atlan, glossary terms can be attached to any data asset and leveraged to power quick and easy data discovery.\nWhy do I need a glossary?\nâ\nIn today's diverse data teams, which include people from different backgrounds and use cases, not all of them think about their data in the same way.\nFor example, one team might think that a particular metric is showing an annualized rate, but the actual rate may be calculated quarterly. This could lead to some real confusion down the road. Defining data terms and sharing those definitions across your team can make a huge difference to data users at all levels of the organization.\nFor teams made up of data analysts, data engineers, data scientists, and decision makers, having a shared language is an important step towards ensuring better collaboration. Building a glossary allows your team to define the metrics, columns, and assets with the same meaning for everyone.\nHighlights of the Atlan glossary\nâ\nHere's how the Atlan glossary can help your organization:\nPowers search and makes it easier to discover data assets\nEncourages the creation, maintenance, and enrichment of business and functional terms due to their direct and visible use in searches\nAllows crowdsourcing the task of attaching appropriate glossary terms to data assets\nSupports automated metadata management through auto-glossary suggestions from the Atlan bot\nAnatomy of the Atlan glossary\nâ\nAtlan gives users the option to build hierarchical glossaries. A glossary term is the lowest unit that can exist independently inside a glossary. These terms can then be grouped into categories and linked together as related terms.\nDid you know?\nThis structure allows for glossaries from multiple domains.\nLet's look at how terms and categories work together to build a glossary.\nTerm\nâ\nA term is the lowest unit that is unique to each glossary.\nIt describes the content of the data assets in a useful and precise way.\nIt can exist independently, without belonging to any particular category or subcategory.\nDid you know?\nOnce youâve\nadded terms\nto your glossaries, you can also\nlink them to your assets\n. You can then use terms on the\nAssets\npage to quickly\nfilter your assets\n.\nCategory\nâ\nA category is a way of organizing the terms in a glossary.\nIt can be used to group together similar terms.\nSubcategories can be added within categories to provide more context in a glossary.\nAssociated terms\nâ\nWho can do this?\nYou will need your Atlan administrator to\nenable associated terms\n-  except related terms.\nWith associated terms, you can define semantic relationships between your terms. These provide additional context for common definitions in your organization.Â\nRelated term\nâ\nSimilar in definition   -  serves the purpose of a \"see also\" section in a dictionary.\nClient\nis a related term for\nCustomer\n.\nRecommended term\nâ\nPreferred form of usage for the current term applied.\nUser\nmay be preferred over\nCustomer\nin the context of your organization.\nSynonym\nâ\nInterchangeable in meaning as another term.\nGlossary\nand\nDictionary\n, or\nClient\nand\nCustomer\n.\nAntonym\nâ\nOpposite in meaning to a particular term.\nMinimum\nis an antonym for the term\nMaximum\n, or\nLoss\nand\nProfit\nare antonyms.\nTranslated term\nâ\nTranslated version of the same term in additional languages.Â\nCliente\nis the Spanish term for\nCustomer\n.\nValid values for\nâ\nDefines values that are considered appropriate for a related term.\nRed\n,\nGreen\n,\nBlue\n, and\nYellow\nare valid values for the term\nColor\n.\nClassifies and Classified by\nâ\nThese have a reciprocal relationship that helps provide more context for both terms.\nCountry\nclassifies\nUnited States\n, while\nUnited States\nis classified by\nCountry\n.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nLink terms to assets\nNext\nGlossary update request approval issue\nWhy do I need a glossary?\nHighlights of the Atlan glossary\nAnatomy of the Atlan glossary\nAssociated terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-owners",
    "content": "Use data\nDiscovery\nAsset Management\nAdd owners\nOn this page\nAdd owners\nAtlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data.\nThe owner is responsible for maintaining the data\nasset. They are the right person to contact for questions about the data's frequency of update, progress, status, and more.\nAdd owners to your assets\nâ\nTo add or update owners for a data asset, follow these steps:\nOn the Atlan homepage, click\nAssets\nin the left menu.\nClick on an asset to view its asset profile.\nIn the\nOverview\nmenu to the right, click\n+\nunder\nOwners\nto access a list of users.\nTo assign owners, you can either:\nClick the\ncheckbox\nnext to an individual user's name.Â\nToggle to\ngroups\nin the top right and select an entire group of users.Â\nClick\nSave\n.\nYour asset profile will now display the asset owners! ð\nSearch by asset owners\nâ\nYou can also filter your assets by asset owners. Here are the steps:\nIn the\nFilters\nmenu on the\nAssets\npage, click\nOwners\n.Â\nClick the\ncheckbox\nnext to an owner name to view their owned assets.\nDid you know?\nYou can select\nNo Owners\nin the\nOwners\nfilter to view assets that currently do not have assigned owners and assign accordingly if needed.\nTags:\natlan\ndocumentation\nPrevious\nStar assets\nNext\nAdd descriptions\nAdd owners to your assets\nSearch by asset owners"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-certificates",
    "content": "Use data\nDiscovery\nAsset Management\nAdd certificates\nOn this page\nAdd certificates\nHow many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data!\nWouldn't it be really convenient if the data could answer these questions?\nCertificates in Atlan can help! The certification tags help users quickly identify whether a data asset is ready to use, a work in progress, or has some issues.\nYou can add the following four certification tags to any data asset:\nVerified\nfor ready to be used.\nDraft\nfor work in progress.\nDeprecated\nif the asset no longer exists.\nNo certificate\nif not required or needs documentation down the line.\nDid you know?\nCertificates can be used to quickly filter data assets on the\nAssets\npage. This helps build trust in your data assets among users.\nAdd certificates to your assets\nâ\nTo add or update the certificate for your data assets, follow these steps:\nOn the Atlan homepage, click\nAssets\nin the left menu.\nClick on the\nasset\nto open its asset profile.\nIn the right menu, click\n+\nunder\nCertificate\nand choose the relevant certification option.\nWrite a message to add more context.Â\nNow your data can proudly display its status for all to see! ð\nOnce you have selected a certification tag for your data asset, you will get a popup that your certificate has been saved.\nTags:\natlan\ndocumentation\nPrevious\nAdd descriptions\nNext\nAdd a resource\nAdd certificates to your assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/link-terms-to-assets",
    "content": "Build governance\nGlossary\nTerm Management\nLink terms to assets\nOn this page\nLink terms to assets\nOnce you've\nset up a glossary\n, you can link terms from your glossary to your data assets in Atlan.\nLinking glossary terms with your data assets can help you:\nProvide additional context for your assets to other users in your organization.\nCreate common definitions once and apply them many times to multiple assets.\nOffer an abstract point for applying\ntags\nto be propagated to all linked assets   -  including their downstream and child assets   -  if\npropagation is enabled\n.\nExample\nâ\nIf your data assets include personal information   -  for example, email addresses   -  you can link your assets to an\nEmail Address\nterm to provide context to your users.\nYou can define the term\nEmail Address\nonce in the glossary.\nYou can link the term to all the columns where an individual's email address appears.\nYou can also tag the term as\nPII\n-  and all of the linked assets will be\ntagged\nas\nPII\n.\nLink terms to your assets\nâ\ndanger\nYou will first need to\ncreate a glossary\nand add terms to it before you can link terms to your assets.\nTo link a term to an asset:\nFrom a term\nâ\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of your glossary.\nUnder your glossary name, click the category in which your term is nested and then click the term you would like to link to your assets.\nIn the term profile, next to\nOverview\n, click\nLinked assets\n.\nClick\n+\nLink Assets\nto get started.Â\n(Optional) In the sidebar on the right, under the search bar, click an asset type to filter your assets   -  for example,\nColumn\n.\nIn the sidebar on the right, select the asset(s) to which you would like to link the term.\nAt the bottom of the sidebar, click\nLink asset(s)\nto confirm your selections.\n(Optional) Under\nLinked Assets\n, next to the search bar, click the export icon to\nexport linked assets\nfor terms to spreadsheets.\nFrom an asset\nâ\nFrom the left menu on any screen, click\nAssets\n.\nOn the\nAssets\npage, select the asset to which you would like to link a term.\nUnder\nTerms\nin the asset sidebar, click the\n+\nsign to add a term to your asset.\nIn the dialog, expand the glossary menu and then click the term you would like to link to your assets.\nClick\nSave\nto confirm your selections.\nYou can now view linked assets for your glossary term! ð\nDid you know?\nYou can also\nset up playbooks\nto automate the task of updating asset metadata, such as terms and more.\nUnlink terms from your assets\nâ\nTo unlink a term from an asset:\nFrom a term\nâ\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of your glossary.\nUnder your glossary name, click the category in which your term is nested and then click the term you would like to unlink from your assets.\nIn the term profile, next to\nOverview\n, click\nLinked assets\n.\n(Optional) Under\nLinked Assets\n, next to the search bar, click the export icon to\nexport linked assets\nfor terms to spreadsheets before unlinking them.\nUnder\nLinked assets\n, navigate to the asset(s) from which you would like to unlink the term.\nTo the right of the asset name, click the\nthree dots\nand then click\nUnlink asset\n.\nFrom an asset\nâ\nFrom the left menu on any screen, click\nAssets\n.\nOn the\nAssets\npage, select the asset from which you would like to unlink a term.\nUnder\nTerms\nin the asset sidebar, hover over the term, and in the top right of the term popover, click the\nunlink button\nto unlink the term from the asset.\nYour assets will now be unlinked from the glossary term.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nBulk upload terms in the glossary\nNext\nWhat is a glossary?\nExample\nLink terms to your assets\nUnlink terms from your assets"
  },
  {
    "url": "https://docs.atlan.com/tags/get-started",
    "content": "3 docs tagged with \"get-started\"\nView all tags\nAdministrators\nLearn about administrators.\nContributors\nLearn about contributors.\nData consumers\nLearn about data consumers."
  },
  {
    "url": "https://docs.atlan.com/tags/quick-start",
    "content": "3 docs tagged with \"quick-start\"\nView all tags\nAdministrators\nLearn about administrators.\nContributors\nLearn about contributors.\nData consumers\nLearn about data consumers."
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/k3s/install-secure-agent-on-virtual-machine-k3s",
    "content": "Connect data\nSecure Agent\nManage Agent\nK3s\nInstall on Virtual Machine (K3s)\nOn this page\nInstall on Virtual Machine (K3s)\nDid you know?\nSecure Agent installation can be done by a non-root user. Root access is\nonly\nneeded for setting up system prerequisites before installation.\nThis page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying\nK3s in a rootless execution mode\n.\nSystem requirements\nâ\nBefore installing the Secure Agent, ensure that the virtual machine (VM) meets the following requirements:\nAt least 80GB of available disk space.\nA Linux-based OS running on an amd64 (x86_64) architecture with\nsystemd\nenabled.\nThe Secure Agent requires the following ports for internal services. Ensure these ports are open and accessible:\nKubernetes API:\n6443\nInternal K3s proxy:\n10443\n,\n10080\nMinIO storage:\n9000\n,\n32075\nMinIO console:\n9001\n,\n30614\nTraefik ingress:\n31037\n,\n32547\nPrerequisites\nâ\nBefore installing the Secure Agent, complete the following setup steps to prepare your Atlan tenant and virtual machine.\nConfigure Atlan tenant\nâ\nIn Atlan, complete the following steps to configure the tenant:\nSign in to your tenant as an Atlan admin.\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\nclick\nLabs\n.\nNavigate to\nWorkflow Center\n.\nEnable the\nCrawl assets using Secure Agent\ntoggle.\nConfigure virtual machine\nâ\nOn the virtual machine, complete the following steps to configure it:\nLog in as a root user.\nCreate the required directory to configure cgroup delegation with:\nsudo mkdir -p /etc/systemd/system/\n[email protected]\nUse the below\ncat\ncommand to create the delegation file with required configuration:\ncat <<EOF | sudo tee /etc/systemd/system/\n[email protected]\n/delegate.conf\n[Service]\nDelegate=cpu cpuset io memory pids\nEOF\nUse the below command to reload systemd:\nsudo systemctl daemon-reload && sudo reboot\nTo keep the Secure Agent running after logout, the root user must enable service persistence for the user installing it by running the following command:\nsudo loginctl enable-linger ``<user_installing_secure_agent>``\nReplace\n<user_installing_secure_agent>\nwith the actual username of the user installing the Secure Agent.\nRun the following commands to enable IP forwarding so Secure Agent can communicate with other Secure Agent instances and make network requests to the Atlan tenant.\nIPv4 forwarding:\nsudo sysctl -w net.ipv4.ip_forward=1\nIPv6 forwarding:\nsudo sysctl -w net.ipv6.conf.all.forwarding=1\nTo manage containerized workloads, install fuse-overlayfs with:\nsudo yum install fuse-overlayfs\nThe VM must have access to the source systemâs secret manager to retrieve secrets. For more information, see how to provide access for some popular secret managers listed below:\nAWS:\nConfigure access for AWS Secrets Manager.\nAzure:\nConfigure access for Azure Key Vault.\nGCP:\nConfigure access for GCP Secret Manager.\nPermissions required\nâ\nBefore installing the Secure Agent, the user must have the following permissions:\nCreate and modify directories in the userâs home directory:\n~/.config/systemd/user\n,\n~/bin\n,\n~/.local/bin\n, and\n~/.rancher\n.\nCreate and write log files.\nExecute standard Linux commands:\nmkdir\n,\nchmod\n,\ntar\n, and\nsed\n.\nDownload Agent packages\nâ\nFollow these steps to download the necessary packages for setting up the Secure Agent.\nDid you know?\nThe steps require Internet access to download files. In case the VM has no Internet connectivity, one can download them separately and copy the files to the VM.\nCreate a folder for deployment and navigate to it:\nmkdir -p atlan-secure-agent && cd atlan-secure-agent\nRun the following commands to download the required packages:\nDownload the\nKubernetes install package\n, which contains files to run K3s on an air-gapped VM:\ncurl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/k3s_offline_package_main.tar\nDownload the\nContainer images package\nif an image registry isn't available:\ncurl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_images_main.tar\nDownload the\nSecure Agent install package\n, which contains files for running the Secure Agent:\ncurl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_install_config_main.tar.gz\nVerify that all the files are downloaded.\nInstall Secure Agent\nâ\nFollow these steps to install and configure the Secure Agent on the virtual machine.\nDid you know?\nThe installation can be performed by both root (administrative) and non-root (standard) users.\nNavigate to the deployment folder (if not already):\ncd atlan-secure-agent\nRun the following command to extract the Secure Agent install package:\ntar -xvf atlan_install_config_main.tar.gz\nThe\nrootless-install\nfolder is extracted from the Secure Agent install package. Run the following command to create an environment file using the\nenv.sample\nfile located in the\nrootless-install\nfolder:\ncp ./rootless-install/.env.sample .env\nOpen the\n.env\nfile and update these variables:\nVAR_ATLAN_SECURE_AGENT_NAME=prod-atlan-agent-vm\nVAR_ATLAN_DOMAIN=tenant.atlan.com\nVAR_ATLAN_TOKEN=<atlan-api-token>\nVAR_ATLAN_DATA_PATH=</absolute/path/to/atlan-secure-agent>\nReplace the environment variable values:\nVAR_ATLAN_SECURE_AGENT_NAME:\nSpecify a meaningful and unique name for the Secure Agent. For example,\nprod-atlan-agent-vm\n.\nVAR_ATLAN_DOMAIN:\nEnter your Atlan tenant domain. For example,\ntenant.atlan.com\n.\nVAR_ATLAN_TOKEN:\nProvide the API key (Bearer token). For more information on generating an API key, see\nCreate a bearer token\n.\nVAR_ATLAN_DATA_PATH:\nSpecify the path where the\natlan-secure-agent\ndirectory is located.\nRun the following command to grant execution permission for the setup script:\nchmod +x rootless-install/setup.sh\nThe extracted\nsetup.sh\nfile installs the Secure Agent and K3s. Run the following command to execute the installer:\n./rootless-install/setup.sh .env\nWhile the installation is in progress, you can run the following command to verify the progress:\nkubectl get pods -A\nVerify installation\nâ\nAfter installing the Secure Agent, verify that it's running correctly. You can check its status through the Atlan UI or by accessing the Agent UI on K3s.\nLog in as an Atlan admin or a similar role to access your tenant. For example:\nhttps://<tenant>.atlan.com\n.\nNavigate to the\nAgent\ntab.\nIn the\nSecure Agents\nlist, use the\nSearch for agents\nbox to enter your Secure Agent name.\nIf the agent appears in the list and is marked\nActive\n, installation is complete.\nTroubleshooting\nâ\nIf you encounter issues during installation, follow these steps:\nCheck the logs using the following command for detailed error messages that may indicate the root cause:\ntail -f logs/k3s.log\nFor K3s rootless mode issues, follow the\nK3s official documentation\nfor troubleshooting rootless issues.\nIf you continue to face issues, contact Atlan support by\ncreating a ticket\n.\nTags:\natlan\ndocumentation\nPrevious\nSecure Agent\nNext\nInstall on AWS EKS\nSystem requirements\nPrerequisites\nPermissions required\nDownload Agent packages\nInstall Secure Agent\nVerify installation\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
    "content": "Connect data\nSecure Agent\nManage Agent\nAWS EKS\nInstall on AWS EKS\nOn this page\nInstall on AWS EKS\nThis guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster.\nSystem requirements\nâ\nTo deploy the Secure Agent on AWS EKS, ensure the following system requirements are met:\nConfigure network access between your Secure Agent and Atlan tenant. For more information, see\nWhitelisting Secure Agent\n.\nYou need Kubernetes version 1.19 or higher.\nYou need to install Helm and kubectl on the machine you're using to connect to the AWS EKS cluster.\nYou need at least 1 node for base services with a disk space of 20 GB and instance configuration as below:\nEnvironment\nMinimum instance type\nRecommended instance type\nProduction\nt3.large\nCustom based on workload\nNon-production\nt3.large\nt3.xlarge\ninfo\nðª\nDid you know?\nFor optimal autoscaling, scale nodes based on the number of concurrent workflows.\nPermissions required\nâ\nBefore installing the Secure Agent, make sure the following permissions are in place:\nPermissions for the Installer\nâ\nThe user, service or system account performing the installation needs access to the EKS cluster and permissions to manage Custom Resource Definitions (CRDs).\nEnsure the kubeconfig is correctly configured for your target EKS cluster. If needed, use the following command to configure or update your kubeconfig file.\naws eks update-kubeconfig --region ``<region>`` --name ``<cluster-name>``\nReplace\n<region>\nwith your AWS region (for example, us-east-1) and\n<cluster-name>\nwith the name of your EKS cluster.\nThe installer needs permission to create, update, and delete Custom Resource Definitions (CRDs). If not using the cluster-admin role, grant the following:\nCreate a file named\nagent-crd-permissions.yaml\non your machine.\nCopy the following content into the file:\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n# Use a descriptive name\nname: helm-crd-installer-role\nrules:\n- apiGroups: [\"apiextensions.k8s.io\"]\nresources: [\"customresourcedefinitions\"]\n# Recommended verbs for Helm CRD management\nverbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n# Use a descriptive name\nname: helm-crd-installer-binding\nsubjects:\n# *** IMPORTANT: Modify this section based on who is running Helm ***\n# Choose ONE of the following options and replace placeholders.\n# Option 1: Bind to a specific User\n- kind: User\nname: \"your-kubernetes-username\" # Replace with the installing user's K8s username recognized by the cluster\napiGroup: rbac.authorization.k8s.io\n# Option 2: Bind to a specific Group\n# - kind: Group\n#   name: \"your-kubernetes-groupname\" # Replace with the installing user's K8s group name\n#   apiGroup: rbac.authorization.k8s.io\n# Option 3: Bind to a Service Account (e.g., for CI/CD pipelines)\n# - kind: ServiceAccount\n#   name: \"installer-sa-name\" # Replace with the installer SA's name\n#   namespace: \"installer-sa-namespace\" # Replace with the installer SA's namespace\nroleRef:\n# This refers to the ClusterRole created above\nkind: ClusterRole\nname: helm-crd-installer-role\napiGroup: rbac.authorization.k8s.io\nFollow the comments in the file to replace the placeholders. In the above file:\nResource:\ncustomresourcedefinitions\n- needed for managing CRDs in the cluster.\nAPI Group:\napiextensions.k8s.io\n- required to work with CRDs.\nVerbs: create, get, list, update, delete - necessary for installing, inspecting, updating, and cleaning up CRDs using Helm.\nClusterRoleBinding: needed to assign the role to the user or group performing the installation.\nOnce youâve updated the placeholders, use the below\nkubectl\ncommand to apply the configuration:\nOnce youâve updated the placeholders, use the below kubectl command to apply the configuration:\nkubectl apply -f agent-crd-permissions.yaml\nPermissions for the Secure Agent Pod (Runtime)\nâ\nThe Secure Agent runs as pods in your EKS cluster and requires permissions to interact with AWS services like S3. These permissions are granted through IAM Roles for Service Accounts (IRSA).\nCreate a new IAM role for the Secure Agent pod.\nConfigure the trust policy to enable the Secure Agentâs Kubernetes service account to assume the role. Make sure the argo-workflow service account exists in the same namespace where you plan to install the agent. For more information, see the AWS documentation on\nIAM roles for service accounts (IRSA)\n.\nExample: Trust policy for the argo-workflow service account:\n\"Condition\": {\n\"StringEquals\": {\n\":sub\": \"system:serviceaccount::argo-workflow\"\n}\n}\nReplace\n<namespace>\nwith the namespace where you plan to install agent.\nCreate an S3 bucket (or use an existing one), and attach the following permissions to the IAM role used by the Secure Agent:\ns3:PutObject\n: Needed to write logs and artifacts\ns3:GetObject\n: Needed to read logs and artifacts.\ns3:ListBucket\n: Needed by Argo artifact repository for listing objects.\nDid you know?\nThe Helm chart automatically configures the necessary Kubernetes RBAC for Argo Workflows, which the Secure Agent uses. No additional configuration is required for the Secure Agent pod..\nPrerequisites\nâ\nBefore proceeding, complete the following setup steps to prepare your Atlan tenant and AWS EKS cluster.\nConfigure Atlan tenant\nâ\nIn your Atlan tenant:\nSign in as an Atlan admin.\nGo to\nAdmin\nfrom the left menu.\nUnder\nWorkspace\n, click\nLabs\n.\nNavigate to\nWorkflow Center\n.\nEnable the\nCrawl assets using Secure Agent\ntoggle.\nConfigure Secure Agent settings\nâ\nThe\nagent_config_values.yaml\nfile is used to configure the Secure Agent, Argo Workflows, and storage for the AWS EKS cluster. Follow these instructions on the machine where you're performing the installation.\nCreate a file named\nagent_config_values.yaml\nfile.\nCopy the configuration below into the file:\n# -----------------------------------------------------------------------------------------\n# Agent core settings   -  Follow the comments to update:\n# 1. Image registry settings - To be updated only if you are using a private image registry\n# 2. Atlan connection settings - To be updated only if you want agent to use the S3 bucket\n# 3. Argo Private repository settings - To be updated only if you are using private repository for Argo workflows\n# 4. Kubernetes Pod Annotation settings - To be updated only if you want to customize the Kubernetes podâs metadata\n# 5. Argo Private repository settings - To be updated only if you are using private repository for Argo workflows\n# 6. S3 storage settings - To be updated with S3 bucket details.\n# -----------------------------------------------------------------------------------------\nagent:\nenabled: true\nenableStorageProxy: false\nca:\ncrt: \"\"\n#Provide a base64-encoded string of a JSON object, e.g., {\"client_id\": 123, \"client_secret\": 1243}.\n#Set this only if you need to include custom headers in API calls made by the agent.\nrestAPIHeaders: \"\"\nversions:\nk3s: \"\"\nk8s: \"\"\nhelm: \"\"\n# 1. Image Registry Settings\nimage:\n# Only update if you're using a private image registry\nregistry: \"public.ecr.aws\"\nrepository: \"atlanhq\"\n# Only update if you're using custom images\nrestImageName: \"rest-2\"\nrestImageTag: \"1.0\"\n# Only update if you're using custom images\njdbcImageName: \"jdbc-metadata-extractor-with-jars\"\njdbcImageTag: \"1.0\"\n# Only update if you're using custom images\ncredentialImageName: \"connector-auth\"\ncredentialImageTag: \"1.0\"\n# Only update if you're using custom images\ncsaScriptsImageName: \"marketplace-csa-scripts\"\ncsaScriptsImageTag: \"1.0\"\n# Marketplace scripts image details - keep these values as is unless using custom images\nmarketplaceScriptsImageName: \"marketplace-scripts-agent\"\nmarketplaceScriptsImageTag: \"1.0\"\npullPolicy: IfNotPresent\npullSecrets: []  # Add pull secrets if using private registry\nannotations: {}\nlabels: {}\nserviceAccountName: \"\"\nautomountServiceAccountToken: true\nresources: {}\n# 2. Atlan connection settings - Only update if you want to agent to use the S3 bucket\natlan:\nargoToken: \"\"\nvaultEnvEnabled: false\n# Set to true only if the agent should store metadata\n# in your bucket instead of sending it to Atlan via presigned URL.\nuseAgentBucket: false\nmetadataBucket: \"\"\npersistentVolume:\nscripts:\nenabled: false\ndata:\nenabled: false\nminio:\nenabled: false\nargo-workflows:\nimages:\npullPolicy: IfNotPresent\npullSecrets: []\ncrds:\ninstall: true\nkeep: true\nannotations: {}\nsingleNamespace: true\nworkflow:\nserviceAccount:\ncreate: true\nrbac:\ncreate: true\ncontroller:\n# 3. Argo Private repository settings - Only update if you are using a private image repository for Argo\nimage:\n# update the private image repository details\nregistry: quay.io\nrepository: argoproj/workflow-controller\ntag: \"\"\nparallelism: 10\nresourceRateLimit:\nlimit: 10\nburst: 5\nrbac:\ncreate: true\nsecretWhitelist: []\naccessAllSecrets: false\nwriteConfigMaps: false\nconfigMap:\ncreate: true\nname: \"\"\nnamespaceParallelism: 10\nworkflowDefaults:\n# 4. Kubernetes Pod Annotation settings - Only update if you want to customize the Pod metadata.\n## For example, the annotation might be used by external systems such as proxies, or monitoring tools, and more.\nspec:\npodMetadata:\nannotations:\nargo.workflow/agent-type: \"atlan-agent-service\"\nlabels:\napp.kubernetes.io/name: \"atlan-agent\"\npodGC:\nstrategy: OnPodSuccess\nserviceAccountName: argo-workflow\nautomountServiceAccountToken: true\nttlStrategy:\nsecondsAfterCompletion: 84600\ntemplateDefaults:\ncontainer:\nsecurityContext:\nallowPrivilegeEscalation: false\nresources: {}\nenv:\n- name: CA_CERT\nvalueFrom:\nconfigMapKeyRef:\nname: cert-config\nkey: ca.crt\noptional: true\n- name: REST_API_HEADERS\nvalueFrom:\nconfigMapKeyRef:\nname: agent-registry-settings\nkey: restAPIHeaders\noptional: true\nserviceAccount:\ncreate: true\nname: workflow-controller\nworkflowNamespaces:\n- default\nreplicas: 1\nrevisionHistoryLimit: 10\nnodeEvents:\nenabled: false\nserver:\nenabled: true\n# 5. Argo Private repository settings - Only update if you are using a private image repository for Argo\nimage:\nregistry: quay.io\nrepository: argoproj/argocli\ntag: \"\"\nrbac:\ncreate: true\nserviceAccount:\ncreate: true\nreplicas: 1\nautoscaling:\nenabled: false\ningress:\nenabled: false\nannotations:\ningress.kubernetes.io/ssl-redirect: \"false\"\nresources: {}\nexecutor:\nsecurityContext: {}\nresources: {}\nartifactRepository:\narchiveLogs: true\nuseStaticCredentials: false\n# 6. S3 bucket settings - needed by the secure agent to store logs and artifacts\ns3:\n# S3 bucket name - Update with the bucket name you created in the Permissions required section.\nbucket: \"atlan--bucket\"\n# S3 endpoint\nendpoint: \"s3.us-east-2.amazonaws.com\"\n# AWS region - Update with the region where you created bucket in the Permissions required section.\nregion: \"us-east-2\"\n# Artifact path format\nkeyFormat: \"argo-artifacts/{{workflow.namespace}}/{{workflow.name}}/{{pod.name}}\"\n# Whether to use insecure connections\ninsecure: false\n# Use AWS SDK credentials (IAM role)\nuseSDKCreds: true\nIn the configuration file, follow the comments to replace the necessary attributes. You may want to update the below configurations if:\nYou are using a private image registry (Image registry settings)\nYou want the agent to use an S3 bucket (Atlan connection settings)\nYou are using a private repository for Argo workflows (Argo Private repository settings)\nYou want to customize the Kubernetes pod's metadata (Kubernetes Pod Annotation settings)\nYou need specific S3 storage configuration (S3 storage settings)\nInstall using Helm chart\nâ\nFollow these steps to install the Secure Agent and its dependencies into your AWS EKS cluster using Helm charts.\nInstall the Argo Custom Resource Definitions (CRDs) required by the Secure Agent. This step installs only the CRDs. The Secure Agent is installed in the subsequent step using a Helm upgrade.\nhelm install <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent \\\n--version 0.1.0 \\\n-n <namespace> \\\n--create-namespace -f <path/to/agent_config_values.yaml> \\\n--set agent.name=\"<secure-agent-name>\" \\\n--set agent.atlan.domain=\"<atlan-tenant-domain>\" \\\n--set agent.atlan.token=\"<atlan-api-token>\" \\\n--set argo-workflows.controller.workflowNamespaces={<namespace>} \\\n--set IsUpgrade=false\nReplace the placeholders:\n<namespace>\n: The Kubernetes namespace where you want to deploy the Secure Agent.\n<path/to/agent_config_values.yaml>\n: The path to the YAML config file.\n<secure-agent-name>\n: Unique name, like agent-us-east-cdw.\n<helm-app-name>\n: Unique Helm release name, like atlan-agent-v1.\n<atlan-tenant-domain>\n: Your Atlan domain, e.g., mycompany.atlan.com.\n<atlan-api-token>\n: Token used for authentication. See\nCreate a bearer token\n.\nUse the following kubectl command to associate the IAM role with the service account. This enables the Secure Agent to access the S3 bucket securely using IAM Roles for Service Accounts (IRSA). Make sure the IAM roleâs trust policy enables the argo-workflow service account to assume the role.\nkubectl annotate serviceaccount argo-workflow \\\n-n  \\\neks.amazonaws.com/role-arn=arn:aws:iam:::role/\nReplace the placeholders:\n<namespace\n: The Kubernetes namespace where you want to deploy the Secure Agent.\n<AWS_ACCOUNT_ID>\n: Your AWS Account ID.\n<YourAgentIAMRoleName>\n: The IAM role name you created for the Secure Agent using IRSA.\nInstall the Secure Agent by upgrading the Helm release. This step performs the actual Secure Agent installation after CRDs are in place.\nhelm upgrade <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent \\\n--version 0.1.0 \\\n-n <namespace> \\\n--create-namespace -f <path/to/agent_config_values.yaml> \\\n--set agent.name=\"<secure-agent-name>\" \\\n--set agent.atlan.domain=\"<atlan-tenant-domain>\" \\\n--set agent.atlan.token=\"<atlan-api-token>\" \\\n--set argo-workflows.controller.workflowNamespaces={<namespace>} \\\n--set IsUpgrade=true\nReplace the placeholders:\n<namespace>\n: The Kubernetes namespace where you want to deploy the Secure Agent.\n<path/to/agent_config_values.yaml>\n: The path to the YAML config file.\n<secure-agent-name>\n: Unique name, like agent-us-east-cdw.\n<helm-app-name>\n: Unique Helm release name, like atlan-agent-v1.\n<atlan-tenant-domain>\n: Your Atlan domain, e.g., mycompany.atlan.com.\n<atlan-api-token>\n: Token used for authentication. See\nCreate a bearer token\n.\nWhile the installation is in progress, you can run the following command to verify the progress:\nkubectl get pods -n <namespace>\nReplace\n<namespace>\nwith the Kubernetes namespace used for deployment.\nVerify installation\nâ\nTo confirm successful installation:\nSign in to your Atlan tenant as an admin. For example,\nhttps://<tenant>.atlan.com\n.\nNavigate to the\nAgent\ntab.\nSearch for your Secure Agent name.\nIf the agent appears in the list and is marked\nActive\n, installation is complete.\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nInstall on Virtual Machine (K3s)\nNext\nConfigure workflow execution\nSystem requirements\nPermissions required\nPrerequisites\nInstall using Helm chart\nVerify installation"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/configure-secure-agent-for-workflow-execution",
    "content": "Connect data\nSecure Agent\nManage Agent\nConfigure workflow execution\nOn this page\nConfigure workflow execution\nWhen using Secure Agent for extraction, source system credentials (secrets) required for workflow execution are stored in a Secret Manager. This guide provides steps to set up workflows with Secure Agent and specify the secret details it uses during workflow execution.\nBefore you begin\nâ\nBefore configuring Secure Agent for workflow execution, ensure you have:\nA registered and active Secure Agent.\nAccess to one of the supported secret stores: AWS Secrets Manager, Azure Key Vault, GCP Secret Manager, environment variable-based secret injection technique, or a custom secret store.\nConfigure secrets retrieval for workflow execution\nâ\nFollow these steps to configure Secure Agent to retrieve secrets from a secret store required for the workflow execution. This is necessary for secure data access while running your workflows.\nðª\nDid you know?\nFor each field, you can enter either the name of a secret stored in your secret manager or the actual value. Use secret names when using a secret store with Secure Agent, or enter values directly if no secret is required.\nAWS\nAzure\nGCP\nEnvironment variables\nCustom store\nSecure Agent retrieves the required secrets from AWS Secrets Manager during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section:\nSecret path in Secret Manager:\nProvide the Amazon Resource Name (ARN) or the path of the secret that contains the sensitive configuration details required for the connector. These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution.\nAWS region:\nSelect the region where your AWS Secrets Manager is located.\nAWS authentication method:\nSelect how you want the Secure Agent to authenticate when executing the workflow. Choose one:\nIAM (Recommended)\n: Use this method if the secure agent was configured to use the AWS IAM permissions to access secrets.\nIAM Assume Role\n: Use this method if the agent was configured to access secrets via cross-account roles.\nAWS Assume Role ARN\n: Provide the IAM Role ARN that grants the Secure Agent permission to retrieve secrets.\nAccess Key & Secret Key\n: Use this method if the agent was configured to use the AWS Access Key ID and Secret Access Key via environment variables or Kubernetes secrets.\nSecure Agent retrieves secrets from Azure Key Vault during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section:\nSecret path in Secret Manager:\nProvide the URL of the Azure Key Vault secret that contains the sensitive configuration details required for the connector. These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution.\nAzure authentication method:\nSelect how you want the Secure Agent to authenticate when accessing the Azure Key Vault secret. Choose one:\nManaged Identity (Recommended)\n: Use this method if the agent was configured to use an Azure-managed identity assigned to the agent environment for authentication.\nService Principal Authentication\n: Use this method if the agent was configured to authenticate via a Service Principal using Tenant ID, Client ID, and Client Secret.\nAzure Key Vault Name:\nProvide the name of your Azure Key Vault that stores your secrets.\nSecure Agent retrieves secrets from GCP Secret Manager during workflow execution. The secret is uniquely identified by its name in GCP Secret Manager, without requiring additional attributes.\nSecure Agent retrieves secrets from environment variables during workflow execution.\nSecure Agent retrieves secrets from Custom Secret Store during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section:\nAgent Custom configuration:\nSecure agent needs information for connecting to the custom secret store. Add the configuration details in JSON format to specify the connection settings and the secrets to retrieve during workflow execution. For example, the JSON configuration to initiate a sample custom store may look like below:\n{\n\"store_url\"\n:\n\"https://custom-secret-store.example.com\"\n,\n\"secret_name\"\n:\n\"my-custom-secret\"\n}\nNext steps\nâ\nAfter configuring the Secure Agent, return to your connectorâs setup guide and continue the workflow setup.\nTags:\nintegration\nconnectors\nworkflow\nautomation\norchestration\nPrevious\nInstall on AWS EKS\nNext\nDeployment architecture\nBefore you begin\nConfigure secrets retrieval for workflow execution\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/references/deployment-architecture",
    "content": "Connect data\nSecure Agent\nReferences\nDeployment architecture\nOn this page\nDeployment architecture\nThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.\nHigh-level architecture\nâ\nThis section describes how the Secure Agent is structured and deployed. It explains the core components that enable metadata extraction, job execution, and communication with Atlan.\nFigure 1:\nAtlan Secure Agent deployment architecture.\nCore components\nâ\nThe Secure Agent runs as a Kubernetes-based application within a customer's private cloud or on-premises environment. It consists of several key components that work together to execute metadata extraction tasks.\nArgo Workflows\nâ\nAn Argo Workflow server is deployed to coordinate all activities and launch Kubernetes workloads.\nThe Secure Agent uses Argo Workflows to orchestrate and manage metadata extraction jobs.\nEach workflow represents a unit of work, such as extracting metadata from a source system.\nAgent orchestrator\nâ\nA scheduled job that runs every five minutes to check for jobs that need to be executed.\nIt connects to the Atlan tenant, retrieves job details, and initiates workflows accordingly.\nAuxiliary services\nâ\nAdditional services that support agent operations:\nHealth monitoring service\nsends periodic heartbeats to Atlan to confirm the agent is active.\nLogging service\nuploads execution logs to Atlan for monitoring and debugging.\nMetadata extraction workflows\nâ\nConnector-specific jobs that extract metadata from source systems.\nWorkflows run in isolated containers, ensuring security and reliability.\nData flow\nâ\nThe Secure Agent supports two modes of metadata transfer. Each mode determines how extracted metadata is delivered to Atlan.\nBucket relay\nâ\nMetadata extraction in bucket relay mode stores metadata in enterprise-managed cloud storage before Atlan retrieves it.\nFigure 2:\nData flow in bucket relay mode.\nThe Secure Agent extracts metadata and writes it to a storage bucket in the customerâs cloud environment (such as AWS S3, Azure Blob Storage, or Google Cloud Storage). This is managed by providing the agent write access to cloud storage.\nAtlan retrieves metadata from the storage bucket and processes it further. This is managed by providing Atlan read access to list and read files in cloud storage.\nThis mode ensures the extracted data remains within the customerâs infrastructure until Atlan explicitly fetches it. Customers can also use this data for auditing.\nDirect ingestion\nâ\nIn direct ingestion mode, metadata is transferred directly from the Secure Agent to Atlan.\nFigure 3:\nData flow in direct ingestion mode.\nThe Secure Agent uses pre-signed URLs to upload metadata directly to Atlan. Some cloud storage providers that use pre-signed URLs include:\nAWS reference\nAzure reference\nGCP reference\nA pre-signed URL grants temporary access to upload files without exposing long-term credentials.\nEach URL has an expiration time, ensuring access is only available for a limited duration.\nSee also\nâ\nSecure Agent - Security\n: Details on security mechanisms.\nTags:\nintegration\nconnectors\nsecurity\naccess-control\npermissions\nPrevious\nConfigure workflow execution\nNext\nSecurity\nHigh-level architecture\nCore components\nData flow\nSee also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/references/security",
    "content": "Connect data\nSecure Agent\nReferences\nSecurity\nOn this page\nSecurity\nThe Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring.\nAuthentication and authorization\nâ\nThe Secure Agent implements security measures for authentication, encryption, and access control. This section details authentication mechanisms, including API key management and secret handling.\nAPI key management\nâ\nThe Secure Agent uses API keys for authentication when communicating with Atlan. These keys verify the agentâs identity and define its access scope.\nAuthentication:\nAPI keys authenticate the Secure Agent, allowing it to interact securely with Atlan. Each key is associated with a specific tenant and grants access based on permissions.\nStorage:\nAPI keys are stored in enterprise-managed vaults, such as AWS Secrets Manager, Azure Key Vault, or Kubernetes Secrets. The Secure Agent retrieves the key dynamically during operation, eliminating manual configuration.\nExpiration:\nAPI keys can have an expiration period, such as 90-180 days, or be configured based on internal security policies.\nRotation:\nWhen an API key nears expiration, a new key can be generated and stored in the secret vault. The Secure Agent automatically fetches the latest key from the vault.\nRevocation:\nIf an API key is compromised, it can be revoked. Once revoked, the Secure Agent retrieves a newly assigned key from the vault without requiring manual intervention.\nSecret management\nâ\nThe Secure Agent retrieves credentials securely without storing them locally.\nEnterprise-managed vaults:\nThe Secure Agent integrates with AWS Secrets Manager, Azure Key Vault, and other vaults to securely store credentials, keeping them within the organizationâs security perimeter.\nJust-in-time access:\nCredentials, such as database secrets, are retrieved dynamically from enterprise vaults when needed and are never stored locally.\nNo credential transmission:\nSecrets are never transmitted to or stored on Atlan, ensuring complete isolation of sensitive information.\nData security and encryption\nâ\nThe Secure Agent protects metadata using encryption and strict access controls.\nCompliance with security standards:\nThe Secure Agent aligns with ISO 27001 and SOC 2 security standards, ensuring strong encryption, data protection, and access control measures.\nData in transit:\nAll communication between the Secure Agent and Atlan is encrypted using TLS 1.2 over HTTPS. For network-level protections, see\nNetwork security\n.\nData at rest:\nMetadata stored in customer-managed storage or Atlanâs tenant bucket is encrypted using AES-256.\nData minimization:\nOnly essential metadata is extracted and transmitted. Customers can configure data filters to exclude specific metadata fields from processing.\nRetention control:\nAtlan doesn't require metadata post-ingestion, and customers can delete metadata from their storage buckets based on internal security policies.\nContainer security\nâ\nThe Secure Agent implements security measures to protect container images, ensuring their integrity and mitigating security risks.\nContainer image hosting:\nSecure Agent container images are hosted on public repositories, such as Docker Hub and Amazon ECR. Organizations can deploy the Secure Agent from a private container registry to meet their compliance and security requirements.\nVulnerability scanning:\nTrivy scans container images for known vulnerabilities, outdated dependencies, misconfigurations, and exposed secrets. Scans are conducted weekly and whenever new changes are checked in.\nImage signing and verification:\nCosign signs container images to ensure authenticity. Image verification includes:\nValidating the image signature against Sigstore's transparency log.\nVerifying the signerâs identity through GitHub workflows.\nConfirming the certificate issued by GitHubâs OpenID Connect (OIDC) provider.\nLicense compliance:\nTrivy scans for software license compliance to ensure proper licensing for all components within the container images.\nNetwork security\nâ\nThe Secure Agent operates within a controlled network environment to facilitate secure metadata extraction and communication with Atlan.\nSSL certificates\nâ\nThe Secure Agent encrypts communications with Atlan, source systems, proxy servers, and secret managers.\nEncryption in transit:\nAll data communication between the Secure Agent and Atlan is encrypted using TLS 1.2 over HTTPS.\nCertificate management:\nIf trusted or well-known certificate authorities are used, no additional configuration is needed. The Default Trusted Certificate Authorities store contains certificates from the most common and trusted CAs, which the Secure Agent uses to secure connections.\nIf internal or private certificate authorities are used, the Secure Agent trusts these custom certificate authorities through the infrastructureâs default certificate store.\nWhitelisting\nâ\nConfiguring network access ensures only trusted communication between the Secure Agent and Atlan.\nDomain whitelisting:\nThe Secure Agent requires outbound access to Atlan through the domain\ntenant.atlan.com\n. Domain-based whitelisting simplifies network configurations while maintaining security.\nDNS resolution:\nThe Secure Agent relies on standard DNS resolution to reach Atlan domains. Network configurations must allow name resolution for\ntenant.atlan.com\n.\nIP-based whitelisting:\nIf domain-based whitelisting isnât feasible and specific IP ranges must be allowed, refer to the\nlist of required IP ranges\nto be whitelisted. If you need further assistance, contact\nAtlan Support\n.\nLogging and monitoring\nâ\nThe Secure Agent captures logs for workflow execution, system orchestration, and Kubernetes operations while also providing monitoring capabilities.\nTypes of logs\nâ\nWorkflow logs:\nCapture job execution details, including start and completion status, connections to source systems and secret managers, metadata extraction results, and authentication status. These logs are sent to Atlan and accessible from the workflow status page.\nOrchestration logs:\nTrack the Secure Agentâs scheduled operations, including connection attempts to Atlan, retrieval of workflow requests, and workflow submission to the Argo engine. Logs also include error messages and performance metrics.\nArgo logs:\nProvide visibility into workflow execution, including job scheduling, resource allocation, state transitions, and error handling.\nKubernetes logs:\nCapture system-level events, such as pod lifecycle changes, container startup and shutdown, resource allocation, network connectivity, and health checks.\nMonitoring\nâ\nHealth checks:\nSecure Agent components run periodic health checks to verify connectivity, resource availability, and system integrity.\nResource utilization:\nCPU, memory, and storage usage are monitored to track system load and detect potential bottlenecks.\nLogs can be viewed in Atlan or integrated with external monitoring systems.\nTags:\ndata\napi\nauthentication\nPrevious\nDeployment architecture\nAuthentication and authorization\nData security and encryption\nContainer security\nNetwork security\nLogging and monitoring"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/set-up-playbooks",
    "content": "Configure Atlan\nPlaybooks\nGet Started\nSet up playbooks\nOn this page\nSet up playbooks\nwarning\nð¤ Who can do this?\nYou will need to be an admin user in Atlan to create playbooks.\nA common question that data teams often face is how to automate metadata at scale.\nHaving started out as a data team ourselves, we know that automating repetitive tasks can help data teams maximize the value they provide to their organization. One way of doing so is through Atlan's playbooks!\nPlaybooks help power metadata automation for your data assets in Atlan. You can create rule-based automations at scale and update metadata in bulk, helping you streamline your workflows.\nYou can update the following asset metadata using playbooks:\nCertificates\nDescriptions\nOwners\nTerms\nTags\nDomains\nCustom metadata\nFor example, imagine your organization needs to transfer ownership of several data assets. Instead of your data team manually updating the ownership of each and every asset, you can create a playbook to automate this process and update the metadata of your assets at scale.\nPlaybook recommendations\nâ\nBefore you begin, review some general guidelines on running playbooks in Atlan:\nAvoid running multiple playbooks simultaneously on the same set of assets. Allow one playbook run to be completed before proceeding with another operation on the same set of assets. Otherwise, you may experience performance issues and inconsistencies.\nReview and understand the depth of your asset lineage or hierarchy prior to enabling a\ntag propagation\nplaybook. For assets with complex lineage,\ntag propagation may take longer\nto complete than the playbook runtime. You may want to review and judiciously select a list of assets that need to be tagged directly. For their child and/or downstream assets, Atlan recommends that you\nenable tag propagation\n.\nCreate a playbook\nâ\nTo create a playbook in Atlan:\nFrom the left menu in Atlan, you can either:\nClick\nAssets\nto navigate to the assets page.\nFrom the\nFilters\nmenu on the left or the tabs along the top,\napply any asset filters\n.\nNext to the search bar, click the 3-dot icon and then click\nCreate playbook\nto create a playbook for the filtered assets   -  this option is only visible to admin users.\nClick\nGovernance\nto navigate to the governance center.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPlaybooks\n.\nClick\nCreate New\nto get started.\nIn the\nCreate new playbook\ndialog box, enter the following details:\nFor\nName\n, enter a name for the task to be accomplished   -  for example,\nUpdate ownership\n. (Atlan recommends that the length of a playbook name must be no longer than 46 characters.)\n(Optional) For\nDescription\n, enter a description.\n(Optional) Select an icon for your playbook.\nClick\nCreate\nto save your playbook.\nSet up rules as filters\nâ\nTo set up rules as filters for your playbook:\nIn the\nBuild Rules\npage of your playbook, click\nFilters\n.\nFor name, add a name to your filter.\nTo set a matching condition for the filters, select\nMatch all\nor\nMatch any\n.\nMatch all\nwill logically\nAND\nthe criteria, while\nMatch any\nwill logically\nOR\nthe criteria.\nFor\nAttributes\n, select a relevant option:\nFor this example, we'll click\nConnection\nand then select a Snowflake connection. (Optional) To further refine your asset selection:\nClick\nAll databases\nto filter by databases in a selected connection.\nClick\nAll schemas\nto filter by schemas in a selected connection.\nClick\nConnector\nto filter assets by\nsupported connectors\n.\nClick\nAsset type\nto filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more.\nClick\nCertificate\nto filter assets by\ncertification status\n.\nClick\nOwners\nto filter assets by\nasset owners\n.\nClick\nTags\nto filter assets by your\ntags\nin Atlan, including imported\nSnowflake\nand\ndbt\ntags.\n(Optional) For\nSnowflake tags\nonly, to the left of the checkbox, click\nSelect value\n, and then from the\nSelect tag value\ndialog, select any value(s) to filter assets by tag value.\nClick\nGlossary, terms, & categories\nto filter by a specific\nglossary\nor\ncategory\nto bulk update all the nested terms or by multiple glossaries and categories.\nClick\nLinked terms\nto filter assets by\nlinked terms\n.\nClick\nDomains\nto filter by specific\ndomains or subdomains\nto bulk update all the assets included in those data domains or subdomains.\nClick\nProducts\nto filter for\ndata products\nby specific data domains or subdomains.\nClick\nSchema qualified Name\nto filter assets by the qualified name of a given schema.\nClick\nDatabase qualified Name\nto filter assets by the qualified name of a given database.\nClick\ndbt\nto filter assets by dbt-specific filters and then select a\ndbt Cloud\nor\ndbt Core\nfilter.\nClick\nProperties\nto filter assets by\ncommon asset properties\n.\nClick\nUsage\nto filter assets by\nusage metrics\n.\nClick\nMonte Carlo\nto filter assets by\nMonte Carlo-specific filters\n.\nClick\nSoda\nto filter assets by\nSoda-specific filters\n.\nClick\nTable/View\nto filter tables or views by row count, column count, or size.\nClick\nColumn\nto filter columns by\ncolumn-specific filters\n, including parent asset type or name, data type, or\ncolumn keys\n.\nClick\nProcess\nto filter\nlineage processes\nby the SQL query.\nClick\nQuery\nto filter assets by associated\nvisual queries\n.\nClick\nMeasure\nto filter\nMicrosoft Power BI measures\nusing the external measures filter.\nFor\nOperator\n, select\nIs one of\nfor values to include or\nIs not\nfor values to exclude. Depending on the selected attribute(s), you can also choose from\nadditional operators\n:\nSelect\nEquals (=)\nor\nNot Equals (!=)\nto include or exclude assets through exact match search.\nSelect\nStarts With\nor\nEnds With\nto filter assets using the starting or ending sequence of values.\nSelect\nContains\nor\nDoes not contain\nto find assets with or without specified values contained within the attribute.\nSelect\nPattern\nto filter assets using supported\nElastic DSL regular expressions\n.\nSelect\nIs empty\nto filter assets with null values.\nSelect\nBelongs to\nor\nDoesn't belong to\nto filter\ndata products\nby specific\ndata domains or subdomains\n.\nFor\nValues\n, select the relevant values. The values will vary depending on the selected attributes.\n(Optional) To add more filters, click\nAdd filter\nand select\nFilter\nto add individual filters or\nFilter\nGroup\nto nest more filters in a group.\n(Optional) To view all the assets that match your rules, in the\nFilters\ncard, click\nView\nall\nfor a preview.\n(Optional) To remove a playbook filter, to the right of any filter, click the three horizontal dots and then click\nDelete\n.\n(Optional) To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click\nDisable\n. Click\nEnable\nto turn on any disabled filters.\nSelect the actions\nâ\nTo select the actions to be performed based on your rules:\nIn the\nBuild Rules\npage of your playbook, click\nActions\n.\nFor\nSelect Action\n, select the relevant metadata option to update:\nClick\nCertificate\nto update the\ncertification status\nof assets to\nVerified\n,\nDraft\n,\nDeprecated\n, or\nNo certificate\n.\nClick\nDescription\nto update the\ndescription\nof your assets.\nClick\nOwners\nto add, remove, or replace\nasset owners\n. In this example, we'll update the ownership of the assets.\nClick\nTerms\nto add\nterms\nto your assets or remove or replace them from\nlinked assets\n.\nClick\nTags\nto add\ntags\nto your assets or remove or replace them from\ntagged\nor\npropagated\nassets. Note that if there are multiple tag actions to be performed, Atlan will execute them in the following order:\nADD\n,\nREMOVE\n, and then\nREPLACE\n.\nClick\nDomain\nto\nadd your assets to a specific domain or subdomain\nor remove them from an existing linked\ndomain or subdomain\n.\nClick any\ncustom metadata structure\nand then select a\ncustom metadata property\nto update or unlink it from your assets.\nFor\nSelect operator\n, select the relevant option. The operators will vary depending on the selected action.\nFor\nValues\n, select the relevant option(s). The values will vary depending on the selected actions.\n(Optional) To add more actions, click\nAdd\nAction\n.\nDid you know?\nYou can control tag propagation when adding tags as an action in playbooks.\nTag propagation\nis disabled by default. If you enable tag propagation, you will also be able to\nconfigure how tags are propagated\n.\nRun the playbook\nâ\nIf you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it.\nTo run the playbook:\nYou can either:\nTo run the playbook once immediately, click\nRun once\n.\nTo schedule the playbook to run hourly, daily, weekly, or monthly, click\nSchedule\nand choose the preferred frequency, timezone, and time.\ndanger\nIf you're scheduling multiple playbooks, Atlan recommends spacing out the schedules as much as possible to minimize any overlap between the playbook workflow runs. For more about workflows in general, see\nworkflow recommendations\n.\nClick\nComplete\nto run the playbook.\nIn the resulting screen, click\nGo to profile\nto view your playbook profile.\nOnce your playbook has completed its run, you will see the metadata updated for your assets! ð\nDid you know?\nIf you have any questions about setting up playbooks, head over\nhere\n.\nTags:\natlan\ndocumentation\nPrevious\nPlaybooks\nNext\nManage playbooks\nPlaybook recommendations\nCreate a playbook\nSet up rules as filters\nSelect the actions\nRun the playbook"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/manage-playbooks",
    "content": "Configure Atlan\nPlaybooks\nManagement\nManage playbooks\nOn this page\nManage playbooks\nOnce you've\ncreated a playbook\n, you can monitor, modify, or delete it at any time. You can also\nenable notifications\nto monitor your playbook runs directly in Slack or Microsoft Teams.\nMonitor a playbook\nâ\nTo monitor your playbooks runs:\nWhen running a playbook immediately, you will be redirected to the monitoring page within 5 seconds.\nAt any other moment:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPlaybooks\n.\nIn the playbooks manager, select\nthe playbook\nyou'd like to view.\nIn the\nOverview\nsection of your playbook, you'll be able to monitor:\nA summary of the rules, actions, and updated assets.\nAn activity log for recent runs and updates over time.\nDid you know?\nThe activity log in the playbooks manager can help you keep track of playbook runs, ranging from 24 hours to 30 days. Select any of the entries to navigate to the corresponding playbook.\nModify a playbook\nâ\nTo modify an existing playbook:\nFrom the left menu of any screen in Atlan, click\nGovernance.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPlaybooks\n.\nIn the playbooks manager, click the playbook you'd like to modify.\nOn your playbook page:\nTo edit the name and description of your playbook, hover over your playbook and click\nEdit\n.\nTo modify the rules of your playbook, click\nRules\nto make your changes and then click\nUpdate\nto save them.\n(Optional) To add a new rule to an existing playbook, in the left menu for playbook rules, click\n+ Add new Rule\n.Â\nTo turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click\nDisable\n. Click\nEnable\nto turn on any disabled filters.\nTo modify the schedule for your playbook, in the upper right of your screen:\nClick\nRun Now\nto run it immediately.\nClick the\npencil icon\nto modify or remove the schedule.\nDelete a playbook\nâ\nTo delete an existing playbook:\nFrom the left menu of any screen in Atlan, click\nGovernance.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPlaybooks\n.\nIn the playbooks manager, hover over the playbook you'd like to delete and click\nDelete Playbook\n.\nClick\nDelete\nto confirm.\nEnable playbook notifications\nâ\nYou can set up Slack or Microsoft Teams alerts for your playbook runs in Atlan. This can help you monitor your playbooks directly in Slack or Microsoft Teams. You can also choose to receive alerts for failed playbook runs only.\nBefore you can enable notifications for playbooks, you will need to either:\nIntegrate Slack and Atlan\nIntegrate Microsoft Teams and Atlan\nTo enable notifications for playbook runs:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPlaybooks\n.\nIn the upper-right of the playbooks manager, under\nActivity\n, click the bell icon.\nIn the\nEnable notifications\npopup:\nClick\nSetup now\nto integrate\nSlack\nor\nMicrosoft Teams\n.\nIf you have already integrated Slack or Microsoft Teams, click\nEnable\n.\nIn the notifications setup dialog, configure the following:\nFor\nNotifications channel\n, you can either:\nIf you have already configured a\nSlack\nor\nMicrosoft Teams\nchannel to receive workflow alerts, that channel will be preselected. You can use the same channel to receive both workflow and playbook run alerts and skip to the next step.\nIf you have not configured a workflow alerts channel or want to add a different one, enter the channel name to receive notifications for playbook runs. This channel will be displayed as the\nPlaybooks alert channel\nÂ in yourÂ\nSlack\nor\nMicrosoft Teams\nintegration.\nTo select the type of notifications you want to receive, you can either:\nClick\nBoth success and failure alerts\nto receive notifications for both successful and failed playbook runs.\nClick\nFailure alerts only\nto limit notifications to failed playbook runs only.\nClick\nSave\nto save your notification preferences.\n(Optional) To disable notifications, from the notifications setup dialog, remove the playbook alerts channel configured for\nSlack\nor\nMicrosoft Teams\n.\nYou will now receive\nSlack\nor\nMicrosoft Teams\nnotifications for all your playbook runs in Atlan! ð\nThe Atlan bot will share playbook run alerts, including details like run status, start time, run time, trigger type, last three runs, and more.\nTags:\natlan\ndocumentation\nPrevious\nSet up playbooks\nNext\nAutomate data profiling\nMonitor a playbook\nModify a playbook\nDelete a playbook\nEnable playbook notifications"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/troubleshooting/troubleshooting-playbooks",
    "content": "Configure Atlan\nPlaybooks\nTroubleshooting\nTroubleshooting playbooks\nOn this page\nTroubleshooting playbooks\nwarning\nð¤ Who can do this?\nYou will need to be an admin user in Atlan to create playbooks.\nHere are a few things to know about\nsetting up playbooks\n:\nWhat are the known limitations of the domain action?\nâ\nFollowing are the known issues or limitations when using the\ndomain action\n:\nAtlan currently does not support adding glossaries, categories, and terms to domains.\nIf you do not have read permission on the assets you want to add to a domain, those assets will be removed from the playbook workflow during processing.Â\nIf you do not have update permission on the assets you want to add to a domain, the playbook workflow will fail. However, some assets may still be linked to the domain before the failure occurs.\nWhat type of infrastructure costs can I expect to incur?\nâ\nAtlan uses Elasticsearch to run playbooks, so expect infrastructure costs to be minimal and not a determining factor for utilizing playbooks.\nWhat is the maximum number of playbooks that can be run?\nâ\nWe recommend building no more than a maximum of 20 rules per playbook. However, the total number of playbooks that can be run is still to be determined. From a technical standpoint, playbooks leverage the workflow infrastructure, which means there are no hard limits. Depending on the number of playbooks that need to be run, the infrastructure will have to be scaled accordingly.Â\nDo I also need to have update permissions for playbooks?\nâ\nYes. You need to have the permission to update assets in Atlan in order to run playbooks for updating them.Â If you do not have the permission to update an asset, you will be unable to update it using playbooks.\nAdditionally, Atlan uses the permissions of the playbook creator in determining the assets to be updated and not that of the user who runs the playbook. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access.\nCan I automate requests for updates through playbooks?\nâ\nNo, Atlan currently does not support automating asset update\nrequests\nthrough playbooks.\nIs there a way to undo updates made through playbooks?\nâ\nCurrently, there is no button to undo asset updates. However, you can\nmodify your existing playbooks\n. You can either turn off the filters or add new rules to reverse the updates.\nIs there a way to view or download a report of updated assets from previous playbook runs?\nâ\nCurrently, no. You can\nmonitor your existing playbooks\nto view a high-level summary of asset updates from previous playbook runs. Observability of results is on the roadmap.\nCan I get email notifications for playbook run successes or failures?\nâ\nCurrently, no. However, you can\nset up Slack or Microsoft Teams alerts\nfor your playbook runs in Atlan.\nHow to handle an\noffload node status is not supported\nerror?\nâ\nIf you encounter an\noffload node status is not supported\nerror message, the playbook workflow may have exceededÂ the EtcD size limit. Playbooks use Argo workflow templates, which are stored as Kubernetes resources. This creates a limit to their size.\nTo handle this error, Atlan recommends the following:\nReduce the number of rules in your playbook\nOptimize filters for asset selection\nTags:\natlan\ndocumentation\nPrevious\nAutomate data profiling"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/automate-data-profiling",
    "content": "Configure Atlan\nPlaybooks\nManagement\nAutomate data profiling\nOn this page\nAutomate data profiling\nâ\nAvailable via the Data Quality Studio package\nwarning\nð¤ Who can do this?\nYou need to be an\nadmin user\nin Atlan to create profiling playbooks.\nMonitoring and improving data quality is critical to building trust in your data assets. Atlan solves for this with profiling playbooks!\nProfiling playbooks help power data observability for your assets in Atlan. You can create profiling playbooks to scan your assets at scale, identify any issues or inconsistencies, and improve the data quality of your assets.\nSupported sources\nâ\nAtlan currently supports column profiling for the following connectors:\nAmazon Athena\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nSnowflake\nTrino\nCreate a profiling playbook\nâ\nTo create a profiling playbook:\nIn the left menu in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPlaybooks\n.\nTo the right of the\nCreate New\nbutton, click the downward arrow and then select\nProfiling Playbook\n.\nIn the\nCreate new profiling playbook\ndialog, enter the following details:\nFor\nName\n, enter a name for the task to be accomplished   -  for example,\nTables scan\n. (Atlan recommends that the length of a playbook name must be no longer than 46 characters.)\nFor\nConnection\n, select a\nsupported connection\nfrom the dropdown menu   -  in this example, we'll select a Google BigQuery connection\ndevelopment\n.\n(Optional) For\nDescription\n, enter a description for your playbook.\n(Optional) Select an icon for your playbook.\nClick\nCreate\nto save your playbook.\nSet up rules as filters\nâ\nDid you know?\nThe assets to be scanned are pre-filled based on your selected connection.\nTo set up rules as filters for your profiling playbook:\nIn the\nBuild Rules\npage of your profiling playbook, click\nFilters\n.\nFor the name field, add a name to your filter   -  for example,\nProfiling action\n.\nTo set a matching condition for the filters, select\nMatch all\nor\nMatch any\n.\nMatch all\nwill logically\nAND\nthe criteria, while\nMatch any\nwill logically\nOR\nthe criteria.\nFor\nAttributes\n, select the relevant option. For this example, we'll select\nName\nlisted under\nProperties\n. (Optional) To further refine your asset selection:\nClick\nConnection\nto select a specific connection.Â\nClick\nAll databases\nto filter by databases in a selected connection.\nClick\nAll schemas\nto filter by schemas in a selected connection.\nClick\nConnector\nto filter assets by\nsupported connectors\n.\nClick\nAsset type\nto filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more.\nClick\nCertificate\nto filter assets by\ncertification status\n.\nClick\nOwners\nto filter assets by\nasset owners\n.\nClick\nTags\nto filter assets by your\ntags\nin Atlan, including imported\nSnowflake\nand\ndbt\ntags.\n(Optional) For\nSnowflake tags\nonly, to the left of the checkbox, click\nSelect value\n, and then from the\nSelect tag value\ndialog, select any value(s) to filter assets by tag value.\nClick\nGlossary, terms, & categories\nto filter by a specific\nglossary\nor\ncategory\nto bulk update all the nested terms or by multiple glossaries and categories.\nClick\nLinked terms\nto filter assets by\nlinked terms\n.\nClick\nSchema qualified Name\nto filter assets by the qualified name of a given schema.\nClick\nDatabase qualified Name\nto filter assets by the qualified name of a given database.\nClick\ndbt\nto filter assets by dbt-specific filters and then select a\ndbt Cloud\nor\ndbt Core\nfilter.\nClick\nProperties\nto filter assets by\ncommon asset properties\n.\nClick\nUsage\nto filter assets by\nusage metrics\n.\nClick\nMonte Carlo\nto filter assets by\nMonte Carlo-specific filters\n.\nClick\nSoda\nto filter assets by\nSoda-specific filters\n.\nClick\nTable/View\nto filter tables or views by row count, column count, or size.\nClick\nColumn\nto filter columns by\ncolumn-specific filters\n, including parent asset type or name, data type, or\ncolumn keys\n.\nClick\nProcess\nto filter\nlineage processes\nby the SQL query.\nClick\nQuery\nto filter assets by associated\nvisual queries\n.\nClick\nMeasure\nto filter\nMicrosoft Power BI measures\nusing the external measures filter.\nFor\nOperator\n, select\nIs one of\nfor values to include or\nIs not\nfor values to exclude. Depending on the selected attribute(s), you can also choose from\nadditional operators\n:\nSelect\nEquals (=)\nor\nNot Equals (!=)\nto include or exclude assets through exact match search.\nSelect\nStarts With\nor\nEnds With\nto filter assets using the starting or ending sequence of values.\nSelect\nContains\nor\nDoes not contain\nto find assets with or without specified values contained within the attribute.\nSelect\nPattern\nto filter assets using supported\nElastic DSL regular expressions\n.\nSelect\nIs empty\nto filter assets with null values.\nFor\nValues\n, select the relevant values. The values will vary depending on the selected attributes.\n(Optional) To add more filters, click\nAdd filter\nand select\nFilter\nto add individual filters or\nFilter\nGroup\nto nest more filters in a group.\n(Optional) To view all the assets that match your rules, in the\nFilters\ncard, click\nView\nall\nfor a preview.\nConfirm profiling actions\nâ\ndanger\nColumn profiling is currently only supported for number and text data types. The profiled column assets will be populated with preconfigured metrics.\nTo select the actions to be performed based on your rules:\nThe default profiling actions to be performed include:\nBase metrics\n:\nDistinct count   -  number of rows that contain distinct values, relative to the column.\nMissing count   -  number of rows that do not contain specific values.\nNumeric metrics\n:\nMinimum and maximum values   -  smallest and greatest values in a numeric column.\nAverage   -  calculated average of values in a numeric column.\nStandard deviation   -  calculated standard deviation of values in a numeric column.\nVariance   -  calculated variance of values in a numeric column.\nString metrics\n:\nAverage length   -  average length of string values in a column.\nMinimum and maximum length   -  minimum and maximum length of string values in a column.\nClick\nNext\nto proceed to the next step.\nIn the\nOptimize your Profiling query\npopup, the following message will appear:\nThis Profiling playbook will query\nx\nrows across\ny\nassets. To avoid significant computing costs, review your applied filters before proceeding\n. Click\nReview filters\nto review your existing filters or click\nContinue anyway\nto proceed.\nNote that Atlan is working to support sampling functionality in the future.\nRun the playbook\nâ\nIf you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it.\nTo run the playbook:\nYou can either:\nTo run the playbook once immediately, click\nRun once\n.\nTo schedule the playbook to run hourly, daily, weekly, or monthly, click\nSchedule\nand choose the preferred frequency, timezone, and time.\nClick\nComplete\nto confirm your selections.\nIn the resulting screen, click\nGo to profile\nto view your playbook profile.\nOnce your playbook run is completed, you will see the data profile updated for your assets! ð\nView profiled assets\nâ\nTo view the profiled assets for your playbook:\nIn the\nOverview\npage of your playbook, to the right of\nProfiling action\n, click the total count of profiled assets.\nIn the sidebar to the right, profiled assets will be indicated with a bar graph icon. Click any profiled asset to proceed to viewing profiling data.\nFrom the table sidebar, click the\nColumn\ntab to view column assets and then select any of the profiled columns.\nIn the column sidebar to the right, click\nProfile\nto view profiling data for the selected column asset.\nDid you know?\nOnce you've created a profiling playbook, you can\nmonitor, modify, or delete\nit at any time.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nManage playbooks\nNext\nTroubleshooting playbooks\nSupported sources\nCreate a profiling playbook\nSet up rules as filters\nConfirm profiling actions\nRun the playbook\nView profiled assets"
  },
  {
    "url": "https://docs.atlan.com/tags/playbooks",
    "content": "One doc tagged with \"playbooks\"\nView all tags\nPlaybooks\nCreate and manage automated workflows for metadata management and data governance."
  },
  {
    "url": "https://docs.atlan.com/tags/automation",
    "content": "25 docs tagged with \"automation\"\nView all tags\nAlways On\nIntegrate Atlan with Always On to enable continuous automation and suggestions.\nAutomate data governance\nYou can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nAutomatically assign roles based on group names\nLearn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app.\nAutomation Integrations\nIntegrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On.\nAWS Lambda\nIntegrate Atlan with AWS Lambda to automate workflows and triggers.\nBrowser Extension\nIntegrate Atlan with the Browser Extension to enhance your data catalog experience.\nConfigure workflow execution\nLearn about configure workflow execution.\nConnections Integration\nIntegrate Atlan with Connections to create webhooks and automate notifications.\nCreate governance workflows\n:::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/.\nCreate webhooks\nIf your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or [submit a request](/support/submit-request).\nDelegate administration\nThe workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows.\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance.\nGenerate lineage between assets App\nLearn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app.\nHow often does Atlan crawl Snowflake?\nLearn about how often does atlan crawl snowflake?.\nManage connectivity\nOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.\nManage governance workflows\n:::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/.\nManage requests\nIf your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected.\nPlaybooks\nCreate and manage automated workflows for metadata management and data governance.\nRequests\nRequests allow users to suggest changes to assets that they cannot directly change themselves.\nRevoke data access\nAs an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the [scope of governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows).\nSend alerts for workflow events\nLearn how to configure alerts for workflow events in Atlan via email or Google Chat.\nStewardship\nLearn how to implement data stewardship in Atlan through automated workflows, policies, and task management.\nTroubleshooting Anomalo connectivity\nLearn about troubleshooting anomalo connectivity.\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings.\nWebhooks Integration\nIntegrate Atlan with Webhooks to automate actions and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/workflows",
    "content": "5 docs tagged with \"workflows\"\nView all tags\nAlteryx\nIntegrate, catalog, and govern Alteryx assets in Atlan using OpenLineage.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nPlaybooks\nCreate and manage automated workflows for metadata management and data governance.\nSend alerts for workflow events\nLearn how to configure alerts for workflow events in Atlan via email or Google Chat.\nStewardship\nLearn how to implement data stewardship in Atlan through automated workflows, policies, and task management."
  },
  {
    "url": "https://docs.atlan.com/tags/metadata",
    "content": "12 docs tagged with \"metadata\"\nView all tags\nAtlan MCP Overview\nLearn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup.\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nPlaybooks\nCreate and manage automated workflows for metadata management and data governance.\nTroubleshooting Amazon DynamoDB connectivity\nLearn about troubleshooting amazon dynamodb connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Tableau connectivity\nLearn about troubleshooting tableau connectivity.\nWhat does Atlan crawl from CrateDB?\nComplete list of CrateDB assets and metadata properties extracted by Atlan during crawling\nWhat does Atlan crawl from Dagster\nLearn about the Dagster metadata that Atlan captures and visualizes\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/capabilities",
    "content": "10 docs tagged with \"capabilities\"\nView all tags\nAtlan AI\nIntegrate and leverage Atlan AI capabilities for enhanced data documentation, and lineage analysis.\nData Models\nCreate and manage data models to structure and organize your data assets.\nData Products\nCreate and manage data products to organize and govern your data assets by domain.\nDiscovery\nFind, understand, and use data assets across your organization with powerful search, filtering, and browsing features.\nInsights\nQuery and analyze your data using Atlan's powerful query builder and SQL capabilities.\nLineage\nTrack and visualize data lineage across your data landscape to understand data flow and dependencies.\nPlaybooks\nCreate and manage automated workflows for metadata management and data governance.\nReporting\nGenerate comprehensive reports on your data assets, usage, and governance.\nRequests\nRequest and manage changes to assets that you don't have direct edit access to.\nUsage and Popularity\nTrack and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/access-archived-assets",
    "content": "Use data\nDiscovery\nAsset Management\nAccess archived assets\nOn this page\nAccess archived assets\nTo access archived assets in Atlan, complete the f\nollowing steps.\nArchived assets\nâ\nDid you know?\nArchived assets still exist in Atlan. They are only \"soft-deleted\" and do not appear in search results (by default). You can only access archived assets through discovery.\nAssets can be archived when:\nYou lose permissions to an asset at source.\nThe asset name is changed at source.\nAn asset is removed at source or moved to a different schema or database.\nDelete your assets either by\ndeleting a connection\nor\nconfiguring\nit in Atlan.\nSearch for archived assets\nâ\nTo search for archived assets:\nFrom the left menu of any page, click\nAssets\n.\nUnder\nFilters\non the left, expand\nProperties\n.\nAt the bottom of the list of properties, click\nIs Archived\nand then\nYes\n.\nNow\nuse any additional filters\n, and the results will include only archived assets.\nView details of an archived asset\nâ\nTo view the details of an archived asset:\nSearch for the asset using the steps above. Narrow down your results using the other filters or search details.\nClick the card for the asset you want to view.\nUse the right sidebar to view the details of the asset. The icons on the far right of the sidebar allow you to review different aspects of the asset.\nDeleted assets\nâ\nAssets that have been (hard-)deleted no longer exist in Atlan. So you cannot find them by searching, or even access them through a direct link.\nTags:\ndata\nasset-profile\nPrevious\nSearch and discover assets\nNext\nAdd an alias\nArchived assets\nDeleted assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/configure-language-settings",
    "content": "Use data\nDiscovery\nConfiguration\nConfigure language settings\nOn this page\nConfigure language settings\nHow does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level.\nAtlan admins can change the default language for their Atlan workspace from the admin center. Individual users can also set a personal language preference, which overrides the default workspace settings - English or otherwise.\nAtlan currently supports the following additional languages:\nFrench\nJapanese\nPortuguese\nTo enable any of the additional supported languages or request ones not listed in this section in your Atlan workspace, Atlan admins must\ncontact Atlan support\n.\nConfigure workspace language settings\nâ\nWho can do this?\nOnce Atlan has enabled a preferred language for your organization, you need to be an\nadmin user\nin Atlan to configure language settings for your Atlan workspace.\nTo configure workspace language settings:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nWorkspace settings\nheading of the\nLabs\npage, for\nDefault workspace language\n, click the\nEnglish\nlanguage dropdown to set a preferred language.\nYour users can now use Atlan in their preferred language! ð\nConfigure personal language settings\nâ\nWho can do this?\nOnce Atlan has enabled a preferred language for your organization, anyone with access to Atlan - admin, member, or guest user - can update language settings for their Atlan instance.\nTo set a personal language preference:\nIn the top right corner of your Atlan instance, click your name, and then from the dropdown, click\nLanguage\n.\nFrom the\nLanguage\ndropdown, select a preferred language for your Atlan instance.\nYou can now use Atlan in your preferred language! ð\nTroubleshooting\nâ\nIf you wish to improve translations, such as for some specific technical term that's not translating correctly, you can directly submit that through Atlan's\nopen source language support\n.\nNote that you need to have a Github account to be able to do so.\nEnter src/locales/default and choose the language you wish to update translations for. Submit a pull request to edit any existing translations or add new terms.\nAlternatively you can submit a request to\nAtlan support\n.\nTags:\natlan\ndocumentation\nPrevious\nAdd a resource\nNext\nHow to use the filters menu\nConfigure workspace language settings\nConfigure personal language settings\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/concepts/how-to-interpret-timestamps",
    "content": "Use data\nDiscovery\nConcepts\nHow to interpret timestamps\nOn this page\nHow to interpret timestamps\nAtlan displays timestamps for assets in local timezones based on the location of your browser. The date and time display includes a combination of the following components:\nRelative time   -  for example,\n2 hours ago\n,\n1 day ago\n,\n3 months ago\nAbsolute time   -  for example,\nMar 14, 2024, 9:40:01 AM\nAtlan displays the following timestamps in the asset sidebar:\nOverview tab\nâ\nUsage\n-  timestamp for the number of read queries on an asset at\nsource\nwithin a specific date range, as fetched from the miner run. This also includes an absolute time value for\nLast usage data updated (in Atlan)\n. Only applicable to data sources for which Atlan supports mining query history.\nLast queried\n-  timestamp for the latest read query on an assetÂ at\nsource\nas fetched from the miner run or in Atlan within a specific date range. Only applicable to data sources for which Atlan supports mining query history.\ndbt run status\n-  status and timestamp for the last run of the dbt job updating an asset in dbt, as fetched from a\ndbt crawler\nrun.\nUsage tab\nâ\nRow update frequency\n-  timestamps for recent row updates on an asset at\nsource\nwithin a specific date range, as fetched from the miner run. Only applicable to data sources for which Atlan supports mining query history. Up to five recent row updates will be displayed, if available.\nProperties tab\nâ\nLast updated (in Atlan)\n-  timestamp for when any metadata attribute of the asset was last updated in Atlan. For example, when you\nlinked a term\nor\nadded a certificate\nto an asset in Atlan.\nLast synced with source\nÂ   -  timestamp for when a workflow run last checked for this asset at\nsource\n. This timestamp also includes a link to the connection workflow.\nCreated (in Atlan)\n-  timestamp for when this asset was first created and published in Atlan during a crawler run.\nLast updated (on source)\n-  timestamp for when any metadata for the asset was last altered at\nsource\n, as fetched from the crawler run.\nCreated at (on source)\n-  timestamp for when the asset was first created at\nsource\n, as fetched from the crawler run.\nFrequently asked questions\nâ\nWhy are metrics missing after miner runs?\nâ\nIf you notice a time lag from when your miner workflows last ran, note that Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. (Read more about miner logic\nhere\n.) This may also depend on when your miner workflow was scheduled to run, which you can\nmodify\nat any time.\nWhy do some timestamps have variable time ranges?\nâ\nMiners have a configurable property that governs the window of time for which metrics are reported. If a miner has been failing consistently, Atlan may reduce this window from 30 to only 14 days for reporting metrics. This is applicable to all date and time properties populated by miners.\nIs last updated in Atlan the same as last synced with source?\nâ\nNo,\nLast updated (in Atlan)\nrecords the time when any metadata is updated on the asset in Atlan while\nLast synced with source\nrecords the time when a workflow ran successfully updating the asset with changes from source, if any. If no metadata updates were made on the asset in Atlan before the next scheduled workflow run,Â\nLast synced with source\nÂ may be considered as the more current timestamp reflecting any or no changes on the asset as fetched from source.\nWhy are there discrepancies in time for some miner-related timestamps?\nâ\nFor timestamps related to miner runs:\nIf no one has queried the asset at source, the timestamp for\nLast queried\nmay be older than the date range recorded for\nUsage\n.\nIf no one has used the asset at source for the duration of time that query history was mined,\nRow update frequency\nmay not display any time value.\nEven when a miner run fails, it partially publishes assets, resulting in inconsistencies. For example, the date range in the\nLast queried\ntooltip may be for a successful miner run but the absolute time may be a different value if the asset had been partially published.\nTags:\natlan\ndocumentation\nPrevious\nHow do I use the filters menu?\nNext\nWhat are asset profiles?\nOverview tab\nUsage tab\nProperties tab\nFrequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/faq",
    "content": "Use data\nDiscovery\nFAQ\nDiscovery FAQs\nOn this page\nDiscovery FAQs\nHow does Atlan handle archived or deleted assets?\nâ\nIf an\nasset is removed from a workflow\nor a\nuser loses permissions\nto an asset, the asset will be\narchived\nin Atlan. The asset will be unarchived with all the metadata intact if it is\nincluded\nin the next workflow run or users permissions are restored.\nIn Atlan, an asset's\ntypename\nand\nqualifiedName\npair serves as a distinctive identifier. The\nqualifiedName\nis a string that has been concatenated and contains the asset's source, host, and hierarchy. The related asset in Atlan will not change unless a modification is made, such as changing the schema or table name.\nIs it possible to search for fields across all data sources?\nâ\nIf you know exactly what you're looking for, Atlan suggests using exact match search. Wrap the search terms within double quotation marks\n\" \"\nwhen typing it in the search bar. For example,\n\"instacart_total_users\"\n.\nFor\ncontextual search\n, the\nAssets\npage provides a\nnumber of filters\nto help you narrow down your search results. Filter data assets by various properties, asset types, and so on.\nThe search results in Atlan also provide a quick count of all the resulting data assets, organized by asset type. These counts will alter in real time as you apply filters.\nWhat is included in the \"All assets\" view?\nâ\nThe\nAll assets\nview includes\nall\nassets in your Atlan data estate.\nIf your Atlan admin has turned off\nView \"All assets\" in Assets Discovery\nfrom Labs in the admin center\n, you may be unable to see this view. In that case, you'll only be able to view the assets curated for the persona(s) or purpose(s) you belong to in Atlan. Admin users will still have full access to all assets, even when this default behavior is turned off.\nCan I search by a value to find assets with that value?\nâ\nAtlan allows you to search and discover metadata, not data.\nAs a constantly updated data catalog of all your data assets and metadata, Atlan allows you to identify and\naccess your data assets\nas well as the\ntribal knowledge and business context\nassociated with them. The powerful, intelligent search returns relevant search results.\nWhy do I not see more results when searching with the search bar (or Cmd + K)?\nâ\nThe search bar is meant to be a quick search option. It works best when you know the name of the asset. As such, it only loads 20 search results at a time. You can also quickly access your\nrecently visited\nstarred assets\nfrom the search bar.\nFor more in-depth discovery,\nsearch from the\nAssets\npage:\nMore results\nAdditional filters\nWhat is the timezone for data display?\nâ\nFor asset properties such as\nLast updated (in Atlan)\n,\nLast synced with source\n, or\nCreated (in Atlan)\n, the timestamps are displayed in local timezones based on the user's browser location. To learn more, see\nHow to interpret timestamps\n.\nAsset Profile FAQs\nâ\nCan we replace or rename existing assets?\nâ\nThe expected behavior will vary based on the\nsupported connector\nand asset type. If the asset name is reflected in the\nqualifiedName\nof the asset and you rename the asset, Atlan will create a net-new asset. This is because the\nqualifiedName\ndetermines\nasset uniqueness\nin Atlan.\nConsider the following examples:\nThe name of a Snowflake table is part of its\nqualifiedName\n. Any changes to the table name in Snowflake will result in a new\nqualifiedName\n, and thus the creation of a new asset in Atlan.\nConversely, the\nqualifiedName\nof Microsoft Power BI reports are based on the UUIDs of the assets in Microsoft Power BI   -  no names are embedded. In this case, renaming a Microsoft Power BI report does not change its UUID in Microsoft Power BI. This means that the\nqualifiedName\nof that report in Atlan will remain unchanged. Atlan will simply update the existing asset. However, this may not be the case where a Microsoft Power BI table is concerned, as the table name is included in the\nqualifiedName\nof the asset.\nIf your use case is to enable quick discovery in Atlan, consider\nadding a business-friendly alias\nto the asset.\nWhat is an activity log?\nâ\nThe activity log in the asset sidebar provides a changelog for your data assets. Having a record of all the changes made to an asset can help build trust in your data assets and promote transparency across your organization.\nView the activity log of an asset\nâ\nTo view the activity log of an asset:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nClick on an asset to view its asset profile.\nFrom the asset sidebar to the right, click the\nActivity\ntab to view the activity log.\nIf an asset was updated from\nGoogle Sheets\nor\nMicrosoft Excel\n, an\nUpdated using Google Sheets\nor\nUpdated using Microsoft Excel\nstamp will appear, respectively. (Optional) Click the\nGoogle Sheets\nor\nMicrosoft Excel\nlink to view the source spreadsheet.\n(Optional) To filter the activity log by a specific type of activity, under\nActivity\n, click the dropdown arrow and then:\nClick\nAlias\nto view\nalias activity\nby user and timestamp of update.\nClick\nDescription\nto view any changes in the\ndescription\nof an asset.\nClick\nStarred\nto view\nstarred activity\nby user and timestamp.\nClick\nAnnouncement\nto view any changes to the\nannouncement\non an asset.\nClick\nTerms\nto view any updates on linked\nterms\n.\nClick\nCertificate\nto view any changes in the\ncertification status\nof an asset.\nClick\nOwners\nto view any changes to the\nownership\nof an asset.\nClick\nTag Added\nor\nTag Removed\nto view any changes for\ntags directly added\nto an asset.\nClick\nTag Added (Propagation)\nor\nTag Removed (Propagation)\nto view any changes for\ntags propagated\nto an asset.\nClick\nColumn Added\n,\nColumn Deleted\n, or\nColumn Updated\nto view column changes.\nClick\nReadme Added\nor\nReadme Updated\nto view when a\nREADME was created\nfor an asset and which user created or updated it.\n(Optional) Click the\nFilter by column\nmenu to filter your activity logs by specific columns.\nYou can now view all the changes that were made to an asset in the activity log! ð\nDid you know?\nActivity logs for metadata changes are persisted throughout the lifecycle of the Atlan instance for your organization.\nWhy do dbt descriptions keep getting deleted?\nâ\nIf you notice that the descriptions from a Snowflake table, for example, get deleted when dbt is your source of truth, it is likely that you have the data source scheduled to run after the dbt run. Atlan recommends following the order of operations as documented in\nHow to order workflows\n.\nYou can also use dbt's\npersist_docs\nfeature to ensure that your metadata\npersists through workflow runs\n.\nWhat is the timeframe for recently verified assets?\nâ\nAtlan displays up to 20 most recently verified assets at a time in the\nRecently verified assets\nsection on your Atlan homepage. You can scroll down further and click\nLoad more\nto view more recently verified assets. This is not based on any particular timeline. In fact, this list may include assets that were updated as long as a week ago, if no new assets were verified more recently.\nWhat signals Atlan to auto-add the deprecated certifications in Looker?\nâ\nThese certifications are sourced from Looker. Once the\nLooker crawler\nhas run successfully and detected that an asset was deleted, Atlan will attached a\nDeprecated\ncertificate to the asset because it no longer exists at source. For example:\nIf tiles are deleted, the API response generated is titled\nLook Deleted\n. In this case, Atlan will add the deprecated certificate.\nIf Atlan crawls any models that do not have a project associated with them (indicated by a missing\nproject_name\nkey in the model API response), Atlan adds a deprecated certificate to the asset.\nIs it possible to add PDFs to an asset README?\nâ\nAtlan currently does not support embedding a PDF file in an asset README. Alternatively, PDF files can be\nlinked as a resource\n.\nHow does version control work for description changes in source tools vs. Atlan?\nâ\nConsidering that users may update metadata in Atlan and the source tool, Atlan\nmanages descriptions in two fields\n-  populating either or both depending on where it was created or updated. This is the format Atlan follows:\nSource descriptions are stored in the\ndescription\nfield\nAny description added or updated in Atlan is stored in the\nuserDescription\nfield\nSeparating them into two fields ensures that the connection package does not override the descriptions entered by users in Atlan every time the workflow runs and updates the asset. This way, the description field in Atlan becomes the source of truth.\nThere is one exception though. If the description field has not been edited at all in Atlan and the connection package only ever brings the descriptions from the source, the workflow will keep updating the description field with what is available at source   -  only source edits will come through.\nOnce the\ndescription\nfield is edited in Atlan, the\nuserDescription\nfield takes over. If you would like to restore the original source description, simply clear the description added in Atlan and it will automatically revert to the source description.\nAs a best practice, we recommend all subsequent edits to the description be done in Atlan.\nThis is valid across all connector workflows.\nCan Atlan track schema changes?\nâ\nAtlan tracks schema changes for SQL sources, which can be viewed in the\nactivity log\nof an asset. However, Atlan is not a schema registry or a data modeling tool such as dbt.\nAre different fonts supported for READMEs and descriptions?\nâ\nAtlan currently does not support customizing the following for asset\ndescriptions\n,\nREADMEs\n, and\nREADME templates\n:\nFonts\nFont sizes\nFont colors\nCan Atlan handle assets with a large number of rows and columns?\nâ\nYou can safely catalog your assets in Atlan, regardless of the number of rows or columns. Since Atlan only extracts metadata, data volume is not a consideration. Note that if an asset has a large number of columns, the\ncolumn preview\nin the asset profile may take some time to load.\nWhy do I not see any tables or columns under database, only schema?\nâ\nIf you're viewing related assets for databases from the\nRelations\ntab in the asset sidebar, Atlan will only display schemas. You can click the schema asset and then open the\nRelations\ntab of the schema to view tables and views.\nTo find tables and views contained within a database, Atlan also recommends using the\ndatabase asset type filter\nfor quick discovery.\nIs the sample data preview cached?\nâ\nWhen previewing sample data on any asset, Atlan retrieves the sample data from source each time. No data is cached in Atlan.\nDelete an asset\nâ\nHave one or more assets in Atlan that you don't want to be there? You have several options for removing them.\nSpecific assets\nâ\nTo remove targeted, specific assets\nuse one of our SDKs or our open API\n.\nSet of related assets\nâ\nTo remove a set of related assets, such as an entire schema:\nModify the connector's configuration\nwith a filter to exclude the asset(s).\nRe-run the connector's workflow with the new configuration.\nDid you know?\nThis will\narchive\nthe schema and its assets.\nAn entire connection's assets\nâ\nTo remove an entire connection and all its assets, see\nHow to delete a connection\n.\nIf an asset is deleted via API, will workflows recreate the asset on the next run?\nâ\nIf you delete an asset at source, Atlan will\narchive\nthat asset in the next crawler run. Atlan recommends that you manage additions or deletions through source workflows. Considering that Atlan workflows run differential crawls, any changes to your assets will be reflected in subsequent crawler runs. This helps deliver faster runtimes and improves workflow performance.\nIf you delete an asset using APIs, there can be two scenarios:\nArchived:\nAssets are soft-deleted, moving to an\nArchived\nstate.\nThe next workflow run will restore the asset only if any changes are made to any of its source metadata properties   -  for example, a new column was added.\nYou can also\nrestore archived assets\n.\nHard-deleted:\nThe asset is completely removed from the metastore.\nThe next workflow run creates a new asset if the previously deleted asset is still present or reintroduced in the source system. For example, this can happen if the asset was never actually removed from the source or if it was deleted and later recreated. However, the newly created asset doesn't retain any of its original Atlan metadata.\nYou\ncan't\nrestore hard-deleted assets.\nAtlan recommends the deletion of assets to be managed by source workflows. The delete and restore endpoints are generally meant for assets created via APIs.\nAdd a README\nâ\nA README is an essential part of every code repository. The better the README, the more collaborators will want to work on your code. The same holds true for your data assets.\nEach data asset should have its own README, which provides a description of its characteristics and other critical information. Atlan allows users to add a README for every data asset, using an intuitive, rich text editor.\nYou can document the tribal knowledge associated with each data asset in a README and reduce dependencies on your team members. The README appears right below each data table in the asset profile, displaying the data and the metadata on the same page and bridging the gap between the two.\nAtlan currently supports the following file types for asset READMEs:\nGoogle Docs\nGoogle Sheets\nGoogle Slides\nMiro boards\nFigJam boards\nLucidchart\ndbdiagram\nERD Lab\nMicrosoft Word\nMicrosoft Excel\nMicrosoft PowerPoint\nGoogle Data Studio\nGoogle Looker Studio\nCanva\nAdd a README\nâ\nA README can be added to different types of data assets in Atlan, including BI dashboards, widgets, columns, databases, and schemas.\nThe character limit for READMEs is 100,000 characters. A portion of this limit is used to ensure compatibility with rich text formatting, slightly reducing the available character limit.\nTo add a README to an asset, follow these steps:\nOn the Atlan homepage, click\nAssets\nin the left menu.\nClick on an asset to view its asset profile.\nIn the\nReadme\nsection of the asset profile, click\n+Add\n.\nYou can either:\nClick\nBlank Page\nto create a new README from scratch.\nClick\nUse\nto select an\nexisting template\nas a starting point.\nEnter your knowledge into the README. Type\n/\nto use the text editor options to format your text, embed links, and more:\nClick\nHeading 1\nto add a title or main heading.\nClick\nHeading 2\nto add subheadings and create sections.\nClick\nHeading 3\nto create subcategories within sections.\nClick\nBulleted List\nto create a bulleted list.\nClick\nNumbered List\nto create an ordered list.\nClick\nChecklist\nto create a checklist of items.\nClick\nFormula\nto add formulae from a\nlist of supported functions\n.\nClick\nCode\nto add a code snippet.\nClick\nImage\nto embed or upload an image.\nClick\nQuote\nto add block quotations.\nClick\nMention\nto tag another user in your Atlan workspace.\nClick\nTable\nto create a table.\nClick\nGoogle Docs\nto paste a Google Doc link and embed your online documents.\nClick\nGoogle Sheets\nto paste a Google Sheets link and embed your online spreadsheets.\nClick\nGoogle Slides\nto paste a Google Slides link and embed your online presentations.\nClick\nMiro Board\nto paste a Miro board link and embed your boards.\nClick\nFigJam\nto paste a FigJam link and embed your boards.\nClick\nLucidchart\nto paste a Lucidchart link and embed your documents or models.\ndanger\nTo embed a Lucidchart document or model, you will need to activate the embed code. Activating an embed code will disable password protection on published documents and make them accessible publicly.\nClick\nDBDiagram\nto paste a dbdiagram link and embed your database diagrams.\nClick\nERD Lab\nto paste an ERD Lab link and embed your entity relationship diagrams.\nClick\nMicrosoft Word\nto paste a Microsoft Word link and embed your online documents.\nClick\nMicrosoft Excel\nto paste a Microsoft Excel link and embed your online spreadsheets.\nClick\nMicrosoft PowerPoint\nto paste a Microsoft PowerPoint link and embed your online presentations.\nClick\nGoogle Data Studio\nor\nGoogle Looker Studio\nto paste a Google Data Studio or Google Looker Studio link and embed your reports and dashboards.\nClick\nCanva\nto paste a Canva link and embed your Canva graphics and presentations.\nClick\nSave\n.\nYour README is ready to be shared! ð\nAlthough it may take some time to create, a README is a critical step for documenting any data asset and making it trustworthy.\nUse README shortcuts\nâ\nAtlan supports keyboard and markdown shortcuts to supercharge your README documentation.\nKeyboard shortcuts\nâ\nIn the table below,\nMod\nstands for modifier key   -\nCommand\nfor Mac and\nCtrl\nfor Windows.\nShortcut\nAction\nMod + Shift + B\nblock quote\nMod + B\nbold\nMod + I\nitalics\nMod + Shift + 8\nbulleted list\nMod + E\ncode\nMod + Alt + C\ncode block\nEnter\nthrice,\nâ\nexit code block\nMod + Alt + 1\nheading 1\nMod + Alt + 2\nheading 2\nMod + Alt + 3\nheading 3\nMod + Alt + 4\nheading 4\nMod + Alt + 5\nheading 5\nMod + Alt + 1\nheading 6\nEnter\nadd new list item\nShift + Tab\nuplift list item\nTab\nsink list item\nMod + Shift + 7\nordered list\nMod + Alt + 0\nset paragraph\nMod + Shift + X\nstrikethrough\nMod + U\nunderline\nMod + Shift + 9\ntoggle task list\nMod + Shift + L\nleft align text\nMod + Shift + E\ncenter align text\nMod + Shift + R\nright align text\nMod + Shift + J\njustify text\nMod + Shift + H\nhighlight text\nWithin a table:\nÂ\nTab\ngo to next cell\nShift + Tab\ngo to previous cell\nBackspace\ndelete table when all cells are selected\nMod - Backspace\ndelete table when all cells are selected\nDelete\ndelete table when all cells are selected\nMod - Delete\ndelete table when all cells are selected\nMarkdown shortcuts\nâ\nMarkdown shortcuts are triggered by pressing space after the shortcut   -  except for bold and italics. For example, to add a block quote, type\n>\nand then tap the spacebar.\nShortcut\nAction\n>\nblock quote\n**\n(text)\n**\n,\n__\nbold\n*\n(text)\n*\n,\n_\nitalics\n-\nbulleted list\n`\ncode\n```\n,\n~~~\ncode block\n#\nheading 1\n##\nheading 2\n###\nheading 3\n####\nheading 4\n#####\nheading 5\n######\nheading 6\n---\n,\n--\n,\n___\nhorizontal divider\nany numeric digit\nordered list\n~~\nstrikethrough\n[]\nunchecked task\n[x]\nchecked task\n==\nhighlight\nDoes Atlan support asset previews?\nâ\nYes, Atlan provides asset previews for supported tools to help with quick discovery and give you the context you need. Typically, the\nWhat does Atlan crawl from (connector name)?\ndocumentation will indicate whether asset previews are supported for a specific connector.\nFor example, Atlan supports asset previews for:\nTableau worksheets and dashboards\nMicrosoft Power BI reports\nGoogle BigQuery tables, views, and materialized views\nSigma workbooks\nCan I search for assets by README, description, or other metadata?\nâ\nIf the keywords you're searching by is present in the asset name or\ndescription\n, only then will the asset appear in your search results. You can also use a\nvariety of filters\nto narrow down your search.\nNote that asset READMEs are currently not searchable in Atlan. This is because Atlan stores them as a relation to a data asset rather than as a direct metadata attribute.\nCan I add the Google Sheets extension for everyone in my organization?\nâ\nYes! To install the Google Sheets Atlan extension at the workspace level, follow the instructions in this\nguide\n. You will need to be an administrator or have access to the admin console of your organization's Google account for this setup.\nOnce installed, users in your organization can\nconnect Atlan with Google Sheets\nto start using the extension.\nCan I embed presentations or docs from SharePoint in a README?\nâ\nYes, you can embed links to your Microsoft Word, Excel, and PowerPoint files stored in SharePoint or OneDrive in READMEs. Refer to\nHow to add a README\nto learn more.\nWhy can hard deleted assets be immediately restored after deletion?\nâ\nThe\nAPI\nis eventually consistent. This means that there is a short window of time, up to a few minutes, during which you can use the restore API endpoint to restore the asset that was hard deleted   -  even if it no longer appears on the UI.\nHow can I make external documents and spreadsheets appear as assets in Atlan?\nâ\nAtlan supports\ncataloging files\nthrough APIs. The\nsupported file types\ninclude DOC, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files.\nWill Atlan send a notification if there's a new table added to my schema?\nâ\nYou can configure any of the following options to receive notifications on asset additions:\nCreate a webhook\nUse the asset change notification custom package\nTags:\ndiscovery\nfaq\nsearch\nassets\ndata\nfaq-discovery\nPrevious\nProvide credentials to view sample data\nHow does Atlan handle archived or deleted assets?\nIs it possible to search for fields across all data sources?\nWhat is included in the \"All assets\" view?\nCan I search by a value to find assets with that value?\nWhy do I not see more results when searching with the search bar (or Cmd + K)?\nWhat is the timezone for data display?\nAsset Profile FAQs\nCan we replace or rename existing assets?\nWhat is an activity log?\nWhy do dbt descriptions keep getting deleted?\nWhat is the timeframe for recently verified assets?\nWhat signals Atlan to auto-add the deprecated certifications in Looker?\nIs it possible to add PDFs to an asset README?\nHow does version control work for description changes in source tools vs. Atlan?\nCan Atlan track schema changes?\nAre different fonts supported for READMEs and descriptions?\nCan Atlan handle assets with a large number of rows and columns?\nWhy do I not see any tables or columns under database, only schema?\nIs the sample data preview cached?\nDelete an asset\nIf an asset is deleted via API, will workflows recreate the asset on the next run?\nAdd a README\nDoes Atlan support asset previews?\nCan I search for assets by README, description, or other metadata?\nCan I add the Google Sheets extension for everyone in my organization?\nCan I embed presentations or docs from SharePoint in a README?\nWhy can hard deleted assets be immediately restored after deletion?\nHow can I make external documents and spreadsheets appear as assets in Atlan?\nWill Atlan send a notification if there's a new table added to my schema?"
  },
  {
    "url": "https://docs.atlan.com/tags/discovery",
    "content": "9 docs tagged with \"discovery\"\nView all tags\nDiscovery\nFind, understand, and use data assets across your organization with powerful search, filtering, and browsing features.\nDiscovery FAQs\nFrequently asked questions about Atlan's Discovery capabilities.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nTroubleshooting Amazon DynamoDB connectivity\nLearn about troubleshooting amazon dynamodb connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Tableau connectivity\nLearn about troubleshooting tableau connectivity.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/search",
    "content": "2 docs tagged with \"search\"\nView all tags\nDiscovery\nFind, understand, and use data assets across your organization with powerful search, filtering, and browsing features.\nDiscovery FAQs\nFrequently asked questions about Atlan's Discovery capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/browse",
    "content": "One doc tagged with \"browse\"\nView all tags\nDiscovery\nFind, understand, and use data assets across your organization with powerful search, filtering, and browsing features."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts/how-tos/create-data-contracts",
    "content": "Build governance\nContracts\nGet Started\nCreate data contracts\nOn this page\nCreate data contracts\nPrivate Preview\nA data contract is an agreement between a data producer and consumer that specifies requirements for generating and using high-quality, reliable data. As a powerful tool for data management, data contracts can help you standardize contractual obligations between data producers and consumers, organize your assets with embeddable contract metadata, and enforce them with data quality rules.\nIn Atlan, you can directly add a data contract to supported assets and provide helpful context to your downstream users.\nFor a data contract to help build trust in your assets, it should be:\nTemplatized and easily comprehensible   -  use Atlan's YAML contract template to create standardized contracts and push to Atlan.\nVersion-controlled   -  continuously validate and monitor your data contracts either in runtime or real-time.\nEmbeddable   -  embed the contract as metadata for a supported asset.\nEnforceable   -  enforce your contracts with data quality rules.\nExtensible   -  identify new specifications, generate new versions, and then compare and contrast them.\nDid you know?\nYou can\ncreate webhooks for data contracts\nand receive notifications for when a contract is added or updated to a URL of your choice.\nSupported asset types\nâ\nYou can create data contracts for the following asset types:\nTables\nViews\nMaterialized views\nOutput port assets of data products\nSupported asset metadata\nâ\nAtlan maps the following asset metadata properties to it contract properties:\nMetadata property\nContract property\nname\ndataset\ntypeName\ntype\nuserDescription\nor\ndescription\ndescription\nownerUsers\nowner.users\nownerGroups\nowners.groups\ncertificateStatus\ncertification.status\ncertificateStatusMessage\ncertification.message\nannouncementType\nannouncement.type\nannouncementTitle\nannouncement.title\nannouncementMessage\nannouncement.description\nmeaningNames\nterms\nclassificationDef.displayName\ntags.name\nclassifications.propagate\ntags.propagate\nclassifications.restrict_propagation_through_lineage\ntags.restrict_propagation_through_lineage\nclassifications.restrict_propagation_through_hierarchy\ntags.restrict_propagation_through_hierarchy\ncolumn.name\ncolumns.name\ncolumn.userDescription\nor\ncolumn.description\ncolumns.description\ncolumn.dataType\ncolumns.data_type\ncolumn.isPrimary\ncolumns.primary\n!column.isNullable\ncolumns.required\ncolumn.precision\ncolumns.precision\ncolumn.numericScale\ncolumns.scale\ntags\non column\ncolumns.tags\ncolumn.meaningNames\ncolumns.terms\ncustom metadata\n(CM)\ncustom_metadata.<CM>\nAdd a data contract to an asset\nâ\nWho can do this?\nAny non-guest user with\nedit access to an asset's metadata\ncan create, deploy, and manage data contracts. This only includes\nadmin and member users\nin Atlan.\nTo add a data contract to an asset, you can either:\nCreate a contract directly in Atlan from the\nContracts\ntab of the asset profile. You can create and maintain data contracts as easily as editing a word document.\nUse Atlan CLI to import an existing contract from your local machine to Atlan directly or through a CI/CD pipeline.\nAtlan CLI\nis a command-line tool that you can download directly from Atlan to your local machine to create and push data contracts to Atlan. Once you have published the contract, you can also\nsync metadata\nfrom a contract to the governed asset in Atlan.\nOnce created, you will be able to monitor and manage your data contracts in Atlan.\nTo add a data contract:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\n(Optional) From the\nFilters\nmenu on the left, click\nProperties\nand then click\nHas contract\n. Click\nNo\nto filter for assets without a contract.\nFrom the\nAssets\npage, select an asset to open the asset sidebar.\nIn the left\nOverview\nsidebar, click\nAdd contract\n.\nIn the\nContract\ntab of the asset profile, you can either:\nClick\nCreate contract\nto create a draft contract directly in Atlan based on asset metadata.\nClick\nImport contract\nto use\nAtlan CLI\nto import an existing contract from your local environment to Atlan. You will first need to install and connect Atlan CLI and then push the contract to Atlan. Refer to our\ndeveloper documentation\nto complete the steps.\n(Optional) Click the\nEdit\nbutton to edit the contract.\nCongrats on adding a data contract in Atlan! ð\nView a data contract\nâ\nTo view a data contract:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nFrom the\nAssets\npage, select an asset to open the asset sidebar.\nFrom the left\nOverview\nsidebar, click\nView contract\nto navigate to the\nContract\nÂ tab in the asset profile.\n(Optional) In the\nContract\ntab, you can view the contract specifications for your asset in a YAML format. You can also:\nClick the\nDocument\nicon to open a read-only, simplified view of your contract.\nNext to\nPublished version\n, click the version dropdown to view the latest version of the contract. Select an older version and then click\nCompareÂ with published version\nto compare them side by side.\nClick the\nEdit\nbutton to edit the contract.\nClick the clipboard icon to copy the YAML code.\nUnder\nTimeline\n, view a timeline for the evolution of your contract.\nUnder\nSummary\n, view details of who last updated your contract and when.\nTags:\natlan\ndocumentation\nPrevious\nContracts\nNext\nAdd contract impact analysis in GitHub\nSupported asset types\nSupported asset metadata\nAdd a data contract to an asset\nView a data contract"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts/how-tos/add-contract-impact-analysis-in-github",
    "content": "Build governance\nContracts\nImpact Analysis\nAdd contract impact analysis in GitHub\nOn this page\nAdd contract impact analysis in GitHub\nPrivate Preview\nImpact analysis helps you identify how modifications to your data contracts might impact downstream processes, data quality, and overall business operations. This can help you analyze proposed changes and mitigate potential risks before implementation.\nIf you have ever changed a\ndata contract\nonly to find out later that it broke a downstream table or dashboard, Atlan provides a\nGitHub Action\nto help you out.\nThis action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request.\nPrerequisites\nâ\nBefore running the action, you will need to create an\nAtlan API token\n.\nYou will also need to assign a\npersona\nto the API token and add a\nmetadata policy\nthat provides the requisite permissions on assets for the Atlan action to work. For example, you can add the following permissions:\nAsset, such as a table   -\nRead\nonly\nAny downstream connections, such as Microsoft Power BI   -\nRead\nonly\nYou will need to configure the default\nGITHUB_TOKEN\npermissions. Grant\nRead and write permissions\nto the\nGITHUB_TOKEN\nin your repository to allow the\natlan-action\nto seamlessly add or update comments on pull requests. Refer to\nGitHub documentation\nto learn more.\nConfigure the action\nâ\nTo set up the Atlan action in GitHub:\nCreate repository secrets\nin your repository:\nATLAN_INSTANCE_URL\nwith the URL of your Atlan instance.\nATLAN_API_TOKEN\nwith the value of the API token.\nAdd the GitHub Action to your workflow:\nCreate a workflow file in your repository   -\n.github/workflows/atlan-action.yml\n.\nAdd the following code to your workflow file:\nname\n:\nAtlan action\non\n:\npull_request\n:\ntypes\n:\n[\nopened\n,\nedited\n,\nsynchronize\n,\nreopened\n,\nclosed\n]\njobs\n:\nget-downstream-impact\n:\nname\n:\nGet Downstream Assets\nruns-on\n:\nubuntu\n-\nlatest\nsteps\n:\n-\nname\n:\nCheckout\nuses\n:\nactions/checkout@v4\n-\nname\n:\nRun Action\nuses\n:\natlanhq/atlan\n-\naction@v1\nwith\n:\nGITHUB_TOKEN\n:\n$\n{\n{\nsecrets.GITHUB_TOKEN\n}\n}\nATLAN_INSTANCE_URL\n:\n$\n{\n{\nsecrets.ATLAN_INSTANCE_URL\n}\n}\nATLAN_API_TOKEN\n:\n$\n{\n{\nsecrets.ATLAN_API_TOKEN\n}\n}\nATLAN_CONFIG\n:\n.atlan/config.yaml\nTest the action\nâ\nAfter you've completed the configuration above, create a pull request with a changed\nAtlan data contract\nfile to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request:\nThe GitHub workflow will add and update a single comment for every file change.\nThe impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type.\nThe comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms.\nView impacted assets directly in Atlan.\nInputs\nâ\nName\nDescription\nRequired\nGITHUB_TOKEN\nFor\nwriting comments on PRs\nto print downstream assets\ntrue\nATLAN_INSTANCE_URL\nFor making API requests to the user's tenant\ntrue\nATLAN_API_TOKEN\nFor\nauthenticating API requests\nto the user's tenant\ntrue\nATLAN_CONFIG\nFor impact analysis of\nAtlan data contracts\n, if included in a GitHub PR\ntrue\nTags:\ndata\napi\nPrevious\nCreate data contracts\nPrerequisites\nConfigure the action\nTest the action\nInputs"
  },
  {
    "url": "https://docs.atlan.com/tags/contracts",
    "content": "One doc tagged with \"contracts\"\nView all tags\nContracts\nLearn how to manage data contracts and agreements in Atlan to ensure data quality and compliance."
  },
  {
    "url": "https://docs.atlan.com/tags/agreements",
    "content": "One doc tagged with \"agreements\"\nView all tags\nContracts\nLearn how to manage data contracts and agreements in Atlan to ensure data quality and compliance."
  },
  {
    "url": "https://docs.atlan.com/tags/data-quality",
    "content": "20 docs tagged with \"data quality\"\nView all tags\nAnomalo\nIntegrate, catalog, and govern Anomalo assets in Atlan.\nConfigure alerts\nSet up real-time notifications for data quality rule failures via Slack or Microsoft Teams.\nContracts\nLearn how to manage data contracts and agreements in Atlan to ensure data quality and compliance.\nData quality permissions\nReference for data quality permission scopes and configuration in Atlan.\nDatabricks Data Quality Studio\nSet up and configure Databricks for data quality monitoring through Atlan.\nEnable auto re-attachment of rules\nLearn how to enable automatic re-attachment of data quality rules to Snowflake tables and views.\nEnable data quality on connection\nEnable and configure data quality for your Databricks connection in Atlan.\nEnable data quality on connection\nEnable and configure data quality for your Snowflake connection in Atlan.\nMonte Carlo\nIntegrate, catalog, and govern Monte Carlo assets in Atlan.\nOperations\nAtlan crawls and manages the following data quality operations and results from Snowflake.\nRoles and permissions\nExplanation of Snowflake's security model and role requirements for data quality operations.\nRules and dimensions\nReference for available data quality rules and classification dimensions in Snowflake data quality.\nSet up Databricks\nConfigure Databricks to enable data quality monitoring through Atlan.\nSet up Snowflake\nConfigure Snowflake to enable data quality monitoring through Atlan.\nSetup and configuration\nCommon questions about Databricks data quality setup and configuration.\nSnowflake Data Quality Studio\nSet up and configure Snowflake for data quality monitoring through Atlan.\nSoda\nIntegrate, catalog, and govern Soda assets in Atlan.\nUpgrade to Snowflake data quality studio\nUpdate existing Snowflake data quality integration to the latest version\nWhat's auto re-attachment\nUnderstand automatic re-attachment of data quality rules to assets that are dropped and recreated.\nWhat's Data Quality Studio\nUnderstand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/governance",
    "content": "18 docs tagged with \"governance\"\nView all tags\nAccess Control\nLearn how to manage user permissions and access to data assets in Atlan for security and compliance.\nAutomatically assign roles based on group names\nLearn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app.\nContracts\nLearn how to manage data contracts and agreements in Atlan to ensure data quality and compliance.\nCustom Metadata\nLearn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information.\nData Products\nCreate and manage data products to organize and govern your data assets by domain.\nDatabricks Data Quality Studio\nSet up and configure Databricks for data quality monitoring through Atlan.\nDomains\nLearn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way.\nGlossary\nLearn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization.\nRequests\nRequest and manage changes to assets that you don't have direct edit access to.\nSet up Databricks\nConfigure Databricks to enable data quality monitoring through Atlan.\nSet up Snowflake\nConfigure Snowflake to enable data quality monitoring through Atlan.\nSnowflake Data Quality Studio\nSet up and configure Snowflake for data quality monitoring through Atlan.\nStewardship\nLearn how to implement data stewardship in Atlan through automated workflows, policies, and task management.\nTags\nLearn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance.\nUpgrade to Snowflake data quality studio\nUpdate existing Snowflake data quality integration to the latest version\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings.\nUsers and groups\nLearn how to manage users and groups in Atlan to control access and organize your data team.\nWhat's Data Quality Studio\nUnderstand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/integrations",
    "content": "14 docs tagged with \"integrations\"\nView all tags\nAtlan MCP Overview\nLearn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup.\nAutomation Integrations\nIntegrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On.\nCollaboration Integrations\nIntegrate Atlan with collaboration tools like Microsoft Teams and Slack.\nCommunication Integrations\nIntegrate Atlan with communication tools like SMTP and Announcements.\nConnections Integration\nIntegrate Atlan with Connections to create webhooks and automate notifications.\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance.\nExport Assets\n:::warning Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to [enable asset export](enable-asset-export).\nIdentity Management Integrations\nIntegrate Atlan with identity management tools like SCIM and SSO.\nIntegrations\nLearn how to integrate Atlan with project management, communication, collaboration, automation, and identity management tools.\nProject Management Integrations\nIntegrate Atlan with project management tools like Jira and ServiceNow.\nSCIM Integration\nIntegrate Atlan with SCIM to automate user provisioning.\nSMTP and Announcements Integration\nIntegrate Atlan with SMTP to send announcements and notifications.\nSSO Integration\nIntegrate Atlan with SSO to enable secure authentication and access control.\nWebhooks Integration\nIntegrate Atlan with Webhooks to automate actions and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/setup",
    "content": "54 docs tagged with \"setup\"\nView all tags\nConfigure SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning.\nConfigure SMTP\nAtlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and [scheduled queries](/product/capabilities/insights/how-tos/schedule-a-query). We provide an embedded SMTP server to do this, out-of-the-box.\nConfigure Snowflake data metric functions\nConfigure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nCrawl Amazon DynamoDB\nOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.\nCrawl Amazon Redshift\nOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.\nCrawl Apache Kafka\nLearn about crawl apache kafka.\nCrawl AWS Glue\nOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.\nCrawl Confluent Kafka\nLearn about crawl confluent kafka.\nCrawl Databricks\nTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl dbt\nOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.\nCrawl Domo\nOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.\nCrawl Google BigQuery\nOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.\nCrawl Looker\nOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.\nCrawl Matillion\nOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.\nCrawl Metabase\nOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.\nCrawl Mode\nOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.\nCrawl MongoDB\nOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.\nCrawl Monte Carlo\nOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.\nCrawl Oracle\nOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.\nCrawl Qlik Sense Cloud\nOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.\nCrawl Redash\nOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.\nCrawl Salesforce\nOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.\nCrawl SAP ECC\nTo crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl SAP HANA\nOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.\nCrawl SAP S/4HANA\nTo crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Sigma\nOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.\nCrawl Sisense\nOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.\nCrawl Soda\nOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.\nCrawl Teradata\nOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.\nCrawl ThoughtSpot\nOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.\nCreate an AWS Lambda trigger\nOnce you have configured the [AWS Lambda permissions](/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda), you can run an AWS Lambda function.\nEnable data quality on connection\nEnable and configure data quality for your Databricks connection in Atlan.\nEnable data quality on connection\nEnable and configure data quality for your Snowflake connection in Atlan.\nIntegrations\nLearn how to integrate Atlan with project management, communication, collaboration, automation, and identity management tools.\nManage dbt tags\nAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.\nMine Google BigQuery\nOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.\nMine Snowflake\nOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.\nMine Teradata\nOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.\nSet up Claude with Remote MCP\nLearn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access.\nSet up CrateDB\nConfigure authentication and connection settings for CrateDB connector\nSet up cross-workspace extraction\nConfigure a single service principal to crawl metadata from all workspaces within a Databricks metastore using system tables\nSet up Cursor with Remote MCP\nLearn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access.\nSet up Databricks\nConfigure Databricks to enable data quality monitoring through Atlan.\nSet up Informatica CDI\nConfigure authentication and user permissions for Informatica Cloud Data Integration connector\nSet up Local MCP Server\nThe Atlan Model Context Protocol (MCP) server enables seamless interaction between external tools and Atlan services. It defines the data structures and conventions used to prepare and transmit context for model execution, making it easier to integrate Atlan into your existing workflows.\nSet up Matillion\nConfigure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance.\nSet up Microsoft Copilot Studio with Remote MCP\nLearn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication.\nSet up n8n with Remote MCP\nLearn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows.\nSet up SAP ECC\nSet up user accounts and permissions required for SAP ECC metadata extraction in Atlan.\nSet up SAP S/4HANA\nSet up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan.\nSet up Snowflake\nConfigure Snowflake to enable data quality monitoring through Atlan.\nSet up Windsurf with Remote MCP\nLearn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication.\nTroubleshooting Atlan browser extension\nCan I add the browser extension for everyone in my organization?\nTroubleshooting SSO\nCan I change the username of a provisioned user in Atlan?"
  },
  {
    "url": "https://docs.atlan.com/cdn-cgi/l/email-protection",
    "content": "Please enable cookies.\nEmail Protection\nYou are unable to access this email address\natlan.com\nThe website from which you got to this page is protected by Cloudflare. Email addresses on that page have been hidden in order to keep them from being accessed by malicious bots.\nYou must enable Javascript in your browser in order to decode the e-mail address\n.\nIf you have a website and are interested in protecting it in a similar way, you can\nsign up for Cloudflare\n.\nHow does Cloudflare protect email addresses on website from spammers?\nCan I sign up for Cloudflare?\nCloudflare Ray ID:\n97d02f310cb3ff5b\n•\nYour IP:\nClick to reveal\n35.192.169.128\n•\nPerformance & security by\nCloudflare"
  },
  {
    "url": "https://docs.atlan.com/tags/support",
    "content": "2 docs tagged with \"support\"\nView all tags\nCustomer support\nLearn about customer support at Atlan.\nFrequently Asked Questions\nFind answers to common questions about using Atlan, organized by topic area for quick resolution."
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/data-and-metadata-persistence",
    "content": "Get Started\nCore Concepts\nData and metadata persistence\nOn this page\nData and metadata persistence\nAtlan is a fully virtualized solution that does not involve moving\ndata\nfrom existing storage layers. Atlan crawls\nmetadata\nfrom upstream data sources and stores it in a secure VPC (virtual private cloud).\nAtlan pushes any queries to existing processing layers. For example, directly to your database, warehouse, or a processing layer such as Athena or Presto on top of blob storage. So the\ndata\nitself stays put   -  Atlan does not move or store it.\nNot sure on the difference between\ndata\nand\nmetadata\n? Try our\nhelpful primer\n.\nData previews and queries\nâ\nAtlan gives users the ability to see sample data previews for a data asset as well as the results for any queries run on Atlan.\nIn both cases, Atlan pushes the request upstream to the data source, and shows a 100-row sample of the result to Atlan users. Atlan does not cache any of this data. So each time a user previews or queries data, it is re-queried from the source.\nEvery time a user runs a query, Atlan streams query results in batches directly from your data source. Since the data is streamed in real-time from the data source, there is no need to persist the query results in Atlan's cache or storage layer. This ensures that the data displayed is always up to date and accurate, eliminating the need for storing intermediate query results.\nMetadata storage\nâ\nAtlan stores the metadata it collects and creates in applications and databases within the VPC. This includes:\nasset metadata\nuser data\nAsset metadata\nâ\nAtlan stores asset metadata, including lineage, in:\nApache Atlas, a graph database layer that stores entity relationships and attributes\nElasticsearch, to optimize search on the product\nCassandra, as the persistence back-end\nUser data\nâ\nAtlan stores data on users, roles, and groups in its own PostgreSQL database. Keycloak uses this information for access and identity management.\nAtlan hashes all sensitive fields like passwords and stores them securely. Any user data transmitted over the internet is SSL-encrypted over HTTPS.\nTags:\nupstream-dependencies\ndata-sources\nPrevious\nAuthentication and authorization\nNext\nEncryption and key management\nData previews and queries\nMetadata storage"
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/encryption-and-key-management",
    "content": "Get Started\nCore Concepts\nEncryption and key management\nOn this page\nEncryption and key management\nAtlan has adopted global industry standards in security practices and solutions. Amazon S3 server-side encryption secures the S3 bucket launched by Atlan.\nAtlan uses AES-256 as the SSE algorithm in the S3 bucket. All the EBS (Elastic Block Storage) launched by Atlan is encrypted. Atlan uses encrypted storage classes to provision persistent volumes to the microservices running inside the Kubernetes cluster.\nKey and credential management\nâ\nAtlan uses\nHashiCorp Vault\nto manage the following:\nKeys   -  Vault manages encryption keys to encrypt sensitive data at rest and in transit.\nSecretsÂ   -  Vault encrypts and securely stores secrets such as API keys, tokens, and credentials.\nPasswords   -  passwords are hashed and stored encrypted.\nData in transit\nâ\nAtlan uses standard encryption to protect data in transit.\nAtlan uses hypertext transfer protocol secure (HTTPS) for secure communication when data is in transit. This protocol is encrypted using Transport Layer Security (TLS).\nTwo-factor authentication (2FA) is also supported for accessing resources.\nData at rest\nâ\nData-At-Rest Encryption (DARE) is the encryption of data stored in different storage components and not moving through networks.\nCloud storage\nâ\nAtlan encrypts the data at rest in different cloud resources like volumes and cloud storage using cloud provider-managed keys.\nAmazon S3   -  Atlan uses\nserver-side encryption with Amazon S3 managed keys (SSE-S3)\nto encrypt the data at rest in Amazon S3. This encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode (AES-GCM) to encrypt all uploaded objects.\nAzure Blob Storage   -  Atlan uses\nMicrosoft-managed keys\nto encrypt the data at rest in Azure Blob Storage. This encryption uses 256-bit AES encryption and is FIPS 140-2 compliant.\nGoogle Cloud Storage   -  Atlan uses\nGoogle-managed keys\nto encrypt the data at rest in Google Cloud Storage. This encryption uses AES-256 using Galois/Counter Mode (GCM) to encrypt all uploaded objects.\nVolumes\nâ\nVolumes are used by the StatefulSet running in the tenants. These volumes are encrypted at rest by the cloud provider-managed keys.\nAmazon Web Services (AWS)   -  Atlan uses the default\nAmazon Elastic Block Store (EBS) encryption\nfor encrypting the data at rest in all the volumes. Amazon EBS encrypts volume with a data key using industry-standard AES-256 data encryption.\nMicrosoft Azure   -  Atlan uses\nAzure Storage encryption\n, which uses server-side encryption (SSE), for encrypting the data at rest in all the volumes. Data in Azure managed disks is encrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant.\nGoogle Cloud Platform (GCP)   -  Atlan uses Google-managed encryption to encrypt the data at rest in all the volumes. This encryption uses the Advanced Encryption Standard (AES) algorithm, AES-256.\nOver the internet\nâ\nCommunication between the client and Atlan public endpoints is always conducted over hypertext transfer protocol secure (HTTPS). HTTPS is encrypted in order to increase the security of data transfer. Any user data transmitted over the internet is SSL-encrypted over HTTPS.\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nData and metadata persistence\nNext\nHigh availability and disaster recovery (HA/DR)\nKey and credential management\nData in transit\nData at rest"
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr",
    "content": "Get Started\nCore Concepts\nHigh availability and disaster recovery (HA/DR)\nOn this page\nHigh availability and disaster recovery (HA/DR)\nHigh availability (HA)\nâ\nAtlan uses Amazon Elastic Kubernetes Service (EKS) for high availability (HA).\nAmazon EKS\nruns and scales the Kubernetes control plane across multiple AWS Availability Zones to ensure high availability. It automatically scales control plane instances based on load, detects and replaces unhealthy control plane instances, and patches the control plane.\nThe benefits of using this concept are:\nScalability and reliability help the system remain stable\nPromotes self-healing to ensure that containers are running in a healthy state\nHandles node failures gracefully\nAuto-scaling enables automated cluster creation\nApplication HA\nâ\nAtlan ensures application HA through the following:\nMultiple replicas for both stateless and stateful applications\nLoad balancing with services\nRolling updates to maintain the availability threshold\nPod-to-node distribution to ensure that critical application pods are running on dedicated nodes\nUsage of inter-Availability Zone (AZ) data transfers to avoid single-zone failure impact\nDisaster recovery (DR)\nâ\nAtlan follows industry best practices for disaster recovery. Atlan uses Argo Workflows for orchestration to successfully implement a disaster recovery strategy and reduce production downtime, so that business impact is minimized in the event of an outage.\nIf a disaster is detected, the Disaster Assessment Team   -  comprising key stakeholders from IT, platform, operations, and support   -  will be promptly notified through Atlanâs established communication channels. The team will conduct a thorough evaluation to determine the extent of the damage and prioritize remediation based on an internal list of critical services and applications.\nIn case of a disaster, a tenant will be recreated, and the following actions performed to restore the tenant:\nOnboard a new tenant with no data and a different domain.\nUse Argo Workflows to restore the data from last backup.\nScale down the previous tenant.\nChange the domain of the new tenant to that of the previous one.\nUpdate the Cloudflare record with the load balancer of the newly onboarded tenant.\nAll the aforementioned action items are automated. The entire process of restoring all the components of a tenant from backup takes around 3-4 hours. In case of data loss for any particular component, it can also be recovered from the last backup.\nHere are a few parameters that help reduce downtime and expedite the process of disaster recovery:\nInfrastructure\nâ\nSingle-tenant SaaS\nis the default deployment option for most Atlan users. In this model, Atlan manages the infrastructure needs and ensures that all instances are spread across multiple\nAvailability Zones\n(AZ) in each AWS Region where the user instance is deployed.\nAvailability Zones are multiple, isolated locations within a single AWS Region.\nMulti-AZ\ndeployments provide enhanced availability for instances within a single AWS Region. With multi-AZ, your data is synchronously replicated to standby in a different Availability Zone.\ndanger\nAtlan currently does not support multi-region deployment.\nAtlan service overview\nâ\nThe diagram below illustrates the relationships and communication flows between each service.Â The bottom-most layer shows the services that are entirely independent, such as Cassandra, Postgres, and more. Most of the other services depend on these to function.Â\nBackups and restore\nâ\nAtlan runs an automated daily backup of each tenant. By default, the backup is scheduled at 3:00 AM UTC, configurable as per the requirement of an organization.\nThe backup of each tenant is stored in its respective cloud storage. The backups are encrypted at rest by the default cloud provider key. This key uses the Advanced Encryption Standard (AES) 256 algorithm. Since Atlan uses the cloud provider key, the key is rotated by the cloud provider.\nAtlan controls access to the cloud storage where the backup is stored, and only provides access in case of troubleshooting an issue. Each backup process captures a full backup of all the data, with no incremental backups being performed. Atlan also monitors the backup to ensure that backups are not skipped. Alerts are generated in case a backup run fails for the support team to examine the issue.\nThe lifecycle policy for backups in the cloud provider is set to 15 days, which means Atlan will retain backups for all the components for 15 days.\nBackups of the following components are taken on a daily basis:\nArgo Workflows\nElasticsearch\nCassandra\nRedis\nPostgres\nAtlan can restore a single component in case of data corruption for any single point of failure, such as a metastore and its components like Elasticsearch and Cassandra. It is also possible to do a full-cluster restore in case of an unintended operation or a data loss or corruption event.\nDid you know?\nArgo Workflows\npowers all the backup and restore packages in Atlan. It includes a retry mechanism in case of any errors while completing the steps in the workflow. It also sends alerts in case of entire package failure as part of observability.\nMigration\nâ\nAtlan has an easy process to migrate the application to other AWS Regions. In case of total region outage and the need for migrating an instance to another region or account, this migration activity will be performed via Atlanâs backup and restore packages.\nRTO, RPO, and retention\nâ\nGreater RTOs and RPOs as well as system recovery are crucial for ensuring that multiple mission-critical applications are quickly restored. It is now possible to minimize the impact of a disruption and perform a recovery within a few hours of an outage.\nAtlan carries out a daily backup of all critical services once every 24 hours, so in a worst case scenario provides an RPO of 24 hours.\nFor all critical applications, RTO is less than 3 hours.\nAtlan retains daily backups for 15 days.\nPost-recovery validation\nâ\nThe following post-recovery actions are performed:\nPost restoration, Atlan conducts data integrity checks to ensure that the restored data is accurate and complete.\nAtlan performs system tests to confirm that all components of the tenant are functioning correctly after restoration.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nEncryption and key management\nNext\nCloud logging and monitoring\nHigh availability (HA)\nDisaster recovery (DR)\nRTO, RPO, and retention\nPost-recovery validation"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-azure-ad-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGet Started\nHow to enable Azure AD for SSO\nOn this page\nEnable  Azure AD for SSO\nWho can do this?\nYou will need to be an admin user within Atlan to configure SSO. You will also need to work with your Azure AD administrator to carry out the tasks below in Azure AD.\ndanger\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over\nhere\n.\nTo integrate Azure AD SSO for Atlan, complete the\nfollowing steps.\nChoose SSO provider (in Atlan)\nâ\nTo choose Azure AD as your SSO provider, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nAzure AD\nand then clickÂ\nConfigure\n.\nUnderÂ\nService provider metadata\n, copy theÂ\nIdentifier (Entity ID)\n,\nReply URL (Assert Consumer Service URL)\n, and\nLogout Url\n.\nSet up SAML app (in Azure AD)\nâ\nTo set up a SAML app, within\nAzure's portal\n:\nFrom the menu on the left, open\nAzure Active Directory\n.\nUnderÂ\nAzure Active Directory | Overview\nclick theÂ\nAdd\nbutton and thenÂ\nEnterprise application\n.\nUnderÂ\nBrowser Azure AD Gallery\nclick theÂ\nCreate your own application\nbutton:\nForÂ\nWhat's the name of your app?\nenter a name, such asÂ\nAtlan\n.\nForÂ\nWhat are you looking to do with your application?\nselectÂ\nIntegrate any other application you don't find in the gallery (Non-gallery)\n.\nAt the bottom of theÂ\nCreate your own application\ndialog, click the\nCreate\nbutton.\nWait for the application details to be shown   -  this can take around 1 minute.\nUnderÂ\nGetting Started\nand within the\nSet up single sign on\ntile, click the\nGet started\nlink.\nUnderÂ\nSelect a single sign-on method\nclick theÂ\nSAML\ntile.\nIn the upper-right of the\nBasic SAML Configuration\ncard, click the\nEdit\nbutton and enter:\nForÂ\nIdentifier (Entity ID)\nclick\nAdd identifier\nand enter the value you copied from Atlan above.\nForÂ\nReply URL (Assertion Consumer Service URL)\nclick\nAdd reply URL\n(twice) and enter the two values you copied from Atlan above. The longer URL should be enabled under the\nDefault\ncolumn.\nForÂ\nLogout Url (Optional)\nenter the value you copied from Atlan above.\nAt the top of the page, underÂ\nBasic SAML Configuration\n, click theÂ\nSave\nbutton.\nClose theÂ\nBasic SAML Configuration\ndialog by clicking\nX\nin the upper-right.\nIn the upper-right of the\nAttributes & Claims\ncard, click the\nEdit\nbutton:\nNavigate to the\nAdditional claims\nsection, click each of the following claims to modify their\nName\nexactly as suggested below and remove the\nNamespace\nvalue:\nemail   - > user.mail\nfirstName   - > user.givenname\nlastName   - > user.surname\n(Optional) username   - > ExtractMailPrefix(user.mail)\ninfo\nðª\nDid you know?\nFor users assigned to Atlan through SSO, the username will be populated from the username mapping. Otherwise, the username will be the email prefix by default, which users can update while registering on Atlan for the first time.\nTo configure group claims:Â\nFrom the options along the top, click\n+ Add a group claim\n.\nIn the popover, under\nWhich groups associated with the user should be returned in the claim?\n, select\nGroups assigned to the application\n.\nFrom the\nSource attribute\ndropdown, select\nCloud-only group display names (Preview)\n. If you have a\nhybrid setup\n, select\nsAMAccountName\ninstead and then check the\nEmit group name for cloud-only groups\ncheckbox.\ndanger\nPlease ensure that the\nCloud-only group display names\nattribute contains the actual group display names. If not, you will need to update the source attribute with the relevant one that contains group display names.\nClick\nAdvanced options\nto expand the dropdown menu:\nCheck the\nCustomize the name of the group claim\nbox.\nFor\nName\n, enter\nmemberOf\n. This is required if you want to retain group membership in Atlan.\nClick\nSave\nand close the popover by clickingÂ\nX\nin the upper-right.\nDownload Azure AD's metadata file (in Azure AD)\nâ\nTo download Azure AD's metadata file, within the same Azure AD app's\nSAML-based Sign-on\npage:\nWithin theÂ\nSAML Signing Certificate\ncard, to the right ofÂ\nFederation Metadata XML\n, click theÂ\nDownload\nlink.\nWithin theÂ\nSet up\n<application>\ncard, copy theÂ\nLogout URL\n.\nAssign users or groups to the app (in Azure AD)\nâ\nTo assign users or groups to the app, within the Azure AD application's page:\nUnderÂ\nManage,\nclick\nUsers and groups\n.\nAt the top of the table, click theÂ\nAdd user/group\nbutton.\nIn the resulting\nAdd Assignment\ndialog, under the\nUsers\nor\nGroups\nÂ heading, click the\nNone Selected\nlink.\nIn the resulting\nUsers\nor\nGroups\nÂ dialog, search for users or groups to add and click to select them.\nWhen finished, at the bottom of theÂ\nUsers\nor\nGroups\ndialog, click the\nSelect\nbutton.\nAt the bottom of theÂ\nAdd Assignment\ndialog, click theÂ\nAssign\nbutton.\nUpload Azure AD's metadata file (in Atlan)\nâ\nTo complete the configuration of Azure AD SSO, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nAzure AD\nand then clickÂ\nConfigure\n.\nTo the right ofÂ\nIdentity provider metadata\nclick the\nImport from XML\nbutton.\nSelect the XML file downloaded from Azure AD above.\nUnderÂ\nSingle Logout Service URL\n, enter the logout URL copied from Azure AD above.\nAt the bottom of the screen, clickÂ\nSave\n.\nCongratulations   -  you have successfully set up Azure AD SSO in Atlan! ð\nDid you know?\nBy default, users can now log into Atlan with either Azure AD SSO or a local Atlan account (via email). To only allow logins via SSO, enable the\nEnforce SSO\noption in Atlan. Once SSO is enforced, we recommend asking your Azure AD administrator to provision access to users through the Azure portal and\nnot directly from Atlan\n. When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically.\n(Optional) Configure group mappings\nâ\ndanger\nBefore you can configure group mapping, you will first need to\ncreate groups in Atlan\nthat correspond to the groups you want to map from Azure AD to Atlan. In addition, you must configure the\nmemberOf\nattribute and group mapping to retain group membership in Atlan   -  irrespective of whether or not you\nenable SCIM\n.\nTo automatically assign Azure AD users to Atlan groups based on their Azure AD groups, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nChange to the\nGroups Mapping\ntab.\nTo the right of each Atlan group listed:\nUnder the\nSSO Groups\ncolumn, type the name of the corresponding group in Azure AD to map to the Atlan group on that row   -  for example,\nData Engineering\n,\nBusiness Analysts\n, and so on. You will need to provide each Azure AD group with access to Atlan.\nClick theÂ\nSave\nbutton on that row.\nAs each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð\nDid you know?\nYou can configure\nSCIM provisioning in Azure AD\nto manage your users and groups in Atlan. Plus, once you've configured group mapping, you can add the mapped groups to a\npersona\nor\npurpose\nto auto-assign relevant permissions to users as they sign up in Atlan.\nTags:\nintegration\nconnectors\nPrevious\nSSO Integration\nNext\nHow to enable Google for SSO\nChoose SSO provider (in Atlan)\nSet up SAML app (in Azure AD)\nDownload Azure AD's metadata file (in Azure AD)\nAssign users or groups to the app (in Azure AD)\nUpload Azure AD's metadata file (in Atlan)\n(Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-google-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGet Started\nHow to enable Google for SSO\nOn this page\nEnable  Google for SSO\nWho can do this?\nYou will need to be an admin user within Atlan to configure SSO. You will also need to work with your Google domain administrator to carry out the tasks below in the Google Admin Center.\ndanger\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over\nhere\n.\nTo integrate Google SSO for Atlan, complete the fo\nllowing steps.\nChoose SSO provider (in Atlan)\nâ\nTo choose Google as your SSO provider, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nGoogle\nand then clickÂ\nConfigure\n.\nUnderÂ\nService provider metadata\n, copy theÂ\nACS URL\nandÂ\nEntity ID\n.\nSet up SAML app (in Google Admin Center)\nâ\nTo set up a SAML app, within\nGoogle Admin Center\n:\nFrom the menu on the left, expandÂ\nApps\nand then click onÂ\nWeb and mobile apps\n.\nAt the top of the table, click theÂ\nAdd app\nlink and then clickÂ\nAdd custom SAML app\n.\nEnter a name for your app, such asÂ\nAtlan\nand then click the\nContinue\nÂ button.\nUnderÂ\nOption 1: Download IdP metadata\nclick theÂ\nDownload metadata\nbutton, save the file, and then click the\nContinue\nbutton.\nUnder\nService provider details\n, enter your Atlan SAML settings:\nForÂ\nACS URL\n, enter the value you copied from Atlan above.\nForÂ\nEntity ID\n, enter the value you copied from Atlan above.\nClick theÂ\nContinue\nbutton.\nUnderÂ\nAttributes\n, define the following mappings from\nGoogle Directory attributes\non the left to\nApp attributes\non the right:\nPrimary email   - > email\nFirst name   - > firstName\nLast name   - > lastName\n(Optional) To\nconfigure group mapping in Atlan\n, under\nGroup membership (optional)\n, enter the following details:\nFor\nGoogle Groups\n, select all the Google groups you want to map to Atlan. You can select up to 75 groups in total.\nFor\nApp attribute\n, enter\nmemberOf\n. This is required if you want to retain group membership in Atlan.\nAt the end of the form, click the\nFinish\nbutton.\nAssign users to the app (in Google Admin Center)\nâ\nTo assign users to the app, within\nGoogle Admin Center\n:\nFrom the app page, expand\nUser access\n.\nUnderÂ\nService status\nchange toÂ\nON for everyone\nÂ and then clickÂ\nSave\n.\nUpload Google's metadata file (in Atlan)\nâ\nTo complete the configuration of Google SSO, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nGoogle\nand then clickÂ\nConfigure\n.\nTo the right ofÂ\nIdentity provider metadata\nclick the\nImport from XML\nbutton.\nSelect the\nGoogleIDPMetadata.xml\nfile downloaded from Google above.\nAt the bottom of the screen, clickÂ\nSave\n.\nCongratulations   -  you have successfully set up Google SSO in Atlan! ð\nDid you know?\nBy default, users can now log into Atlan with either Google SSO or a local Atlan account (via email). To only allow logins via SSO, enable the\nEnforce SSO\noption in Atlan. Once SSO is enforced, we recommend asking your Google domain administrator to provision access to users through the Google Admin Center and\nnot directly from Atlan\n. When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically.\n(Optional) Configure group mappings\nâ\ndanger\nBefore you can configure group mapping, you will first need to\ncreate groups in Atlan\nthat correspond to the groups you want to map from Google to Atlan. In addition, you must configure the\nmemberOf\nattribute and group mapping to retain group membership in Atlan.\nTo automatically assign Google users to Atlan groups based on their Google groups, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nChange to the\nGroups Mapping\ntab.\nTo the right of each Atlan group listed:\nUnder theÂ\nSSO Groups\ncolumn, type the name of the corresponding group in Google to map to the Atlan group on that row   -  for example,\nData Engineering\n,\nBusiness Analysts\n, and so on. You will need to provide each Google group with access to Atlan.\nClick theÂ\nSave\nbutton on that row.\nAs each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð\nDid you know?\nOnce you've configured group mapping, you can add the mapped groups to a\npersona\nor\npurpose\nto auto-assign relevant permissions to users as they sign up in Atlan.\nTags:\nintegration\nconnectors\nPrevious\nHow to enable Azure AD for SSO\nNext\nHow to enable JumpCloud for SSO\nChoose SSO provider (in Atlan)\nSet up SAML app (in Google Admin Center)\nAssign users to the app (in Google Admin Center)\nUpload Google's metadata file (in Atlan)\n(Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-jumpcloud-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGet Started\nHow to enable JumpCloud for SSO\nOn this page\nEnable  JumpCloud for SSO\nWho can do this?\nYou will need to be an admin user within Atlan to configure SSO. You will also need to work with your JumpCloud administrator to carry out the tasks below in JumpCloud.\ndanger\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over\nhere\n.\nTo integrate JumpCloud SSO for Atlan, complete the\nfollowing steps.\nChoose SSO provider (in Atlan)\nâ\nTo choose JumpCloud as your SSO provider, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nJumpcloud\nand then clickÂ\nConfigure\n.\nUnderÂ\nService provider metadata\n, copy the\nIdP Entity ID\n,\nSP Entity ID\n, and\nACS URL\n.\nSet up SAML app (in JumpCloud)\nâ\nTo set up a SAML app, within\nJumpCloud Console\n:\nFrom the menu on the left, under\nUser Authentication\nclick\nSSO\n.\nTo the left of the search box, click the large circular\n+\nicon.\nAt the bottom of the page, click theÂ\nCustom SAML App\nbutton.\nUnder theÂ\nGeneral Info\ntab, forÂ\nDisplay Label\nenter a name such asÂ\nAtlan\n.\nChange to the\nSSO\ntab and enter your Atlan SAML settings:\nForÂ\nIdP Entity ID\nenter the value you copied from Atlan above.\nForÂ\nSP Entity ID\nenter the value you copied from Atlan above.\nForÂ\nACS URL\nenter the value you copied from Atlan above.\nBelowÂ\nSignature Algorithm\nensureÂ\nSign Assertion\nis enabled.\nScroll to the bottom of theÂ\nSSO\ntab and underÂ\nUser Attribute Mapping\nclick theÂ\nadd attribute\nbutton. Define the following mappings fromÂ\nService Provider Attribute Name\non the left toÂ\nJumpCloud Attribute Name\non the right:\nemail   - > email\nfirstName   - > firstname\nlastName   - > lastname\ngroup   - > group (you may need to selectÂ\nCustom User or Group Attribute\nfrom the\nJumpCloud Attribute Name\ndrop-down, and then type in\ngroup\n)\nusername   - > username\nUnder theÂ\nGroup Attributes\nheading, enable theÂ\ninclude group attribute\nbox and enter the value\nmemberOf\n. This is required if you want to retain group membership in Atlan.\nChange to theÂ\nUser Groups\ntab and check the box for each user group you want to be enabled for SSO.\nBelow the form, click theÂ\nactivate\nbutton and when prompted click the\ncontinue\nbutton.\nDownload JumpCloud metadata file (in JumpCloud)\nâ\nTo download the JumpCloud metadata file, within\nJumpCloud Console\n:\nFrom the SSO app page, click your Atlan SSO application to open it.\nChange to the\nSSO\ntab and under\nJumpCloud Metadata\nclick the\nExport Metadata\nbutton.\nUpload JumpCloud's metadata file (in Atlan)\nâ\nTo complete the configuration of JumpCloud SSO, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nJumpcloud\nand then clickÂ\nConfigure\n.\nTo the right ofÂ\nIdentity provider metadata\nclick the\nImport from XML\nbutton.\nSelect the\nJumpCloud-saml2-metadata.xml\nfile downloaded from JumpCloud above.\nAt the bottom of the screen, clickÂ\nSave\n.\nCongratulations   -  you have successfully set up JumpCloud SSO in Atlan! ð\nDid you know?\nBy default, users can now log into Atlan with either JumpCloud SSO or a local Atlan account (via email). To only allow logins via SSO, enable the\nEnforce SSO\noption in Atlan. Once SSO is enforced, we recommend asking your JumpCloud administrator to provision access to users through JumpCloud and\nnot directly from Atlan\n. When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically.\n(Optional) Configure group mappings\nâ\ndanger\nBefore you can configure group mapping, you will first need to\ncreate groups in Atlan\nthat correspond to the groups you want to map from JumpCloud to Atlan. In addition, you must configure the\nmemberOf\nattribute and group mapping to retain group membership in Atlan.\nTo automatically assign JumpCloud users to Atlan groups based on their JumpCloud groups, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nChange to the\nGroups Mapping\ntab.\nTo the right of each Atlan group listed:\nUnder the\nSSO Groups\ncolumn, type the name of the corresponding group in JumpCloud to map to the Atlan group on that row   -  for example,\nData Engineering\n,\nBusiness Analysts\n, and so on. You will need to provide each JumpCloud group with access to Atlan.\nClick theÂ\nSave\nbutton on that row.\nAs each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð\nDid you know?\nOnce you've configured group mapping, you can add the mapped groups to a\npersona\nor\npurpose\nto auto-assign relevant permissions to users as they sign up in Atlan.\nTags:\nintegration\nconnectors\nPrevious\nHow to enable Google for SSO\nNext\nHow to enable Okta for SSO\nChoose SSO provider (in Atlan)\nSet up SAML app (in JumpCloud)\nDownload JumpCloud metadata file (in JumpCloud)\nUpload JumpCloud's metadata file (in Atlan)\n(Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-okta-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGet Started\nHow to enable Okta for SSO\nOn this page\nEnable  Okta for SSO\nWho can do this?\nYou will need to be an admin user within Atlan to configure SSO. You will also need to work with your Okta administrator to carry out the tasks below in Okta.\ndanger\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over\nhere\n.\nTo integrate Okta SSO for Atlan, complete the foll\nowing steps.\nChoose SSO provider (in Atlan)\nâ\nTo choose Okta as your SSO provider, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nOkta\nand then clickÂ\nConfigure\n.\nUnderÂ\nService provider metadata\n, copy theÂ\nSingle sign on URL\nÂ andÂ\nAudience URI (SP Entity ID)\n.\nSet up SAML app (in Okta)\nâ\nTo set up a SAML app, within Okta's administration console:\nFrom the menu on the left, expandÂ\nApplications\nand then click onÂ\nApplications\n.\nAt the top of the table, click theÂ\nCreate App Integration\nbutton.\nIn theÂ\nCreate a new app integration\ndialog, selectÂ\nSAML 2.0\nand then click\nNext\n.\nUnderÂ\nGeneral Settings\nenter:\nForÂ\nApp name\n, enter a name for the application, such asÂ\nAtlan\n.\nClick theÂ\nNext\nbutton.\nUnderÂ\nSAML Settings - General\nenter:\nForÂ\nSingle sign on URL\nenter the value you copied from the field of the same name in Atlan above.\nEnsureÂ\nUse this for Recipient URL and Destination URL\nis enabled.\nForÂ\nAudience URI (SP Entity ID)\nenter the value you copied from the field of the same name in Atlan above.\nUnderÂ\nAttribute Statements (optional)\ndefine the following mappings from\nName (Name format)\non the left to\nValue\non the right:\nfirstName (Basic)   - > user.firstName\nlastName (Basic)   - > user.lastName\nemail (Basic)   - > user.email\ngroup (Basic)   - > user.group\ninfo\nðª\nDid you know?\nFor users assigned to Atlan through SSO, the username will be populated from the username mapping. Otherwise, the username will be the email prefix by default, which users can update while registering on Atlan for the first time.\nUnderÂ\nGroup Attribute Statements (optional for SSO login, required for group sync)\ndefine the following mappings fromÂ\nName (Name format)\non the left to\nFilter\non the right:\nmemberOf (Unspecified)   - > Matches regex\n[\\s\\S]+\n-  for examples of how to filter groups with regex in Okta, refer to\nOkta documentation\n. This is required if you want to retain group membership in Atlan.\nWhile this step is optional for basic SSO authentication, you must configure the memberOf attribute if you want to sync Okta groups to Atlan and use group-based access control\nAt the bottom of the form, click the\nNext\nbutton.\nUnderÂ\nHelp Okta Support understand how you configured this application\nselect\nI'm an Okta customer adding an internal app\nand forÂ\nApp type\nenableÂ\nThis is an internal app that we have created\n.\nClick theÂ\nFinish\nbutton.\nDownload Okta's metadata file (in Okta)\nâ\nTo download Okta's metadata file, within the Okta app's page:\nOpen theÂ\nSign On\ntab.\nUnder theÂ\nSAML Signing Certificates\nheading, in the table, click theÂ\nActions\nlink under theÂ\nActions\ncolumn.\nFrom the drop-down, clickÂ\nView IdP metadata\n.\nSave the XML file, if it appears in plain text in your browser.\nAssign users to the app (in Okta)\nâ\nTo assign users to the app, within the Okta app's page:\nOpen theÂ\nAssignments\ntab.\nAt the top of the table, click theÂ\nAssign\nbutton and select\nAssign to People\nto add individual users or\nAssign to Groups\nto add groups.\nTo the right of each user to whom you want to assign the application, click\nAssign\n. To assign the application to a group, you may have to locate it first.\nFor individual users, confirm that the data is correct in the\nAssign Atlan to People\ndialog. For groups, complete the fields in the\nAssign Atlan to Groups\ndialog if it appears.\nClick\nSave and Go Back\n. Repeat steps 3 to 5 for each user or group to which you want to assign the application.\nWhen finished, in the respective dialog box, click\nDone\n.\nUpload Okta's metadata file (in Atlan)\nâ\nTo complete the configuration of Okta SSO, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nOkta\nand then clickÂ\nConfigure\n.\nTo the right ofÂ\nIdentity provider metadata\nclick the\nImport from XML\nbutton.\nSelect the XML file saved from Okta above.\nAt the bottom of the screen, clickÂ\nSave\n.\nCongratulations   -  you have successfully set up Okta SSO in Atlan! ð\nDid you know?\nBy default, users can now log into Atlan with either Okta SSO or a local Atlan account (via email). To only allow logins via SSO, enable the\nEnforce SSO\noption in Atlan. Once SSO is enforced, we recommend asking your Okta administrator to provision access to users through Okta and\nnot directly from Atlan\n. When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically.\n(Optional) Configure group mappings\nâ\ndanger\nBefore you can configure group mapping, you will first need to\ncreate groups in Atlan\nthat correspond to the groups you want to map from Okta to Atlan. In addition, you must configure the\nmemberOf\nattribute and group mapping to retain group membership in Atlan   -  irrespective of whether or not you\nenable SCIM\n.\nTo automatically assign Okta users to Atlan groups based on their Okta groups, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nChange to the\nGroups Mapping\ntab.\nTo the right of each Atlan group listed:\nUnder theÂ\nSSO Groups\ncolumn, type the name of the corresponding group in Okta to map to the Atlan group on that row   -  for example,\nData Engineering\n,\nBusiness Analysts\n, and so on. You will need to provide each Okta group with access to Atlan.\nClick theÂ\nSave\nbutton on that row.\nAs each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð\nDid you know?\nYou can configure\nSCIM provisioning in Okta\nto manage your users and groups in Atlan. Plus, once you've configured group mapping, you can add the mapped groups to a\npersona\nor\npurpose\nto auto-assign relevant permissions to users as they sign up in Atlan.\nTags:\nintegration\nconnectors\nPrevious\nHow to enable JumpCloud for SSO\nNext\nHow to enable OneLogin for SSO\nChoose SSO provider (in Atlan)\nSet up SAML app (in Okta)\nDownload Okta's metadata file (in Okta)\nAssign users to the app (in Okta)\nUpload Okta's metadata file (in Atlan)\n(Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-onelogin-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGet Started\nHow to enable OneLogin for SSO\nOn this page\nEnable  OneLogin for SSO\nWho can do this?\nYou will need to be an admin user within Atlan to configure SSO. You will also need to work with your OneLogin administrator to carry out the tasks below in OneLogin.\ndanger\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over\nhere\n.\nTo integrate OneLogin SSO for Atlan, complete the\nfollowing steps.\nChoose SSO provider (in Atlan)\nâ\nTo choose OneLogin as your SSO provider, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nOneLogin\nand then clickÂ\nConfigure\n.\nUnderÂ\nService provider metadata\n, copy theÂ\nAudience (EntityID)\n,\nRecipient\n,\nACS (Consumer) URL Validator\n, and\nACS (Consumer) URL\n.\nSet up SAML application (in OneLogin)\nâ\nTo set up a SAML application, within OneLogin admin console:\nFrom the menu along the top, navigate to\nApplications\nand then click onÂ\nApplications\n.\nIn the upper right, click the\nAdd App\nbutton.\nIn the search box, enterÂ\nsaml custom\nand then click\nSAML Custom Connector (Advanced)\n.\nUnderÂ\nDisplay Name\nenter a name for your app, such asÂ\nAtlan\nand then click theÂ\nSave\nbutton.\nChange to theÂ\nConfiguration\ntab and underÂ\nApplication details\nenter your Atlan SAML settings:\nForÂ\nAudience (EntityID)\nenter the value you copied from Atlan above.\nForÂ\nRecipient\nenter the value you copied from Atlan above.\nForÂ\nACS (Consumer) URL Validator\nenter the value you copied from Atlan above.\nForÂ\nACS (Consumer) URL\nenter the value you copied from Atlan above.\nForÂ\nLogin URL\nenter the same value used for the fields above.\nChange to theÂ\nSSO\ntab and change the following:\nForÂ\nSAML Signature Algorithm\nsetÂ\nSHA-512\n.\nUnderÂ\nLogin Hint\nensureÂ\nEnable login hint\nis checked.\nChange to the\nParameters\ntab and use the circular\n+\nicon to add mappings for the following:\nemail   - > Email\nfirstName   - > First Name\nlastName   - > Last Name\nIn the upper right, click the\nSave\nbutton.\nDownload OneLogin's metadata file (in OneLogin)\nâ\nTo download the metadata file for the application, within OneLogin:\nFrom the application page, in the upper right navigate to\nMore Actions\nand click\nSAML Metadata\n.\n(Optional) Map groups to the app (in OneLogin)\nâ\nTo map OneLogin groups to the app, within the OneLogin application:\nIn the top left, click the\nUsers\ntab, and from the dropdown, select\nMappings\n.\nUnder\nMappings\n, click\nNew Mapping\nto create a new group mapping for Atlan.\nIn the\nNew Mapping\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for your group mapping   -  for example,\nSSOGroupA\n.\nUnder\nConditions\n, click the\n+\nbutton and enter the following details:\nFrom the attributes dropdown, select\nGroup\nto map all your OneLogin groups to Atlan.Â\nFrom the operators dropdown, select\nis\n.\nFrom the values dropdown, select the group name.\nUnder\nActions\n, enter the following details:\nFrom the\nSet role\ndropdown, select\nSet memberOf\n. This is required if you want to retain group membership in Atlan.\nFrom the\nSet memberOf to\ndropdown, enter the group name.\nClick\nSave\nto confirm your selections.\nUnder\nMappings\n, click\nNew Mapping\nto remove any group mappings if none are selected.\nIn the\nNew Mapping\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for your group mapping   -  for example,\nclearMemberOf\n.\nUnder\nConditions\n, click the\n+\nbutton and enter the following details:\nFrom the attributes dropdown, select\nGroup\n.Â\nFrom the operators dropdown, select\nis\n.\nFrom the values dropdown, keep the default selection\nNone\n.\nUnder\nActions\n, enter the following details:\nFrom the\nSet role\ndropdown, select\nSet memberOf\n.\nFrom the\nSet memberOf to\ndropdown, leave as blank.\nClick\nSave\nto confirm your selections.\nUnder\nMappings\n, click the\nReapply All Mappings\ntab, and in the corresponding screen, click\nContinue\nto confirm.\nIn the top left, click the\nApplications\ntab, and from the dropdown, click\nApplications\n.\nUnder\nApplications\n, select your SAML application.\nFrom the left menu of\nSAML Custom Connector (Advanced)\n, click\nParameters\n.\nIn the upper right of the parameters page, click the\n+\nbutton to add a new parameter.\nIn the\nNew Field\ndialog, enter the following details:\nFor\nField name\n, enter\nmemberOf\n.\nFor\nFlags\n, check the\nInclude in SAML assertion\nbox.\nClick\nSave\nto proceed to the next step.\nIn the corresponding\nEdit Field memberOf\ndialog, from the\nValue\ndropdown, select\nMemberOf\n.\nClick\nSave\nto confirm your selections.\nIf any of your OneLogin users do not belong to any groups, you can either add them to an existing group or create a new one. Once you have\nconfigured group mapping in Atlan\n, they will be able to log in to Atlan and assigned the same permissions as their OneLogin group.\nUpload OneLogin's metadata file (in Atlan)\nâ\nTo complete the configuration of OneLogin SSO, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nOneLogin\nand then clickÂ\nConfigure\n.\nTo the right ofÂ\nIdentity provider metadata\nclick the\nImport from XML\nbutton.\nSelect the\nonelogin_metadata_1234567.xml\nfile downloaded from OneLogin above.\nAt the bottom of the screen, clickÂ\nSave\n.\nCongratulations   -  you have successfully set up OneLogin SSO in Atlan! ð\nDid you know?\nBy default, users can now log into Atlan with either OneLogin SSO or a local Atlan account (via email). To only allow logins via SSO, enable the\nEnforce SSO\noption in Atlan. Once SSO is enforced, we recommend asking your OneLogin administrator to provision access to users through OneLogin and\nnot directly from Atlan\n. When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically.\n(Optional) Configure group mappings\nâ\ndanger\nBefore you can configure group mapping, you will first need to\ncreate groups in Atlan\nthat correspond to the groups you want to map from OneLogin to Atlan. In addition, you must configure the\nmemberOf\nattribute and group mapping to retain group membership in Atlan.\nTo automatically assign OneLogin users to Atlan groups based on their OneLogin groups, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nChange to the\nGroups Mapping\ntab.\nTo the right of each Atlan group listed:\nUnder the\nSSO Groups\ncolumn, type the name of the corresponding group in OneLogin to map to the Atlan group on that row   -  for example,\nData Engineering\n,\nBusiness Analysts\n, and so on. You will need to provide each OneLogin group with access to Atlan.\nClick theÂ\nSave\nbutton on that row.\nAs each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð\nDid you know?\nOnce you've configured group mapping, you can add the mapped groups to a\npersona\nor\npurpose\nto auto-assign relevant permissions to users as they sign up in Atlan.\nTags:\nintegration\nconnectors\nPrevious\nHow to enable Okta for SSO\nNext\nHow to enable SAML 2.0 for SSO\nChoose SSO provider (in Atlan)\nSet up SAML application (in OneLogin)\nDownload OneLogin's metadata file (in OneLogin)\n(Optional) Map groups to the app (in OneLogin)\nUpload OneLogin's metadata file (in Atlan)\n(Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-saml-2-0-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGet Started\nHow to enable SAML 2.0 for SSO\nOn this page\nEnable  SAML 2.0 for SSO\nWho can do this?\nYou will need to be an admin user within Atlan to configure SSO. You will also need to work with your SAML 2.0 administrator to carry out the tasks below in your custom IdP.\ndanger\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over\nhere\n.\nTo integrate SAML 2.0 SSO for Atlan, complete the\nfollowing steps.\nChoose SSO provider (in Atlan)\nâ\nTo choose SAML 2.0 as your SSO provider, within Atlan:\nFrom the left menu on any screen, navigate to\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nSSO\n.\nUnder\nChoose SAML provider\n, select\nSAML 2.0\nand then click\nConfigure\n.\nFor\nAlias\n, type in an alias for the SAML 2.0 connection and then click\nNext\n.\nUnder\nService provider metadata\n, copy the\nAtlan SAML Assertion URL\nand\nAtlan Audience URI (SP Entity ID)\n.\nSet up SAML app (in custom IdP)\nâ\nIf you have PingFederate as your IdP, refer to\nSSO integration with PingFederate using SAML\nfor the SAML assertion URL to use.\nTo set up a SAML app within your custom IdP:\nCreate a new SAML application in your IdP with the name\nAtlan\n.\nFor\nEntity/Issuer ID\n, enter the\nAtlan Audience URI (SP Entity ID)\nvalue you copied from above.\nFor\nAssertion Consumer Service (ACS) URL\n, enter the\nAtlan SAML Assertion URL\nvalue you copied from above.\nAdd the required users and groups to the application.\nConfigure the IdP to return the following attributes in the SAML response:\nfirstName\nlastName\nemail\nmemberOf\n(listing the user's group memberships, which will be required for group mapping in Atlan)\nSave the SAML metadata XML file for the SSO URL and X.509 public certificate file of the IdP.\ndanger\nThe SSO URL must be accessible from Atlan via an internet connection.\nConfigure IdP details (in Atlan)\nâ\nTo complete the configuration of SAML 2.0 SSO, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nUnderÂ\nChoose SAML provider\n, selectÂ\nSAML 2.0\nand then clickÂ\nConfigure\n.\nFor\nAlias\n, type in an alias for the SAML 2.0 connection and then click\nNext\n.\nTo the right ofÂ\nIdentity provider metadata,\nclick the\nImport from XML\nbutton.\nSelect the XML file saved from the IdP above.\nFor\nAttribute Mapper\n, modify the IdP attribute names for email, first name, and last name if these will be different in the IdP SAML response.\n(Optional) For\nCustomize\n, under\nSign in button text\n, type any custom message you'd like your users to see on the Atlan login screen.Â\nAt the bottom of the screen, click\nSave\n.\nCongratulations   -  you have successfully set up SSO for your custom IdP in Atlan! ð\nDid you know?\nBy default, users can now log into Atlan with either SAML 2.0 SSO or a local Atlan account (via email). To only allow logins via SSO, enable the\nEnforce SSO\noption in Atlan. Once SSO is enforced, we recommend asking your SAML 2.0 administrator to provision access to users through your custom IdP and\nnot directly from Atlan\n. When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically.\n(Optional) Configure group mappings\nâ\ndanger\nBefore you can configure group mapping, you will first need to\ncreate groups in Atlan\nthat correspond to the groups you want to map from your custom IdP to Atlan. In addition, you must configure the\nmemberOf\nattribute and group mapping to retain group membership in Atlan.\nTo automatically assign SSO users to Atlan groups based on their custom IdP groups, within Atlan:\nFrom the left menu on any screen, navigate toÂ\nAdmin\n.\nUnder theÂ\nWorkspace\nheading, clickÂ\nSSO\n.\nChange to the\nGroups Mapping\ntab.\nTo the right of each Atlan group listed:\nUnder the\nSSO Groups\ncolumn, type the name of the corresponding group in your custom IdP to map to the Atlan group on that row   -  for example,\nData Engineering\n,\nBusiness Analysts\n, and so on. You will need to provide each custom IdP group with access to Atlan.\nClick theÂ\nSave\nbutton on that row.\nAs each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð\nDid you know?\nOnce you've configured group mapping, you can add the mapped groups to a\npersona\nor\npurpose\nto auto-assign relevant permissions to users as they sign up in Atlan.\nTags:\nintegration\nconnectors\nPrevious\nHow to enable OneLogin for SSO\nNext\nAuthenticate SSO credentials to query data\nChoose SSO provider (in Atlan)\nSet up SAML app (in custom IdP)\nConfigure IdP details (in Atlan)\n(Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/enable-azure-ad-for-scim-provisioning",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSCIM\nHow to enable Azure AD for SCIM provisioning\nOn this page\nEnable  Azure AD for SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with\nSystem for Cross-domain Identity Management\n(SCIM).\nTo enable Azure AD for SCIM provisioning, complete the following steps.\nDid you know?\nFor any questions about SCIM provisioning, head over\nhere\n.\nPrerequisites\nâ\nAzure AD SSO must be enabled for Atlan\n.\nAzure AD users or groups must be assigned to Atlan\n.\nGroup mapping must be configured\n, only required if syncing mapped groups from Azure AD to Atlan. For any new groups created in Azure AD, you will first need to\ncreate corresponding groups in Atlan\nand then\nmap the groups\nto sync them through SCIM provisioning.\nRetrieve SCIM token in Atlan\nâ\nWho can do this?\nYou will need your\nAtlan admin\nto complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Azure AD administrator.\nÂ You will need to generate a SCIM token in Atlan for\nauthentication in Azure AD\n.\nTo retrieve the SCIM token, within Atlan:\nFrom the left menu on any screen, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nSSO\n.\nOn the\nSingle Sign on\npage for Azure AD, under\nOverview\n, navigate to\nAutomate Provisioning with SCIM\nand toggle it on.\nUnder\nSCIM token\n, click the\n+ Generate token\nbutton to create a SCIM token.\nIn the\nSCIM token generated\ndialog, click the\nCopy\nbutton to copy the SCIM token and store it in a secure location.\ndanger\nThe SCIM token will only be displayed once after it has been generated, you cannot retrieve it later.\nEnable SCIM provisioning in Azure AD\nâ\nWho can do this?\nYou will need your Azure AD administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your\nAtlan admin\n.\nYou can enable SCIM provisioning in Azure AD to automatically sync your users and groups to Atlan.\nConfigure SCIM provisioning in Azure AD\nâ\nTo configure SCIM provisioning, within Azure AD:\nLog in to your\nAzure portal\nand search for and select\nAzure Active Directory\n.\nFrom the left menu under\nManage\n, select\nEnterprise applications\n.\nFrom the\nAll applications\npage, select the\nSAML application\nyou created to configure SSO in Atlan.\nIn the left menu of your application page,Â under\nManage\n, click\nProvisioning\n.\nFrom the\nProvisioning mode\ndropdown, click\nAutomatic\n.\nUnder\nAdmin credentials\n, enter the following details:\nFor\nTenant URL\n, enter your Atlan tenant URL in the following format   -\nhttps://<your-tenant-dns>/api/service/scim\n.\nFor\nSecret Token\n, enter the\nSCIM token you copied\nin Atlan.\nClick the\nTest connection\nbutton to confirm connectivity to Atlan.\nWhen successful, in the top right, click\nSave\nto save the configuration.\nIn the\nMappings\nsection, verify that\nProvision Azure Active Directory Groups\nand\nProvision Azure Active Directory Users\nare enabled. Under\nMappings\n:\nClick\nProvision Azure Active Directory Groups\n, and under\nAttribute Mappings\n, define the following mappings from Azure AD on the left to Atlan on the right:\ndisplayName\n- >\ndisplayName\n-  Note that this field is currently unsupported in Atlan.\nobjectId\n- >\nexternalId\nmembers\n- >\nmembers\nClick\nProvision Azure Active Directory Users\n, and under\nAttribute Mappings\n, define the following mappings from Azure AD on the left to Atlan on the right:\nmailNickname\n- >\nuserName\n-  If the username is not mapped, the default username will be the\nUserPrincipalName\n(UPN).\nSwitch([IsSoftDeleted], , \"False\", \"True\", \"True\", \"False\")\n- >\nactive\ndisplayName\n- >\ndisplayName\nmail\n- >\nemails[type eq \"work\"].value\ngivenName\n- >\nname.givenName\nsurname\n- >\nname.familyName\nobjectId\n- >\nexternalId\nTo save any changes, click\nSave\n.\nProvision users and groups\nâ\ndanger\nYou will need to\nassign users or groups to Atlan\nfrom Azure AD before you can provision them. You will also need to\nconfigure group mapping\nÂ to sync mapped groups from Azure AD to Atlan. For any new groups created in Azure AD, you will first need to\nmap the groups\nin Atlan to sync them through SCIM provisioning.\nAfter you have\nenabled SCIM provisioning\nand\nassigned users and groups to Atlan\nin Azure AD, you can provision them to Atlan. In Azure AD, users and groups can be provisioned in two ways   -\nprovisioning cycle\nand\non-demand provisioning\n.\nNote the following:\nThe username and email address of new and existing users cannot be changed once users have been provisioned to Atlan.\nIf provisioning any users that already exist in Atlan, ensure that their Azure AD credentials match the existing credentials in Atlan for provisioning to be successful.\nTo provision users and groups, within Azure AD:\nLog in to your\nAzure portal\nand search for and select\nAzure Active Directory\n.\nFrom the left menu under\nManage\n, select\nEnterprise applications\n.\nFrom the\nAll applications\npage, select the\nSAML application\nyou created to configure SSO in Atlan.\nIn the left menu of your application page,Â under\nManage\n, click\nProvisioning\nand select a provisioning method:\nTo enable\nprovisioning cycle\n, in the upper left of the\nOverview\npage, click\nStart provisioning\nand toggle the\nProvisioning Status\nto\nOn\n.\nTo enable\non-demand provisioning\n, from the left menu, click\nProvision on demand\n. To provision users or groups on demand:\nFor\nSelect a user or group\n, search for and select a user or group.\nAt the bottom of the screen, click\nProvision\n. Repeat the steps for every user or group you want to provision.\nOnce you have enabled SCIM provisioning, Azure AD will automatically provision and update user accounts in Atlan. However, the\nsync typically happens every 40 minutes\n. So, it may take up to 40 minutes for user provisioning to be completed in Atlan.\nDid you know?\nThere are\nknown limitations\nto on-demand provisioning in Azure AD.\nTags:\natlan\ndocumentation\nPrevious\nConfigure SCIM provisioning\nNext\nHow to enable Okta for SCIM provisioning\nPrerequisites\nRetrieve SCIM token in Atlan\nEnable SCIM provisioning in Azure AD"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/enable-okta-for-scim-provisioning",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSCIM\nHow to enable Okta for SCIM provisioning\nOn this page\nEnable Okta for SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with\nSystem for Cross-domain Identity Management\n(SCIM).\nTo enable Okta for SCIM provisioning, complete the following steps.\nDid you know?\nFor any questions about SCIM provisioning, head over\nhere\n.\nPrerequisites\nâ\nOkta SSO must be enabled for Atlan\n.\nOkta users must be assigned to Atlan\n.\nGroup mapping must be configured\n, only required if syncing mapped groups from Okta to Atlan. For any new groups created in Okta, you will first need to\ncreate corresponding groups in Atlan\nand then\nmap the groups\nto sync them through SCIM provisioning.\nRetrieve SCIM token in Atlan\nâ\nWho can do this?\nYou will need your\nAtlan admin\nto complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Okta administrator.\nÂ You will need to generate a SCIM token in Atlan for\nauthentication in Okta\n.\nTo retrieve the SCIM token, from within Atlan:\nFrom the left menu on any screen, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nSSO\n.\nOn the\nSingle Sign on\npage for Okta, under\nOverview\n, navigate to\nAutomate Provisioning with SCIM\nand toggle it on.\nUnder\nSCIM token\n, click the\n+ Generate token\nbutton to create a SCIM token.\nIn the\nSCIM token generated\ndialog, click the\nCopy\nbutton to copy the SCIM token and store it in a secure location.\ndanger\nThe SCIM token will only be displayed once after it has been generated, you cannot retrieve it later.\nEnable SCIM provisioning in Okta\nâ\nWho can do this?\nYou will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your\nAtlan admin\n.\nYou can\nenable SCIM provisioning in Okta\nto automatically sync your users and groups to Atlan.\nConfigure SCIM provisioning in Okta\nâ\nTo configure SCIM provisioning, from within Okta:\nLog in to your Okta admin console.\nFrom the menu on the left, expand the\nApplications\nmenu and then click\nApplications\n.\nUnder\nApplications\n, select the\nSAML application you created\nto configure SSO in Atlan.\nFrom the tabs along the top of your application page, click the\nGeneral\ntab and then click\nEdit\n.\nUnder\nApp Settings\n, for\nProvisioning\n, click\nSCIM\nand then click\nSave\nto confirm.\nFrom the tabs along the top of your application page, click the\nProvisioning\ntab and then click\nEdit\n.\nFor\nSCIM connection\n, enter the following details:\nFor\nSCIM connector baseÂ URL\n, enter your Atlan tenant URL in the following format   -\nhttps://<your-tenant-dns>/api/service/scim/\n.\nFor\nUnique identifier field for users\n, enter\nuserName\nas the field name of the unique identifier for your users on your SCIM server.\nFor\nSupported provisioning actions\n, click to enable the following provisioning actions:\nImport New Users and Profile Updates\n-  this allows Okta to import new users and user profile updates to Atlan.\nPush New Users\n-  this allows user information to flow from Okta to Atlan.\nPush Profile Updates\n-  this allows profile information to flow from Okta to Atlan.\nPush Groups\n-  this allows group information to flow from Okta to Atlan.\nImport Groups\n-  this allows Okta to import new groups and group profile updates to Atlan.\nFor\nAuthentication Mode\n, click the dropdown and then select\nHTTP Header\n.\nTo authenticate using\nHTTP Header\n, you will need to provide a bearer token that will provide authorization against Atlan. For\nAuthorization\n, in the\nToken\nfield, enter the\nSCIM token you copied\nin Atlan.\nClick the\nTest Connector Configuration\nÂ button to confirm connectivity to Atlan.\nOnce successful, at the bottom of the form, click\nSave\nto save the configuration.\nUnder the left\nSettings\nmenu of the\nProvisioning\ntab, two new tabs will appear   -\nTo App\nand\nTo Okta\n. Click\nTo App\nto configure settings for SCIM provisioning to Atlan.\nFor\nProvisioning to App\npage, click\nEdit\nand then click to enable the following:\nCreate Users\n-  assigns a new Atlan account to each user managed by Okta. Okta does not create a new account if it detects that the username specified in Okta already exists in Atlan. The user's Okta username is assigned by default.\nUpdate User Attributes\n-  updates the user profiles of users assigned to Atlan. Profile changes made in Atlan will be overwritten with their respective Okta profile values.\nDeactivate Users\n-  automatically deactivates user accounts when they are unassigned in Okta or their Okta accounts are deactivated. Okta will also reactivate the Atlan account if the app integration is reassigned to a user in Okta.\nÂ Click\nSave\nto save the configuration.\nMap Okta user attributes to Atlan\nâ\ndanger\nYou will need to\nassign users to Atlan\nfrom Okta before you can provision them.\nAfter you\nhave enabled SCIM provisioning\nand\nassigned users to Atlan\nin Okta, you can provision them to Atlan. Note the following:\nThe username and email address of new and existing users cannot be changed once users have been provisioned to Atlan.\nIf provisioning any users that already exist in Atlan, ensure that their Okta credentials match the existing credentials in Atlan for provisioning to be successful.\nTo provision users to Atlan, from within Okta:\nLog in to your Okta admin console.\nFrom the menu on the left, expand the\nDirectory\nmenu and then click\nProfile Editor\n.\nOn the\nProfile Editor\npage, in the left menu under\nUsers\n, click\nApps\nand select the\nSAML application you created\nto configure SSO in Atlan.\nOn your application page, under\nAttributes\n, click\nMappings\n.\nIn the\nUser Profile Mappings\ndialog box, click\nOkta User to App\n.\nIn the\nOkta User to App\npage,\nuserName\nis already set by Atlan. Define the following mappings from Okta on the left to Atlan on the right:\nuser.firstName\n- >\ngivenName\nuser.lastName\n- >\nfamilyName\nuser.email\n- >\nemail\nClick\nSave\nto save your selections.\nOnce saved, at the bottom of the dialog, click\nApply updates now\n.\n(Optional) Navigate to the\nProvisioning\ntab of the\nSAML application you created\nto configure SSO in Atlan to confirm the attribute mappings.\nEnable group push in Okta to Atlan\nâ\ndanger\nYou will need to\nconfigure group mapping in Atlan\nbefore you can enable group push from Okta to Atlan.\nTo enable group push to Atlan, from within Okta:\nLog in to your Okta admin console.\nFrom the menu on the left, expand the\nApplications\nmenu and then click\nApplications\n.\nUnder\nApplications\n, select the\nSAML application you created\nto configure SSO in Atlan.\nFrom the tabs along the top of your application page, click the\nPush Groups\ntab and then click\nEdit\n.Â\nUnder\nPush Groups to App\n, click the settings icon. FromÂ the\nGroup Push Settings\ndialog, click\nRename app groups to match group name in Okta\nand then click\nSave\nto rename groups in Atlan when linking groups.\nUnder\nPush Groups to App\n, click the\nPush Groups\nbutton and then select\nFind groups by name\nto push your Okta groups to Atlan:\nFor\nPush groups by name\n, in the\nEnter a group to push...\nfield, enter the name of an Okta group you want to push to Atlan.\nTo the right of your selected Okta group, under\nMatch result & push action\n, click the\nCreate Group\ndropdown and then select\nLink Group\n.\nClick\nSave\nto save your selections.\n(Optional) Repeat steps 1 to 3 to push additional Okta groups to Atlan.\nTags:\nintegration\nconnectors\nPrevious\nHow to enable Azure AD for SCIM provisioning\nNext\nTroubleshooting SCIM provisioning\nPrerequisites\nRetrieve SCIM token in Atlan\nEnable SCIM provisioning in Okta"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles",
    "content": "Configure Atlan\nAccess control\nConcepts\nWhat are user roles?\nOn this page\nWhat are user roles?\nOverview\nâ\nAll users in Atlan need to be assigned one of the following predefined roles.\ndanger\nUser roles play a relatively small part in determining access to metadata and data. For more details on all the possible access control mechanisms, see\nHow do I control access to metadata and data?\nAdmin\nâ\nAnÂ\nadmin\nuser can manage Atlan:\nSet up integrations with external collaboration tools\nSet up data connections and run workflows\nManage users, groups, tags, and access policies\nMaintain extensions to the metadata\nTurn experimental features on and off\nIn addition, theÂ\nadmin\nuser can do everything theÂ\nmember\nuser can do.\nThere are two optional sub roles within the\nadmin\nto delegate adminitration for workflows or governance without full platform level admin access.\nThese\nworkflow and governance sub roles\ncan be enabled by admins.\nMember\nâ\nAÂ\nmember\nuser can discover, maintain, and query assets:\nFind and view metadata for assets\nUpdate metadata for specific assets (via personas and policies)   -  for example,\nattach tags\nSuggest metadata updates for all other assets\nApprove or reject suggested metadata changes (via personas and policies)\nPreview sample data and query data in specific assets\nGuest\nâ\nAÂ\nguest\nuser can only discover assets:\nFind and view metadata for assets\nSuggest updates\nto metadata for any assets (if\nenabled from the admin center\n)\nNever update metadata for any assets\nPreview sample data or query data in specific assets (via personas and policies)\nDetailed permissions\nâ\nTo understand the table of permissions, note the following:\nThe permission to\nmanage\nallows a user to create, read, update, and delete objects.\nâ   -  capability included.\nâ   -  capability will be a paid addition, reach out to your customer success manager for more information.\nBasic metadata   -  read asset name, description, certificates, and more.\nPermission to act may be limited\n.\nPermission\nAdmin\nMember\nGuest\nManage tags\nâ\nÂ\nÂ\nManage custom metadata and options\nâ\nÂ\nÂ\nManage users and groups\nâ\nÂ\nÂ\nManage access (personas, purposes, policies)\nâ\nÂ\nÂ\nEdit the organization's profile\nâ\nÂ\nÂ\nCreate API tokens\nâ\nÂ\nÂ\nSet up SSO\nâ\nÂ\nÂ\nCreate workflows\nâ\nÂ\nÂ\nApprove or reject suggested metadata changes\nâ\nâ\nÂ\nManage glossaries\nâ\nÂ\nÂ\nManage categories and terms\nâ\nâ\nÂ\nBulk upload terms (via glossary policies)\nâ\nâ\nÂ\nPreview sample data\nâ\nâ\nâ\nSuggest changes to metadata\nâ\nâ\nâ\nEdit metadata (via personas and policies)\nâ\nâ\nÂ\nView basic metadata for assets\nâ\nâ\nâ\nCreate Jira issues on assets\nâ\nâ\nâ\nShare assets on Slack or Teams\nâ\nâ\nâ\nInsights\nIncluded\nAdd-on\nAdd-on\nCreate and run new queries\nâ\nâ\nâ\nCreate collections, folders, and saved queries\nâ\nâ\nÂ\nView and run saved queries\nâ\nâ\nâ\nSchedule queries\nâ\nâ\nÂ\nTags:\nintegration\nconnectors\nPrevious\nWhat are groups?\nNext\nWhat are the sidebar tabs?\nOverview\nDetailed permissions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/concepts/what-are-groups",
    "content": "Configure Atlan\nAccess control\nConcepts\nWhat are groups?\nWhat are groups?\nGroups provide a way to organize many users together. You can use them in the same way as individual users, but at a more abstract level.\nThey help you scale your governance of data and metadata in Atlan as your teams grow.\nFor example, you can use groups to:\nIsolate the administration of different data sources\nAssign asset ownership\nMaintain access policies across a team of users\nDid you know?\nCreating groups comes in handy when a new person joins the team. You can add the new person to one or two groups instead of adding them to many objects and policies.\nTags:\natlan\ndocumentation\nPrevious\nWhat are purposes?\nNext\nWhat are user roles?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/concepts/what-are-tags",
    "content": "Build governance\nTags\nConcepts\nWhat are tags?\nTags\nAtlan allows\nadmin users\nto create tags for classifying data assets. You can use these tags in several ways:\nTo identify important characteristics of data assets\nFor grouping different assets together\nTo apply\ngranular access policies\nagainst those groupings\nDid you know?\nAs the volume of your data and its consumption increases, tags help you maintain data security.\nFor example, you can use tags to identify assets according to:\nData protection guidelines such as GDPR and CCPA compliance,Â for example:\nRight to be forgotten (RTBF) as a tag for GDPR compliance\nRight to know or right to delete for CCPA regulations\nDrive data cleanup for user data management at source\nIndustry information security practices, such as:\nCIA (Confidentiality, Integrity, and Availability) ratings of data assets\nISO 2700x data classification schemes   -  for example, Public, Internal, Confidential, and Restricted\nGovernmental schemes for classified information such as Top Secret, Secret, and Confidential\nDepartment-specific requirements   -  for example, HR, Finance, Sales, and Engineering\nPersonal identification information   -  for example, PII\nPII-tagged data can also be hashed, redacted, or nullified in Atlan\nFor details on tagging assets, see\nHow to attach a tag\n.\nFor details on managing tags from supported sources in Atlan, see:\nHow to manage Databricks tags\nHow to manage dbt tags\nHow to manage Google BigQuery tags\nHow to manage Snowflake tags\nTags:\natlan\ndocumentation\nPrevious\nRemove a tag"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/atlan-s-open-api",
    "content": "Get Started\nQuick Start Guides\nDevelopers\nAtlan's open API\nAtlan's open API\nLike you, we hate being locked into closed, black-box platforms. That's why we built Atlan's core platform on leading open-source projects and made every action API-driven.\nDid you know?\nEverything visible on Atlan is powered by APIs.\nOur mission is to help you\nactivate your metadata\n, to help your team do its best work. So we are gradually opening up these APIs for everyone.\nFor details of what's available, see our\ndedicated developer portal\n.\nTo get started,\ncreate an API token\nfrom within Atlan.\nTags:\napi\nrest-api\ngraphql\nPrevious\nSoftware development kits (SDKs)\nNext\nAuthentication and authorization"
  },
  {
    "url": "https://docs.atlan.com/platform/how-tos/generate-har-files-and-console-logs",
    "content": "Get Started\nAdministration\nGenerate HAR files and console logs\nOn this page\nGenerate HAR files and console logs\nAtlan is built on\nREST APIs\n, so you can see the requests being sent by the UI to the API gateway through your browser's developer console.\nTo help Atlan troubleshoot issues, you may be asked to create and send a HAR file and browser console logs:\nConsole or network logs\nfrequently provide critical error details that are required to determine the underlying cause of the issue or bug that you are experiencing.\nHAR files\ninclude all the network traffic from when you started recording, including sensitive information like passwords and private keys. To avoid including such information in a HAR file, Atlan recommends using a text editor to manually edit the file and remove any sensitive content before sending it to Atlan support.\nGenerate in Google Chrome\nâ\nLaunch\nGoogle Chrome\nand navigate to the relevant webpage in Chrome.\nIn the upper-right corner of your screen, click the three vertical dots.Â\nFrom the\nChrome\nmenu, click\nMore Tools\nand then click\nDeveloper Tools\n.\nIn the left menu, click on the\nNetwork\ntab and then select\nFetch/XHR\nas your filtering option.\nUnder the\nNetwork\ntab, click\nPreserve log\n.Â A red circle will appear on the left to show that you have started recording the network log. If you see a black circle, click on it to turn it red and start recording.\nTo allow Google Chrome to record the interaction between the browser and website, refresh the page. Confirm if you can view new entries in the console.\nNext to the red circle icon, click the circle slash icon to clear logs.\nReplicate the issue that you experienced in the browser. For example, if it's a particular click that triggers an error, perform this action so that the error is recorded in the console.\nOnce the page has loaded, navigate to the\nConsole\ntabÂ and right-click in the console box. Select\nSave as...\nand enter a name for the file.\nReturn to the\nNetwork\ntab and right-click the element that triggered an error. This is typically marked in red in the console. Click\nSave as HAR with content\nto save the HAR file.\n(Recommended) To remove any sensitive information from the HAR file:\nOpen the HAR file in a text editor of your choice.\nSearch for all instances of passwords and replace the values with a placeholder value such as\n*****\n. For example, in the following sample password content, you can replace\n<YourPrivateKey>\nand\n<YourPrivateKeyPassword>\nÂ with placeholder values:\n\"headersSize\": -1,\n\"bodySize\": 3762,\n\"postData\": {\n\"mimeType\": \"application/json\", \"text\": \"{\\\"host\\\":\\\"<YourHostName>\\\",\\\"port\\\":<port>,\\\"authType\\\":\\\"keypair\\\",\\\"username\\\":\\\"PRD_SDCSHOP_TU_ETL_CATALOG\\\",\\\"password\\\":\\\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\\\<YourPrivateKey>\\\\n-----END ENCRYPTED PRIVATE KEY-----\\\",\\\"extra\\\":{\\\"role\\\":\\\"GLOBAL_TALEND_CATALOG_DBADMIN\\\",\\\"warehouse\\\":\\\"PRD_SDCSHOP_ETL\\\",\\\"private_key_password\\\":\\\"<YourPrivateKeyPassword>\\\"},\\\"connectorConfigName\\\":\\\"atlan-connectors-snowflake\\\",\\\"query\\\":\\\"show atlan schemas\\\",\\\"schemaExcludePattern\\\":[\\\"INFORMATION_SCHEMA\\\"]}\"\n}\n}\nSave the HAR file.\nUpload the HAR file and browser console log to your Atlan support ticket.\nTags:\napi\nrest-api\ngraphql\nPrevious\nCloud logging and monitoring\nNext\nTenant access management\nGenerate in Google Chrome"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-access-management",
    "content": "Get Started\nAdministration\nTenant access management\nOn this page\nTenant access management\nFor any Atlan tenant, there are three types of access required for troubleshooting an issue:\naccess to cloud resources\naccess to vCluster or dedicated Kubernetes (K8s) cluster\naccess to product\nAccess to cloud resources\nâ\nIdentify need for access   -  access to the cloud resources of a tenant is generally required to debug an issue related to infrastructure components. For any tenant, cloud resources include cloud storage, secrets, Kubernetes cluster, network rules, and more.\nRequest access   -  if an Atlan engineer troubleshooting an issue requires access to any cloud resource, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification.\nApproval process   -  the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control.\nAccess granting   -  once approved, the IT team at Atlan will grant access to the resource for a specified period of time.\nAccess revocation   -  after the specified period of time is over or troubleshooting has been completed, the IT team will revoke access to avoid any unauthorized or prolonged access.\nAccess to cluster\nâ\nIdentify need for access   -  access to vCluster or dedicated Kubernetes (K8s) cluster is required to troubleshoot an issue related to Kubernetes resources.\nManagement of Kubernetes clusters   -  Atlan uses\nLoft\nfor managing K8s clusters and vclusters. All K8s clusters and vclusters are added to and managed from Loft.\nRequest access   -  if an Atlan engineer troubleshooting an issue requires access to a vcluster or dedicated cluster, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification.\nApproval process   -  the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control.\nAccess granting   -  once approved, the IT team at Atlan will grant access to the resource for a specified period of time.\nAccess revocation   -  after the specified period of time is over or troubleshooting has been completed, the IT team will revoke access to avoid any unauthorized or prolonged access.\nAccess to product\nâ\nIdentify need for access   -  access to the product may be required to troubleshoot an issue with product features or connector workflows.\nCreation of support user   -  Atlan creates a support user named\natlansupport\nwhile creating a tenant. The credentials for this user is stored with the IT team.\nSecurity of credentials   -  the credentials for this user are securely stored in\n1Password\n, and managed by the IT team. Access to these credentials is tightly controlled, requiring explicit permission from the IT team to access them, and support user passwords are reset every 90 days.\nRequest access   -  if an Atlan engineer troubleshooting an issue requires access to the product, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification.\nApproval process   -  the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control.\nAccess granting   -  once approved, the IT team at Atlan will share the password of the Atlan support user for a specified period of time.\nMonitoring and logging   -  all actions performed using the\natlansupport\naccount are monitored and logged in\nKeycloak\n, with logs retained for 60 days.\nTags:\natlan\ndocumentation\nPrevious\nGenerate HAR files and console logs\nNext\nTenant logs\nAccess to cloud resources\nAccess to cluster\nAccess to product"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-logs",
    "content": "Get Started\nAdministration\nTenant logs\nOn this page\nTenant logs\nAtlan can help you understand the events that occur in your tenants, including user and administrative actions. Learn more about logging and retention as follows:\nTenant logs\nâ\nNote the following:\nLoad balancer logs for Azure and GCP tenants are currently not enabled.\nAuditSearch and SearchLog records are persisted forever in Elasticsearch. The 30-day retention period pertains to application logs written to logging Elasticsearch.\nAn example of block storage mentioned below is Amazon Elastic Block Store (EBS) for AWS.\nProduction tenants\nâ\nLog types\nRetention\nStorage\nAWS\nAzure\nGCP\nActive tenant overall backup\n15 days\nObject storage\nâ\nâ\nâ\nOffboarded tenant overall backup\nAWS   -  30 days, Azure and GCP   -  15 days\nObject storage\nâ\nâ\nâ\nLoad balancer logs\n30 days\nObject storage\nâ\nâ\nâ\nAudit   -\nuser events\n60 days\nPostgreSQL\nâ\nâ\nâ\nAudit   -\nadmin events\nUnlimited\nPostgreSQL\nâ\nâ\nâ\nApplication logs\n30 days\nElasticsearch and object storage\nâ\nâ\nâ\nWorkflow logs\n90 days\nClickHouse\nâ\nâ\nâ\nWorkflow artifacts\n180 days\nObject storage\nâ\nâ\nâ\nApplication metrics\n60 days\nVictoriaMetrics (block storage)\nâ\nâ\nâ\nProof of value (POV) tenants\nâ\nLog types\nRetention\nStorage\nAWS\nAzure\nGCP\nActive tenant overall backup\n15 days\nObject storage\nâ\nâ\nâ\nOffboarded tenant overall backup\nAWS   -  3 days, Azure and GCP   -  15 days\nObject storage\nâ\nâ\nâ\nLoad balancer logs\n30 days\nObject storage\nâ\nâ\nâ\nAudit   -\nuser events\n60 days\nPostgreSQL\nâ\nâ\nâ\nAudit   -\nadmin events\nUnlimited\nPostgreSQL\nâ\nâ\nâ\nApplication logs\n30 days\nElasticsearch and object storage\nâ\nâ\nâ\nWorkflow artifacts\n180 days\nObject storage\nâ\nâ\nâ\nApplication metrics\n60 days\nVictoriaMetrics (block storage)\nâ\nâ\nâ\nAtlan logs\nâ\nService\nType\nLogging pipeline\nDestination\nHeracles\napplication\nFluent Bit\nS3\nArgo\napplication, server\nArgo, Fluent Bit\nS3\nAtlas\napplication, audit, perf\nFluent Bit\nS3\nNumaflow\napplication\nFluent Bit\nS3\nKube events\napplication\nFluent Bit\nS3\nWisdom\napplication, audit\nFluent Bit\nS3\nChronos\napplication\nFluent Bit\nS3\nRedis\napplication\nFluent Bit\nS3\nKong\napplication, audit\nFluent Bit, PostgreSQL, Keycloak REST API\nS3\nKeycloak\napplication\nFluent Bit\nS3\nElasticsearch\napplication\nFluent Bit\nS3\nCassandra\napplication\nFluent Bit\nS3\nHeka\napplication\nFluent Bit\nS3\nPgpool\napplication, server\nFluent Bit\nS3\nKafka\nevents\nFluent Bit\nS3\nCloud storage lifecycle\nâ\nThe cloud storage created for each tenant has its own lifecycle. The lifecycle policy is attached to paths in the cloud storage. The lifecycle policy applied to a production tenant is as follows:\nAmazon Web Services (AWS)\nâ\nLifecycle policy\nPath\nAction\nDeleteClusterLogsAfter30Days\nlogs/\nExpires\nDeleteArgoArtifactsAfter180Days\nargo-artifacts/\nTransitions to S3 Glacier Flexible Retrieval, then expires\nDeleteArgoBackupAfter15Days\nbackup/argo/\nExpires\nDeleteAltanScheduleQuery\nargo-artifacts/default/schedule-query/\nExpires\nDeletePostgresBackupAfter15Days\nbackup/postgres/\nExpires\nDeleteRedisBackupAfter15Days\nbackup/redis/\nExpires\nDeleteCassandraBackupAfter15Days\nbackup/cassandra/\nExpires\nDeletePrometheusBackupAfter15Days\nbackup/prometheus/\nExpires\nDeleteALBLogsAfter30Days\nAWSLogs/\nExpires\nMicrosoft Azure\nâ\nLifecycle policy\nPath\nAction\nDeleteClusterLogsAfter30Days\nlogs/\nDelete\nDeleteArgoArtifactsAfter180Days\nargo-artifacts/\nMoves to archive after 90 days and delete after 180 days\nDeleteArgoBackupAfter15Days\nbackup/argo/\nDelete\nDeletePostgresBackupAfter15Days\nbackup/postgres/\nDelete\nDeleteRedisBackupAfter15Days\nbackup/redis/\nDelete\nDeleteCassandraBackupAfter15Days\nbackup/cassandra/\nDelete\nDeleteAltanScheduleQuery\nargo-artifacts/default/schedule-query/\nDelete if blobs not modified in 1 day\nDeleteSparkEventLogsAfter15Days\nspark-event-logs/\nDelete\nGoogle Cloud Platform (GCP)\nâ\nLifecycle policy\nPath\nAction\nDeleteClusterLogsAfter15Days\nlogs/\nDelete\nDeleteArgoArtifactsAfter180Days\nargo-artifacts/\nArchive\nDeleteArgoArtifactsAfter270Days\nargo-artifacts/\nDelete\nDeleteArgoBackupAfter3Days\nbackup/argo/\nDelete\nDeletePostgresBackupAfter3Days\nbackup/postgres/\nDelete\nDeletePrometheusBackupAfter3Days\nbackup/prometheus/\nDelete\nDeleteRedisBackupAfter3Days\nbackup/redis/\nDelete\nDeleteScheduleQueryAfter1Day\nargo-artifacts/default/schedule-query/\nDelete\nDeleteSparkEventLogsAfter15Days\nspark-event-logs/\nDelete\nDeleteCassandraBackupAfter3Days\nbackup/cassandra/\nDelete\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nTenant access management\nNext\nTenant monitoring\nTenant logs\nAtlan logs\nCloud storage lifecycle"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-monitoring",
    "content": "Get Started\nAdministration\nTenant monitoring\nTenant monitoring\nAtlan monitors tenants from a centralized observability platform.\nThe following essential metrics are monitored:\nInfrastructure metrics such as CPU utilization and memory usage\nApplication performance metrics\nAvailability and uptime metrics\nLatencies\nThe setup for monitoring tenants consists of Zenduty, Grafana, VictoriaMetrics server, vmagent, and Prometheus exporters such as Node Exporter and Blackbox Exporter.\nThe architecture for the tenant monitoring system is as follows:\nTenant monitoring in Atlan includes the following processes:\nAll the metrics of a tenant are collected in the centralized monitoring stack that runs in the Atlan control plane.\nThese metrics can be visualized in\nGrafana\n, which also runs in the Atlan control plane.\nAtlan associates have access to Grafana over Atlan VPN and can use available dashboards to monitor tenants.\nBased on the alerting rules, alerts are triggered for the tenant and managed through\nZenduty\n, an incident alerting tool.\nTags:\natlan\ndocumentation\nPrevious\nTenant logs\nNext\nTenant offboarding"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-offboarding",
    "content": "Get Started\nAdministration\nTenant offboarding\nOn this page\nTenant offboarding\nAn Atlan tenant consists of multiple infrastructure resources, such as EC2 nodes, S3 buckets, ALB, EKS, and IAM roles, Loft vcluster, and more. Offboarding an Atlan vcluster tenant involves removing all the infrastructure resources associated with that particular tenant or instance.Â\nWhile most of the resources can be deleted immediately without any data loss, we handle the cleanup of storage resources like S3 buckets with care.\nS3 buckets associated with a tenant are not immediately removed   -  they are retained for 30 days after a tenant is offboarded. This ensures that there is a\ndisaster recovery mechanism\nin place if a tenant is offboarded by mistake or faces any unrecoverable errors.\nTenant decommissioning\nâ\nYou can\nraise a support ticket\nfor tenant decommissioning. The customer success team at Atlan will notify the infrastructure support team responsible for executing tenant offboarding. These requests are usually resolved within the first 48 hours to ensure that no unnecessary costs are incurred for infra resources.\nFrequently asked questions\nâ\nWhere is customer data stored?\nâ\nAny customer-specific data in Atlan   -  for example, query logs, assets, service logs, and more   -  are stored in EBS volumes and S3 buckets.\nHow long does customer data persist after tenant offboarding?\nâ\nEBS volumes are immediately deleted during offboarding. However, S3 buckets are retained for a period of 30 days.\nTags:\natlan\ndocumentation\nPrevious\nTenant monitoring\nNext\nInfrastructure security\nTenant decommissioning\nFrequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/tags/monitoring",
    "content": "9 docs tagged with \"monitoring\"\nView all tags\nAutomate data governance\nYou can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring.\nLink your Microsoft Teams account\nTo get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users.\nManage tasks\n:::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can use the inbox.\nSecurity monitoring\nLearn about security monitoring.\nSupported sources\nLearn about supported sources.\nTroubleshooting Fivetran connectivity\nLearn about troubleshooting fivetran connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity.\nWhat's Data Quality Studio\nUnderstand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/logs",
    "content": "One doc tagged with \"logs\"\nView all tags\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/compliance",
    "content": "One doc tagged with \"compliance\"\nView all tags\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/siem",
    "content": "One doc tagged with \"siem\"\nView all tags\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/opentelemetry",
    "content": "One doc tagged with \"opentelemetry\"\nView all tags\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/otlp",
    "content": "One doc tagged with \"otlp\"\nView all tags\nCloud logging and monitoring\nLearn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/platform/references/security-monitoring",
    "content": "Get Started\nSecurity & Compliance\nSecurity monitoring\nOn this page\nSecurity monitoring\nAtlan has built-in monitoring systems that help users manage the behind-the-scenes infrastructure. These ensure adherence to the highest standards of security.\nFull visibility into infrastructure performance\nâ\nWe provide admins complete visibility of CPU, memory, and storage metrics through\nGrafana\n. These are industry-standard dashboards.\nSlack alerts and notifications\nâ\nFor proactive alerting, Slack notifications can also be enabled across Atlanâs infrastructure.\nSee\nsecurity.atlan.com\nfor the latest policies and standards, reports and certifications, architecture, diagrams and more.\nTags:\ndashboards\nvisualization\nanalytics\nalerts\nmonitoring\nnotifications\nsecurity\naccess-control\npermissions\nPrevious\nHow are resources isolated?\nNext\nCompliance standards and assessments\nFull visibility into infrastructure performance\nSlack alerts and notifications"
  },
  {
    "url": "https://docs.atlan.com/platform/references/compliance-standards-and-assessments",
    "content": "Get Started\nSecurity & Compliance\nCompliance standards and assessments\nCompliance standards and assessments\nAtlan adheres to various industry standards and regulations to ensure the security, privacy, and integrity of the platform. This entails conducting both external audits and internal assessments to continuously improve compliance standards.\nFollowing is an overview of Atlan's key compliance certifications and internal assessment practices:\nCompliance\nDescription\nStatus\nFrequency\nISO 27001\nThe Information Security Management System (ISMS) standard ensures data confidentiality, integrity, and availability.\nCertified\nAnnual\nISO 27701\nThe Privacy Information Management System (PIMS) standard manages PII and ensures compliance with privacy regulations like GDPR and CCPA.\nCertified\nAnnual\nSOC 2 Type II\nThe SOC (System and Organization Controls) 2 Type II report attests to the security, availability, confidentiality, and privacy controls for service organizations.\nCertified\nAnnual\nGDPR\nThe General Data Protection Regulation (GDPR) is an EU regulation that ensures the protection of personal data by enforcing strict privacy and security measures, along with giving individuals control over their data. Atlan adheres to GDPR through ongoing compliance, including breach notifications, data subject rights, and consent management.\nCertified\nAnnual\nEU-U.S. Data Privacy Framework\nThe Data Privacy Framework outlines policies and controls that govern how Atlan handles personal information to ensure data protection and compliance with privacy regulations like GDPR.\nCompliant\nAnnual\nHIPAA\nHIPAA, or the Health Insurance Portability and Accountability Act, safeguards protected health information (PHI).\nCertified\nAnnual\nVAPT assessments\nAnnual third-party Vulnerability Assessment and Penetration Testing (VAPT) assessments help identify and mitigate potential vulnerabilities within the Atlan platform.\nOngoing\nAnnual\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nSecurity monitoring\nNext\nIncident response plan"
  },
  {
    "url": "https://docs.atlan.com/tags/integration",
    "content": "123 docs tagged with \"integration\"\nView all tags\nAdd custom metadata\n<div style={{position: \"relative\", paddingBottom: \"calc(66.33333333333333% + 41px)\", height: 0}}> <iframe src=\"https://demo.arcade.software/1dT1bPneM5fp1O71lb.\nAdd descriptions\nYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.\nAdministration and Configuration\nComplete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization.\nAI and Automation Features\nGuide to Atlan's AI capabilities and automation features for enhanced data governance and productivity.\nAlways On\nIntegrate Atlan with Always On to enable continuous automation and suggestions.\nAtlan browser extension security\nLearn about atlan browser extension security.\nAWS Lambda\nIntegrate Atlan with AWS Lambda to automate workflows and triggers.\nBrowser Extension\nIntegrate Atlan with the Browser Extension to enhance your data catalog experience.\nBulk enrich metadata\nAtlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan.\nCan Atlan integrate with Airflow to generate lineage?\nAtlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage).\nCan I add Atlan's browser extension for everyone in my organization?\nRefer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nYou can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events.\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nRefer to our [troubleshooting Jira documentation](/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira) to learn more.\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nCan I create backups of glossaries?\nAtlan also allows you to [export your glossary assets](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) to spreadsheets and keep a record of your contextual information.\nCan I query any DW/DL?\nYou can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources#data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature.\nCan site renaming affect the Jira integration?\nLearn about can site renaming affect the jira integration?.\nCan the Hive crawler connect to an independent Hive metastore?\nLearn about can the hive crawler connect to an independent hive metastore?.\nCan we use a Microsoft SSO login?\nLearn about can we use a microsoft sso login?.\nConfigure SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning.\nConfigure Snowflake data metric functions\nConfigure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nConfigure workflow execution\nLearn about configure workflow execution.\nConnectors and capabilities\nLearn about connectors and capabilities.\nCreate an AWS Lambda trigger\nOnce you have configured the [AWS Lambda permissions](/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda), you can run an AWS Lambda function.\nCreate announcements\nAdding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions.\nCreate README templates\nAdmin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and [enrich their assets with READMEs](/product/integrations). They will also be able to see a rich preview of each template before adding the relevant documentation.\nCustom solutions\nLearn about custom solutions.\nData Connections and Integration\nComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.\nDelete a connection\nLearn about delete a connection.\nDeployment architecture\nThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nDownload impacted assets in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nEnable  Azure AD for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  Google for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  JumpCloud for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  Okta for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  OneLogin for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  SAML 2.0 for SSO\nSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).\nEnable  Snowflake OAuth\nAtlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware.\nEnable  SSO for Amazon Redshift\nYou will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift).\nEnable  SSO for Google BigQuery\nCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.\nEnable Okta for SCIM provisioning\nYou can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM).\nETL tools connectors\nOverview and entry point for all ETL tools connectors in Atlan.\nHow are product updates deployed?\nLearn about how are product updates deployed?.\nHow do I send messages or search assets from Slack?\nSending messages and searching assets from Slack are disabled. Refer to [Troubleshooting Slack](/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack) to learn more.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nInfrastructure security\nLearn about infrastructure security.\nIntegrate Anomalo\nOnce you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo.\nIntegrate Atlan with Google Sheets\nThe Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.\nIntegrate Atlan with Microsoft Excel\nThe Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel.\nIntegrate Jira Cloud\nYou must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work.\nIntegrate Jira Data Center\nYou will need to [configure an incoming link](https://confluence.atlassian.com/adminjiraserver/configure-an-incoming-link-1115659067.html) with an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider.\nIntegrate Microsoft Teams\nOnce you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams.\nIntegrate ServiceNow\nIf your Atlan admin has [enabled the governance workflows and inbox module](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) in your Atlan workspace, you can create a ServiceNow integration to allow your users to [grant or revoke data access](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) for governed assets in Atlan or any other data source.\nIntegrate Slack\nTo integrate Slack and Atlan, follow these steps.\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nJira\nIntegrate Atlan with Jira to automate ticket creation and link your Jira account.\nLink your account\nTo [export assets to and bulk enrich metadata from](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) a supported spreadsheet tool,.\nLink your Jira account\nTo create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that [set up the Jira integration](/product/integrations/project-management/jira/how-tos/integrate-jira-cloud), but not for other users.\nLink your Microsoft Teams account\nTo get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users.\nLink your ServiceNow account\nTo request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that [set up the ServiceNow integration](/product/integrations/project-management/servicenow/how-tos/integrate-servicenow), but not for other users.\nLink your Slack account\nTo see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that [set up the Slack integration](/product/integrations/collaboration/slack/how-tos/integrate-slack), but not for other users.\nManage connectivity\nOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.\nManage custom metadata structures\n:::warning Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones.\nManage requests\nIf your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected.\nMicrosoft Teams\nIntegrate Atlan with Microsoft Teams to enable collaboration and notifications.\nMonitor connectivity\nAtlan runs its crawlers through an orchestrated set of automated tasks.\nOkta first-time login authentication error\nLearn about why do i get an authentication error when logging in via okta for the first time?.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nPingFederate SSO 404 error\nIf you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion.\nProvide credentials to query data\nLearn about provide credentials to query data.\nProvide credentials to view sample data\nLearn about provide credentials to view sample data.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nReport on assets\nLearn about report on assets.\nReport on automations\nYou can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions.\nSchedule a query\nYou must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients.\nServiceNow\nIntegrate Atlan with ServiceNow to automate ticket creation and link your ServiceNow account.\nSet default user roles for SSO\n:::warning Who can do this? You will need to be an admin user and [configure SSO](/product/integrations/identity-management/sso) with a provider first.\nSet up a private network link to Hive\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nSet up a private network link to Trino\n:::warning Who can do this? You will need your AWS administrator involved - you may not have access to run these tasks yourself.\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.\nSet up Fivetran\nLearn about set up fivetran.\nSet up MongoDB\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up Salesforce\nLearn about setting up Salesforce authentication for Atlan.\nSlack\nIntegrate Atlan with Slack to enable collaboration and notifications.\nSSO integration with PingFederate using SAML\nTo use both IdP- and SP-initiated SSO, add both the URLs mentioned above.\nSupported sources\nLearn about supported sources.\nTroubleshooting Atlan browser extension\nCan I add the browser extension for everyone in my organization?\nTroubleshooting AWS Glue connectivity\nLearn about troubleshooting aws glue connectivity.\nTroubleshooting connector-specific SSO authentication\nLearn about troubleshooting connector-specific sso authentication.\nTroubleshooting Jira\nWhat fields are supported when creating tickets or requesting access?\nTroubleshooting Metabase connectivity\nLearn about troubleshooting metabase connectivity.\nTroubleshooting Microsoft Teams\nWhy do I get an error while adding Atlan to Microsoft Teams?\nTroubleshooting Mode connectivity\nLearn about troubleshooting mode connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity.\nTroubleshooting SCIM provisioning\nLearn about troubleshooting scim provisioning.\nTroubleshooting ServiceNow\nWhy is the security\\_admin role required to complete the ServiceNow integration?\nTroubleshooting Sisense connectivity\nLearn about troubleshooting sisense connectivity.\nTroubleshooting Slack\nWhat do the colors in Slack notifications for modified assets mean?\nTroubleshooting spreadsheets\nWhy do I need admin consent for exporting assets to Microsoft Excel?\nTroubleshooting SSO\nCan I change the username of a provisioned user in Atlan?\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nupdate column metadata in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.\nUpdate column metadata in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.\nUse the filters menu\nYou can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you.\nView event logs\nEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.\nWhat are user roles?\nLearn about what are user roles?.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Confluent Schema Registry?\nAtlan crawls and maps the following assets and properties from Confluent Schema Registry.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase.\nWhat does Atlan crawl from Microsoft SQL Server?\nAtlan crawls and maps the following assets and properties from Microsoft SQL Server.\nWhat does Atlan crawl from Mode?\nAtlan crawls and maps the following assets and properties from Mode.\nWhat is included in the Jira integration?\nWith two of your most important workspaces connected, you can save time and improve the way you track issues for your data.\nWhat is included in the Microsoft Teams integration?\nWith two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.\nWhat is included in the Slack integration?\nLearn about the features and capabilities of the Slack integration with Atlan.\nWhat is the crawler logic for a deprecated asset?\nLearn about what is the crawler logic for a deprecated asset?.\nWhat type of user provisioning does Atlan support for SSO integrations?\nAtlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:.\nWhat's the difference between connecting to Athena and Glue?\nLearn about what's the difference between connecting to athena and glue?.\nWhy did my users not receive an invite email from Atlan?\nIf you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:.\nWhy do I get an error message when I click on Atlan's browser extension?\nRefer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).\nWhy is Atlan's browser extension not loading?\nRefer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension)."
  },
  {
    "url": "https://docs.atlan.com/faq/basic-platform-usage",
    "content": "Get Started\nFAQs\nBasic Platform Usage\nOn this page\nBasic Platform Usage\nEssential information about using Atlan's core features, from browser requirements to data querying and asset management.\nWhich browsers does Atlan support?\nâ\nAtlan supports browsers for which all functionalities can be validated. Atlan recommends using Google Chrome for an optimal user experience.\nWhat's the availability of Atlan?\nâ\nAtlan software is constantly available (24x7). You'll be notified in advance if Atlan experiences any downtime or outage. See also\nHigh availability and disaster recovery (HA/DR)\n.\nHow does Atlan access the data for queries?\nâ\nAtlan pushes all queries to the source. Each query generated in Atlan is a pushdown SQL query into the underlying source database. This data is neither stored nor cached anywhere in Atlan. It's encrypted in transit using AWS Key Management Service (AWS KMS). See also\nEncryption and key management\n.\nIn addition, Atlan applies\naccess policies\nto the results before displaying them.\nWhy replace my current IDE with Atlan?\nâ\nAtlan's\nInsights\nfeature is far and beyond any other IDE:\nIt provides a quick preview of the tables and data.\nScheduled queries\n- you can create a collection of saved queries and schedule the output to be delivered to users via email.\nVisual Query Builder\n- querying for people who aren't proficient in SQL.\nParameterization\n- add variables to your SQL queries that can be edited by users during runtime.\nCan I use Python or R to query data?\nâ\nAtlan currently supports using Python to query data. Refer to the\ndeveloper documentation\nto learn more. Support for R is unavailable.\nCan I switch off the query feature for data security concerns?\nâ\nYou can block all users from querying data across all data assets. You'll need to be an admin user to\ntoggle the Insights option off\nfrom the admin workspace.\nWhat types of files can be received into Atlan?\nâ\nAtlan supports cataloging\nfiles\nthrough APIs. The\nsupported file types\ninclude document, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files.\nCan I export all the data from Atlan?\nâ\nAtlan enables you to export all your assets or a\nfiltered subset of assets\nand asset metadata to Google Sheets and Microsoft Excel spreadsheets. Learn more\nhere\n.\nHow do I update my profile picture in Atlan?\nâ\nTo update your profile picture in Atlan:\nIn the top right of your instance, click your name, and then from the dropdown, click\nProfile\n.\nIn the\nProfile\nsidebar, click the edit icon and then click\nChange avatar\nto upload a new image.\n(Optional) You can add or update other relevant details in your Atlan profile - for example, primary role, skills, and even your Slack member ID.\nClick\nSave\nto save your changes.\nWhy does an emoji appear as one color in the menu and another on Atlan UI?\nâ\nEmojis may appear differently across different browsers and operating systems. This is because emojis are rendered by the browser or operating system, and each has its own way of displaying them. For a consistent experience, consider using Google Chrome.\nWhy is the count of assets in the connection dropdown different than the asset pill?\nâ\nWhen you select a\nsupported source\nfrom the\nConnector\ndropdown, the\nAll\nassets tab displays an exact count of assets for each connection. If you haven't selected a connector, then the\nAll\nassets tab displays a rounded count of all assets in your Atlan instance.\nDisabling all assets view\nâ\nHere are a few things to know about\nrestricting asset visibility\n:\nWhat's the default view for users with multiple personas or purposes?\nâ\nThe default view depends on the alphabetical order of the\npersonas\nor\npurposes\nthat the member and guest users belong to. Personas are first sorted alphabetically, and if the user doesn't belong to any persona, then purposes are sorted next to determine the default view on the\nAssets\npage.\nCan access to glossaries be restricted?\nâ\nYes, an Atlan admin can\ndisable all glossaries view\n.\nCan users view linked assets they don't have access to for announcements?\nâ\nMember and guest users are able to view the names of linked assets in the\nannouncements\non the homepage. However, if they open the asset profile of a restricted asset, they'll encounter a\nNot found\npage.\nTags:\natlan\ndocumentation\nfaq-platform\nPrevious\nGetting Started and Onboarding\nNext\nSupport and Technical Help"
  },
  {
    "url": "https://docs.atlan.com/faq/support-and-technical-help",
    "content": "Get Started\nFAQs\nSupport and Technical Help\nOn this page\nSupport and Technical Help\nComplete guide to getting support, understanding API limits, and accessing technical assistance for Atlan.\nWhat's the availability of support?\nâ\nAtlan support provides full coverage during business hours across most time zones. See also\nCustomer support\n.\nIs there a limit on the number of API requests that can be performed?\nâ\nMost automated operations that need to process many records follow one of these patterns:\nSearch-based extract\nEnd-to-end bulk update\nThese patterns minimise the total number of API calls because each request extracts or updates many records, so you're unlikely to hit rate limits.\nIf you make raw API requests directly, different endpoints have different limits. As a rule of thumb, assume\nabout 1,000 requests per minute per host\n(your Atlan domain). Whenever possible, adopt the patterns mentioned earlier to avoid these limits.\nWhat's the recommended path for pagination of asset searches with scroll API?\nâ\nRefer to\nPaging search results\nin the developer documentation for the recommended scroll API implementation.\nIs Atlan compatible with data quality tools?\nâ\nYes. Atlan integrates with several data-quality and observability platforms:\nNative integrations such as\nMonte Carlo\n,\nSoda\n, and\nAnomalo\nAbility to surface quality scores and alerts in asset profiles\nREST APIs to ingest additional quality metadata from any tool you choose\nFor the latest list, see\nsupported sources\n.\nTags:\napi\nrest-api\ngraphql\nfaq-support\nPrevious\nBasic Platform Usage"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-platform",
    "content": "2 docs tagged with \"faq-platform\"\nView all tags\nBasic Platform Usage\nEssential information about using Atlan's core features, from browser requirements to data querying and asset management.\nGetting Started and Onboarding\nEverything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/use-the-filters-menu",
    "content": "Use data\nDiscovery\nConfiguration\nHow to use the filters menu\nOn this page\nUse the filters menu\nYou can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you.\nOnce you have added filters, you can also:\nBookmark your search results with applied filters for quick access.\nCopy the browser URL with applied filters and share it with other users in your organization   -  they may need to log into Atlan first to view the search results.\nExport filtered assets to spreadsheets\nand enrich asset metadata in bulk.\nDid you know?\nYou can resize the filters panel. Hover over the edge of the panel and then click and drag the slider arrow to resize it. Atlan will remember your preferred size for the duration of your session. The panel will be returned to the default size when you log in again.\nUse the following filters to help with asset disco\nvery. Start by either clicking\nAssets\nin the left panel or the search bar from any screen in Atlan:\nSource\nâ\nYou can use\nsource-specific\nfilters to curate a list of relevant assets to search from.\nTo filter by a specific source:\nIn the\nFilters\nmenu on the left, click\nSource\n.\nClick\nChoose connection\nto filter by a\nsupported connector\n.\n(Optional) Select an existing connection for a selected connector:\nClick\nAll Databases\nto filter by databases for a selected connection.\nClick\nAll Schemas\nto filter by schemas for a selected connection.\nAsset type\nâ\nYou can filter your search results by specific types of assets. The asset type filter also includes a quick count of all the resulting assets grouped by type. You can select multiple asset types to group search results by asset types.\nFor example, if you would only like to view column assets:\nUnder the search bar on the\nAssets\npage, click the\nAsset type\ndropdown.\nFrom the\nAsset type\ndropdown, click the\nColumn\ntab to only view column assets. Only column assets will be displayed in the results, with the tab showing a total count of the column assets.\n(Optional) In the\nFilters\nmenu on the left, under\nAsset Type Filters\n, click\nColumn\nto add a type-specific property filter to further refine your search:\nClick\nParent asset type\nto filter columns by a parent asset type   -  tables and views.\nClick\nParent asset name\nto filter columns by the name of a table or view, or set a matching condition such as pattern match.\nClick\nData type\nto filter columns by data types.\nClick any of the\ncolumn keys\nto filter by column keys.\n(Optional) If an asset type you're filtering for does not match the search keywords but there are other asset types that match, click\nCheck other matches\nto view those assets or click\nClear search\nto clear your search and start over.\nDomains\nâ\nDomains\nprovide a logical way of mapping and organizing assets within a specific domain or business entity. You can filter assets by a single domain, multiple domains, or no domains.\nTo filter assets by domains:\nIn the\nFilters\nmenu on the left, click\nDomains\nto expand the menu.\nUnder\nDomains\n, to filter assets by domains:\nCheck the boxes to select one or more domains or subdomains to filter your assets.\nClick\nNo domains\nto filter assets not mapped to any domain.\nMetadata filters\nâ\nCertificate\nâ\nYou can filter your asset search based on the\ncertificate\nattached to the data assets   -\nVerified\n,\nDraft\n,\nDeprecated\n, and\nNo certificate\n.\nFor example, if you would only like to view verified assets:\nIn the\nFilters\nmenu on the left, click\nCertificate\nto expand the menu.\nUnder\nCertificate\n, click\nVerified\nto only view verified assets in your search results.Â\nOwners\nâ\nYou can filter your asset search by selecting one or more\nowners\n. You can also toggle between individual users and groups to filter results based on a group of users. Or, select\nNo Owners\nin the owners filter to view assets that currently do not have any owners and assign them if needed.\nFor example, if you would like to filter assets by a group of owners:\nIn the\nFilters\nmenu on the left, click\nOwners\nto expand the menu.\nUnder\nOwners\n, click the\ngroup\nicon\n.\nClick the group name by which you want to filter your assets.\nTags\nâ\nYou can filter your assets by user-generated\ntags\n, such as\npublic\n,\nPII\n, and more. You can also select\nNo Tag\nin the tags filter to view assets that currently do not have any tags and add them if needed.\nFor example, if you would like to filter assets for a data compliance check:\nIn the\nFilters\nmenu on the left, click\nTags\nto expand the menu.\nUnder\nTags\n, click the relevant option   -  for example,\nPII\n.\n(Optional) Filter for assets by tags imported from supported sources:\nSelect a\nsynced Snowflake tag\nto view tagged Snowflake assets only.\nTo filter by Snowflake tag values, next to the tag name, click the rightward arrow to open the tag value menu.\nIn the\nFilter by tag value\ndialog, click the\nSelect tag value\ndropdown and then select a tag value to filter assets. You can either search by predefinedÂ\nallowed values\nor tag values.\nSelect a\nsynced dbt tag\nto view tagged dbt Cloud or dbt Core assets only.\nSelect a\nsynced Databricks tag\nto view tagged Databricks assets only. You can also filter by Databricks tag values.\nTerms\nâ\nYou can filter your asset search by terms from your\nglossaries\n. You can also select\nNo Terms\nin the terms filter to view assets that currently do not have any linked terms and add them if needed.\nFor example, if you would like find assets linked to a specific term:\nIn the\nFilters\nmenu on the left, click\nTerms\nto expand the menu.\nUnder\nTerms\n, click the relevant term   -  for example,\nMarketing Analysis\n-  to discover assets linked to that term.\n(Optional) Next to the search bar in the\nTerms\nfilter, click the\nAdvanced options\nicon to set a matching condition. Click the operators dropdown, and then:\nClick\nOr\nto filter assets that match any selected term(s).\nClick\nAnd\nto filter assets that match all selected terms.\nClick\nNone of\nto filter assets that do not match any of the selected term(s).\nClick\nNot empty\nto filter assets that have one or more\nlinked terms\n.\nClick\nEmpty\nto filter assets without any\nlinked terms\n.\nProperties\nâ\nProperties offer a variety of filters to narrow down your asset search. You can filter your assets by common asset properties, such as name, description, last updated, and more.Â\nTo search by common asset properties:\nIn the\nFilters\nmenu on the left, click\nProperties\nÂ to expand the menu.\nFrom the\nProperties\nmenu, you can:\nClick\nTitle\nto search by the technical name or\nalias\nof an asset. From the dialog, set your preferred matching condition (see\nDescription\nfilter).\nClick\nDescription\nto search by the description of an asset in Atlan or from the source.Â To set a matching condition, from the\nDescription\nÂ dialog, click the operators dropdown to:\nClick\nEquals (=)\nor\nNot Equals (!=)\nto include or exclude assets through exact match search.\nClick\nStarts With\nor\nEnds With\nto filter assets using the starting or ending sequence of values.\nClick\nContains\nto find assets with specified values contained within the property.\nClick\nPattern\nto filter assets using supported\nElastic DSL regular expressions\n.\nClick\nIs Empty\nor\nIs Not Empty\nto filter assets with or without null values.\nClick\nStarred assets\nto filter for all your\nstarred assets\n.\nClick\nHas lineage\nto filter for assets with or without\ndata lineage\n.\nClick\nHas readme\nto filter for assets with or without\nREADME files\n.\nClick\nHas resources\nto filter for assets with or without\nresources\n.\nClick\nAnnouncement\nto find assets with a specific\nannouncement\ntype. From the\nAnnouncement\ndialog, click the dropdown menu to select\nInformation\n,\nIssue\n,\nWarning\n, or\nNo announcement\n.\nClick\nUnique identifier\nto find assets with a unique ID. From the dialog, select your preferred matching condition and type the relevant information.\nClick\nQualified name\nto filter by a unique name for the asset. From the dialog, select your preferred matching condition and type the relevant information (see\nDescription\nfilter).\nClick\nLast updated (in source)\nor\nLast updated (in Atlan)\nto find assets by when they were last updated and where. From the dialog, set the condition to\nBefore\nor\nAfter\nand then select a date.\nClick\nCreated (in source)\nor\nCreated (in Atlan)\nto find assets by when they were created and where. From the dialog, set the condition to\nBefore\nor\nAfter\nand then select a date.\nClick\nCreated by (Atlan)\nor\nLast updated by\n(Atlan)\nto filter assets by a user who created or last updated the asset in Atlan. From the dialog, from the\nSelect user\ndropdown, select the user name.\nClick\nIs archived\nto view\narchived assets\nin the search results.\n(Optional) Click the\n+\nsign to add more filtering options   -  currently only available with the\nTitle\n,\nDescription\n,\nUnique identifier\n,\nQualified name\n,\nLast updated\n(in source and Atlan), and\nCreated\n(in source and Atlan) filters.\ndbt\nâ\ndanger\nThe dbt filters will only appear in the filters menu if\ndbt assets have been crawled\n.\nThe dbt filters allow you to filter your\ndbt Cloud\nand\ndbt Core\nassets to find the most relevant results. For example, if you need to find assets from a specific project in dbt:\nIn the\nFilters\nmenu on the left, click\ndbt\nto expand the menu.\nUnder\ndbt\n, click\nProject name\nto filter by a specific project.\nIn the\nProject name\ndialog:\nSelect the relevant matching condition.\nFor\nType\n, type the project name to filter your assets   -  for example,\nfood-beverage\n.\n(Optional) Click the\n+\nsign to add more filtering options. (Note: This may not be available for all the dbt filters).\nUsage\nâ\nThe usage filters allow you to\nfilter your assets by usage\nmetadata. For example, you can:\nFilter assets with zero queries and archive them.\nFind costly assets to better optimize your operations.\nDiscover recently updated assets and follow up on the updates.Â\nCustom metadata filters\nâ\ndanger\nYou first need to set up custom metadata properties and toggle on the\nShow in filter\nslider during setup.\nWhen you\nadd custom metadata\nin Atlan, you can also choose to set custom metadata properties as filters to help with quicker asset discovery.\nFor example, if you would like to filter your assets by custom user roles metadata:\nIn the\nFilters\nmenu on the left, click the custom metadata filter   -  for example,\nStewards\n.\nUnder the custom metadata filter, select the custom metadata property   -  such as\nData Steward\n.\nIn the property dialog, select the matching condition and user to filter your assets.\n(Optional) For\ncustom metadata option properties\nonly:\nYou can either:\nFor custom metadata properties with five or fewer options, click the operators dropdown.\nFor custom metadata properties with more than five options, next to the search bar, click the\nAdvanced options\nicon and then click the operators dropdown.\nFrom the operators dropdown, you can:\nClick\nOr\nto filter assets that match any selected value(s).\nClick\nAnd\nto filter assets that match all selected values   -  only supported if\nmultiple values are allowed\nfor custom metadata options.\nClick\nNone of\nto filter assets that do not match any of the selected value(s)   -  only supported if\nmultiple values are allowed\nfor custom metadata options.\nClick\nNot empty\nto filter assets that have one or more assigned values for the selected property.\nClick\nEmpty\nto filter assets without any assigned values for the selected property.\nAsset type filters\nâ\nYou can choose from two different sets of filters   -  type-specific or connector-specific.\nType-specific filters\nâ\nIf you're\nfiltering by a specific asset type\n, you can select type-specific property filters to further refine your search. For example, if you're filtering by:\nTables or views   -  you can filter these asset types by a specific row or column count.\nColumns   -  you can filter column assets by parent asset type and name, data type, or\ncolumn keys\n.\nProcess   -  you can filter\nprocess assets\nby the SQL query.\nQuery   -  you can filter saved queries by\nvisual queries\n.\nConnector-specific filters\nâ\nConnector-specific filters will appear in the filters menu only if there are crawled assets for a supported source and asset-type filters specific to the connector are applied.\nConnector-specific filters are currently supported for the following sources:\nAnomalo\nApache Kafka\n,\nConfluent Kafka\n,\nAiven Kafka\n,\nRedpanda Kafka\n, and\nAmazon MSK\nApache Airflow/OpenLineage\n,\nAmazon MWAA\n,\nAstronomer\n,\nGoogle Cloud Composer\n, and\nApache Spark\nGoogle BigQuery\nMatillion\nMicrosoft Azure Cosmos DB\nMicrosoft Azure Event Hubs\nMicrosoft Power BI\nMicroStrategy\nMongoDB\nMonte Carlo\nRedash\nSalesforce\nSisense\nSnowflake\nSoda\nTableau\nThoughtSpot\nQlik Sense Cloud\nand\nQlik Sense Enterprise on Windows\nGoogle Cloud Storage\nbuckets and objects\nMicrosoft Azure Data Lake Storage\naccounts, containers, and objects\nPreset\ndatasets, dashboards, and workspaces\nAPI\npaths\nFiles\n-  supported file types include DOC, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files\nTo use a connector-specific filter:\nFrom the\nAssets\npage, click the\nAsset type\nfilter, and then from the dropdown, select an asset type from a supported connector   -  in this example, we'll select Soda\nChecks\n.\nIn the\nFilters\nmenu on the left, click the\nSoda\nfilter to expand the menu.\nFrom the\nSoda\nfilter menu, filter your Soda checks by check status, owner, or last scanned at date.\n(Optional) For properties that allow selecting multiple values, you can set your preferred matching condition. Click the operators dropdown and then:\nClick\nOr\nto filter assets that match any selected value(s).\nClick\nAnd\nto filter assets that match all selected values.\nClick\nNone of\nto filter assets that do not match any selected value(s).\nClick\nNot empty\nto filter assets that have one or more assigned values for the selected property.\nClick\nEmpty\nto filter assets without any assigned values for the selected property.\nTags:\ndata\nintegration\nPrevious\nConfigure language settings\nNext\nAdd custom metadata\nSource\nAsset type\nDomains\nMetadata filters\nCustom metadata filters\nAsset type filters"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-descriptions",
    "content": "Use data\nDiscovery\nAsset Management\nAdd descriptions\nOn this page\nAdd descriptions\nYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a\nREADME\n. Doing so will enrich your data asset with the relevant contextual information.\nDid you know?\nThe description editor in Atlan also supports Markdown syntax. You can crawl descriptions in Markdown at source, view them in the asset sidebar and profile, and make edits in Markdown directly in Atlan.\nAdd descriptions to your assets\nâ\nTo add or update a description for your data asset, follow these steps:\nFrom the left menu on any screen, click\nAssets\n.\nOn the\nAssets\npage, click on an asset to view its\nOverview\nin the right menu.\nUnder\nDescription\n, click on the text box to add a description.\nOnce you've added new text or updated the existing one, click anywhere to automatically save your changes.\nYour asset description is now live! ð\nYou can also add or edit asset descriptions directly from the asset profile.\nDid you know?\nThe size limit for description values is 32766 bytes. Depending on the types of characters used, the character limit for descriptions can range from 8191 to 32766 characters.\nAdd column descriptions\nâ\nYou can also add a description for a single column rather than an entire data asset. To add column descriptions, follow these steps:\nOn the\nAssets\npage, click on an asset to view its asset profile.\nUnder\nColumn Preview\nin the asset profile, navigate to a column and click\n+Add\nunder\nDescription\n.\nOnce you've entered the text, click anywhere to save it.\nSearch using asset descriptions\nâ\nYou can also filter your assets by the keywords in your asset descriptions. Here are the steps:\nIn the\nFilters\nmenu on the\nAssets\npage, click\nProperties\n.\nNext, click\nDescription\nfrom the dropdown menu.\nIn the\nDescription\npopup display, enter a keyword in the text box.\nClick on the\ndownward arrow\nand then select the preferred matching condition.\nTo add multiple description filters, click\n+\nin the\nDescription\npopup.\nThe\nAssets\npage will now display a list of assets filtered by your description text. You can click\nClear All\nin the\nFilters\nmenu at any time to remove all your filters.\ndanger\nIf you're using integration code or\ncustom packages\nto update asset descriptions, there may be\nadditional nuances\nto consider since these can override either (or both) description attributes:\ndescription\nand\nuserDescription\n.\nTags:\ndata\nintegration\ncrawl\nasset-profile\nPrevious\nAdd owners\nNext\nAdd certificates\nAdd descriptions to your assets\nAdd column descriptions\nSearch using asset descriptions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/star-assets",
    "content": "Use data\nDiscovery\nAsset Management\nStar assets\nOn this page\nStar assets\nWho can do this?\nAnyone with access to Atlan   -  admin, member, or guest user   -  can star assets.\nAtlan allows you to star your most used assets and bookmark them for quick and easy access.\nOnce you have starred your assets, you can:\nAccess your starred assets from\nanywhere\nin Atlan\nGet a personalized view of your homepage with starred assets\nFilter by your starred assets\nSort your assets by star count\nView starred activity in the\nactivity log\nSet up Slack or Microsoft Teams notifications for metadata updates\nStar an asset\nâ\nNavigate to the left menu of any screen in Atlan and click\nAssets\nto begin:\nFrom asset sidebar\nâ\nTo star an asset from the asset sidebar:\nFrom the\nAssets\npage, click an asset you want to star.\nNavigate to the the upper right of the\nOverview\nsidebar and click the star button to star the asset and add it to your list of starred assets.\nFrom asset profile\nâ\nTo star an asset from the asset profile:\nFrom the\nAssets\npage, right-click an asset you want to star and select\nOpen profile\n.\nNavigate to the top right of the asset profile and click the star button to star the asset and add it to your list of starred assets.\nClick the star button again to remove the asset from your list of starred assets.\nDid you know?\nOnly\nyou\nhave the power to edit stars from your starred assets. However, for assets you've starred, other users can view your username while hovering over the star button and from the activity log.\nView starred assets\nâ\nOnce you have starred your assets, you can use the\nStarred assets\nwidget to view them from anywhere in Atlan. The starred assets widget will show you a total count and complete list of your starred assets.\nTo view starred assets:\nFrom the top right of any screen in Atlan, click the star icon. This will also show you a total count of your starred assets.\nFrom the\nStarred assets\npopup, you can view starred assets sorted by most recently starred or use the search bar to search for specific starred assets. Click\nOpen in Assets\nto view all your starred assets.\nThe corresponding\nAssets\npage will only show all your starred assets with the\nStarred assets\nfilter applied. To further refine your search, add\nmore filters\n.\n(Optional) Next to the search bar on the\nAssets\npage, click the sort button. From the\nStar count\nsorting menu, click\nMost starred\nto view most starred assets or\nFewest starred\nto view least starred assets.Â\nClick any starred asset to view more details:\nIn the top right of the\nOverview\nsidebar, hover over the star button to view total star count and users who have starred it in a popover.\nFrom the sidebar tabs on the right, click the\nActivity\ntab to view\nstarred activity\nby user and timestamp.\nEnable metadata update alerts\nâ\nYou can set up Slack or Microsoft Teams notifications for metadata updates on all your starred assets in Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts directly delivered to your\nSlack\nor\nMicrosoft Teams\naccount, you can stay informed about the latest changes to your starred assets.\nBefore you can set up notifications for starred assets, you will need to:\nSlack:\nIntegrate Slack and Atlan\nLink your Slack account to Atlan\nMicrosoft Teams:\nIntegrate Microsoft Teams and Atlan\n:\nFor any existing Microsoft Teams integrations prior to February 22, 2024 only, you will need to\nupdate the Atlan app in your Microsoft Teams workspace\nto use this feature.\nFor all new Microsoft Teams integrations from February 22, 2024 onward, no additional configuration required.\nLink your Microsoft Teams account to Atlan\nTo enable notifications for starred assets:\nFrom the top right of any screen in Atlan, click the star icon.\nFrom the bottom left of the\nStarred assets\npopup, click\nEnable notifications\n.\nTo link your Slack or Microsoft Teams account:\nIf you have already\nlinked your Slack account\n, skip this step. To\nlink your Slack account to Atlan\n, in the\nReceive notifications for starred assets\ndialog, click\nSlack\n, and from the corresponding screen, click\nAllow\nto continue.\nIf you have already\nlinked your Microsoft Teams account\n, skip this step. To\nlink your Microsoft Teams account to Atlan\n, in the\nReceive notifications for starred assets\ndialog, click\nTeams\n, and from the corresponding screen, click\nAllow\nto continue.\nIn the notifications setup dialog, for\nNotify me about\n, you can either:\nClick\nAll updates\nto receive notifications for all the changes listed in\nCustom updates\nmade to your starred assets.\nClick\nCustom updates\nto limit notifications to specific types of metadata updates:\nClick\nName\nto receive an alert when the name of an asset is updated.\nClick\nDescription\nto receive an alert when a\ndescription\nis added to, updated, or removed from an asset.\nClick\nAnnouncement\nto receive an alert when an\nannouncement\nis added to, updated, or removed from an asset.\nClick\nCertificate\nto receive an alert when a\ncertificate\nis added to, updated, or removed from an asset.\nClick\nOwners\nto receive an alert when an\nowner\nis assigned to or removed from an asset.\nClick\nReadme\nto receive an alert when a\nREADME\nis added to, updated, or removed from an asset.\nClick\nTerms\nto receive an alert when a\nterm is linked to or unlinked from an asset\n.\nClick\nTags\nÂ to receive an alert when a tag is\nattached to\nor\nremoved from\nan asset.\nClick\nPropagated Tags\nto receive an alert when a tag is\npropagated to\nor\nremoved from\nan asset.\nÂ Click\nSave\nto save your notification preferences.\n(Optional) To edit notification settings, from the bottom left of the\nStarred assets\npopup, click\nEnabled\n. In the notifications setup dialog, you can further customize your notifications.\n(Optional) To remove notifications, from the bottom left of the\nStarred assets\npopup, click\nEnabled\n. In the notifications setup dialog, click\nDisable notifications\nto reset your notification settings.\nYou will now receive\nSlack\nor\nMicrosoft Teams\nnotifications for changes made to all your starred assets in Atlan! ð\nTags:\ndata\nasset-profile\nPrevious\nAdd an alias\nNext\nAdd owners\nStar an asset\nView starred assets\nEnable metadata update alerts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-an-alias",
    "content": "Use data\nDiscovery\nAsset Management\nAdd an alias\nOn this page\nAdd an alias\nWho can do this?\nAny\nnon-guest user\nwith\nedit access to an asset's metadata\ncan add an alias. This only includes admin and member users.\nAn alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use\nAtlan AI\nto do the same, if\nAtlan AI is enabled in your Atlan workspace\n. This can help you improve the readability of your asset names while providing useful context to your users.\nAtlan recommends adding an alias that's unique to each asset, in a one-to-one relationship. To relate your assets, you can instead\nattach tags\nto group them by use case or\nlink terms\nto group them conceptually.\nAtlan currently supports adding an alias to everything EXCEPT the following asset types:\nDatabase\nSchema\nConnection\nProcess, including\nConnectionProcess\nBIProcess\nColumnProcess\nQuery\nExample\nâ\nFor an asset with a technical name like\nFCT_SUPPLIER_TRANSACTIONS\n, you can add\nSupplier Transaction Records\nas an alias. Once you have added an alias, you can:\nSearch for assets\neither with their technical names or aliases, Atlan will match on the most relevant title.\nUse the\nTitle\nproperty filter\nto filter for assets either by their technical names or aliases.\nSet\nasset name display preferences for personas\n, choosing whether the technical name or alias should be displayed prominently in search and discovery.\nView aliases in\nsearch results\n, asset preview,\nasset profile\n,\nasset sidebar\n, and\nlineage graph\n.\nAdd an alias\nâ\nTo add an alias to your asset:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nOn the\nAssets\npage, click any asset with a technical name. To add an alias, you can either:\nOpen the asset profile, and to the right of the asset name, click the pencil icon.\nNavigate to the\nOverview\nsidebar, and to the right of the asset name, click the pencil icon.\nIn the\nAdd an alias\ndialog, you can either:\nIn the\nType a business-friendly name\nfield, enter an alias for your asset.\nIf\nAtlan AI is enabled\n, under\nAtlan AI Suggested\n, click an Atlan AI-suggested alias for your asset. (Optional) Click the refresh icon to regenerate Atlan AI suggestions and compare different sets of suggestions.\nClick\nAdd\nto save your preferred alias for the asset.\n(Optional) Once you have added an alias, you can:\nHover over the asset name to view both the technical name and alias in a popup.\nFrom the asset sidebar tabs on the right, click the\nActivity\ntab to view\nalias activity\nby user and timestamp of update.\nTo edit your alias, click the pencil icon to make any changes.\nFrom the\nfilters menu\non the left, click\nProperties\nand then click\nTitle\nto filter assets by the technical name or alias.\nYour asset will now display an alias! ð\nYou can also\nset asset name display preferences\nto technical name or alias for your personas. If no preference has been specified and an alias is available, then Atlan will display the alias over the technical name on an asset by default.\nTags:\natlan\ndocumentation\nPrevious\nAccess archived assets\nNext\nStar assets\nExample\nAdd an alias"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/supported-sources",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nReferences\nSupported sources\nOn this page\nSupported sources\nAt Atlan, we have two core philosophies on integrations:\nSimple setup with out-of-the-box connectors\n.\nIn three steps\n, you'll have your sources connected, with activity logs and automated\nSlack\nor\nMicrosoft Teams\nalerts for easy monitoring.\nExtensible through\nOpen APIs\n. We've built Atlan on an Open API architecture, so every action on Atlan can be driven by APIs. This means you can bring in any data product you want, from any source.\nOut-of-the-box connectors\nâ\nFor more information on supported capabilities of Atlan's current integrations, see\nConnectors and capabilities\n.\nData sources\nâ\nAmazon Athena\nAmazon Redshift\nAWS Glue\n(including S3)\nDatabricks\nGoogle BigQuery\nHive\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nPrestoSQL\nRedash\nSalesforce\nSAP ECC\nSAP HANA\nSnowflake\nTeradata\nTrino\nNoSQL data sources\nâ\nAmazon DynamoDB\nMicrosoft Azure Cosmos DB\nMongoDB\nBusiness intelligence tools\nâ\nAmazon QuickSight\nDomo\nIBM Cognos Analytics\nLooker\nMetabase\nMicrosoft Power BI\nMicroStrategy\nMode\nQlik Sense Cloud\nQlik Sense Enterprise on Windows\nSigma\nSisense\nTableau\nThoughtSpot\nData movement tools\nâ\ndbt Cloud\ndbt Core\nFivetran\nMatillion\nMicrosoft Azure Data Factory\nData quality tools\nâ\nAnomalo\nMonte Carlo\nSoda\nEvent buses\nâ\nAiven Kafka\nAmazon MSK\nApache Kafka\nConfluent Kafka\nMicrosoft Azure Event Hubs\nRedpanda Kafka\nSchema registry\nâ\nConfluent Schema Registry\nOrchestration tools\nâ\nAirflow/OpenLineage\nAmazon MWAA/OpenLineage\nAstronomer/OpenLineage\nGoogle Cloud Composer/OpenLineage\nAlteryx\nData processing tools\nâ\nApache Spark/OpenLineage\nDidn't find a tool in your stack?\nâ\nYou can use Atlan's\nOpen APIs\nto bring in metadata from any source in the modern data stack. For example:\nData sources\nâ\nAWS S3 buckets and objects\nGoogle Cloud Storage buckets and objects\nMicrosoft Azure Data Lake Storage containers and objects\nNetSuite\nVertica\nAPIs\nFiles\nBusiness intelligence tools\nâ\nGoogle Data Studio\nPreset\nTags:\nintegration\nconnectors\nalerts\nmonitoring\nnotifications\nfaq\ntroubleshooting\napi\nrest-api\ngraphql\nPrevious\nOpenLineage configuration and facets\nNext\nCan I connect to any source with an ODBC/JDBC driver?\nOut-of-the-box connectors\nDidn't find a tool in your stack?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains/how-tos/manage-domains",
    "content": "Build governance\nDomains\nGet Started\nManage domains\nOn this page\nManage domains\nâ\nAvailable via the Data Marketplace package\nWho can do this?\nYou must be an\nadmin user\nin Atlan to create and manage domains. Any non-guest users must be granted the\nupdate metadata permission\nto be able to add assets to a domain.\nDomain policies\ncurrently don't have any impact outside the\nproducts module\n.\nDomains provide a logical way of mapping and organizing assets within a specific domain or business entity. For example, you can create domains for the following:\nFunctions such as finance, sales, and human resources\nBusiness units or brands for different products and services\nGeographic regions of operation\nEnvironments such as development and production\nMost importantly, domains help promote shared ownership and domain-level governance in your organization.\nTo create a domain, complete the following steps.\nAdd a domain\nâ\nTo add a domain:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nDomains\n. If you have\nenabled the products module\n, refer to\nHow to create data domains\ninstead.\nOn the\nDomains\npage, under\nAll domains\n, click\nCreate domain\n.\nFor\nOverview\n, enter basic details for your domain:\n(Optional) For\nCover\n, click the\nChange\nbutton to select an image from the gallery or upload an image of your own. Click\nReposition\nto drag and reposition the cover image and then click\nSave position\nto save your preferences.\n(Optional) For\nTheme\n, choose from the available color options to add a theme to your domain.\nFor\nName\n, enter a meaningful name for your domain   -  for example,\nProduct Operations\n.\n(Optional) Click the domain icon to change the icon for your domain.\n(Optional) For\nDescription\n, enter a description for your domain.\nFor\nOwners\n, assign additional users or groups as domain owner(s).\nIn the top right of the screen, click the\nCreate\nbutton to complete setup.\nCongrats on creating a domain in Atlan! ð\nYour users can now\nadd assets to your domain\n.\n(Optional) Add a subdomain\nâ\ndanger\nYou will first need to create a domain before you can add subdomains.\nSubdomains help you logically segment your domains according to business needs.\nTo add a subdomain:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nDomains\n. If you have\nenabled the products module\n, refer to\nHow to create data domains\ninstead.\nOnÂ the\nDomains\npage, under\nAll domains\n, select a domain or subdomain to add a subdomain.\nFrom the upper right of your domain page, click the\n+ Add\nbutton and then click\nNew sub-domain\n.\nFor\nOverview\n, enter basic details for your subdomain:\n(Optional) For\nCover\n, click the\nChange\nbutton to select an image from the gallery or upload an image of your own. Click\nReposition\nto drag and reposition the cover image and then click\nSave position\nto save your preferences.\n(Optional) For\nTheme\n, choose from the available color options to add a theme to your subdomain.\nFor\nName\n, enter a meaningful name for your subdomain   -  for example,\nAnalytics\n.\n(Optional) Click the domain icon to change the icon for your subdomain.\n(Optional) For\nDescription\n, enter a description for your subdomain.\nFor\nOwners\n, assign additional users or groups as subdomain owner(s).\nIn the top right of the screen, click the\nCreate\nbutton to complete setup.\nCongrats on creating a subdomain in Atlan! ð\nManage a domain\nâ\nThe domain profile includes essential details about the domain. You can also curate what your domain users will be able to view.\nTo manage a domain:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nDomains\n. If you have\nenabled the products module\n, refer to\nHow to create data domains\ninstead.\nOnÂ the\nDomains\npage, under\nAll domains\n, hover over a domain or subdomain to:\nView domain owners in the\nOwners\ncolumn.\nClick\n+ Add personas\nto\nadd a persona\nfor governing assets within a domain or subdomain.\ndanger\nAny non-guest users must be granted the\nupdate metadata permission\nto be able to add assets to a domain.\nClick the star button to\nstar your domain\nand bookmark it for easy access.\nFor subdomains only, to the right of the subdomain name, click the 3-dot icon and then:\nClick\nMove to\nto move a subdomain to a different parent domain. In the\nMove to\ndialog, select a relevant parent domain within the same or a different domain and then click\nMove\nto confirm the changes.\nClick\nConvert to domain\nto convert a subdomain into a parent domain. In the\nConvert to domain\ndialog, click\nConvert to domain\nto confirm your changes.\nClick a domain or subdomain to navigate to the domain or subdomain profile, respectively.\nOn your domain page, the\nOverview\ntab displays important details about the domain. (Optional) From the top right, click the\n+ Add\nbutton and then click\nNew sub-domain\nto add data subdomains.\nUnder\nSummary\n, view a total count of assets in your domain and the domain description:\n(Optional) Click\n+ Add stakeholder\nto\nadd stakeholders\n.\n(Optional) Click the\nDescription\nfield to update the description.\n(Optional) For\nOwners\n, click the pencil icon to add or remove\nowners\n.\n(Optional) If\ncustom metadata properties\nare available, you can add\ncustom metadata\nto your domain.\n(Optional) Click\n+ Add resource\nto\nadd a resource\nto your domain.\nUnder\nReadme\n, click\n+ Add\nto\nadd a README\nto your data domain or\nuse Atlan AI for documentation\n.\nÂ From the top right of the domain profile:\nClick the user avatars to view a list of recently visited users, total views on your domain, total number of unique visitors, and total views by user.\nUse the days filter to filter domain views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your domain\nand bookmark it for easy access.\nClick the\nSlack\nor\nTeams\nicon to post on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the\n3-dot\nicon to configure the following:\nClick\nAdd announcement\nto\nadd an announcement\nto your domain.\nClick\nAdd a resource\nto\nadd resources\nto your domain.\nClick\nArchive\nto archive your domain   -  ensure that your domain is empty before you archive it.\nChange to the\nAssets\ntab to view assetsÂ within your domain.\nChange to the\nStatistics\ntab to\nmonitor domain usage\n.\nIf you have\nenabled the products module\n, change to the\nLineage\ntab to view\nbusiness lineage for your domain\n.\nDid you know?\nYou can\nset up playbooks\nto bulk add assets to your domains and subdomains or remove them.\nTags:\natlan\ndocumentation\nPrevious\nDomains\nNext\nHow to organize assets\nAdd a domain\n(Optional) Add a subdomain\nManage a domain"
  },
  {
    "url": "https://docs.atlan.com/tags/asset-profile",
    "content": "5 docs tagged with \"asset-profile\"\nView all tags\nAccess archived assets\nLearn about access archived assets.\nAdd custom metadata\n<div style={{position: \"relative\", paddingBottom: \"calc(66.33333333333333% + 41px)\", height: 0}}> <iframe src=\"https://demo.arcade.software/1dT1bPneM5fp1O71lb.\nAdd descriptions\nYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.\nSearch and discover assets\nAtlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context.\nStar assets\n:::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can star assets."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights",
    "content": "Use data\nInsights\nOn this page\nInsights\nâ\nAvailable via the Insights package\nOverview:\nUse Atlan's Insights to query and analyze your data assets. Leverage the visual query builder or write SQL queries to explore your data, create visualizations, and share insights with your team.\nGet started\nâ\nHow to query data\nGuides\nâ\nQuery building\nâ\nHow to make a query interactive\n: Create interactive queries with parameters.\nHow to query without shared credentials\n: Query data without sharing credentials.\nHow to provide credentials to query data\n: Configure credentials for querying.\nQuery management\nâ\nHow to save and share queries\n: Save and collaborate on queries with your team.\nHow to schedule a query\n: Set up automated query execution.\nConcepts\nâ\nQuery builder actions\n: Learn about the different actions available in the query builder.\nReferences\nâ\nTips and tricks\n: Handy shortcuts and advanced capabilities for Insights.\nTroubleshooting\nâ\nTroubleshooting exporting large query results\n: Resolve issues when exporting large query result sets.\nTroubleshooting bring your own credentials\n: Fix credential-related problems when running queries.\nFAQ\nâ\nWhy do I only see tables from the same schema to join from in a visual query\n: Understand schema-based join limitations.\nCan I turn off sample data preview for the entire organization\n: Learn about sample data preview settings.\nAre there any limits on concurrent queries\n: Understand query concurrency limits.\nCan I query any data warehouse or data lake\n: Learn about supported data sources.\nHow to monitor for runaway queries\n: Track and manage query performance.\nCan we restrict who can query our data warehouse\n: Understand query access controls.\nHow can I identify an insights query in my database access log\n: Track query execution in logs.\nHow to use parameterized queries\n: Create dynamic queries with parameters.\nWhat controls the frequency of queries\n: Understand query scheduling and execution.\nTags:\ninsights\nquery\nanalysis\ncapabilities\nNext\nHow to query data\nGet started\nGuides\nConcepts\nReferences\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/query-without-shared-credentials",
    "content": "Use data\nInsights\nCredentials\nHow to query without shared credentials\nOn this page\nquery without shared credentials\nWho can do this?\nYou will need to beÂ a\nconnection admin\nin Atlan to enable\nbring your own credentials\n(BYOC) on a specific connection.\nDon't want to use a single shared service account to access data?\nWith\nbring your own credentials\n(BYOC), users need to provide their\nown\ncredentials before they can query data. Each user's permissions in the data store itself are then applied to each query. This is helpful for organizations that have already invested in defining granular controls in their data stores. With BYOC, you can reuse those controls without any extra work!\nAtlan currently supports the following connectors for BYOC:\nAmazon Athena\nAmazon Redshift\nDatabricks\nMySQL\nPostgreSQL\nPresto\nSnowflake\nTrino\nDid you know?\nTo query data using SSO credentials instead, refer to\nAuthenticate SSO credentials to query data\n.\nEnable BYOC on a connection\nâ\nTo enable BYOC on a connection:\nFrom the left menu of any screen, click\nAssets\n.\nFrom the pills below the search bar at the top of the screen, click\nConnection\n.\nFrom the list of results, select the connection for which you want to enable BYOC.\nFrom the sidebar on the right, next to\nConnection settings\n, click\nEdit\n.\nIn the\nConnection settings\ndialog:\nUnder\nAllow query\n, for\nAuthentication type\n,Â click\nBasic credentials\nto enforce\nuser credentials for querying data\n.\nUnderÂ\nDisplay sample data\n, for\nSource preview\n,Â click\nBasic credentials\nto enforce\nuser credentials for viewing sample data\n.\n(Optional) Toggle on\nEnable data policies created at source to apply for querying in Atlan\nto apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing\ndata policies\non the connection in Atlan will be deactivated and creation of new data policies will be disabled.\nAt the bottom right of the\nConnection settings\ndialog, click\nUpdate\n.\nTags:\natlan\ndocumentation\nPrevious\nSchedule a query\nNext\nProvide credentials to query data\nEnable BYOC on a connection"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/concepts/what-are-the-query-builder-actions",
    "content": "Use data\nInsights\nConcepts\nWhat are the query builder actions?\nOn this page\nWhat are the query builder actions?\nWhen using the\nVisual Query Builder\nyou can use any of the following actions in your query.\nTo illustrate their use, imagine the following sample tables:\nData for examples\nâ\nExpenses table\nâ\nCATEGORY_ID\nMEDIUM\nSPEND\n1\nFacebook\n447000\n1\nGoogle\n94500\n1\nLinkedIn\n12300\n2\nSales\n700000\n2\nMarketing\n400000\n3\nSalesforce\n250000\n3\nHubspot\n75000\nCategory table\nâ\nID\nNAME\n1\nAdvertisement\n2\nSalary\n3\nSoftware\n4\nOther\nActions\nâ\nJoin data\nâ\nJoin data to combine the contents of several assets. To join data you'll need to select:\nThe type of join (see table below).\nThe left asset and the column data to match with the right asset's column.\nThe right asset and the column data to match with the left asset's column.\nType of join\nExplanation\nInner Join\nreturns only matching records from both the left table and the right table.\nLeft Join\nreturns all records from the left table, and only matching records from the right table.\nRight Join\nreturns all records from the right table, and only matching records from the left table.\nOuter Join\nreturns all records from both tables.\nFor example, to get a set of results that shows the category of expenses along with the spend:\nSelect\nInner Join\nor\nLeft Join\nas the join type\nSelect\nExpenses\nfor the left table, and\nCATEGORY_ID\nas its column\nSelect\nCategory\nfor the right table, and\nID\nas its column\nThe resulting table would be:\nCATEGORY_ID\nMEDIUM\nSPEND\nID\nNAME\n1\nFacebook\n447000\n1\nAdvertisement\n1\nGoogle\n94500\n1\nAdvertisement\n1\nLinkedIn\n12300\n1\nAdvertisement\n2\nSales\n700000\n2\nSalary\n2\nMarketing\n400000\n2\nSalary\n3\nSalesforce\n250000\n3\nSoftware\n3\nHubspot\n75000\n3\nSoftware\nDid you know?\nYou can join more than 2 assets by clicking the\nAdd another\nlink at the bottom of the\nJoin data\ntile. Each additional join will add objects to the list of assets that can be used on the left of the join.\nFilter\nâ\nFilter data to return only some of the rows. To filter data you'll need to select:\nThe column data by which to narrow the rows.\nThe operation used to compare that column's data.\nThe value to compare against.\nFor example, to retrieve only the advertising spend:\nSelect\nNAME\nfor the column\nSelect\nEqual to\nfor the operation\nEnter\nAdvertisement\nfor the value\nThe resulting table would be:\nCATEGORY_ID\nMEDIUM\nSPEND\nID\nNAME\n1\nFacebook\n447000\n1\nAdvertisement\n1\nGoogle\n94500\n1\nAdvertisement\n1\nLinkedIn\n12300\n1\nAdvertisement\nGroup\nâ\nGroup data to combine rows together into buckets. To group data you'll need to select the column by which to bucket the data.\nFor example, to bucket the data by category:\nSelect\nNAME\nIf you remove the filter step above, the resulting table would be:\nNAME\nAdvertisement\nSalary\nSoftware\nDid you know?\nIt may look like you've over-simplified the data now. If your query stopped here, that would be true. But think of this action as preparation for other actions, like aggregating.\nAggregate\nâ\nAggregate data to calculate a metric from many rows of data. For example, to calculate a metric against each of the buckets created by grouping. To aggregate data you'll need to select:\nThe column on which to run the calculation\nThe calculation to run:\nCount\n: calculate the total number of rows\nUnique Count\n: calculate the number of unique values for this column across the rows\nSum\n: add all the values of the rows together\nMax\n: find the highest value in the rows\nMin\n: find the lowest value in the rows\nAverage\n: add all the values of the rows together, then divide by the number of rows\nFor example, to calculate the total spending by category:\nSelect\nSPEND\nfor the column\nSelect\nSum\nfor the calculation\nThe resulting table would be:\nNAME\nsum_SPEND\nAdvertisement\n862800\nSalary\n1100000\nSoftware\n325000\nSort\nâ\nSort data to return the rows in a defined order. To sort data you'll need to select:\nThe column by which to order the results\nThe direction in which to order them:\nASC\n: return the lowest value as the first row and highest value last\nDESC\n: return the highest value as the first row and lowest value last\nFor example, to return the category with the highest spending:\nSelect\nsum_SPEND\nfor the column\nSelect\nDESC\nfor the order\nThe resulting table would be:\nNAME\nsum_SPEND\nSalary\n1100000\nAdvertisement\n862800\nSoftware\n325000\nTags:\natlan\ndocumentation\nPrevious\nProvide credentials to query data\nNext\nInsights tips and tricks\nData for examples\nActions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/references/tips-and-tricks",
    "content": "Use data\nInsights\nReferences\nInsights tips and tricks\nOn this page\nInsights tips and tricks\nAt Atlan, we are committed to powering your user experience. Here are a few tips and tricks to help you get the most out of Insights for you and your team!\nExport large query results via email\nâ\nWho can do this?\nYou will need your Atlan administrator to\nenable scheduled queries\n. Once enabled, you need to be a connection admin to\nincrease the query limit\nto more than 100,000 rows. Atlan currently has an upper limit of 30 million rows. Reach out to your data success manager if youâd like to increase the limit for your organization.\nPreviously, users could only scan and see query results for up to 100,000 rows by default. To help you scan more rows and export those results, you can now export queries for more than 100,000 rows directly to your email inbox.\nTo export query results for more than 100,000 rows to your email inbox:\nIn the query editor in\nInsights\n, type a limit for more than 100,000.\n(Note: Be sure to uncheck the\nLimit to 100 rows\nbox in the query editor.)\nClick\nRun\nto run the query.\nThe query results set will only display 100,000 rows. In the yellow bar above the query results set, click\nSend results via email\n.\nYou will need to save the query to get the results via email. In the\nSave Query\ndialog box:\nSelect the relevant\nQuery collection\nto save your query. If you haven't created one, you will get a prompt to\ncreate a query collection\nduring this step.Â\nFor\nQuery name\n, type a name for your query.\n(Optional) Add a description, certification status, and term.Â\nClick\nSave Query\nto receive the query results in your email inbox.\n(Optional) To\nadd an announcement\nto alert others in your team, click\nAdd Announcement\n.Â\nThe query results will be delivered to your email inbox in a CSV file! ð\nIf your saved query consists of multiple queries, you'll receive the results for each query in a separate CSV file.\nDid you know?\nOnce the query run is successful, you will also be able to download the query results as a CSV file directly in Atlan. If you donât wish to run the query, you can abort the query at any time. If you have any questions, head over\nhere\n.\nOpen asset sidebar from Insights\nâ\nTo get all the context you need before querying an asset, you can open the asset sidebar directly from\nInsights\n.\nTo open the asset sidebar:\nIn the\nExplorer\nleft panel in\nInsights\n, hover over a table and click on the\nOpen sidebar\nicon. (The name may vary depending on the asset type, such as\nOpen table sidebar\nfor table assets).\nYou will now be able to view the asset sidebar for the asset you'd like to query! ð\nSearch and sort query results\nâ\nSearch query results\nâ\nWith the search feature enabled for query results, you can use the search bar to type a search term and filter your results by that specific term.Â\nTo search your query results after running a query:\nIn the query results set in\nInsights\n, click the\nSearch results\nbar.\nIn the\nSearch results\nbar, type your search term, such as\nTuesday\n, to see the query results for that term.\nSort query results\nâ\nIf you'd like to sort your query results, use the sort feature to order your query results.Â\nTo sort your query results after running a query:\nIn the query results set in\nInsights\n, click on a\ncolumn name\n, such as\nuser id\n, to sort your results in an ascending or descending order.\nDid you know?\nAtlan currently supports text, number, and Boolean data types for sorting query results.\nAdd inferred data types\nâ\ndanger\nAtlan currently supports inferred data types only for\nDatabricks\nÂ and\nSnowflake\nassets.\nIf your column data is stored in a standard data type   -  for example,\nSTRING\n-  you can add an inferred data type to the column. This will help you query the data\nusing the Visual Query Builder\n.Â\nFor example, if your column contains dates stored in a\nSTRING\nformat, you can infer this column as a\nDATE\ndata type in Atlan.\nTo add an inferred data type:\ndanger\nYou will need to select an acceptable inferred data type as per your data source.\nFrom a query\nâ\nFrom the left menu of any screen, click\nInsights\n.\nAt the top of the screen, to the right of the\nUntitled\ntab, click the\n+\nbutton and select\nNew visual query\n.\nUnder\nSelect from\n, choose the table you want to query.\nIn the column selector to the right, select the column to which you want to add an inferred data type   -  for this example, we'll choose a column with aÂ\nSTRING\ndata type,\nstringDateColumn\n.\nHover over your selected column, and in the top right of the metadata popover, click the\nOpen sidebar\nicon.\nAt the top of the column sidebar, click the\nthree dots\nicon and then click\nAdd inferred datatype\n.\nFor\nAdd Inferred datatype\n, type a secondary data type   -  for this example, we'll add a\nDATE\ndata type.\nClick\nAdd\nto add your inferred data type.Â\n(Optional) To query the column with an inferred data type:\nIn the query editor, click the blue circular\n+\nbutton and then select\nFilter\nas the action.\nFor\nWHERE\n, select the column with an inferred data type.\nUnder the selected column, click the inferred data type option to query the column in Atlan.\nFrom an asset\nâ\nFrom the left menu of any screen, click\nAssets\n.\nOn the\nAssets\npage, click a column asset with a\nSTRING\ndata type to open the asset sidebar   -  for example,\nstringDateColumn\n.\nAt the top of the column sidebar, click the\nthree dots\nicon and then click\nAdd inferred datatype\n.\nFor\nAdd Inferred datatype\n, type a secondary data type   -  for this example, we'll add a\nDATE\ndata type.\nClick\nAdd\nto add your inferred data type.\nThe inferred data type for your selected column will now be displayed in the asset sidebar! ð\nTo remove the inferred data type from a column asset, click the\npencil\nicon. In the dialog, click\nx\nto remove the data type and then click\nRemove\nto confirm removal.Â\nView querying costs\nâ\nTo help you gain the most value-driven insights in Atlan, you will receive a cost nudge before querying your Google BigQuery\nviews\nand\nmaterialized views\n. This will inform you about the precise bytes that will be spent during the execution of the query, helping you decide if you would still like to run the query. For all subsequent runs of the same query, Atlan will use\ncached query results\nfrom Google BigQuery   -  note that the query text must be the same as the original query.\nGoogle BigQuery table assets are already cost-optimized for querying.\nTo\nview querying costs\nfor a Google BigQuery view:\nUnder the\nExplorer\ntab in\nInsights\n, hover over a Google BigQuery view and click the play icon.\nIn the cost preview dialog, click\nCancel\nto cancel running the query based on the costs shown or click\nRun\nto proceed with running the query.\nDid you know?\nYou will also receive a cost nudge before viewing\nGoogle BigQuery sample data previews\n.\nTags:\natlan\ndocumentation\nPrevious\nWhat are the query builder actions?\nNext\nAre there any limits on concurrent queries?\nExport large query results via email\nOpen asset sidebar from Insights\nSearch and sort query results\nAdd inferred data types\nView querying costs"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/concurrent-queries-limit",
    "content": "Use data\nInsights\nFAQ\nAre there any limits on concurrent queries?\nAre there any limits on concurrent queries?\nAtlan allows up to 8 concurrent queries per user.Â If a user submits more than 8 queries simultaneously, any additional queries will be placed in a waiting state.\nThere is currently no limit on the total number of concurrent queries across multiple users in a tenant. Atlan is benchmarked to perform optimally with up to 200 users querying simultaneously.\nFor\nscheduled queries\n, Atlan sets a limit of 50 concurrent queries to ensure stability and optimal resource consumption.\nTags:\natlan\ndocumentation\nfaq-insights\nPrevious\nInsights tips and tricks\nNext\nCan I query any DW/DL?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/troubleshooting/troubleshooting-bring-your-own-credentials",
    "content": "Use data\nInsights\nTroubleshooting\nTroubleshooting bring your own credentials\nOn this page\nTroubleshooting bring your own credentials\nOnce your connection admins have\nconfigured\nbring your own credentials\n(BYOC)\nin Atlan, users will need to provide their own credentials before they can\nquery data\nor\npreview sample data\n. On that note, here are a few things to keep in mind:\nHow do I distinguish between credentials for BYOC and the crawler?\nâ\nIf user credentials are enforced:\nThere will be an admin credential setup for the crawler, which is part of the workflow configuration.Â\nThere will also be a separate user credential setup required for each user who wants to query the connection via Insights or view sample data.Â\nThe crawler's credentials will be used for the miner as well   -  such as for reading DDL commands and query history at source.\nWhat credentials do I need for sample data preview?\nâ\nWhether you need to provide credentials for sample data preview will depend on the connection settings. If default credentials are enabled, you'll be able to view sample data without having to provide your own credentials. If user credentials are enforced, you'll be prompted to\nprovide your credentials\nfor viewing sample data.\nCan I use admin or user credentials for both workflow and BYOC setup?\nâ\nBoth types of credentials can be used for configuring a crawler or enabling user credentials, as long as they are valid and have the required permissions.\nWhat type of credentials should be used for the workflow setup?\nâ\nThe credentials used must be associated with an account that has many permissions. However, this can be somewhat limited for certain sources. For instance, if\nusing the account usage method\nfor setting up a Snowflake workflow instead of the information schema method.\nTags:\natlan\ndocumentation\nPrevious\nWhy do I only see tables from the same schema to join from in a visual query?\nNext\nTroubleshooting exporting large query results"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/schedule-a-query",
    "content": "Use data\nInsights\nQuery Management\nSchedule a query\nOn this page\nSchedule a query\nWho can do this?\nBefore scheduling a query, you will need your Atlan administrator to\nenable scheduled queries\n.\nYou might want to schedule a query to run repeatedly. For example:\nIf you want to fetch data for the same query at different times.\nIf you want to share data with business teams on a consistent basis, for example, weekly.\nIf a data analyst is out of office but wants the data to be shared with users periodically.\ndanger\nYou must\nsave your query\nbefore you can schedule it. Your\nSMTP configuration\nmust also be in a working state to send results to recipients.\nSchedule a query\nâ\nTo schedule a query:\nOpen\nInsights\n.\nIn the upper left, click the\npapers-in-a-box\nicon to open your query collections.\nChoose the query collection where the query you want to schedule is saved.\nHover over the saved query you want to schedule, click the\nthree dots\nto its right, and then click\nSchedule\n.\nIn the\nNew Schedule Query\ndialog, enter the scheduling details:Â\nFor\nName\n, enter a meaningful name for the scheduled query.\nFor\nRuns every\n, choose how often the query should run:\nFor\nHour\nand\nDay\n, you can go down to the minute for hourly and daily runs.\nFor\nWeek\n, you can select multiple days for a weekly run.\nFor\nMonth\n, you can select multiple dates for a monthly run.\nFor\nCustom cron\n, write your own custom cron.\nFor\nTimezone\n, select a relevant option or keep the default one.\nFor\nShare Results\n, select the user(s) to whom you want to send the results from each run.\n(Optional) If there are any existing schedules for your query, from the top right, click the existing schedule link to view more details in a sidebar.\nAt the bottom right of the dialog, click\nDone\n.\n(Optional) Under\nQuery successfully scheduled\n, click the\nRun Now\nlink if you want to test the scheduled query.\nAt the bottom right of the dialog, click\nFinish\n.\nYour query will now run on the defined schedule and the results will be sent out automatically! ð\nNote that the download link for the results is only valid for\n6\nhours.Â\nDid you know?\nIf a scheduled query fails, you'll receive an email notification with the name of the query, so you can troubleshoot and get it running in no time.\nChange a scheduled query\nâ\nTo change the schedule for a query:\nOpen\nInsights\n.\nIn the upper left, click the repeating-arrow icon to open your scheduled queries.\nHover over the scheduled query you want to change, click the\nthree dots\nto its right, and then:\n(Optional)Â Click\nRun now\nto run the scheduled query immediately.\n(Optional) Click\nPause\nto pause the scheduled query.\nClick\nEdit\nto update the scheduled query.\nIn the\nUpdate Schedule Query\ndialog box, make any desired changes to the schedule or recipients.\nAt the bottom right of the dialog box, click\nUpdate\nto save the changes.\n(Optional) Under\nQuery successfully updated\n, click the\nRun Now\nlink if you want to test the scheduled query.\nAt the bottom right of the dialog, click\nFinish\n.\nYour query will now run on the changed schedule and the results will be sent out automatically! ð\nRemove a scheduled query\nâ\nTo remove a scheduled query:\nOpen\nInsights\n.\nIn the upper left, click the repeating-arrow icon to open your scheduled queries.\nHover over the scheduled query you want to stop, click the\nthree dots\nto its right, and then click\nDelete\n.\nIn the\nDelete Schedule\ndialog box, click\nDelete\n.\nYour query will no longer run automatically.\nTags:\ndata\nintegration\nconfiguration\nPrevious\nMake a query interactive\nNext\nHow to query without shared credentials\nSchedule a query\nChange a scheduled query\nRemove a scheduled query"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/make-a-query-interactive",
    "content": "Use data\nInsights\nQuery Management\nMake a query interactive\nOn this page\nMake a query interactive\nIf you want to share a query with others, but limit how they can change the query, you can make it\ninteractive\n.\nOthers can then only change the value(s) that are interactive in your query   -  not the query itself.\nWho can do this?\nThe owner of the query, or anyone with edit access to the query can make it interactive. Once interactive, anyone with read-only access to the query can also edit\nonly\nthe interactive part.\nSQL queries\nâ\nTo make a SQL query interactive:\nOpen the query in Insights.\nHighlight the value you want to be interactive.\nOn the bar separating the query from the results, click the curly braces icon and then click the\nAdd variable\nbutton.\nClick the settings icon to the right of the\nEnter a string\ntext box to define the custom variable:\nFor\nName\n, enter a meaningful name for the custom variable.\nFor\nType\n, change the data type of the custom variable, if necessary.\nFor\nDefault value\n, enter the value to use for the custom variable if other users do not change it.\nAt the bottom of the\nEdit\ndialog for the custom variable, click\nSave\n.\nIn the upper right of the query editor, click\nRun\nto confirm the query still operates as expected.\nThat's it, your query is now interactive! ð\nDid you know?\nYou can select from a wide range of data types for your custom variables   -  string, number, date, date and timestamp ranges, and multi-value options.\nVQB queries\nâ\nTo make a Visual Query Builder (VQB) query interactive:\nOpen the query in Insights.\nFind the value you want to be interactive.\nIn the right of the box for that value, click the lightning bolt icon.\nIn the upper right of the query editor, click\nRun\nto confirm the query still operates as expected.\nThat's it, your query is now interactive! ð\nTags:\natlan\ndocumentation\nPrevious\nSave and share queries\nNext\nSchedule a query\nSQL queries\nVQB queries"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/how-tos/integrate-slack",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nHow-tos\nHow to integrate Slack\nOn this page\nIntegrate Slack\nWho can do this?\nYou will need to be an admin in Atlan to configure the Slack integration. You will also need inputs and approval from an administrator of your Slack workspace.\nTo integrate Slack and Atlan, follow these steps.\nRetrieve Slack tokens\nâ\nTo retrieve Slack's integration tokens:\nAccess your Slack apps console at:\nhttps://api.slack.com/apps\nAt the bottom of the page, in theÂ\nYour App Configuration Tokens\nbox click theÂ\nGenerate Token\nbutton.\nIn theÂ\nGenerate Your App Configuration Token\ndialog, from the\nWorkspace\ndrop-down choose your Slack workspace and then click theÂ\nGenerate\nbutton.\nFrom theÂ\nYour App Configuration Tokens\nbox:\nUnderÂ\nAccess Token\nclick theÂ\nCopy\nbutton and save this temporarily.\nUnderÂ\nRefresh Token\nclick theÂ\nCopy\nbutton and save this temporarily.\ndanger\nThese tokens will usually expire after 12 hours, so will need to be used the same day they are generated.\nConnect Atlan to Slack\nâ\nTo connect Atlan to Slack, from within Atlan:\nFrom the left menu, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, clickÂ\nIntegrations\n.\nIn theÂ\nSlack\ntile, click theÂ\nConnect\nbutton.\nEnter the tokens copied above:\nForÂ\nAccess token\nenter the access token value.\nForÂ\nRefresh token\nenter the refresh token value.\nClickÂ\nNext\nto continue.\nUnderÂ\nInstall the Atlan app in your Slack workspace\nclick theÂ\nInstall now\nbutton.\nAt the bottom of the resulting Slack popup, click the\nAllow\nbutton. (If you want more details on what each permission does, see\nWhat does Atlan do with each Slack permission?\n)\n(Optional) Request permission from your Slack admin\nâ\nIf you are not a workspace administrator in Slack, you will be prompted to request permission to install. To request permission to install the integration:\nUnderÂ\nAdd a message for your App Managers\nenter an explanation for installing the app.\nAt the bottom of the form, click theÂ\nSubmit\nbutton.\nContact your Slack workspace administrator and ask them to approve the Atlan app.\nOnce approved, you'll get an alert in Slack from Slackbot.\nOnce you receive the Slackbot alert, return to theÂ\nIntegrations\nmenu in Atlan (in the Admin Center) and click theÂ\nAdd to Slack\nbutton.\nAtlan is now connected to Slack! ð\nConfigure integration from Atlan to Slack\nâ\nTo configure the Slack integration from Atlan, from the\nIntegrations\nsub-menu:\nExpand the\nSlack\ntile.\nUnder theÂ\nConfigurations\ntab, enter channels to use in your Slack workspace. Enter the channel name or provide a link to the channel without a\n#\n, and press tab or enter after each channel to add multiple channels.\n(Optional) For\nChannels\n, add any channels that users should be able to post to from within Slack or\nget notified on glossary updates\n.\n(Optional) For\nAnnouncements channel\n, enter the name of a single channel that can be used to view\nannouncements on assets\nin Atlan.\n(Optional) For\nWorkflows alert channel\n, enter the name of a single channel that can be used to view\nalerts for workflow activities\nin Atlan. You can also choose to receive failure alerts only by toggling on the\nReceive failure alerts only\nslider.\n(Optional) For\nPlaybooks alert channel\n, enter the name of a single channel that can be used to view\nalerts for playbook runs\nin Atlan. You can also choose to receive failure alerts only by toggling on the\nReceive failure alerts only\nslider.\n(Optional) ForÂ\nQuery output share channels\n, add any channels where users should be able to share query output.\n(Optional) For\nRequest notifications\n, toggle on the slider to receive Slack notifications when\nrequests\nare raised in Atlan and\napprove or reject them\ndirectly from Slack.\nAt the bottom of the tile, click theÂ\nUpdate\nbutton.\nUsers can now post to Slack without leaving Atlan! ð\nDid you know?\nChannels need to be public for Atlan to be able to post to them. If you try to integrate a private channel you will see an error for that channel when you try to update.\nConfigure integration from Slack to Atlan\nâ\nTo configure the Atlan integration from Slack, from within Slack:\nOpen each channel you want Slack users to be able to query Atlan from within.\nAt the top of the channel, click the name of the channel.\nChange to theÂ\nIntegrations\ntab.\nIn theÂ\nApps\ntile, click theÂ\nAdd an app\nbutton.\nFind the\nAtlan\napp under the\nIn your workspace\nheading and click the\nAdd\nbutton next to it.\n(Optional) To add an icon to the Atlan app in Slack, from the\nApps\npage, click the\nAtlan\napp. On the\nDisplay Information\npage, under\nApp icon & Preview\n, click\n+ Add App Icon\nand upload the\nAtlan icon\nfor the app.Â\nUsers can now search for assets in Atlan without leaving Slack! ð\nDid you know?\nYou can even add the Atlan app to private channels.\nTags:\nintegration\napi\nconfiguration\nPrevious\nSlack\nNext\nLink your Slack account\nRetrieve Slack tokens\nConnect Atlan to Slack\nConfigure integration from Atlan to Slack\nConfigure integration from Slack to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nMicrosoft Teams\nHow-tos\nHow to integrate Microsoft Teams\nOn this page\nIntegrate Microsoft Teams\nWho can do this?\nYou will need to be an\nadmin\nin Atlan to configure the Microsoft Teams integration. You will also need inputs and approval from the\nApplication Administrator\nand\nTeams Administrator\nof your Microsoft Teams workspace.\nTo integrate Microsoft Teams and Atlan, follow the\nse steps.\nRetrieve the team link\nâ\nTo retrieve the team link, from within Microsoft Teams:\nIn the left menu in\nMicrosoft Teams\n, click\nTeams\n.\nUnder\nTeams\n, navigate to your team, and to the right of the team name, click the\nthree dots\n.\nFrom the dropdown menu, click\nGet link to team\n.\nFrom the\nGet a link to the team\ndialog, click\nCopy\nto copy the team link and save it in a temporary location.\nConnect Atlan to Microsoft Teams\nâ\nOnce you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams.\nAtlan requires the following delegated permissions for the Microsoft Teams integration:\noffline_access\n-  allows the Atlan app to access resources on behalf of the users, even when users are not currently using the app.\nUser.Read\n-  allows users to sign in to the app, and allows the app to read the profile of signed-in users.\nAppCatalog.Submit\n-  allows the app to submit application packages to the catalog and cancel submissions that are pending review on behalf of the signed-in user.\nChannelMessage.Read.All\n-  allows the app to read a channel's messages in Microsoft Teams, on behalf of the signed-in user (only required if you have admin access to publish the app in Microsoft Teams).\nAppCatalog.ReadWrite.All\n-  allows the app to create, read, update, and delete apps in the app catalogs (only required if you have admin access to publish the app in Microsoft Teams).\nTeamsAppInstallation.ReadWriteSelfForUser\n-  allows a Teams app to read, install, upgrade, and uninstall itself for the signed-in user.\nFrom within Atlan:\nFrom the left menu on any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nIntegrations\n.\nIn the\nMicrosoft Teams\ntile, click\nConnect\n.\nIn the\nAdd to Microsoft Teams\ndialog, for\nTeam link\n, paste the\nteam link you copied\nfrom Microsoft Teams above.\nClick\nNext\nto continue.\nFor\nPublish the Atlan app\n:\nIf you have\nApplication Administrator\naccess to Microsoft Teams:\nClick\nPublish now\nto publish the Atlan app in your Microsoft Teams workspace.\nIn the\nPermissions requested\npopover, check the box for\nConsent on behalf of your organization\nto automatically grant access to the application for all users and then click\nAccept\nto publish the Atlan app.\nIf you do not have admin access to Microsoft Teams:\nClick\nRequest to publish\n.\nIn the\nPermissions requested\npopover, click\nAccept\nto publish the Atlan app.\nContact your Microsoft Teams administrator to\napprove\nand\npublish\nthe Atlan app in your Microsoft Teams workspace.\nDid you know?\nIf your global administrator has enabled the\nadmin consent workflow\n, you will be prompted to request admin approval while attempting to install the Atlan app in your Microsoft Teams workspace. Reach out to your admin to\napprove the admin consent request\nfrom the Microsoft Entra admin center. Additionally, if there is any\nexpiry date\nset for such requests, ensure that the request is approved within that period of time, otherwise you will need to request approval again.\nAdd Atlan to the Microsoft Teams directory\nâ\ndanger\nYou\nmust\nallow 24 hours to elapse after\nsetting up the Microsoft Teams integration in Atlan\nand before adding the Atlan app to your Microsoft Teams directory. This is required for the Atlan app to be activated in Microsoft Teams. To learn more, refer to\nMicrosoft Teams documentation\n.\nOnce the Atlan app has been published in your Microsoft Teams workspace, you will need your\nTeams Administrator\nto add Atlan to your Microsoft Teams directory.\nFrom within Microsoft Teams:\nLog in to the\nMicrosoft Teams admin center\nwith\nTeams Administrator\naccess.\nIn the left menu of\nMicrosoft Teams\n, click\nTeams apps\nand then click\nManage apps\n.\nOn the\nManage apps\npage, use the search bar to search for the\nAtlan\napp.\nTo the left of the app name, click the circle to select the\nAtlan\napp.\nFrom the tabs along the top of the\nAll apps\nsection, click\nAdd to a team\n.\nIn the\nAdd to a team\nform, configure the following:\nClick the search bar and enter the name of the team to which you want to add the Atlan app. Hover over the team name and then click\nAdd\n.\nClick\nApply\nto confirm your selection(s).\nAtlan is now connected to Microsoft Teams! ð\nConfigure integration from Atlan to Microsoft Teams\nâ\nNow that Atlan is connected to Microsoft Teams, you can configure the Microsoft Teams integration from Atlan.\nFrom the\nIntegrations\nsub-menu:\nExpand the\nMicrosoft Teams\ntile.\nUnder the\nConfigurations\ntab, configure the following:\nFor any existing Microsoft Teams integrations prior to February 22, 2024 only, you will need to update the Atlan app in your Microsoft Teams workspace to use new features like\nenabling notifications for starred assets\n:\nIf you have\nApplication Administrator\naccess to Microsoft Teams:\nClick\nInstall now\nto update the Atlan app in your Microsoft Teams workspace.\nIn the\nPermissions requested\npopover, click\nAccept\nto update the Atlan app.\nIf you do not have admin access to Microsoft Teams:\nClick\nRequest update\n.\nContact your Microsoft Teams administrator to\nupdate\nthe Atlan app in your Microsoft Teams workspace.\nOnce updated, click\nCheck status\nin Atlan to view the status of your requested update.\ninfo\nðª\nDid you know?\nFor all new Microsoft Teams integrations from February 22, 2024 onward, no additional configuration required.\nFor\nChannels\n, enter channels to use in your Microsoft Teams workspace to allow users to post to from within Microsoft Teams or\nget notified about glossary updates\n. Enter the channel name without a\n#\n, and press tab after each channel if you want to enter multiple channels.\n(Optional) For\nAnnouncements channel\n, enter the name of a single channel that can be used to view\nannouncements on assets\nin Atlan.\n(Optional) For\nWorkflows alert channel\n, enter the name of a single channel that can be used to see\nalerts for workflow activities\nin Atlan. You can also choose to receive failure alerts only by toggling on the\nReceive failure alerts only\nslider.\n(Optional) For\nPlaybooks alert channel\n, enter the name of a single channel that can be used to see\nalerts for playbook runs\nin Atlan. You can also choose to receive failure alerts only by toggling on the\nReceive failure alerts only\nslider.\nAt the bottom of the tile, click the\nUpdate\nbutton.\nUsers can now post to Microsoft Teams without leaving Atlan! ð\nDid you know?\nChannels need to be\nstandard\nfor Atlan to be able to post to them. If you try to integrate a private or shared channel, you will see an error for that channel when you try to update.\nTags:\nintegration\nconnectors\nPrevious\nMicrosoft Teams\nNext\nLink your Microsoft Teams account\nRetrieve the team link\nConnect Atlan to Microsoft Teams\nAdd Atlan to the Microsoft Teams directory\nConfigure integration from Atlan to Microsoft Teams"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/concepts/what-are-requests",
    "content": "Use data\nRequests\nConcepts\nWhat are requests?\nOn this page\nRequests\nDid you know?\nAtlan supports\ngovernance workflows\n! Once you have\nenabled governance workflows and inbox\n, Atlan will channel requests and approvals for your governed assets through governance workflows and land them in your\ninbox\n.\nWho can do this?\nAny\nuser\nwithout\nedit access to an asset's metadata\ncan request changes to an asset.\nRequests allow users to suggest changes to assets that they cannot directly change themselves.\nLet's break that down a little bit:\nAdmin users\ncan directly change assets where they have permission, and suggest changes to all other assets.\nMember users\ncan directly change assets where they have permission, and suggest changes to all other assets.\nGuest users\ncan only suggest changes to an asset if the\nadmin has enabled requests for guests\n.\nExamples\nâ\nThe key point to understand is the statement \"where they have permission\". As described in\nHow do I control access to metadata and data?\nthere are many options in Atlan to control these permissions.\nSo let's look at a few examples:\nConnection admin permissions\nâ\nIn the simplest case,\nconnection admins\n:\nHave direct edit access to all assets in a connection\nBoth admin users and member users can be connection admins\nConnection admins will generally not be able to suggest changes to assets in their connection, because they can make changes to them directly. (The only exception to this is described below   -  when the edit permission granted to connection admins by default is overridden by a policy that explicitly denies edit permissions.)\nPolicy-based permissions\nâ\nYou can grant more granular permissions through\naccess policies\n:\nControl the subset of assets in a connection that can be directly changed (or not)\nControl the specific characteristics of assets that can be directly changed (or not)\nFor example, in a\nmetadata policy\nyou could specify that:\nFor any assets (tables, columns) in a certain schema,\na certain group of users can only add or remove terms (no other changes).\nIn this example, any admin users or member users in that group will still be able to suggest other changes to the assets in that schema. (For example, changes to the description, owners, certification, or tags.)\nDid you know?\nRemember that\naccess is denied by default\n. This means if an admin user or member user is\nnot\na connection admin, and is\nnot\npart of any policy giving them permission to change metadata, then\nby default\nthe user will only be able to suggest changes to metadata.\nOverridden permissions\nâ\nKeep in mind, though, that\nexplicit restrictions always take priority\n. This means even when one policy (or being a connection admin) would otherwise give permission to change assets directly, it is possible to\noverride\nthat permission.\nFor example, in a\nmetadata policy\nyou could specify that:\nFor any assets (tables, columns) in a certain database,\nall updates are explicitly denied to a certain group of users.\nEven if a user\ndoes have\npermission to directly update an asset from some other policy, this explicit deny policy will take priority. In this case, the user will only be able to suggest changes to the assets in that database. It does not matter if they are an admin user or member user, or even a connection admin.\nTags:\nworkflow\nautomation\norchestration\nPrevious\nManage requests\nExamples"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/slack-integration",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nFAQ\nWhat is included in the Slack integration?\nOn this page\nWhat is included in the Slack integration?\nWith two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.\nOnce you\nintegrate Slack with Atlan\nyou can do all of the following â all without leaving Atlan!\nShare data assets on Slack\nLink important Slack threads to Atlan assets\nStart Slack conversations\nSet up Slack notifications for\nannouncements\n,\ngovernance workflows\n,\nglossary updates\n,\nstarred assets\n, and\nplaybook runs\nWhen users share assets on Slack (like assets, glossaries and saved queries) Atlan provides an embedded preview and context for the asset directly in Slack.\ninfo\nðª\nDid you know?\nTo use all of the features outlined, each user needs to\nconnect their individual Slack account\nto their Atlan profile.\nAsk questions about assets, without leaving Atlan\nâ\nTo ask questions about assets on Slack, without leaving Atlan:\nFor any asset, in the asset sidebar, click the\nSlack\nicon.\nIn the\nShare on Slack\ndialog:\nFor\nChannel\n, choose the Slack channel where you want to post your question.\nFor\nMessage\n, enter the question you want to ask on the Slack channel. (Optional) Type\n@\nto select another user from your Atlan workspace and tag them in your message.\n(Optional) For\nAdd as a resource\n, toggle the slider on to add the message as a\nresource\nto your asset. When you add the message as a resource, anyone viewing the asset in Atlan will be able to see previous discussions in Slack about that asset.\nClick\nShare\nto post on Slack.\nYour message is now on Slack and linked to the asset for future reference! ð\nAdd existing Slack discussions to assets\nâ\nwarning\nð¤\nWho can do this?\nAny\nnon-guest user with edit access to an asset's metadata\ncan add existing Slack discussions to assets.\nEver discussed a data asset in Slack? It's important to bring that context back to your asset.\nFrom within Slack\nâ\nTo add existing discussions in Slack into Atlan, from Slack:\nOn any discussion in Slack, open the context menu for actions and click\nSend to Atlan\n.\nFind the asset you want to link the discussion to:\nFor\nTitle\nenter a brief summary of the discussion.\nFor\nAsset Types\nselect the type of asset you want to link the discussion to.\nFor\nAsset\nstart typing in the asset you want to link the discussion to. Slack will run a search against Atlan and give you a list of options â select your asset from the list.\nAt the bottom right of the dialog, click the\nSubmit\nbutton.\nYour existing discussion on Slack is now linked to the asset for future reference. And Atlan has even updated the discussion thread with a link to the asset for everyone's convenience within Slack! ð\nFrom within Atlan\nâ\nTo add existing discussions in Slack into Atlan, from Atlan:\nFrom the asset, on the right of the screen, click the\nSlack\nsidebar icon:\nIf there are existing discussions on the asset, to the right of\nSlack Conversations\n, click the\nAdd\nlink.\nIf there are no existing discussions on the asset, click the\nAdd Slack Thread\nbutton.\nIn the\nAdd Slack Thread\ndialog, under the\nLink\nheading, paste the link to the Slack discussion.\nUnder the\nTitle\nheading, write a brief description of the discussion. This will only be displayed to users who have not\nlinked their Slack account\n.\nAt the bottom of the dialog, click the\nAdd\nbutton.\nYour existing discussion on Slack is now linked to the asset for future reference! ð\nStart direct chats from Atlan user profiles\nâ\nYou can only start a direct chat with Atlan users who have\nlinked their Slack account\nto their Atlan profile. Once linked, the\nSay Hi\nSlack icon will appear next to the user's name.\nTo start a direct chat with another Atlan user:\nHover over any username.\nIn the resulting dialog, click the\nSay Hi\nSlack icon next to the user's name.\nYou are now in a direct chat with that user in Slack! ð\nEnable alerts for glossary updates in Atlan\nâ\nwarning\nð¤ Who can do this?\nYou will need to be an\nadmin user\nin Atlan to enable Slack notifications for glossary updates. Once enabled, anyone in the selected Slack channel will be able to view these alerts directly in Slack.\nYou can set up Slack notifications for updates made to your\nglossaries\nin Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts sent to a\nSlack channel of your choice\n, you'll be able to stay informed about the latest changes to your glossaries.\nSlack notifications can only be set up at the glossary level. Even if configured from a specific term or category profile, the notification settings will be applicable to the entire glossary.\nTo enable Slack notifications for glossary updates, from Atlan:\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of the glossary for which you want to set up change notifications.\nFrom the top right of the glossary profile, click the bell icon to set up notifications for glossary updates.\nIn the notifications setup dialog, click\nSlack\nand then enter the following details:\nFor\nNotification channel\n, click the dropdown to select a Slack channel where you want to receive notifications from the channel(s) configured in your\nSlack integration in Atlan\n.\nFor\nNotify me about\n, you can either:\nClick\nAll updates\nto receive notifications for all the changes listed in\nCustom updates\nmade to your glossary.\nClick\nCustom updates\nto limit notifications to specific types of updates:\nClick\n+ New term or category\nto receive an alert when a new term or category is added to the glossary.\nClick\nName\nto receive an alert when the name of the glossary or that of a nested category or term is updated.\nClick\nDescription\nto receive an alert when a\ndescription\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nAnnouncement\nto receive an alert when an\nannouncement\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nCertificate\nto receive an alert when a\ncertificate\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nOwners\nto receive an alert when an\nowner\nis assigned to or removed from the glossary or a nested category or term.\nClick\nReadme\nto receive an alert when a\nREADME\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nTags (for terms)\nto receive an alert when a tag is\nattached to\nor\nremoved from\na term within the glossary.\nClick\nCategories updated (for terms)\nto receive an alert when a\nterm is assigned to a different category\nwithin the same glossary. (Notifications for moving terms across glossaries are currently not supported.)\nClick\nRemove term\nto receive an alert when a term is removed from the glossary. (Notifications for removal of categories are currently not supported.)\nClick\nSave\nto save your notification preferences.\n(Optional) To edit notification settings, click the bell icon. In the notifications setup dialog, you can change the channel or further customize your notifications.\n(Optional) To remove notifications, click the bell icon. In the notifications setup dialog, click the\nEnabled (by username)\ndropdown. Click\nRemove notification\nto reset your notification settings.\nYou will now receive Slack notifications for changes made to your glossary! ð\nIf the\nSlack channel\nyou selected for sending notifications is removed from your Slack integration in Atlan, glossary alerts will continue to be sent to that channel. In that case, you can either remove the notifications or select a different channel.\ninfo\nðª\nDid you know?\nYou can also set up Slack notifications for metadata updates on all your\nstarred assets\nin Atlan.\nQuery Atlan from within Slack\nâ\nFind terms\nâ\nTo search for terms in Atlan from within Slack:\nEnter\n/search-term <search words>\n.\nReview the results (only visible to you).\nSend any result(s) you want to the channel.\nYou've now shared terms with others without ever leaving Slack! ð\nFind queries\nâ\nTo search for queries in Atlan from within Slack:\nEnter\n/search-query <search words>\n.\nReview the results (only visible to you).\nSend any result(s) you want to the channel.\nYou've now shared queries with others without ever leaving Slack! ð\nProvide context for any asset link\nâ\nOther times you may have a link for an asset already. You can do more than just share the link â Atlan's bot will expand the context of that link directly in Slack.\nTo provide detail for an asset in Slack:\nPaste an asset link into Slack, along with any other context in your message.\nThe Atlan bot will expand the link with a card of contextual metadata.\nThe channel can now understand the basics about the asset without ever leaving Slack! ð\ninfo\nðª\nDid you know?\nOnce your organization's Slack account is integrated with Atlan, your users will be able to receive\nSlack notifications on their requests\n.\nView workflow alerts in Slack\nâ\nYou can\nconfigure the Slack integration\nto receive alerts for workflow activities from Atlan. This can help you\nmonitor your workflows\ndirectly in Slack. You can also choose to receive failure alerts only.\nTo view workflow alerts in Slack:\nOpen the Slack channel you\nconfigured\nto view workflow notifications.\nThe Atlan bot will share workflow alerts â including details like run status, start time, run time, trigger type, and last three runs.\n(Optional) Click\nView on Atlan\nto open the workflow link in Atlan.\nYou can now inspect crawled assets or troubleshoot in case of any failed workflows.\ninfo\nðª\nDid you know?\nAtlan also supports workflow alerts for all\ncustom packages\n. If you have set up any custom packages on your Atlan instance, you will be able to monitor your workflows directly in Slack.\nTake action on requests in Slack\nâ\nYou can\nconfigure the Slack integration\nto receive notifications for metadata update\nrequests\nraised in Atlan, and approve or reject them directly from Slack.\nTo take action on requests in Slack:\nFrom the left\nApps\nmenu in Slack, click the Atlan app.\nIf\nrequest notifications are enabled\n, the app will display requests raised in Atlan â including asset name and basic information, and the type of metadata update requested. To take action on a request:\nClick\nReject\nto reject the update.\nClick\nAccept\nto accept the update.\nThe status of the request will be updated in Slack to inform your team that no further action is required.\nTags:\nslack\nintegration\ncollaboration\ndata\nfaq\nfaq-integrations\nPrevious\nSlack permissions explained\nNext\nBulk enrich metadata\nAsk questions about assets, without leaving Atlan\nAdd existing Slack discussions to assets\nFrom within Slack\nFrom within Atlan\nStart direct chats from Atlan user profiles\nEnable alerts for glossary updates in Atlan\nQuery Atlan from within Slack\nFind terms\nFind queries\nProvide context for any asset link\nView workflow alerts in Slack\nTake action on requests in Slack"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/faq/microsoft-teams-integration",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nMicrosoft Teams\nFAQ\nWhat is included in the Microsoft Teams integration?\nOn this page\nWhat is included in the Microsoft Teams integration?\nWith two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.\nOnce you've\nintegrated Microsoft Teams with Atlan\n, you can do all of the following   -  all without leaving Atlan!\nShare data assets on Microsoft Teams\nLink important Microsoft Teams threads to Atlan assets\nSet up Microsoft Teams notifications for\nannouncements\n,\nglossary updates\n,\nstarred assets\n, and\nplaybook runs\nWhen users share assets on Microsoft Teams   -  including asset links, glossaries, and saved queries   -  Atlan will provide an embedded preview and context for the asset directly in Microsoft Teams.\nShare assets without leaving Atlan\nâ\nTo share assets on Microsoft Teams, without leaving Atlan:\nFrom any asset in Atlan, in the asset sidebar, click the\nShare on Teams\nicon.\nIn the\nShare on Teams\ndialog:\nFor\nChannel\n, choose the Microsoft Teams channel where you want to post.\nFor\nMessage\n, enter the message you want to share for the asset on the channel. (Note that tagging another user from your Atlan workspace in the message is currently not supported.)\n(Optional) For\nAdd as a resource\n, toggle the slider on to add the message as a resource to your asset. When you add the message as a resource, anyone viewing the asset in Atlan will be able to see previous discussions in Microsoft Teams about that asset.\nClick\nShare\nÂ to post on Microsoft Teams.\nYour message is now on Microsoft Teams and linked to the asset for future reference! ð\nAdd existing Microsoft Teams discussions to assets\nâ\nWho can do this?\nAny\nnon-guest user with edit access to an asset's metadata\ncan add existing Microsoft Teams discussions to assets.\nIf you've ever discussed a data asset in Microsoft Teams, it could be helpful to bring that context back to your asset.\nTo add existing discussions from Microsoft Teams, within Atlan:\nFrom the asset, on the right of the screen, click theÂ\nTeams\nsidebar icon:\nIf there are existing discussions on the asset, to the right of\nTeams conversations\n, click the\nAdd\nlink.\nIf there are no existing discussions on the asset, click the\n+\nAdd Teams thread\nbutton.\nIn the\nAdd Teams thread\ndialog, under the\nLink\nheading, paste the link to the Microsoft Teams thread.\nUnder the\nTitle\nheading, write a brief description of the thread.\nAt the bottom of the dialog, click the\nAdd\nbutton.\nYour existing discussion on Microsoft Teams is now linked to the asset for future reference! ð\nEnable alerts for glossary updates in Atlan\nâ\nwarning\nð¤ Who can do this?\nYou will need to be an\nadmin user\nin Atlan to enable Microsoft Teams notifications for glossary updates. Once enabled, anyone in the selected channel will be able to view these alerts directly in Microsoft Teams.\nYou can set up Microsoft Teams notifications for updates made to your\nglossaries\nin Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts sent to a\nMicrosoft Teams channel of your choice\n, you'll be able to stay informed about the latest changes to your glossaries.\nMicrosoft Teams notifications can only be set up at the glossary level. Even if configured from a specific term or category profile, the notification settings will be applicable to the entire glossary.\nTo enable Microsoft Teams notifications for glossary updates, from Atlan:\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of the glossary for which you want to set up change notifications.\nFrom the top right of the glossary profile, click the bell icon to set up notifications for glossary updates.\nIn the notifications setup dialog, click\nTeams\nand then enter the following details:\nFor\nNotification channel\n, click the dropdown to select a Microsoft Teams channel where you want to receive notifications from the channel(s) configured in your\nMicrosoft Teams integration in Atlan\n.Â\nFor\nNotify me about\n, you can either:\nClick\nAll updates\nto receive notifications for all the changes listed in\nCustom updates\nmade to your glossary.\nClick\nCustom updates\nto limit notifications to specific types of updates:\nClick\n+ New term or category\nto receive an alert when a new term or category is added to the glossary.\nClick\nName\nto receive an alert when the name of the glossary or that of a nested category or term is updated.\nClick\nDescription\nto receive an alert when a\ndescription\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nAnnouncement\nto receive an alert when an\nannouncement\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nCertificate\nto receive an alert when a\ncertificate\nis added to, updated, or removed from the glossary or a nested category or term.\nClick\nOwners\nto receive an alert when an\nowner\nis assigned toÂ or removed from the glossary or a nested category or term.\nClick\nReadme\nto receive an alert when a\nREADME\nÂ is added to, updated, or removed from the glossary or a nested category or term.\nClick\nTags (for terms)\nto receive an alert when a tag is\nattached to\nor\nremoved from\na term within the glossary.\nClick\nCategories updated (for terms)\nto receive an alert when a\nterm is assigned to a different category\nwithin the same glossary. (Notifications for moving terms across glossaries are currently not supported.)\nClick\nRemove term\nto receive an alert when a term is removed from the glossary. (Notifications for removal of categories are currently not supported.)\nÂ Click\nSave\nto save your notification preferences.\n(Optional) To edit notification settings, click the bell icon. In the notifications setup dialog, you can change the channel or further customize your notifications.\n(Optional) To remove notifications, click the bell icon. In the notifications setup dialog, click the\nEnabled (by username)\ndropdown. Click\nRemove notification\nto reset your notification settings.\nYou will now receive Microsoft Teams notifications for changes made to your glossary! ð\nIf the\nMicrosoft Teams channel\nyou selected for sending notifications is removed from your Microsoft Teams integration in Atlan, glossary alerts will continue to be sent to that channel. In that case, you can either remove the notifications or select a different channel.\nDid you know?\nYou can also set up Microsoft Teams notifications for metadata updates on all your\nstarred assets\nin Atlan.\nExpand any asset link in Microsoft Teams\nâ\nIf you share an asset link in Microsoft Teams, Atlan's bot will expand that link directly in the Microsoft Teams channel and provide an embedded preview of the asset.\nTo provide detail for an asset in Microsoft Teams:\nPaste an asset link into Microsoft Teams, along with any other context in your message.\nThe Atlan bot will expand the link with a card of contextual metadata.\n(Optional) Click\nView in Atlan\nto open the asset link in Atlan.\nThe channel can now understand basic information about the asset without leaving Microsoft Teams! ð\nView workflow alerts in Microsoft Teams\nâ\nYou can\nconfigure the Microsoft Teams integration\nto receive alerts for workflow activities from Atlan. This can help you\nmonitor your workflows\ndirectly in Microsoft Teams. You can also choose to receive failure alerts only.\nTo view workflow alerts in Microsoft Teams:\nOpen the Microsoft Teams channel you\nconfigured\nto view workflow notifications.\nThe Atlan bot will share workflow alerts   -  including details like run status, start time, run time, trigger type, and last three runs.\n(Optional) Click\nView in Atlan\nto open the workflow link in Atlan.\nYou can now inspect crawled assets or troubleshoot in case of any failed workflows.\nDid you know?\nAtlan also supports workflow alerts for all\ncustom packages\n. If you have set up any custom packages on your Atlan instance, you will be able to monitor your workflows directly in Microsoft Teams.\nTags:\ndata\nintegration\nfaq\nfaq-integrations\nPrevious\nTroubleshooting Microsoft Teams\nNext\nSlack\nShare assets without leaving Atlan\nAdd existing Microsoft Teams discussions to assets\nEnable alerts for glossary updates in Atlan\nExpand any asset link in Microsoft Teams\nView workflow alerts in Microsoft Teams"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo",
    "content": "Connect data\nData Quality & Observability\nMonte Carlo\nOn this page\nMonte Carlo\nOverview:\nCatalog Monte Carlo monitors, incidents, and rules in Atlan. Gain visibility into data quality metrics, alerts, and governance for your data assets.\nGet started\nâ\nFollow these steps to connect and catalog Monte Carlo assets in Atlan:\nSet up the connector\nCrawl Monte Carlo assets\nReferences\nâ\nWhat does Atlan crawl from Monte Carlo\n: Learn about the Monte Carlo assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Monte Carlo\n: Verify prerequisites before setting up the Monte Carlo connector.\nTags:\nmonte carlo\nconnector\nobservability\ndata quality\nconnectivity\nNext\nSet up Monte Carlo\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo",
    "content": "Connect data\nData Quality & Observability\nMonte Carlo\nCrawl Monte Carlo Assets\nCrawl Monte Carlo\nOn this page\nCrawl Monte Carlo\nOnce you have\nconfigured the Monte Carlo permissions\n, you can establish a connection between Atlan and Monte Carlo.\nTo crawl metadata from Monte Carlo, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Monte Carlo as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nMonte Carlo\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Monte Carlo credentials:\nFor\nAuthentication\n,\nAPI Key Authentication\nis the default selection.\nFor\nAPI Key ID\n, enter the\nAPI key ID you copied\n.\nFor\nAPI Secret\n, enter the\nAPI secret you copied\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Monte Carlo.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Monte Carlo connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Monte Carlo crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click the\nInclude filter\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click the\nExclude filter\n. (This will default to no assets, if none specified.)\nToÂ enable crawling assets with specific\nincident statuses\n, click\nInclude Incident Statuses\nand select the relevant option(s). To include unresolved incidents by default, we recommend selecting the\nNo Status\nand\nAcknowledged\nfilters. (This will default to all incident statuses, if none are specified.)\nFor\nIncidents and Alerts time range\n, specify a date range for which you want to crawl alerts and incidents from Monte Carlo. The default date range is set to the last 30 days, you can either keep the default selection or change to the last 14 or 45 days.\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the enrichment:\nTo map Monte Carlo metadata enrichment to assets from specific connections only, for\nInclude Connections\n, specify the connections in Atlan, or leave it blank to include all connections. If you have specified any connections, Atlan will map monitors, alerts, and incidents only to the assets included in those connections.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Monte Carlo crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Monte Carlo\nNext\nWhat does Atlan crawl from Monte Carlo?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/references/what-does-atlan-crawl-from-monte-carlo",
    "content": "Connect data\nData Quality & Observability\nMonte Carlo\nReferences\nWhat does Atlan crawl from Monte Carlo?\nOn this page\nWhat does Atlan crawl from Monte Carlo?\nPrivate Preview\nAtlan supports both automated and custom monitors as native assets for search and discovery. Atlan also supports crawling incidents for all\ntypes of monitors\n.\nOnce you've\ncrawled Monte Carlo\n, you can\nuse connector-specific filters\nÂ to search for your Monte Carlo assets as well as assets from other supported sources to which Monte Carlo monitors have been applied   -  for example, Snowflake tables.\nThe following filters are currently supported for Monte Carlo assets:\nMonitor type   -  filter monitors by type of monitor\nMonitor status   -  filter monitors by monitor status\nIncident count   -  filter monitors by the total count of incidents\nLast synced in Atlan   -  filter monitors by timestamp for last updated in Atlan\nThe following Monte Carlo filters are currently available for supported SQL assets:\nMonitor status   -  filter SQL assets associated with any monitors by monitor status\nMonitor type   -  filter SQL assets associated with any monitors by type of monitor\nAlert priority   -  filter SQL assets by priority level of alerts, ranging from 1 to 4\nAlert type   -  filter SQL assets by specific types of alerts\nIncident severity   -  filter SQL assets by severity level of incidents, ranging from 1 to 4\nAlert subtype   -  filter SQL assets by alert subtypes\nAlert status   -  filter SQL assets by specific\nalert statuses\nAlert owner   -  filter SQL assets by alert owners\nLast synced in Atlan   -  filter SQL assets by timestamp for when any associated monitors and incidents were updated in Atlan\nAtlan crawls and maps the following assets and properties from Monte Carlo.\nMonitors\nâ\nAtlan maps automated and custom monitors from Monte Carlo to its\nMCMonitor\nasset type.\nSource property\nAtlan property\nname\nname\nAudience\nmcLabels\nuuid\nmcMonitorId\nmonitorStatus\nmcMonitorStatus\nmonitorType\nmcMonitorType\nwarehouseName\nmcMonitorWarehouse\nscheduleType\nmcMonitorScheduleType\nnamespace\nmcMonitorNamespace\nruleType\nmcMonitorRuleType\nruleSql\nmcMonitorRuleCustomSql\nscheduleConfig\nmcMonitorRuleScheduleConfig\nruleComparisons\nmcMonitorRuleComparisons\nmonitorUrl\nsourceUrl\nbreachRate\nmcMonitorBreachRate\nscheduleConfig\nmcMonitorRuleScheduleConfigHumanized\nalertCondition\nmcMonitorAlertCondition\npriority\nmcMonitorPriority\nisOotbMonitor\nmcMonitorIsOotb\nAlerts and incidents\nâ\nAtlan maps alerts and incidents from Monte Carlo to its\nMCIncident\nasset type.\nAlerts inherit priority levels from custom monitors. If an alert is confirmed as an issue or requires resolution, Monte Carlo enables you to mark it as an incident and apply a severity level. Atlan will display the status of your assets at source. Refer to\nMonte Carlo documentation\nto learn more.\nSource property\nAtlan property\nname\nname\ntableLinked\nmcAssetReferences\nuuid\nmcIncidentId\nincidentType\nmcIncidentType\nincidentSubTypes\nmcIncidentSubTypes\nseverity\nmcIncidentSeverity\nfeedback\nmcIncidentState\nwarehouse.name\nmcIncidentWarehouse\nincidentOwner\nsourceOwners\nincidentUrl\nsourceUrl\npriority\nmcIncidentPriority\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Monte Carlo\nNext\nPreflight checks for Monte Carlo\nMonitors\nAlerts and incidents"
  },
  {
    "url": "https://docs.atlan.com/tags/api",
    "content": "72 docs tagged with \"api\"\nView all tags\nAdd contract impact analysis in GitHub\nAdd contract impact analysis in GitHub <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nAdd impact analysis in GitHub\nLearn about add impact analysis in github.\nAdd impact analysis in GitLab\nLearn about add impact analysis in gitlab.\nAdministration and Configuration\nComplete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization.\nAI and Automation Features\nGuide to Atlan's AI capabilities and automation features for enhanced data governance and productivity.\nAPI authentication\nLearn about api authentication.\nAtlan's open API\nLearn about atlan's open api.\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nConnectors and capabilities\nLearn about connectors and capabilities.\nCrawl Fivetran\nLearn about crawl fivetran.\nextract lineage and usage from Databricks\nOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.\nGenerate HAR files and console logs\nAtlan is built on [REST APIs](https://apidocs.atlan.com/), so you can see the requests being sent by the UI to the API gateway through your browser's developer console.\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIntegrate Anomalo\nOnce you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo.\nIntegrate Apache Spark/OpenLineage\nAtlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Slack\nTo integrate Slack and Atlan, follow these steps.\nInterpret usage metrics\nAtlan currently supports usage and popularity metrics for the following connectors:\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nPreflight checks for Amazon QuickSight\nThe [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission.\nPreflight checks for Anomalo\nThis check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid.\nPreflight checks for Databricks\nBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.\nPreflight checks for Domo\nAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.\nPreflight checks for Fivetran\nLearn about preflight checks for fivetran.\nPreflight checks for Google BigQuery\nEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).\nPreflight checks for Looker\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project#get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management).\nPreflight checks for MicroStrategy\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html#/Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions.\nPreflight checks for Monte Carlo\nBefore [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c.\nPreflight checks for Redash\nBefore [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti.\nPreflight checks for Sigma\nFirst, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission.\nPreflight checks for Sisense\nAtlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.\nPreflight checks for Tableau\nThe [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm#server_info) REST API is used to fetch the `restApiVersion` value.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nSecurity\nThe Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring.\nSet up an AWS private network link to Databricks\nFor all details, see [Databricks documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.\nSet up Confluent Kafka\nAtlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata.\nSet up Confluent Schema Registry\n:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.\nSet up Fivetran\nLearn about set up fivetran.\nSet up IBM Cognos Analytics\n:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.\nSet up Looker\n:::warning Who can do this? You will probably need your Looker administrator to run these commands - you may not have access yourself.\nSet up Microsoft Azure Data Factory\nAtlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up Qlik Sense Cloud\n:::warning Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps - you may not have access yourself.\nSet up Qlik Sense Enterprise on Windows\n:::warning Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps - you may not have access yourself.\nSet up Redash\n:::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself.\nSet up Sigma\n:::warning Who can do this? You will probably need your Sigma administrator to complete these steps - you may not have access yourself.\nSet up Sisense\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nSet up Soda\n:::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -.\nSet up Tableau\n:::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself.\nSoftware development kits (SDKs)\nLearn about software development kits (sdks).\nSupport and Technical Help\nComplete guide to getting support, understanding API limits, and accessing technical assistance for Atlan.\nSupported sources\nLearn about supported sources.\nTags and Metadata Management\nComplete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization.\nTroubleshooting Databricks connectivity\nLearn about troubleshooting databricks connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Matillion connectivity\nLearn about troubleshooting matillion connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting Sigma connectivity\nLearn about troubleshooting sigma connectivity.\nTroubleshooting Trino connectivity\nLearn about troubleshooting trino connectivity.\nView query logs\nYou can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization.\nWhat does Atlan crawl from Databricks?\nAtlan crawls and maps the following assets and properties from Databricks.\nWhat does Atlan crawl from Fivetran?\nLearn about what does atlan crawl from fivetran?.\nWhat does Atlan crawl from Microsoft SQL Server?\nAtlan crawls and maps the following assets and properties from Microsoft SQL Server.\nWhat does Atlan crawl from Salesforce?\nAtlan only performs GET requests on these five endpoints:.\nWhat lineage does Atlan extract from Matillion?\nAtlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nAtlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\nLearn about what lineage does atlan extract from microsoft azure synapse analytics?.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage",
    "content": "Use data\nLineage\nOn this page\nLineage\nOverview:\nData lineage in Atlan helps you understand how data moves across your data landscape. Use lineage to perform root cause analysis, impact analysis, and automate metadata propagation.\nGet started\nâ\nHow to view lineage\nGuides\nâ\nLineage Visualization\nâ\nHow to download and export lineage\n: Export lineage information for external use.\nConcepts\nâ\nWhat is lineage\n: Understand the fundamentals of data lineage.\nWhat is column-level lineage\n: Learn about column-level lineage tracking.\nWhat are processes\n: Understand how processes are represented in lineage.\nWhat are partial assets\n: Learn about partial asset representation in lineage.\nTags:\nlineage\ndata-flow\ndependencies\ncapabilities\nNext\nHow to view lineage\nGet started\nGuides\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/view-lineage",
    "content": "Use data\nLineage\nGet Started\nHow to view lineage\nOn this page\nview lineage\nThe\nlineage\ngraph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps.\nView the lineage graph\nâ\nTo view the lineage graph:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\n(Optional) In the\nFilters\nmenu\non the left, expand the\nProperties\nmenu and then click\nHas lineage\nto filter for assets with data lineage.\nSelect an asset, and from the top right of the asset card, click the\nView lineage\nicon to open the lineage graph.\nOn the lineage graph, the home icon indicates the base asset:\nClick the\n+\nbutton to the left of the base asset to view upstream assets.\nClick the\n+\nbutton to the right of the base asset to view downstream assets.\n(Optional) Hover over any asset to view the metadata popover for more context on the selected asset   -  including asset type, database and schema names,\nowners\n,\nusage and popularity metrics\n,\nannouncements\n, and\nMonte Carlo\nand\nSoda\ndata quality details for supported assets. From the popover, you can also:\nClick the\nView impact\nbutton to\nview impacted assets\n.\nClick the\nDownload impact\nbutton to\ndownload the lineage report\nfor the asset.\nClick the outward-facing arrow icon to open lineage in a new tab.\n(Optional) Hover over the\n+\nbutton to the right of any asset and then click the\nExpand all\nÂ button to view assets further upstream or downstream horizontally.\n(Optional) Click any circular process button to view more details about the\nlineage process\nin the sidebar.\n(Optional) Click\nShow all\nto view more upstream or downstream assets vertically. If there are more than 100 assets, a popup will appear asking you to scroll to the bottom of the list and click\nShow all\nagain to view all dependencies.\n(Optional) For any asset on the lineage graph, click the\nview columns\nmenu to expand the columns view:\nThe default view shows 10 columns. Click\nShow more\ncolumns\nto view the full list of columns.\nUse the search bar to search for specific columns.\nView a total count of columns that have lineage.\nNext to the search bar, click the sort icon to sort columns in an alphabetical or reverse alphabetical order, ascending or descending order, or by last updated in Atlan.\nHover over any column name and then click the curvedÂ downward arrow to\nview impacted assets\nor click the downward arrow to\ndownload the lineage report\ndirectly from the column.\nSelect a column, and then from the column sidebar, click the\nLineage\ntab. From the\nLineage\ntab:\nClick\nView impact\nto\nview and download impacted assets\ndirectly from the sidebar.\nClick the eye icon to customize your view of column-level lineage in the sidebar:\nFor\nDepth\n, select the level of depth up to\nMax Depth\nfor column-level lineage.\nTurn on\nShow process nodes\nÂ to view\nlineage processes\nin the sidebar.\nFor\nShow hierarchy\n, keep the default view of child assets or toggle it off.\nFor any asset on the lineage graph, from the sidebar,Â click the\nColumns\ntab. From the\nColumns\ntab:\nYou can view a list of columns for the selected asset. Click\nShow columns with lineage\nto only view columns with lineage:\nFor any column with lineage in the sidebar, click the play button to locate that asset on the lineage graph.\n(Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow.\n(Optional) From the top right of the lineage graph:\nClick the\nFind in canvas\nsearch bar to search for any specific assets on the lineage graph.\nClick the downward arrow to view more options:\nClick\nView impact\nto\nview impacted assets\n.\nClick\nDownload impact\nto\ndownload the lineage report\nfor the asset.\nClick\nDownload lineage as image\nto\ndownload an image\nof the lineage graph.\nClick the eye icon to set preferences for the lineage graph:\nFor\nAdditional metadata\n, show or hide the following context for your assets   -  schema name, database name, asset type,\nannouncements\n, data quality, or\npopularity metrics\n.\nFor\nOrder columns and fields\n, order assets on the lineage graph alphabetically, in an ascending or descending order, or by last updated in Atlan.\nFor\nLine arrows\n, show or hide the arrows that indicate data flows on the lineage graph.\nFor\nPin metadata sidebar\n, toggle on the slider to pin the sidebar open on the lineage graph.\nClick the question mark icon to share feedback or view documentation.\n(Optional) From the bottom right of the lineage graph:\nToggle the\nVisibility\nslider to customize the visibility of assets not included in your selected lineage path.\nClick the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base asset.\nClick the minimap icon to view an abridged version of the lineage graph.\nClick the information icon to view the map legend.\nClick the fullscreen icon to expand the lineage view to fullscreen mode.\nClick the minus or plus icons to zoom out or zoom in on the lineage graph, respectively.\nDid you know?\nIfÂ the\nproducts module is enabled\nin your Atlan workspace, you can also\ncreate data products directly from the lineage graph\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nLineage\nNext\nDownload and export lineage\nView the lineage graph"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/download-and-export-lineage",
    "content": "Use data\nLineage\nManage lineage\nDownload and export lineage\nOn this page\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to\nview\n,\ndownload\n, and\nexport\nyour impacted assets and share it with others in your organization.\nThe impact report includes upstream and downstream assets, along with the following attributes:\nStatus\n-  export status and total count of assets exported, only for\nexport\noption\nName\n-  asset name and link\nType\n-  asset type\nBusiness Name (Alias)\n-  business-oriented\nalias\nof assets, if any\nConnector\n-  name of\nsupported source\nDatabase\nÂ   -  name of the database, if applicable\nSchema\nÂ   -  name of the schema, if applicable\nTable\n-  name of the parent table or view, only for column assets\nLineage\ndepth\n-  starts at 1 and increases with each lineage level\nDescription\n-\nasset description\n, if any\nOwner Users\nand\nOwner Groups\n-\nasset owners\n, if any\nCertification Status\nand\nCertification Message\n-\ncertification status of asset\n, if any\nTags\nand\nPropagated Tags\n-\ntags\ndirectly attached or\npropagated\nto an asset, if any\nTerms\n-  glossary terms for\nlinked assets\nAnnouncement Type\n,\nAnnouncement Title\n, and\nAnnouncement Message\n-\nannouncements on assets\n, if any\nSource URL\n-  only supported for dbt Cloud and Core, Matillion, Microsoft Power BI, Mode, Monte Carlo, Salesforce, Sigma, Sisense, Soda, and Tableau assets\nUsage\n-\nnumber of queries by number of users\n, only for\nsupported sources\nPopularity\n-\npopularity indicator\n, only for\nsupported sources\nLast refresh on source\n-  metadata last altered at source\nWorkbook\n,\nData source\n, and\nSource owner\n-  only applicable to Tableau assets\nQualified Name\n-  fully qualified name of the asset\nSource Asset GUID\n-  globally unique identifier of the source asset\nGUID\n-  globally unique identifier of the exported asset\nImmediate Upstream\nand\nImmediate Downstream\n-  one level directly upstream and downstream, respectively, of the dependent asset.\nView impacted assets\nâ\nThe impacted assets view includes asset metadata, BI source URLs, and Atlan URLs for both upstream and downstream assets.\nTo view impacted assets:\nIn the right menu from any screen, click\nAssets\n.\nClick an asset to navigate to its asset profile.\nIn the asset profile, click\nLineage\n.\nIn the top right of the lineage graph, click the downward arrow and then click\nView impact\nto view impacted assets. Note that if the total number of impacted assets exceeds 200, you will only be able to download lineage. In the impacted assets report:\nTo view downstream assets for\nimpact analysis\n, click\nDownstream\n.\nTo view upstream sources for\nroot cause analysis\n, click\nUpstream\n.\n(Optional) To view the source URL for BI assets, click the source URL option   -  for example,\nView in Looker\n.\n(Optional) To navigate to the impacted asset in Atlan, click the Atlan URL option.\n(Optional) Click\nImpact Report\nto download the impacted assets report in a CSV file or\nexport to Google Sheets or Microsoft Excel\n.\nClick\nClose\nto return to the lineage graph.\nDownload lineage\nâ\nYou can download lineage for your data assets in a CSV file or as an image.\nDownload lineage in a report\nâ\nYou can download a lineage report for your assets in a CSV format for further analysis. When you share the CSV file with team members, they will also be provided with the BI source URLs and asset links in Atlan. As long as they have access to the asset in the BI tool and Atlan, they'll be able to open both directly from the CSV file.\nTo download the lineage report:\nIn the right menu from any screen, click\nAssets\n.\nClick on an asset to navigate to its asset profile.\nIn the asset profile, click\nLineage\n.\nIn the top right of the lineage graph, click the\ndownward arrow\nand then click\nCSV File\nto download the lineage report for the asset.Â\nOnce the report has been fetched, click\nDownload report\nto download it as a CSV file.\nYou will now be able to view the upstream and downstream lineage of your assets in a CSV file! ð\ndanger\nClicking the\nCSV File\nbutton may not instantly result in a link to the CSV file. When you click the button, it triggers a workflow to scan assets with lineage, retrieve a list of those assets via the API, and then generate a CSV file with the lineage report. For high-volume impact reports, the workflow can take a few minutes to run before the link to the CSV file is ready. The progress bar under the button displays the status of the workflow and will turn green once all the tasks in the workflow have been executed.\nDownload lineage as an image\nâ\nAtlan offers you the option to download lineage as a high-resolution PNG file, providing you with the visual clarity you need for your presentations.\nTo download lineage as an image:\nIn the right menu from any screen, click\nAssets\n.\nClick on an asset to navigate to its asset profile.\nIn the asset profile, click\nLineage\n.\nIn the top right of the lineage graph, click the\ndownward arrow\nand then click\nAs an image\nÂ to download an image of the lineage graph.\nYou will now be able to view a high-resolution image of your asset lineage! ð\nExport lineage to spreadsheets\nâ\nAtlan enables you to export lineage to spreadsheets. This can help you assess the downstream impact of any changes made to an upstream asset for\nimpact analysis\n. Atlan currently supports exporting impacted assets to:\nGoogle Sheets\nMicrosoft Excel online\nOnce your\nAtlan admin\nhas integrated a supported tool, you will be able to export impacted assets to spreadsheets. Your existing\npermissions\nand\naccess policies\nin Atlan will determine whether you can export the impact report, but at a minimum you'll require read permission on the assets you want to export.\nDid you know?\nAtlan currently limits the total number of assets you can export to 20,000 rows each for both upstream and downstream assets. Reach out to your customer success manager if you'd like to increase the limit for your organization.\nEnable lineage export\nâ\nThe export to Google Sheets and Microsoft Excel icons and buttons will only be visible if your Atlan admin has integrated a supported tool. If you cannot see the export icon or button, reach out to your Atlan admin to integrate Google Sheets or Microsoft Excel.\nTo integrate a supported tool, your Atlan admin\nmust\nfollow the steps in\nEnable asset export\n.\nExport impacted assets\nâ\nWho can do this?\nOnce an Atlan admin has integrated a supported tool, any\nadmin, member, or guest user\nin Atlan with read permission on assets can export the impact report to spreadsheets.\ndanger\nAtlan recommends that you avoid exporting assets during workflow runs. Exporting assets while you have workflows running in the background may lead to duplicate assets on the spreadsheet.\nAtlan allows you to export your impacted assets to spreadsheets and view asset metadata in bulk. At an individual user level, only one export is allowed for each supported tool at a time, the rest will be auto-queued for execution. There are currently no limitations at a tenant level.\nYou can export lineage from the following areas:\nOverview\nand\nLineage\ntabs in sidebar for assets with lineage\nLineage graph for asset-level lineage\nImpacted assets view in lineage graph\nColumn assets with lineage from lineage graph\nTo export lineage:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nTo export lineage, in the\nAssets\npage, you can:\nSelect an asset, and then from the asset sidebar:\nIn the\nOverview\ntab, click the 3-dot icon and then click\nImpact report\n.\nClick the\nLineage\ntab and then click the export icon.\nSelect an asset, and from the top right of the asset card, click the\nView lineage\nicon to open the lineage graph:\nFrom the top right of the lineage graph, click the downward arrow.\nFor any asset with column-level lineage on the lineage graph, click the\nview columns\nmenu and hover over a column name to view export options.\nTo export lineage to a spreadsheet tool:\nClick\nGoogle Sheets\nto export impacted assets to a Google Sheets spreadsheet.\nClick\nMicrosoft Excel\nto export impacted assets to a Microsoft Excel workbook.\nA sign-in dialog will appear and you will be redirected to sign in with your Google or Microsoft account. From the corresponding screen, click\nAllow\nto connect to Google Sheets or Microsoft Excel. If you're already signed in, skip this step.\nTo track the progress of the export, navigate to the spreadsheet. The\nQueued\nstatus will change to\nSuccess\nonce impacted assets have been exported.\nOn the spreadsheet, you can view metadata for your impacted assets. Atlan exports the impact report to two individual sheets within the main spreadsheet, one each for\nUpstream\nand\nDownstream\n. For any metadata attribute not applicable to a particular asset, the column will display an empty value.\n(Optional) To view your asset export history, navigate to the\nAssets\npage. Next to the search bar on the\nAssets\npage, click the 3-dot icon and then click\nExport\n. From the\nExport\ndialog, expand the\nHistory\ndropdown to view your last 10 exports. Note that only\nyou\ncan currently view your own export history.\nThat's it, you've successfully exported your impacted assets from Atlan! ð\ndanger\nAtlan currently does not support updating asset metadata in the spreadsheet and syncing impacted assets back to Atlan.\nTags:\nlineage\ndata-lineage\nimpact-analysis\ndownstream-impact\ndependencies\nupstream-dependencies\ndata-sources\nPrevious\nHow to view lineage\nNext\nGenerate lineage between assets App\nView impacted assets\nDownload lineage\nExport lineage to spreadsheets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-is-column-level-lineage",
    "content": "Use data\nLineage\nConcepts\nWhat is column-level lineage?\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nFor example, instead of stating that the upstream table\nSTG CUSTOMERS\nis dependent on the downstream table\nRAW CUSTOMERS\n, you can point to the specific column that has an impact on the downstream asset.\nKnowing the impact on the downstream asset is essential for impact analysis. However, in order to make more granular decisions, users must understand\nhow\nthe impact occurs. Column-level lineage reveals exactly what is impacted and how.\nTo learn more about data lineage, read\nthis guide\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\ndownstream-impact\ndependencies\nupstream-dependencies\ndata-sources\nPrevious\nGenerate lineage between assets App\nNext\nWhat is lineage?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-are-partial-assets",
    "content": "Use data\nLineage\nConcepts\nWhat are partial assets?\nOn this page\nWhat are partial assets?\nIf a\nsupported data source\nhas not been crawled, mined, or cataloged via APIs but is present in lineage, Atlan will create partial assets to provide a complete representation of data lineage. For example, if your\norchestration pipelines\nreference any assets as a source or target that are yet to be cataloged in Atlan, these will be represented as partial assets on the lineage graph.\nKey characteristics of partial assets:\nDisplayed with a dotted border on the lineage graph.\nAvailable in the visual representation of lineage in Atlan - lineage or pipeline view.\nYou can search for and view a partial asset profile and lineage.\nYou can not enrich metadata for partial assets.\nYou can view metadata for partial assets in the sidebar.\nYou can view and download the impact report for partial assets.\nColumn-level lineage for partial assets is supported.\nSupported sources\nâ\nAtlan currently supports the creation of partial assets for the following tools:\nAirflow/OpenLineage\nAmazon MWAA/OpenLineage\nAstronomer/OpenLineage\nGoogle Cloud Composer/OpenLineage\nApache Spark/OpenLineage\nFivetran\nView partial assets\nâ\nThere are multiple ways to view partial assets :\nLike any asset in Atlan\nâ\nFrom the left menu of any screen, click\nAssets\n.\nSearch for asset by name, or use Atlan discovery filter to find partial assets.\nClick an asset to open its profile.\nFrom pipeline lineage or lineage graph\nâ\nFrom the top left of an asset profile, click the\nLineage\ntab to view lineage or the\nPipeline\ntab for orchestration assets.\nA partial asset is represented with a dotted border on the lineage graph. Select a partial asset to view more details.\n(Optional) Open the asset sidebar to view metadata, lineage, and properties of the partial asset, and navigate to the profile.\nHover over the partial asset to view a metadata popover:\nYou can view the following details   -  asset name and type, connection, database, and schema names, and owner, if assigned.\nClick the\nView impact\nbutton to\nview impacted assets\n.\nClick the\nDownload impact\nbutton to\ndownload the lineage report\nfor the asset.\nClick the\nview columns\nmenu to view column assets.\nHover over any column name and then click the curvedÂ downward arrow to\nview impacted assets\nor click the straight downward arrow to\ndownload the impact report\ndirectly from the column.\nConvert partial assets to catalogued asstes\nâ\nTo convert a partial asset into a fully published asset, complete the following steps in any order:\nIf the sourcer is supported, run the crawler for the\nsupported data source\nof the partial asset.\nRun the\norchestration pipeline\nthat initially cataloged the partial asset.\nTags:\natlan\ndocumentation\nPrevious\nWhat is lineage?\nNext\nWhat are processes?\nSupported sources\nView partial assets\nConvert partial assets to catalogued asstes"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-are-processes",
    "content": "Use data\nLineage\nConcepts\nWhat are processes?\nOn this page\nWhat are processes?\nProcesses represent the movement and transformation of assets in Atlan. On the\nlineage graph\n, processes are the circular buttons with icons denoting the sources. Processes help you understand the transformations taking place from one asset to another and whether to take action.\nBy default, process assets are only visible on the lineage graph and sidebar. If\nenabled\nby an admin user, you will be able to\nsearch\n,\nfilter\n, and discover process assets and track metrics in the\nreporting center\n.\nView processes\nâ\nFrom lineage graph and sidebar\nâ\nTo view processes on the lineage graph and sidebar:\nFrom the left menu of any screen, click\nAssets\n.\nClick an asset to open its asset profile.\nFrom the top left of the asset profile, click the\nLineage\ntab.\nClick any circular process button to view more details in the sidebar.\n(Optional) For\nOverview\nin the sidebar, under\nQuery\n, click the full screen icon to expand the SQL code snippet.\n(Optional) In the right sidebar, click\nLineage\nto view the input and output for the process.\nFrom asset preview\nâ\nWho can do this?\nYou will need your Atlan admin to\nenable discovery of process assets\n.\nTo view process assets in asset preview:\nFrom the left menu of any screen, click\nAssets\n.\nUnder the search bar on the\nAssets\npage, slide through the asset type tabs or click the three dots icon to select the\nProcess\ntab.\nClick any process asset to view details in the sidebar.\n(Optional) For\nOverview\nin the sidebar, under\nQuery\n, click the full screen icon to expand the SQL code snippet.\n(Optional) In the right sidebar, click\nLineage\nto view the input and output for the process.\n(Optional) From the left\nfilters menu\n, click\nProcess\nand then click\nQuery\nto filter process assets by the SQL query.\nFrom reporting center\nâ\nWho can do this?\nYou must be an admin user in Atlan to view the\nreporting center\n.\nTo track process assets from the\nreporting center\n:\nFrom the left menu in Atlan, click\nReporting\nand then click\nAssets\n.\nFrom the\nAssets\ndashboard, for the\nAll Asset Types\nfilter, select\nProcess\n.\n(Optional) To further\nrefine your search\n, click\nMore filters\n.\n(Optional) Once the process assets are displayed, click any data point to view processes on the\nAssets\npage.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nWhat are partial assets?\nNext\nHow can Atlan generate upstream lineage from the data warehouse layer?\nView processes"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/references/how-can-atlan-generate-upstream-lineage-from-the-data-warehouse-layer",
    "content": "Use data\nLineage\nReferences\nHow can Atlan generate upstream lineage from the data warehouse layer?\nHow can Atlan generate upstream lineage from the data warehouse layer?\nAtlan has 2 API endpoints to generate lineage:\nlineage between assets\ncolumn-level lineage\nAtlan generates upstream lineage in the following ways:\nCustom jobs:\nConstraint   -  these jobs do not have an interface for Atlan to extract lineage. For example, custom Python jobs that move data from RDBMS to S3 to Snowflake.\nRecommended path   -  use the\nlineage API\nto push source-to-target mappings to Atlan.\nData integration tools:Â\nConstraint   -  these tools have historically not had metadata APIs for Atlan to extract lineage.\nRecommended path   -  Atlan has released support for data integration tools that have released metadata APIs, such as\nFivetran\n.\nOrchestration tools:Â\nConstraint   -  you need to run\nDAGs\nor\njobs\nto catalog assets and lineage in Atlan, unlike other workflows that catalog assets after a workflow run.\nRecommended path   -  use the\nlineage API\nto either create lineage directly or using OpenLineage.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nupstream-dependencies\ndata-sources\napi\nrest-api\ngraphql\nPrevious\nWhat are processes?\nNext\nLineage Generator (no transformations)"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/airflow-lineage-integration",
    "content": "Use data\nLineage\nFAQ\nCan Atlan integrate with Airflow to generate lineage?\nCan Atlan integrate with Airflow to generate lineage?\nAtlan currently supports native integration with\nApache Airflow/OpenLineage\n.\nAtlan also supports integrating with the following Apache Airflow distributions:\nAmazon MWAA\nAstronomer\nGoogle Cloud Composer\nTags:\nconnectors\nintegration\nfaq\nfaq-lineage\nPrevious\nSource asset type\nNext\nCan Atlan read a dump of SQL statements to create lineage?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/troubleshooting/troubleshooting-lineage",
    "content": "Use data\nLineage\nTroubleshooting\nTroubleshooting lineage\nOn this page\nTroubleshooting lineage\nSo you've crawled your source, and mined the queries, but lineage is missing. Why?\nWhere to look first?\nâ\nViews\nâ\nCheck the SQL attribute of the view data asset   -  this must have SQL in it for view lineage to appear.\nThe crawler workflows populate the SQL attribute. If it's empty on the view asset, the crawler is the suspect.\nTables\nâ\nThe miner workflows populate table lineage. If it's missing, the miner is the suspect.\nCheck the SQL picked up by the miner (for example, in\nS3\n).\nIf the miner picks up the necessary SQL but lineage is not produced, check if any of the assets involved are missing.\nData stores to BI assets\nâ\nFor Atlan to link these assets, the upstream assets (data stores) must\nfirst\nexist.\nIf they are only created\nafter\nthe downstream assets, lineage will stay unlinked.\nOr if some of the assets are missing, lineage may have gaps preventing linkage.\nShow more menu\nâ\nLineage may appear missing if the linked asset is hidden in the\nShow more\nmenu. Although it will still appear in the list of upstream or downstream assets in the\nLineage\ntab in the side profile, it will not appear visually in the lineage graph. Click\nShow more columns\nto see the rest of the assets and their lineage.\nMiner logic\nâ\nWhen setting up the miner for the first time, you will need to provide a start date   -  ranging from the last two days up to past two weeks of query history. If an asset has not been queried during the selected time period, data lineage will be unavailable.\nFor subsequent runs, the miner will fetch query history based on the following logic:\nSTART_TIME\nâ¤\nCURRENT_DATE\n-\nINTERVAL '1 DAY'\nFor example, the miner logic for January 23 will be:\nJan 22 5 p.m. â¤ Jan 23 00:00 - 1 day\nJan 22 5 p.m. â¤ Jan 22 00:00\nThe miner will not fetch the data for the previous day (January 22) on the current day (January 23). Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session.\nCauses of missing lineage\nâ\nThere are several reasons why lineage may be missing:\nWorkflow ordering\nâ\nThe order of operations you run in Atlan is important. To have lineage across tools, you need to:\nCrawl data stores first.\nMine query logs (and dbt) second.\nCrawl BI tools last.\nIf you've used a different order, the upstream assets (data stores) may not yet exist when you load the BI metadata. Then you can have lineage within the BI metadata, but not between the BI metadata and the data sources.\nIf that's the case for you, don't worry. Re-run your existing workflows in the order above and Atlan should resolve it.\nCrawling filters\nâ\nAnother reason lineage may have gaps is that linking assets do not exist, even after re-running the crawler.\nWhen crawling a source, you can specify filters on which metadata to include and exclude. If you've excluded metadata needed to link assets into lineage, then end-to-end lineage will have gaps.\nCheck that you have not excluded any of the asset(s) you're expecting to be in lineage. (And remember that using an\ninclude\nfilter means that not all metadata is being crawled   -  some is being excluded.)\nIf in doubt, try running your workflows without include or exclude filters.\nSource permissions\nâ\nAtlan is not the\nonly\nplace where you can filter metadata.\nAtlan accesses your sources through credentials you provide. Those credentials have assigned permissions controlling what (meta)data they can access in the source. If those permissions prevent access to some (meta)data, then Atlan cannot crawl that metadata.\nSo if ordering and filter don't fix the problem, check your source permissions. Are they providing access to all the data assets you need for lineage?\nDifferent connections, same source\nâ\nWe currently do not resolve lineage across different connections for the same source. You need to crawl (and mine) all assets from a given source through the same connection to generate lineage.\ndanger\nThis one is the most subtle of the causes. The assets may even appear to be in the environment in this case. Check the\nqualifiedName\nof the asset matches\nexactly\nwhat lineage expects.\nTemporary tables\nâ\nIf your data processing tool uses temporary tables, Atlan can still support generating lineage accurately.\nFor example:\nTable A â temporary table â table B\nLineage will be represented as table A â table B in Atlan.\nIn this case, Atlan assumes that tables A and B are present in Atlan. However, if table A is missing, then Atlan will not be able to generate lineage.\nCross-connection links\nâ\nIf the combination of database, schema, and table name for an asset is the same across different connections, it is possible that Atlan may create unexpected links for these assets.\nFor example, if your\nProduction\nenvironment has the same set of databases, schemas, and tables as your\nStaging\nenvironment and both these source systems are crawled, Atlan may connect BI reports to either of these assets due to the name-match algorithm.\nIndirect data flow\nâ\nAtlan currently only processes and visualizes direct data flow on the lineage graph. However, assets can be related through other means such as control flow or conditional statements, in which case there is no data movement between them. Atlan currently neither processes nor visualizes such relationships on the lineage graph.\nFor example, when processing the following query:\ninsert\ninto\ntgt_tab\n(\ncol_x\n,\ncol_y\n)\nselect\ncol_x\n,\ncase\nwhen\ncol_y\n>\n100\nthen\n'High'\nelse\n'Low'\nend\nfrom\nsrc_tab\nAtlan will display the following links in the lineage graph:\nsrc_tab â tgt_tab (table-level data flow)\nsrc_tab.col_x â tgt_tab.col_x (column-level data flow)\nNote that there is no lineage generated for col_y. This is because the data present in src_tab.col_y does not actually flow or get transferred to tgt_tab.col_y.\nLineage persistence\nâ\nLineage in Atlan is reflective of the last valid set of transformations performed for a particular\ntarget\ntable in the external (\nsource\n) system. Atlan retains these transformations as lineage and does not auto-delete or sunset the\nprocess entities\n(\nlinks\n).\nThe exception to this rule is when new information pertaining to the same\ntarget\ntable is inferred in the latest job run. In this case, Atlan will replace the previous\nlinks\nwith new ones.\nTags:\ndata\ncrawl\nPrevious\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?\nNext\nWhy is my Databricks lineage API not working?\nWhere to look first?\nCauses of missing lineage\nLineage persistence"
  },
  {
    "url": "https://docs.atlan.com/faq/tags-and-metadata-management",
    "content": "Configure Atlan\nFrequently Asked Questions\nTags and Metadata Management\nOn this page\nTags and Metadata Management\nComplete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization.\nWhat are some examples of tags?\nâ\nTo learn more about examples of tags, see\nWhat are tags?\nWhy does tag propagation take time to apply?\nâ\nWhen\ntag propagation is enabled\n, it automatically triggers a background task in the metastore. This background task is created to reduce the API load and response time. After each background task has been created, the API simply returns a 200 OK response code.\nTag propagation is completed once the background tasks have been executed - including\ntag attachment\nor\nremoval\nby propagation.\nThe lifecycle of a background task:\nWhen a background task is created for tag propagation, its status changes to\nPENDING\n. Multiple tasks may be in the same\nPENDING\nstate, depending on the number of assets to be propagated.\nAs a task gets picked up, its status changes to\nIN PROGRESS\n. As each task is executed, the tag is propagated to an asset.\nOnce the task is completed, its status changes to\nCOMPLETE\n. As the next task gets picked up, this cycle repeats until tag propagation is completed for all the assets.\nHow are tags propagated for new assets?\nâ\nTag propagation\nis disabled by default in Atlan. If you have\nenabled tag propagation\n, tags are automatically propagated to a child or downstream asset created after running a workflow. This means that when a new asset is registered, tag propagation is automatically triggered in the metastore and runs as a background task.\nFor example, if a workflow adds an additional column to a table, a new background task for adding tags is created in the metastore. This new task is executed when all the previous tasks have been completed in the queue. The speed with which these tasks are completed depends on the number of pending tasks and the volume of tags to be added or removed. The same process also applies to\ntag deletion\nand updating tags through\nplaybooks\n.\nCan I delete a tag?\nâ\nTags can be deleted, but this requires careful consideration due to their impact on data governance. For detailed instructions on tag deletion, see the\ntag deletion guide\n.\nIs reverse tag sync supported for column-level tags?\nâ\nReverse tag sync capabilities depend on your data source and configuration. For specific data source support and configuration options, consult the connector documentation or contact Atlan support.\nTags:\ndata\napi\nfaq-metadata\nPrevious\nSecurity and Compliance\nNext\nUser Management and Access Control"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/mine-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nCrawl Redshift Assets\nMine Amazon Redshift\nOn this page\nMine Amazon Redshift\nOnce you have\ncrawled assets from Amazon Redshift\n, you can mine its query history to construct lineage and retrieve\nusage and popularity metrics\n.\nTo mine lineage from Amazon Redshift, review the\norder of operations\nand then complete the following steps.\nSelect the miner\nâ\nTo select the Amazon Redshift miner:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the filters along the top, click\nMiner\n.\nFrom the list of packages, select\nRedshift Miner\nand click on\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the Amazon Redshift miner:\nFor\nConnection\n, select the connection to mine. (To select a connection,\nthe crawler\nmust have already run.)\nFor\nMiner extraction method\n, select\nQuery History\n.\nFor\nStart time\n, choose the earliest date from which to mine query history.\ndanger\nAmazon Redshift only stores query history for\n2-5 days\n. If you need to query more history, for example in an initial load, consider using the\nS3 miner\nfirst. After the initial load, you can\nmodify the miner's configuration\nto use query history extraction.\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nAdvanced\nto further configure the miner:\nFor\nCross Connection\n, click\nYes\nto extract lineage across all available data source connections or click\nNo\nto only extract lineage from the selected Amazon Redshift connection.\nFor\nControl Config\n, if Atlan support has provided you with a custom control configuration, select\nCustom\nand enter the configuration into theÂ\nCustom Config\nbox. You can also:\nEnter\n{âignore-all-caseâ: true}\nto enable crawling assets with case-sensitive identifiers.\n(Optional) For\nEnable Popularity\n, click\nYes\nto retrieve\nusage and popularity metrics\nfor your Amazon Redshift assets from query history:\nFor\nExcluded Users\n, type the names of users to be excluded while calculating\nusage metrics\nfor Amazon Redshift assets. Press\nenter\nafter each name to add more names.\nRun the miner\nâ\nTo run the Amazon Redshift miner, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the miner, click\nPreflight checks\n.\nYou can either:\nTo run the miner once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the miner has completed running, you will see lineage for Amazon Redshift assets that were created in Amazon Redshift between the start time and when the miner ran! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl Amazon Redshift\nNext\nWhat does Atlan crawl from Amazon Redshift?\nSelect the miner\nConfigure the miner\nRun the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/crawl-dbt",
    "content": "Connect data\nETL Tools\ndbt\nCrawl dbt Assets\nCrawl dbt\nOn this page\nCrawl dbt\nOnce you have\nconfigured dbt Cloud service token\nor\nuploaded your dbt Core project files to cloud storage\n, you can crawl dbt metadata into Atlan.\nTo enrich metadata in Atlan from dbt, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select dbt as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\ndbt Assets\nand then click\nSetup Workflow\n.\nProvide your credentials\nâ\ndbt core\nâ\nTo enter your\ndbt Core\ncredentials:\nFor\nExtraction method\n, click\nObject Storage\n.\nEnter the details for the object storage location of your project files.\nClick the\nTest Authentication\nbutton to confirm connectivity to object storage using these details.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\ndbt cloud\nâ\nTo enter your\ndbt Cloud\ncredentials:\nFor\nExtraction method\n, clickÂ\nCloud\n.\nFor\nHost Name\n, enter the domain name of your dbt Cloud instance, if not the default. Include the\nhttps://\n. For more information on access URLs, refer to\ndbt documentation\n.\nFor\nAuthentication Type\n,\nService Account\nis the default selection for\nservice account token\n. Change to\nPAT\nto enter a\npersonal access token\n(PAT) instead.\nFor\nToken\n, enter the\ndbt Cloud token you generated\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to dbt Cloud using these details.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the dbt connection configuration:\nProvide a\nConnection name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you don't specify any user or group, no one can manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the dbt crawler, you can further configure it.\nDid you know?\nIf a project appears in both the include and exclude filters, the exclude filter takes precedence.\ndbt core\nâ\nOn the\nConfiguration\npage for dbt Core, you can override the defaults for any of these options:\nTo limit the enrichment to a particular connection with materialized assets, click\nConnection\nand select the relevant option. (This defaults to all connections, if none are specified.)\nTo\nimport existing tags from dbt to Atlan\n, for\nImport Tags\n, click\nYes\n.\ndbt cloud\nâ\nOn the\nConfiguration\npage for dbt Cloud, you can override the defaults for any of these options:\nTo select the dbt projects and environments you want to exclude from crawling, click\nExclude Metadata\n. (This defaults to no projects, if none are specified.)\nTo select the dbt projects and environments you want to include in crawling, click\nInclude Metadata\n. (This defaults to all projects, if none are specified.)\nTo limit the enrichment to a particular connection with materialized assets, click\nConnection\nand select the relevant option. (This defaults to all connections, if none are specified.)\nTo\nimport existing tags from dbt to Atlan\n, for\nImport Tags\n, click\nYes\n.\nFor\nAdvanced options\n, click\nYes\nto configure the crawler further:\nFor\nEnrich Metadata in Materialized Assets\n, click\nYes\nto enable enrichment for both dbt and materialized assets or\nNo\nfor dbt assets only.\nRun the crawler\nâ\nTo run the dbt crawler, after completing the previous steps:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you can see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up dbt Core\nNext\nManage dbt tags\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/mine-queries-through-s3",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nHow-tos\nMine queries through S3\nOn this page\nMine queries through S3\nOnce you have crawled assets from a supported connector,Â you can mine query history.\nSupported connectors include the following:\nAmazon Redshift\nGoogle BigQuery\nHive\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nSnowflake\nTeradata\nFor each of the supported connectors, Atlan supports mining query history via S3. This is useful when you have files that hold query history beyond what the source itself retains.\nTo mine lineage from these sources from S3, complete the following steps.\nStructure the query files\nâ\nTo make the query history files available for Atlan, ensure the files:\nUse a\n.json\nextension.\nAre present in a single S3 bucket and prefix (directory).\nTo structure the contents of the files for Atlan, ensure:\nEach line is a single JSON value. (The JSON object cannot be pretty-formatted or span multiple lines.)\nEach SQL query is on its own line.\nCommas are\nnot\nused to separate the lines.\nDid you know?\nYou can also provide a default database and schema, and session IDs in the JSON.\nIf a SQL query has only the name of the table or view it queries, Atlan will use the default database and schema to generate lineage for the query.\nIncluding the session ID speeds up lineage processing. If provided, ensure that all queries belonging to the same session are next to each other in the file.\nHere is an example of what your JSON should look like. (Here it is split across multiple lines to assist reading, but remember it must all be on a single line in the file!)\n{\n\"QUERY_TEXT\"\n:\n\"insert into NETFLIX_DB.PUBLIC.MOVIES_FILTERED as select m.* from MOVIES m where m.RATING > 5;\"\n,\n\"DATABASE_NAME\"\n:\n\"NETFLIX_DB\"\n,\n\"SCHEMA_NAME\"\n:\n\"PUBLIC\"\n,\n\"SESSION_ID\"\n:\n\"5c2f0a41-5d02-46f1-b9bd-ef80ad571013\"\n}\nThe name of the keys or properties in the JSON can be configured while setting up the miner package. In the example above, the default database (\nDATABASE_NAME\n) and schema (\nSCHEMA_NAME\n) will be used to qualify the query against the table\nMOVIES\nas\nNETFLIX_DB.PUBLIC.MOVIES\n.\nSet up the S3 bucket\nâ\nThe query files must be available in an S3 bucket. You can either upload these files to the Atlan deployment bucket or use your own S3 bucket.\nOption 1: Use the Atlan S3 bucket\nâ\nTo avoid access issues, we recommend uploading the required files to the same S3 bucket as Atlan.\nRaise a support request\nto get the details of your Atlan bucket and include the ARN value of the IAM user or IAM role we can provision access to.\nTo configure access, add the following IAM policy to the default EC2 instance role used by the Atlan EKS cluster.\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"VisualEditor0\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:GetBucketLocation\"\n,\n\"s3:ListBucket\"\n,\n\"s3:GetObject\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::<bucket-name>\"\n,\n\"arn:aws:s3:::<bucket-name>/<prefix>/*\"\n]\n}\n]\n}\nReplace\n<bucket-name>\nwith the bucket where the data is uploaded.\nReplace\n<prefix>\nwith the prefix (directory) where all the files have been uploaded.\nIf you instead opt to use your own S3 bucket, you will need to complete the following steps:\nOption 2: Use your own S3 bucket\nâ\ndanger\nS3 buckets with VPC endpoints currently do not support\ncross-region requests\n. This may result in workflows not picking up objects from your bucket.\nYou'll first need to create a cross-account bucket policy giving Atlan's IAM role access to your bucket. A cross-account bucket policy is required since your Atlan tenant and S3 bucket may not always be deployed in the same AWS account. The permissions required for the S3 bucket include   -\nGetBucketLocation\n,\nListBucket\n, and\nGetObject\n.\nTo create a cross-account bucket policy:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new policy to allow access by this ARN and update your bucket policy with the following:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"VisualEditor0\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<role-arn>\"\n}\n,\n\"Action\"\n:\n[\n\"s3:GetBucketLocation\"\n,\n\"s3:ListBucket\"\n,\n\"s3:GetObject\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::<bucket-name>\"\n,\n\"arn:aws:s3:::<bucket-name>/<prefix>/*\"\n]\n}\n]\n}\nReplace\n<role-arn>\nwith the role ARN of Atlan's node instance role.\nReplace\n<bucket-name>\nwith the name of the bucket you are creating.\nReplace\n<prefix>\nwith the name of the prefix (directory) within that bucket where you will upload the files.\nOnce the new policy has been set up, please notify the support team. Your request should include the S3 bucket name and prefix. This should be done prior to setting up the workflow so that we can create and attach an IAM policy for your bucket to Atlan's IAM role.\n(Optional) Update KMS policy\nâ\nIf your S3 bucket is encrypted, you will need to update your KMS policy. This will allow Atlan to decrypt the objects in your S3 bucket.\nProvide the KMS key ARN and KMS key alias ARN to the Atlan support team. The KMS key that you provide must be a customer managed KMS key. (This is because you can only change the key policy for a customer managed KMS key, and not for an AWS managed KMS key. Refer to\nAWS documentation\nto learn more.)\nTo whitelist the ARN of Atlan's node instance, update the KMS policy with the following:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"Decrypt Cross Account\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<role-arn>\"\n}\n,\n\"Action\"\n:\n[\n\"kms:Decrypt\"\n,\n\"kms:DescribeKey\"\n]\n,\n\"Resource\"\n:\n\"*\"\n}\n]\n}\nReplace\n<role-arn>\nwith the role ARN of Atlan's node instance role.\nSelect the miner\nâ\nTo select the S3 miner:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the filters along the top, click\nMiner\n.\nFrom the list of packages, select the miner for your source and click on\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the S3 miner:\nFor\nConnection\n, select the connection to mine. (To select a connection, a crawler must have already run against that source.)\nFor\nMiner extraction method\n, select\nS3\n.\nEnter the details for your files:\nFor\nBucket Name\n, enter the name of your S3 bucket or Atlan's bucket, including\ns3://\n.\nFor\nBucket Prefix\n, enter the S3 prefix (directory) within the bucket where the files are located.\n(Optional) For\nBucket Region\n, enter the name of the S3 region in which the bucket exists.\nFor\nSQL Json key\n, enter the JSON key containing the SQL query value. (In the example above, this was\nQUERY_TEXT\n.)\nFor\nDefault Database Json Key\n, enter the JSON key containing the name of the default database. (In the example above, this was\nDATABASE_NAME\n.)\nFor\nDefault Schema Json Key\n, enter the JSON key containing the name of the default schema. (In the example above, this was\nSCHEMA_NAME\n.)\nFor\nSession ID Json Key\n, enter the JSON key containing the session ID under which the query ran. (In the example above, this was\nSESSION_ID\n.)\n(Optional) For\nControl Config\n, if Atlan support has provided you a custom control configuration, select\nCustom\nand enter the configuration into the\nCustom Config\nbox. You can also:\nEnter\n{âignore-all-caseâ: true}\nto enable crawling assets with case-sensitive identifiers.\nRun the miner\nâ\nTo run the S3 miner, after completing the steps above:\nTo run the miner once, immediately, at the bottom of the screen click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly or monthly, at the bottom of the screen click the\nSchedule & Run\nbutton.\nOnce the miner has completed running, you will see lineage for your source's assets created by the queries in S3! ð\nFrequently asked questions\nâ\nIf I remove queries from S3 and run the miner, does it remove the lineage generated from those queries?\nâ\nNo, we do not remove lineage from older queries that are no longer in the bucket.\nDoes the miner reprocess files in the S3 prefix?\nâ\nYes, we process all files present in the S3 prefix and publish any new lineage generated. We recommend removing older files when updating the files in the S3 prefix.\nI used this approach for initial mining. Can I convert the miner I already set up to do its future mining direct from the source (not S3)?\nâ\nYes, just\nedit the workflow configuration\n. Alternatively, you can also set up another miner for the same connection.\nAre the database and schema name parameters always required in the JSON file?\nâ\nThe\nDATABASE_NAME\nand\nSCHEMA_NAME\nfields can be set to null if that data is already available in the query. These properties are used as a fallback option for when queries are run in the context of a certain schema or database.\nWhat SQL statements should be added to the S3 miner JSON file for lineage?\nâ\nYou will need to add DDL and DML statements to the S3 miner JSON file for mining lineage.\nSELECT\nis not required since it is a DQL statement. Both\nUPDATE\nand\nDELETE\ncan be based on values from another table, so these statements will be required for generating lineage.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nConnect data sources for Azure-hosted Atlan instances\nNext\nHow to order workflows\nStructure the query files\nSet up the S3 bucket\nOption 1: Use the Atlan S3 bucket\nOption 2: Use your own S3 bucket\nSelect the miner\nConfigure the miner\nRun the miner\nFrequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/mine-google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nCrawl BigQuery Assets\nMine Google BigQuery\nOn this page\nMine Google BigQuery\nOnce you have\ncrawled assets from Google BigQuery\n, you can mine its query history to construct lineage.\nTo mine lineage from Google BigQuery, review the\norder of operations\nand then complete the following steps.\nSelect the miner\nâ\nTo select the Google BigQuery miner:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the filters along the top, click\nMiner\n.\nFrom the list of packages, select\nBigQuery Miner\nand then click\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the Google BigQuery miner:\nFor\nConnection\n, select the connection to mine. (To select a connection,\nthe crawler\nmust have already run.)\nFor\nMiner Extraction Method\n, select\nQuery History\n.\nFor\nStart time\n, choose the earliest date from which to mine query history.\ninfo\nðª\nDid you know?\nThe miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the\nS3 miner\nfirst. After the initial load, you can\nmodify the miner's configuration\nto use query history extraction.\n(Optional) By default, the miner fetches data from the US region. To fetch data from\nanother region\n, for\nRegion\n, select\nCustom\nand then enter the region where your\nINFORMATION_SCHEMA\nis hosted under\nCustom BigQuery Region\n. Enter the region in the\nfollowing format\nregion-<REGION>\n, replacing\n<REGION>\nwith your specific region   -  for example,\neurope-north1\n.\nTo check for any permissions or other configuration issues before running the miner, click\nPreflight checks\n.\nAt the bottom of the screen, click\nNext\nto proceed.\ndanger\nIf running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic\nhere\n.\nConfigure the miner behavior\nâ\nTo configure the Google BigQuery miner behavior:\n(Optional) For\nCalculate popularity\n, change to\nTrue\nto retrieve\nusage and popularity metrics\nfor your Google BigQuery assets from query history:\nTo select a\npricing model for running queries\n, for\nPricing Model\n, click\nOn Demand\nto be charged for the number of bytes processed or\nFlat Rate\nfor the number of slots purchased.\nFor\nPopularity Window (days)\n, 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days.\nFor\nExcluded Users\n, type the names of users to be excluded while calculating\nusage metrics\nfor Google BigQuery assets. Press\nenter\nafter each name to add more names.\n(Optional) For\nControl Config\n, click\nCustom\nto configure the following:\nFor\nFetch excluded project's QUERY_HISTORY\n, click\nYes\nto mine query history from databases or projects excluded while\ncrawling metadata from Google BigQuery\n.\nIf Atlan support has provided you with a custom control configuration,Â enter the configuration into theÂ\nCustom Config\nbox. You can also:\n(Optional) Enter\n{âignore-all-caseâ: true}\nto enable crawling assets with case-sensitive identifiers.\nRun the miner\nâ\nTo run the Google BigQuery miner, after completing the steps above:\nTo run the miner once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the miner has completed running, you will see lineage for Google BigQuery assets that were created in Google BigQuery between the start time and when the miner ran! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nCrawl Google BigQuery\nNext\nManage Google BigQuery tags\nSelect the miner\nConfigure the miner\nConfigure the miner behavior\nRun the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/crawl-looker",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nCrawl Looker Assets\nCrawl Looker\nOn this page\nCrawl Looker\nOnce you have configured the\nLooker user permissions\n, you can establish a connection between Atlan and Looker.\nTo crawl metadata from Looker, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Looker as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nLooker Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to Looker and crawls metadata directly.\nIn\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Looker credentials:\nFor\nHost Name\n, enter the full URL for your Looker API host, including the\nhttps://\n.\nFor\nPort\n, keep\n443\nfor Looker instances created after July 7, 2020, or switch to\n19999\nfor older instances.\nFor\nClient ID\n, enter the client ID you generated when\nsetting up user permissions\n.\nFor\nClient Secret\n, enter the client secret you generated when\nsetting up user permissions\n.\n(Optional) For\nField Level Lineage\n:\nFor\nPrivate SSH Key\n, paste the\nprivate SSH key for the key you configured in GitHub\n.\nFor\nPassphrase for the private key\n, enter the passphrase that protects the key, if any. (If the key is not protected by a passphrase, leave this blank.)\nFor\nSSH Known Hosts\n, add any value that needs to be hardcoded in the\n~/.ssh/known-hosts\nfile before cloning your project Git repositories using SSH. (If not required, leave this blank.)\nAt the bottom of the form, click the\nTest Authentication\nbutton to confirm connectivity to Looker using these details.\nWhen successful, at the bottom of the screen click the\nNext\nbutton.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Looker. This method uses Atlan's looker-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\nprojects.json\n,\ndashboards.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Looker. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Looker data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the Looker connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the Looker crawler, you can further configure it. (These options are only available when using the\ndirect extraction method\n.)\nYou can override the defaults for any of these options:\nLooker folders contain saved content, such as\ndashboards\n,\nlooks\n, and\ntiles\n:\nTo select the Looker folders you want to include in crawling, click\nInclude Folders\n. (This will default to all folders, if none are specified.)\nTo select the Looker folders you want to exclude from crawling, click\nExclude Folders\n. (This will default to no folders, if none are specified.)\nLooker projects contain LookML files, such as\nmodels\n,\nviews\n, and\nexplores\n:\nTo select the Looker projects you want to include in crawling, click\nInclude Projects\n. (This will default to all projects, if none are specified.)\nTo select the Looker projects you want to exclude from crawling, click\nExclude Projects\n. (This will default to no projects, if none are specified.)\nFor\nUse Field Level Lineage\n, click\nTrue\nto enable crawling field-level lineage for Looker or click\nFalse\nto disable it.\nDid you know?\nIf a folder or project appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Looker crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up on-premises Looker access\nNext\nCrawl on-premises Looker\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nCrawl Tableau Assets\nCrawl Tableau\nOn this page\nCrawl Tableau\nOnce you have\nconfigured Tableau\n, you can establish a connection between Atlan and Tableau. (If you are also using a private network for Tableau, you will need to\nset that up first\n, too.)\nTo crawl metadata from Tableau, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Tableau as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nTableau Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nInÂ\nDirect\nextraction, Atlan connects to Tableau and crawls metadata directly.\nInÂ\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Tableau credentials:\nFor\nHost Name\n, enter the host name of your Tableau Online or Tableau Server instance (or the\nprivate DNS name if your Tableau Server instance uses an SSL certificate\n).\nFor\nPort\n, enter the port number of your Tableau instance.\nFor\nAuthentication\n, choose how you would like to connect to Tableau:\nFor\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou use to log in to Tableau.\nFor\nPersonal Access Token\nauthentication, enter the\nPersonal Access Token Name\nand\nPersonal Access Token Value\nyou generated\n_Create_a_personal_access_token).\nFor\nJWT Bearer\nauthentication, enter your Tableau Server username or Tableau Online email address for\nUsername\n, and the\nClient ID\n,\nSecret ID\n, and\nSecret Value\nyou copied from the connected app\nin Tableau.\n(Optional) For\nSSL\n, keep the default\nEnabled\nto use HTTPS or click\nDisabled\nto use HTTP.\nFor\nSite\n, enter the name of the site you want to crawl. (If left blank, the default site will be used.)\ndanger\nIf you are using Tableau Online, the site is required for Atlan to authenticate properly.\n(Optional) For\nSSL certificate\n, this is only required if\nyour Tableau Server instance uses a self-signed or an internal CA SSL certificate\n, paste a\nsupported SSL certificate in the recommended format\n.\nAt the bottom of the form, click the\nTest Authentication\nbutton to confirm connectivity to Tableau using these details.\nWhen successful, at the bottom of the screen click the\nNext\nbutton.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Tableau. This method uses Atlan's tableau-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nForÂ\nBucket name\n, enter the name of your S3 bucket.\nForÂ\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndashboards/result-0.json\n,\nworkbooks/result-0.json\n, and so on.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Tableau. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Tableau data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the Tableau connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nConfigure the crawler\nâ\nBefore running the Tableau crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the Tableau projects you want to include in crawling, click\nInclude Projects\n. (This will default to all assets, if none are specified.)\nTo select the Tableau projects you want to exclude from crawling, click\nExclude Projects\n. (This will default to no assets, if none are specified.)\nTo have the crawler ignore Tableau projects based on a naming convention, specify a regular expression in the\nExclude Projects Regex\nfield.\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nDid you know?\nIf a project appears in both the include and exclude filters, the exclude filter takes precedence. (The\nExclude Projects Regex\nalso takes precedence.)\nConfigure advanced controls\nâ\nBefore running the Tableau crawler, you can also configure advanced controls for the crawler.\nOn the\nAdvanced\npage, you can override the defaults for any of these options:\nFor\nAlternate Host URL\n, enter the protocol and host name to be used for viewing assets directly in Tableau.\nFor\nCrawl Unpublished Worksheets and Dashboards\n, click\nYes\nto enable crawling hidden worksheets and dashboards or\nNo\nto skip crawling them.\nFor\nHidden Datasource Fields\n, click\nYes\nto enable crawling hidden datasource fields or\nNo\nto skip crawling them.\nCrawl embedded dashboards: Embedded dashboard here means linking or displaying a dashboard inside another dashboard by providing a link to the dashboard in a Web Page item of the embedding dashboard.\nClick\nYes\nto enable relationships between different embedded dashboards.\nClick\nNo\nto skip creating relationships between embedded dashboards.\nRun the crawler\nâ\nTo run the Tableau crawler, after completing the steps above:\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up a private network link to Tableau server\nNext\nCrawl on-premises Tableau\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nConfigure advanced controls\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/tags/lineage",
    "content": "76 docs tagged with \"lineage\"\nView all tags\nAlteryx\nIntegrate, catalog, and govern Alteryx assets in Atlan using OpenLineage.\nAmazon MWAA OpenLineage\nIntegrate, catalog, and visualize Amazon MWAA lineage in Atlan.\nApache Airflow OpenLineage\nIntegrate, catalog, and visualize Apache Airflow lineage in Atlan.\nApache Spark OpenLineage\nIntegrate, catalog, and visualize Apache Spark lineage in Atlan.\nAstronomer OpenLineage\nIntegrate, catalog, and visualize Astronomer lineage in Atlan.\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nConnectors and capabilities\nLearn about connectors and capabilities.\nCrawl Dagster assets\nCreate a crawler workflow in Atlan to capture lineage from Dagster assets\nCreate data products\nYou can either create a data product from the products module or lineage graph.\nDagster\nIntegrate, catalog, and visualize Dagster lineage in Atlan.\nDagster integration\nFrequently asked questions about Dagster integration with Atlan\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nDataStax Enterprise\nCatalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data.\nDoes Atlan support field-level lineage for BI tools?\nAtlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nGenerate lineage between assets App\nLearn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app.\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan.\nHigh availability and disaster recovery (HA/DR)\nLearn about high availability and disaster recovery (ha/dr).\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow do you enable data lineage for different data sources?\nLearn about how do you enable data lineage for different data sources?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nHow is the Atlan lineage graph depicted using Power BI measures?\nLearn about how is the atlan lineage graph depicted using power bi measures?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nLineage\n[Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:.\nLineage\nTrack and visualize data lineage across your data landscape to understand data flow and dependencies.\nLineage Generator (no transformations)\nLearn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior.\nMonitor data domains\nThe _Statistics_ tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nPreflight checks for Apache Airflow\nLearn about preflight checks for apache airflow.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nReport on glossaries\nThe [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further.\nReport on queries\nThe Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs.\nReport on usage and cost\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nSet up Dagster\nConfigure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assets\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSource asset type\nDetailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app.\nTag propagation\nLearn about tag propagation.\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector\nTenant logs\nLearn about tenant logs.\nTeradata\nCatalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage.\nTroubleshooting Amazon DynamoDB connectivity\nLearn about troubleshooting amazon dynamodb connectivity.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting Amazon Redshift connectivity\nLearn about troubleshooting amazon redshift connectivity.\nTroubleshooting Apache Airflow/OpenLineage connectivity\nLearn about troubleshooting apache airflow/openlineage connectivity.\nTroubleshooting Apache Spark/OpenLineage connectivity\nLearn about troubleshooting apache spark/openlineage connectivity.\nTroubleshooting Domo connectivity\nLearn about troubleshooting domo connectivity.\nTroubleshooting Fivetran connectivity\nLearn about troubleshooting fivetran connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Matillion connectivity\nLearn about troubleshooting matillion connectivity.\nTroubleshooting Microsoft Azure Cosmos DB connectivity\nLearn about troubleshooting microsoft azure cosmos db connectivity.\nTroubleshooting Microsoft Azure Data Factory connectivity\nLearn about troubleshooting microsoft azure data factory connectivity.\nTroubleshooting Microsoft Power BI connectivity\nLearn about troubleshooting microsoft power bi connectivity.\nTroubleshooting Mode connectivity\nLearn about troubleshooting mode connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nTroubleshooting on-premises Looker connectivity\nLearn about troubleshooting on-premises looker connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting Sigma connectivity\nLearn about troubleshooting sigma connectivity.\nTroubleshooting Sisense connectivity\nLearn about troubleshooting sisense connectivity.\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nTroubleshooting usage and popularity metrics\nLearn about troubleshooting usage and popularity metrics.\nUse Atlan AI for lineage analysis\nâ Available to customers in Enterprise and Business-Critical platform editions\nView event logs\nEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.\nview lineage\nThe [lineage](/product/capabilities/lineage/concepts/what-is-lineage) graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps.\nWhat are processes?\nLearn about what are processes?.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Dagster\nLearn about the Dagster metadata that Atlan captures and visualizes\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase.\nWhat does Atlan crawl from Sigma?\nAtlan crawls and maps the following assets and properties from Sigma.\nWhat is business lineage?\nLearn about what is business lineage?.\nWhat lineage do you support?\nLearn about what lineage do you support?.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/data-lineage",
    "content": "60 docs tagged with \"data-lineage\"\nView all tags\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nConnectors and capabilities\nLearn about connectors and capabilities.\nCreate data products\nYou can either create a data product from the products module or lineage graph.\nDoes Atlan support field-level lineage for BI tools?\nAtlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nGenerate lineage between assets App\nLearn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app.\nHigh availability and disaster recovery (HA/DR)\nLearn about high availability and disaster recovery (ha/dr).\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow do you enable data lineage for different data sources?\nLearn about how do you enable data lineage for different data sources?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nHow is the Atlan lineage graph depicted using Power BI measures?\nLearn about how is the atlan lineage graph depicted using power bi measures?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nLineage\n[Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:.\nLineage Generator (no transformations)\nLearn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior.\nMonitor data domains\nThe _Statistics_ tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nPreflight checks for Apache Airflow\nLearn about preflight checks for apache airflow.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nReport on glossaries\nThe [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further.\nReport on queries\nThe Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs.\nReport on usage and cost\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSource asset type\nDetailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app.\nTag propagation\nLearn about tag propagation.\nTenant logs\nLearn about tenant logs.\nTroubleshooting Amazon DynamoDB connectivity\nLearn about troubleshooting amazon dynamodb connectivity.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting Amazon Redshift connectivity\nLearn about troubleshooting amazon redshift connectivity.\nTroubleshooting Apache Airflow/OpenLineage connectivity\nLearn about troubleshooting apache airflow/openlineage connectivity.\nTroubleshooting Apache Spark/OpenLineage connectivity\nLearn about troubleshooting apache spark/openlineage connectivity.\nTroubleshooting Domo connectivity\nLearn about troubleshooting domo connectivity.\nTroubleshooting Fivetran connectivity\nLearn about troubleshooting fivetran connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Matillion connectivity\nLearn about troubleshooting matillion connectivity.\nTroubleshooting Microsoft Azure Cosmos DB connectivity\nLearn about troubleshooting microsoft azure cosmos db connectivity.\nTroubleshooting Microsoft Azure Data Factory connectivity\nLearn about troubleshooting microsoft azure data factory connectivity.\nTroubleshooting Microsoft Power BI connectivity\nLearn about troubleshooting microsoft power bi connectivity.\nTroubleshooting Mode connectivity\nLearn about troubleshooting mode connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nTroubleshooting on-premises Looker connectivity\nLearn about troubleshooting on-premises looker connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting Sigma connectivity\nLearn about troubleshooting sigma connectivity.\nTroubleshooting Sisense connectivity\nLearn about troubleshooting sisense connectivity.\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nTroubleshooting usage and popularity metrics\nLearn about troubleshooting usage and popularity metrics.\nUse Atlan AI for lineage analysis\nâ Available to customers in Enterprise and Business-Critical platform editions\nView event logs\nEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.\nview lineage\nThe [lineage](/product/capabilities/lineage/concepts/what-is-lineage) graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps.\nWhat are processes?\nLearn about what are processes?.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase.\nWhat does Atlan crawl from Sigma?\nAtlan crawls and maps the following assets and properties from Sigma.\nWhat is business lineage?\nLearn about what is business lineage?.\nWhat lineage do you support?\nLearn about what lineage do you support?.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/impact-analysis",
    "content": "58 docs tagged with \"impact-analysis\"\nView all tags\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nConnectors and capabilities\nLearn about connectors and capabilities.\nCreate data products\nYou can either create a data product from the products module or lineage graph.\nDoes Atlan support field-level lineage for BI tools?\nAtlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nHigh availability and disaster recovery (HA/DR)\nLearn about high availability and disaster recovery (ha/dr).\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow do you enable data lineage for different data sources?\nLearn about how do you enable data lineage for different data sources?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nHow is the Atlan lineage graph depicted using Power BI measures?\nLearn about how is the atlan lineage graph depicted using power bi measures?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nLineage\n[Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:.\nLineage Generator (no transformations)\nLearn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior.\nMonitor data domains\nThe _Statistics_ tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nPreflight checks for Apache Airflow\nLearn about preflight checks for apache airflow.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nReport on glossaries\nThe [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further.\nReport on queries\nThe Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs.\nReport on usage and cost\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nSet up on-premises Databricks lineage extraction\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nTag propagation\nLearn about tag propagation.\nTenant logs\nLearn about tenant logs.\nTroubleshooting Amazon DynamoDB connectivity\nLearn about troubleshooting amazon dynamodb connectivity.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting Amazon Redshift connectivity\nLearn about troubleshooting amazon redshift connectivity.\nTroubleshooting Apache Airflow/OpenLineage connectivity\nLearn about troubleshooting apache airflow/openlineage connectivity.\nTroubleshooting Apache Spark/OpenLineage connectivity\nLearn about troubleshooting apache spark/openlineage connectivity.\nTroubleshooting Domo connectivity\nLearn about troubleshooting domo connectivity.\nTroubleshooting Fivetran connectivity\nLearn about troubleshooting fivetran connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Matillion connectivity\nLearn about troubleshooting matillion connectivity.\nTroubleshooting Microsoft Azure Cosmos DB connectivity\nLearn about troubleshooting microsoft azure cosmos db connectivity.\nTroubleshooting Microsoft Azure Data Factory connectivity\nLearn about troubleshooting microsoft azure data factory connectivity.\nTroubleshooting Microsoft Power BI connectivity\nLearn about troubleshooting microsoft power bi connectivity.\nTroubleshooting Mode connectivity\nLearn about troubleshooting mode connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nTroubleshooting on-premises Looker connectivity\nLearn about troubleshooting on-premises looker connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting Sigma connectivity\nLearn about troubleshooting sigma connectivity.\nTroubleshooting Sisense connectivity\nLearn about troubleshooting sisense connectivity.\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nTroubleshooting usage and popularity metrics\nLearn about troubleshooting usage and popularity metrics.\nUse Atlan AI for lineage analysis\nâ Available to customers in Enterprise and Business-Critical platform editions\nView event logs\nEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.\nview lineage\nThe [lineage](/product/capabilities/lineage/concepts/what-is-lineage) graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps.\nWhat are processes?\nLearn about what are processes?.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase.\nWhat does Atlan crawl from Sigma?\nAtlan crawls and maps the following assets and properties from Sigma.\nWhat is business lineage?\nLearn about what is business lineage?.\nWhat lineage do you support?\nLearn about what lineage do you support?.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/faq",
    "content": "33 docs tagged with \"faq\"\nView all tags\nCan Atlan integrate with Airflow to generate lineage?\nAtlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage).\nCan Atlan read a dump of SQL statements to create lineage?\nAtlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/.\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nYou can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events.\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nRefer to our [troubleshooting Jira documentation](/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira) to learn more.\nCan I create backups of glossaries?\nAtlan also allows you to [export your glossary assets](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) to spreadsheets and keep a record of your contextual information.\nCan I query any DW/DL?\nYou can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources#data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature.\nCan I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.\nDagster integration\nFrequently asked questions about Dagster integration with Atlan\nDeployment and security\nFrequently asked questions about Secure Agent 2.0 deployment and security\nDiscovery FAQs\nFrequently asked questions about Atlan's Discovery capabilities.\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.\nFrequently Asked Questions\nFind answers to common questions about using Atlan, organized by topic area for quick resolution.\nHow can I identify an Insights query in my database access log?\nAtlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs.\nHow can I use personas to update a term in a glossary?\nBy default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section.\nHow do I send messages or search assets from Slack?\nSending messages and searching assets from Slack are disabled. Refer to [Troubleshooting Slack](/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack) to learn more.\nLineage\n[Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:.\nPermissions and limitations\nFrequently asked questions about CrateDB connector setup, permissions, and limitations\nRoles and permissions\nExplanation of Snowflake's security model and role requirements for data quality operations.\nSetup and configuration\nCommon questions about Databricks data quality setup and configuration.\nSupported sources\nLearn about supported sources.\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector\nTroubleshooting Anomalo connectivity\nLearn about troubleshooting anomalo connectivity.\nTroubleshooting Slack\nWhat do the colors in Slack notifications for modified assets mean?\nWhat are Power BI processes on the lineage graph?\nNote that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets.\nWhat does Atlan do with each Slack permission?\nLearn about what does atlan do with each slack permission?.\nWhat is included in the Jira integration?\nWith two of your most important workspaces connected, you can save time and improve the way you track issues for your data.\nWhat is included in the Microsoft Teams integration?\nWith two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.\nWhat is included in the Slack integration?\nLearn about the features and capabilities of the Slack integration with Atlan.\nWhat is the default permission for a glossary?\nBy default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data#glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access.\nWhat is the difference between Copy Link and Share on Slack or Teams?\nLearn about what is the difference between copy link and share on slack or teams?.\nWhy do I only see tables from the same schema to join from in a visual query?\nWhen [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder.\nWhy is lineage available for table level but not column level?\nThe home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset.\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?\nAtlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/troubleshooting",
    "content": "8 docs tagged with \"troubleshooting\"\nView all tags\nConnection issues\nResolve common connection and authentication issues when setting up CrateDB connector\nDeployment and security\nFrequently asked questions about Secure Agent 2.0 deployment and security\nFrequently Asked Questions\nFind answers to common questions about using Atlan, organized by topic area for quick resolution.\nLineage\n[Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:.\nSetup and configuration\nCommon questions about Databricks data quality setup and configuration.\nSupported sources\nLearn about supported sources.\nTroubleshooting Anomalo connectivity\nLearn about troubleshooting anomalo connectivity.\nTroubleshooting Slack\nWhat do the colors in Slack notifications for modified assets mean?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai",
    "content": "Configure Atlan\nAtlan AI\nOn this page\nAtlan AI\nOverview:\nCatalog and leverage Atlan AI capabilities to enhance your data assets in Atlan. Gain AI-powered documentation, and lineage analysis capabilities for your data estate.\nGet started\nâ\nRemote MCP Overview\n: Learn about Atlan's hosted Remote MCP server for AI agents\nHow to implement the Atlan MCP server\n: Set up the local MCP server\nGuides\nâ\nHow to use Atlan AI for documentation\n: Generate and manage AI-powered descriptions for your data assets.\nHow to use Atlan AI for lineage analysis\n: Understand lineage transformations using natural language explanations.\nConcepts\nâ\nWhat is Atlan AI\n: Learn about Atlan AI's capabilities and features.\nAtlan AI security\n: Understand how Atlan AI handles data security and privacy.\nTags:\natlan-ai\nai\ncapabilities\nNext\nAtlan MCP\nGet started\nGuides\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/atlan-mcp-overview",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nOn this page\nAtlan MCP\nThe\nModel Context Protocol (MCP)\nis an open standard that enables AI agents to access contextual metadata from external systems. It provides a consistent way for large language models and automation frameworks to retrieve the context they need to generate accurate and reliable results.\nAtlan MCP is based on this standard and provides a reference implementation through the\nAtlan MCP server\n. The server acts as a secure bridge between Atlanâs metadata platform and AI tools such as Claude, Cursor, Windsurf, and Microsoft Copilot Studio. With Atlan MCP, you can search and discover assets, explore lineage, update metadata, create glossaries, and more, all using real-time context from Atlan.\nAtlan MCP tools\nâ\nThe Atlan MCP server provides a set of tools that enable AI agents to work directly with Atlan metadata. These tools supply real-time context to AI environments, making it easier to search, explore, and update metadata without leaving your workflow.\nSearch assets\n: Find assets in Atlan using flexible filters such as name, type, tags, and domains. This helps AI agents surface the most relevant assets for a given task.\nQuery by DSL\n: Retrieve specific assets using Atlan's DSL query language. This enables precise lookups that go beyond basic search filters.\nExplore lineage\n: Trace upstream or downstream lineage for a given asset. This provides visibility into data dependencies and impact across your environment.\nUpdate assets\n: Modify metadata attributes, including descriptions, certification status, and README content. This enables AI agents to keep metadata current as part of automated workflows.\nGlossary\n: Create glossaries, categories, and terms. This supports standardized business definitions and improves consistency across teams and tools.\nDeployment options\nâ\nYou can connect to Atlan MCP in two ways:\nRemote MCP\n: A hosted, per-tenant MCP server managed by Atlan. Supports both OAuth and API Key authentication. Currently available in\nprivate preview\n.\nLocal MCP\n: A locally hosted server using Docker or uv. Provides flexibility for development or testing.\nTags:\nAtlan MCP\nAI\nmetadata\nintegrations\nPrevious\nAtlan AI\nNext\nRemote MCP\nAtlan MCP tools\nDeployment options"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
    "content": "Configure Atlan\nAtlan AI\nDocumentation\nHow to use Atlan AI for documentation\nOn this page\nUse Atlan AI for documentation\nâ\nAvailable to customers in Enterprise and Business-Critical platform editions\nWho can do this?\nBefore using Atlan AI, your admin user must\nenable Atlan AI\nin your Atlan workspace.\nAtlan AI helps you automate the process of documenting your data assets. You can use Atlan AI to generate meaningful context for your assets and then simply review the content for accuracy and relevance. Accept, reject, or edit any AI-powered suggestions, the choice is yours!\nYou can use Atlan AI to:\nDocument tables and views with AI-generated descriptions\nDocument child assets with AI-generated descriptions\nDocument\nterms and categories\nwith AI-generated descriptions\nDocument terms with AI-generated READMEs\nAdd an Atlan AI-generated alias to supported assets\nDid you know?\nTo ensure full transparency, any changes made using Atlan AI will be marked as\nUpdated using Atlan AI\nin the\nactivity log\n.\nAdd a description to a table or view\nâ\nYou can use Atlan AI to add descriptions to your assets in Atlan and provide helpful context to your teams. Supported asset types include:\nAmazon QuickSight\nanalyses, dashboards, and datasets\ndbt Cloud\nand\ndbt Core\nmodels, sources, and tests\nLooker\ndashboards, explores, looks, models, tiles, and views\nMicrosoft Power BI\nworkspaces, dashboards, datasets, data sources, pages, reports, tables, and tiles\nMode\ncharts, queries, and reports\nRedash\nÂ dashboards, queries, and visualizations\nSalesforce\nobjects, dashboards, and reports\nSigma\nworkbooks, datasets, pages, and data elements\nSnowflake streams\nSQL tables, views, databases, and schemas\nTableau\nworkbooks, worksheets, dashboards, data sources, and metrics\nThoughtSpot\nanswers and liveboards\nTo add a description to a table or view using Atlan AI:\nFrom the left menu on any screen, click\nAssets\n.\n(Optional) Under the search bar on the\nAssets\npage, click the\nTable\nÂ tab.\n(Optional) In the\nFilters\nmenu on the left, click\nProperties\nto expand the menu and select\nDescription\nto search for assets without a description.\nClick an asset to view the\nOverview\ntab in the sidebar.\nUnder\nDescription\n, you can either:\nFor assets without a description, navigate to the text box and click\nuse Atlan AI\nto add an Atlan AI-suggested description.\nFor assets with an existing description, click the text box and then click the\nImprove using Atlan AI\nbutton to replace the existing description with an Atlan AI-suggested one.\n(Optional) At the bottom of the\nAtlan AI is writing...\nÂ box, click\nEnhance now\nto briefly\ndescribe your organization\nand help Atlan AI make more relevant suggestions   -  this option is only visible to admin users.\nOnce Atlan AI has generated a description, you can:\nClick anywhere in the text box to edit the text and then click\nApply\n.\nClick\nApply\nto add the description to your asset.\nClick\nDiscard\nto discard the AI-generated description.\nClick the retry button to generate a new description, compare the two to select the most relevant option, and then click\nApply\n.\n(Optional) From the sidebar tabs on the right, click the\nActivity\ntab to view the changelog   -  including the\nUpdated using Atlan AI\nstamp, user information, and timestamp for the update.\nYour AI-suggested table or view description is now live! ð\nAdd a description to a column\nâ\nYou can use Atlan AI to add descriptions to your columns from the following:\nOverview\ntab in the sidebar for column assets\nColumn preview\nsection in the asset profile for table and view assets\nColumns\ntab in the sidebar for table and view assets\nSupported asset types include:\nAmazon QuickSight\nanalysis visuals, dashboard visuals, and dataset fields\ndbt Cloud\nand\ndbt Core\ncolumns\nLooker\nfields for explores and views\nMicrosoft Power BI\ncolumns and measures\nSalesforce\nfields\nSigma\ndataset columns and data element fields\nSQL columns\nTableau\ndata source fields and calculated fields\nThoughtSpot\nvisualizations\nIn this example, we'll use Atlan AI to add a description to a column from the\nColumn preview\nsection in a table\nasset profile\n.\nTo add a description to a column using Atlan AI:\nFrom the left menu on any screen, click\nAssets\n.\n(Optional) Under the search bar on the\nAssets\npage, click the\nTable\nÂ tab.\nRight-click an asset and then select\nOpen profile\nto view its asset profile.Â\nFrom the asset profile, navigate to the\nColumn Preview\nsection and select a column to document.\nUnder\nDescription\n, click\nuse Atlan AI\nto add an AI-generated description to the column.\n(Optional) At the bottom of the\nAtlan AI is writing...\nÂ box, click\nEnhance now\nto briefly\ndescribe your organization\nand help Atlan AI make more relevant suggestions   -  this option is only visible to admin users.\nOnce Atlan AI has generated a description, you can:\nClick anywhere in the text box to edit the text and then click\nApply\n.\nClick\nApply\nto add the description to your asset.\nClick\nDiscard\nto discard the AI-generated description.\nClick the retry button to generate a new description, compare the two to select the most relevant option, and then click\nApply\n.\n(Optional) From the sidebar tabs on the right, click the\nActivity\ntab to view the changelog   -  including the\nUpdated using Atlan AI\nstamp, user information, and timestamp for the update.\nYour AI-suggested column descriptions are now live! ð\nAdd a description to glossary assets\nâ\nYou can use Atlan AI to add descriptions to your\nterms and categories\nin Atlan and provide useful business context for your\nlinked assets\n.\nTo add a description to a term using Atlan AI:\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of your glossary.\nUnder your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI.\nYou can either add a description from the term profile or sidebar. Under\nDescription\n, you can either:\nFor terms without a description, navigate to the text box and click\nuse Atlan AI\nto add an Atlan AI-suggested description.\nFor terms with an existing description, click the text box and then click the\nImprove using Atlan AI\nbutton to replace the existing description with an Atlan AI-suggested one   -  this option is only available in the sidebar.\n(Optional) At the bottom of the\nAtlan AI is writing...\nbox, click\nEnhance now\nto briefly\ndescribe your organization\nand help Atlan AI make more relevant suggestions   -  this option is only visible to admin users.\nOnce Atlan AI has generated a description, you can:\nClick anywhere in the text box to edit the text and then click\nApply\n.\nClick\nApply\nto add the description to your term.\nClick\nDiscard\nto discard the AI-generated description.\nClick the retry button to generate a new description, compare the two to select the most relevant option, and then click\nApply\n.\n(Optional) From the sidebar tabs on the right, click the\nActivity\ntab to view the changelog   -  including the\nUpdated using Atlan AI\nstamp, user information, and timestamp for the update.\nYour AI-suggested term description is now live! ð\nAdd a README to terms\nâ\nYou can use Atlan AI to generate comprehensive\nREADMEs\nfor your terms in Atlan. This provides you with a solid foundation for documenting business context. You can then simply edit the Atlan AI-generated README and customize the format accordingly.\nTo add a README to a term using Atlan AI:\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of your glossary.\nUnder your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI.\nIn the\nReadme\nsection of the asset profile, click\nUse Atlan AI\nto add an Atlan AI-suggested\nREADME\nto the term.\nClick anywhere in the text box to edit or format the text and then click\nSave\n.\nDid you know?\nIf there are automated suggestions for asset descriptions, the option to use Atlan AI to document such assets will be unavailable.\nAutomated suggestions\nare based on user-generated descriptions for similar assets and may be more accurate than Atlan AI-generated descriptions.\nTags:\natlan\ndocumentation\nPrevious\nSet up Local MCP Server\nNext\nHow to use Atlan AI for lineage analysis\nAdd a description to a table or view\nAdd a description to a column\nAdd a description to glossary assets\nAdd a README to terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-lineage-analysis",
    "content": "Configure Atlan\nAtlan AI\nLineage Analysis\nHow to use Atlan AI for lineage analysis\nOn this page\nUse Atlan AI for lineage analysis\nâ\nAvailable to customers in Enterprise and Business-Critical platform editions\nWho can do this?\nBefore using Atlan AI, your admin user must\nenable Atlan AI\nin your Atlan workspace.\nAtlan AI can help you understand\nlineage transformations\nusing natural language. You can use Atlan AI to create a natural language explanation for assets with SQL attributes and help you better understand the transformation logic.\nExplain lineage transformations\nâ\nTo use Atlan AI to explain lineage transformations:\nFrom the left menu of any screen, click\nAssets\n.\nSelect an asset, and from the top right of the asset card, click theÂ\nView lineage\nÂ icon to open the lineage graph.\nOn the lineage graph, click any circular\nprocess\nbutton to view more details in the sidebar.\nFrom\nOverview\nin the sidebar, under\nQuery\n, click the\nAtlan AI\nicon to explain the SQL query.\nYou can now understand lineage transformations using Atlan AI! ð\nDid you know?\nThe lineage graph in Atlan provides a granular view of the data flows and transformations for your assets, learn more\nhere\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nHow to use Atlan AI for documentation\nNext\nAtlan AI security\nExplain lineage transformations"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/concepts/security",
    "content": "Configure Atlan\nAtlan AI\nConcepts\nAtlan AI security\nOn this page\nAtlan AI security\nDid you know?\nAtlan uses\nAzure OpenAI Service\nto power Atlan AI. Atlan does\nnot\nsend any data to the AI service and only uses metadata for supported capabilities. For questions about data security, see below.\nLearn more about how\nAtlan AI\nprocesses and stores your data:\nWhat services does Atlan AI use?\nâ\nAtlan uses\nAzure OpenAI Service\nto power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model.\nWhat data does Atlan send to the AI service?\nâ\nAtlan does\nnot\nsend any data to the AI service. Atlan only sends metadata for supported capabilities.\nFor example:\nAtlan AI-suggested asset descriptions\n-  table, view, column, database, or schema name.\nAtlan AI-suggested term descriptions\n-  glossary name and description, category name and description, and term name.\nAtlan AI-suggested lineage explanations\n-  SQL transformations in lineage with upstream and downstream asset names.\nAtlan AI-suggested aliases\n-  table, view, column, database, or schema name.\nAtlan AI-suggested READMEs for terms\n-  glossary, category, and term name and description, and any existing READMEs within the same glossary.\nDoes Atlan use any metadata or data to train Atlan AI?\nâ\nNo, Atlan does\nnot\nuse your metadata or data for fine-tuning or training AI models.\nIs the data processed through Atlan AI encrypted?\nâ\nAtlan makes HTTPS requests from your tenant, applicable to all\nsupported cloud platforms\n. The data is encrypted in transit using TLS 1.2, AWS PrivateLink, or Azure virtual network peering. Atlan uses 256-bit Advanced Encryption Standard (AES-256) algorithm to encrypt data at rest.\nHow does Atlan ensure security development of Atlan AI?\nâ\nAtlan AI follows\nOWASP Top 10\nthat includes application security reviews and\nStatic Application Security Testing (SAST) tools\n.\nDoes Atlan AI comply with any governance or legal frameworks?\nâ\nWhile Atlan is\nHIPAA and GDPR compliant\n, Atlan AI is currently not. As Atlan AI matures, compliance continues to be our key focus.\nDoes Atlan AI process PII or other sensitive data?\nâ\nAtlan AI only processes user input and metadata, which typically do not contain PII or sensitive data. However, it is the organization's responsibility to ensure that PII or other sensitive data is not available in the metadata or shared via user input.\nWhat is the data retention policy for Atlan AI?\nâ\nAtlan does\nnot\nstore any data for Atlan AI.\nThis is enforced in the following two ways:\nAtlan has an\nexemption from Microsoft\nto not store any data. Keeping data sensitivity in mind, Atlan has opted out of abuse monitoring and human review from Azure OpenAI Service.\nOnly the metadata generated using Atlan AI is cataloged in Atlan.\nHow does Atlan manage security vulnerabilities for Atlan AI?\nâ\nVulnerabilities and incidents are managed in accordance with the\nexisting program and policy\n.\nHow does Atlan manage the performance and scale for Atlan AI?\nâ\nWe utilize the scalability of our existing cloud infrastructure while relying on\nAzure OpenAI\n.\nTags:\ndata\nmodel\nPrevious\nHow to use Atlan AI for lineage analysis\nNext\nWhat's Atlan AI?"
  },
  {
    "url": "https://docs.atlan.com/tags/atlan-ai",
    "content": "2 docs tagged with \"atlan-ai\"\nView all tags\nAtlan AI\nâ Available to customers in Enterprise and Business-Critical platform editions\nAtlan AI\nIntegrate and leverage Atlan AI capabilities for enhanced data documentation, and lineage analysis."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags",
    "content": "Build governance\nTags\nOn this page\nTags\nOverview:\nUse tags in Atlan to categorize and organize your data assets. Tags provide a flexible way to add metadata and classifications to your assets, enhancing searchability and governance.\nGet started\nâ\nFollow these steps to implement tags in Atlan:\nCreate a new tag\nGuides\nâ\nDelete a tag\nConcepts\nâ\nWhat are tags\nTags:\ntags\nclassification\ncategorization\norganization\ngovernance\natlan\nNext\nCreate a new tag\nGet started\nGuides\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/create-a-new-tag",
    "content": "Build governance\nTags\nGet Started\nCreate a new tag\nCreate a new tag\nWho can do this?\nYou will need to be an admin user in Atlan to create tags.\nTo create a new tag:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading, click\nTags\nand then add a new tag:\nIf there are no existing tags, click\nAdd tag\n.\nIf you have any existing tags, under the\nTags\nheading, click the\n+ New\nbutton.\nEnter details for the tag:\nFor\nUntitled tag\n, enter a meaningful name for your tag.\n(Optional) For\nAdd description...,\nenter a more detailed description of your tag.\n(Optional) To personalize your tags, click the tag icon. From the upper right of the\nIcons\ndialog:\nClick\nIcons\nto change the icon for your tag. Click the gray box to change the color of your tag icon to green, yellow, or red.\nClick\nEmoji\nto add an emoji to your tag.\nClick\nUpload Image\nto upload an image for your tag. The recommended size for image uploads is 24x24 pixels.\nClickÂ\nCreate\n.\n(Optional) Under\nTags\n, click the funnel icon to filter your tags by source:\nClick\nAtlan\nto filter for tags created in Atlan and not synced to any external sources.\nClick\nSnowflake\nto filter for\ntags imported from Snowflake\n.\nClick\ndbt\nto filter for\ntags imported from dbt\n.\nThat's it   -  you now have a tag ready for your team to use for\ntagging assets\n! ð\nFor tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar.\nDid you know?\nOnce you've created a tag, you can also\ndelete\nit at any time.\nTags:\natlan\ndocumentation\nPrevious\nTags\nNext\nDelete a tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/delete-a-tag",
    "content": "Build governance\nTags\nTag Management\nDelete a tag\nOn this page\nDelete a tag\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to delete tags.\nTo delete\ntags\n, you can either:\nIf a tag is not attached to any assets, you can\ndelete the tag\nright away.\nIf a tag is attached to assets, you will need to\nremove the tag\nfrom the\ntagged assets\nfirst and then you can\ndelete\nit.\nDelete a tag\nâ\nTo delete a tag without linked assets:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter your tags by source:\nClick\nAtlan\nto filter for tags created in Atlan and not synced to any external sources.\nClick\nSnowflake\nto filter for\ntags imported from Snowflake\n.\nClick\ndbt\nto filter for\ntags imported from dbt\n.\nUnder the left\nTags\nmenu, select a tag to delete. (The total count of linked assets will be displayed as zero if there are no linked assets.)\nFrom the top right, click the trash can icon to delete the tag.\nIn the\nDelete tag\ndialog, click\nDelete\nto confirm deletion.\nDelete a tag with linked assets\nâ\ndanger\nIf a tag is attached to assets, you will need to\nremove the tag\nfrom the\ntagged assets\nÂ before deleting it.\nTo delete a tag with linked assets:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter your tags by source:\nClick\nAtlan\nto filter for tags created in Atlan and not synced to any external sources.\nClick\nSnowflake\nto filter for\ntags imported from Snowflake\n.\nClick\ndbt\nto filter for\ntags imported from dbt\n.\nUnder the\nTags\nmenu, select a tag to delete.\nUnder the tag name, click the\nLinked assets\ntab to navigate to the linked assets.\nTo remove the tag from the linked assets, you can either:\nClick a linked asset to open the asset sidebar, and then under\nTags\nin the sidebar, click the pencil icon to edit the tag. Uncheck the tag box and then click\nSave\n. Repeat the steps for each tagged asset.\nCreate a playbook\nto remove the tag from tagged assets at scale.\nOnce the tag has been removed from all the linked assets, from the top right, click the trash can icon to delete the tag.\nIn the\nDelete tag\ndialog, click\nDelete\nto confirm.\nTags:\natlan\ndocumentation\nPrevious\nCreate a new tag\nNext\nAttach a tag\nDelete a tag\nDelete a tag with linked assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/remove-a-tag",
    "content": "Build governance\nTags\nTag Management\nRemove a tag\nOn this page\nRemove a tag\nAtlan allows you to remove\ntags\nfrom a\ntagged asset\n. To remove a tag, you will first need to identify the origin of the tag:\nDirectly added\nto an asset\nPropagated to an asset\nthrough\ntag propagation\nRemove a direct tag\nâ\nTo remove a direct tag:\nIn the left menu from any screen in Atlan, click\nAssets\n.\nOn the\nAssets\npage, select a tagged asset   -  in this example, we'll select the\nORDERS\ntable with the\nMarketing Analysis\ntag.\nUnder\nTags\nin the right menu, hover over the attached tag to view details. In the metadata popover,Â\nLinked by\nindicates that the tag was directly added to the asset. Click the pencil icon to edit the tag.\nIn the popup, next to the tag name, uncheck the box to remove the tag   -  this will also remove the tag from all the assets to which it was\npropagated\n.\nClick\nSave\nto confirm tag removal.\nRemove a propagated tag\nâ\nTo remove a propagated tag:\nIn the left menu from any screen in Atlan, click\nAssets\n.\nOn the\nAssets\npage, select a tagged asset   -  in this example, we'll select the\nAUTHORS\ncolumn with the\nPublications Department\ntag.\nUnder\nTags\nin the right menu, you can either:\nHover over the attached tag to view details. In the metadata popover,\nPropagated from\nindicates that the tag was propagated to the asset. From the popover, click the originating asset to reconfigure tag propagation   -  in this example, the\nBook_rating\ntable.\nClick the pencil icon and then click the\nPropagated\ntab to view information about tags propagated via upstream assets. From the\nPropagated\ntab, click the originating asset to reconfigure tag propagation   -  in this example, the\nBook_rating\ntable.\nFrom the corresponding screen, under\nTags\nin the right menu for the originating asset, click the pencil icon to edit the tag.\nIn the popup, next to the tag name, click\nEdit\n:\nTo remove the tag from downstream assets only, from the\nPropagation\ndialog, click\nHierarchy only (no lineage)\n.\nTo remove the tag from\npropagated assets\nonly and not the originating asset, from the\nPropagation\ndialog, click\nNo propagation\n.\nClick\nUpdate\nto save your changes.\nClick\nSave\nto confirm tag removal.\nNote that it may take some time for the\ntag to be removed\nfrom all the assets it was propagated to.Â\nDid you know?\nYou can also\ncreate playbooks\nto automate the task of removing tags from tagged assets with propagation\nenabled\nor\ndisabled\n. Once the playbook run is completed, tags will be removed from your selected assets.\nTags:\natlan\ndocumentation\nPrevious\nAttach a tag\nNext\nWhat are tags?\nRemove a direct tag\nRemove a propagated tag"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/manage-dbt-tags",
    "content": "Connect data\nETL Tools\ndbt\nManage dbt in Atlan\nManage dbt tags\nOn this page\nManage dbt tags\nDid you know?\nIf you have already set up and crawled dbt, you do\nnot\nneed to make any modifications to your\ndbt Cloud\nor\ndbt Core\nsetup. You only need to configure the\ndbt crawler\nto import dbt tags. Atlan will then import your existing dbt tags automatically for you.\nAtlan imports your\ndbt tags\nand allows you to update your dbt assets with the imported tags.\nOnce you've\ncrawled dbt\n:\nYour dbt assets in Atlan will be automatically enriched with their dbt tags.\nImported dbt tags will be mapped to corresponding\nAtlan tags\nthrough case-insensitive name match   -  multiple dbt tags can be matched to a single tag in Atlan.Â\nYou can also\nattach dbt tags\nto your dbt assets in Atlan   -  allowing you to categorize your assets at a more granular level.\nYou can\nfilter your assets\nby dbt tags.\nImport dbt tags to Atlan\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to import dbt tags. You will also need to work with your dbt Cloud or dbt Core administrator for additional inputs and approval.\nAtlan imports existing dbt tags through one-way tag sync. The imported dbt tags are matched to corresponding tags in Atlan through case-insensitive name match and your dbt assets enriched with the tags synced from dbt.\nTo allow Atlan to import and sync dbt tags, you will need to do the following:\nCreate tags\nor have existing tags in dbt.\nSet up\ndbt Cloud\nor\ndbt Core\nto integrate with Atlan.\nConfigure the dbt crawler\nto import existing tags from dbt to Atlan. For\nImport Tags\n, click\nYes\nto import dbt tags or click\nNo\nto disable it. If you subsequently modify the workflow to disable\ntag import, for any tags already imported, Atlan will preserve those tags.\nOnce the crawler has completed running, tags synced from dbt will be available to use for\ntagging assets\n! ð\nView dbt tags in Atlan\nâ\nOnce you've crawled\ndbt Cloud\nor\ndbt Core\n, you will be able to view and manage your dbt tags in Atlan.\nTo view synced dbt tags:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading of the _Governance cente_r, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter tags by source type. Click\ndbt\nto filter for tags imported from dbt.\nIn the\nOverview\nsection, you can view a total count of synced dbt tags. To the right of\nOverview\n, click\nSynced tags\nto view additional details   -  including tag name, description, total count of linked assets, connection name, and timestamp for last synced.\n(Optional) Click the\nLinked assets\ntab to view linked assets for your dbt tag.\n(Optional) In the top right, click the pencil icon to add a description and change the\ntag icon\n. Tags synced from dbt cannot be renamed.\nYou can now\nattach dbt tags\nto your dbt assets in Atlan! ð\nTags:\nconnectors\ncrawl\nsetup\nPrevious\nCrawl dbt\nNext\nEnrich Atlan through dbt\nImport dbt tags to Atlan\nView dbt tags in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/manage-google-bigquery-tags",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nManage BigQuery in Atlan\nManage Google BigQuery tags\nOn this page\nManage Google BigQuery tags\nAtlan imports your\nGoogle BigQuery tags\nand allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires\nEnterprise edition or higher\n.\nOnce you've\ncrawled Google BigQuery\n:\nYour Google BigQuery assets in Atlan will be automatically enriched with their Google BigQuery tags.\nImported Google BigQuery tags will be mapped to corresponding\nAtlan tags\nthrough case-insensitive name match   -  multiple Google BigQuery tags can be matched to a single tag in Atlan.Â\nYou can also\nattach Google BigQuery tags\n, including tag values and hierarchies, to your Google BigQuery assets in Atlan   -  allowing you to categorize your assets at a more granular level. Atlan supports:\nTags\n-  enrich your Google BigQuery tables, views, and materialized views with tags and tag values.\nPolicy tags\nÂ   -  enrich your Google BigQuery columns with policy tags and tag hierarchies.\nYou can\nfilter your assets\nby Google BigQuery tags.\nAtlan currently does not support crawling\nDataplex tag templates\n.\nPrerequisites\nâ\nBefore you can import tags from Google BigQuery, you will need to do the following:\nCreate tags\nor have existing tags in Google BigQuery.\nGrant permissions\nto import tags from Google BigQuery.\nImport Google BigQuery tags to Atlan\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to import Google BigQuery tags. You will also need to work with your Google BigQuery administrator for additional inputs and approval.\nAtlan imports existing Google BigQuery tags through one-way tag sync. The imported Google BigQuery tags are matched to corresponding tags in Atlan through case-insensitive name match and your Google BigQuery assets enriched with the tags synced from Google BigQuery.\nTo import Google BigQuery tags to Atlan, you can either:\nCreate a new Google BigQuery workflow and\nconfigure the crawler\nto import tags.\nModify the crawler's configuration\nfor an existing Google BigQuery workflow to change\nImport Tags\nto\nYes\n. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags.\nOnce the crawler has completed running, tags synced from Google BigQuery will be available to use for\ntagging assets\n! ð\nView Google BigQuery tags in Atlan\nâ\nOnce you've\ncrawled Google BigQuery\n, you will be able to view and manage your Google BigQuery tags in Atlan.\nTo view synced Google BigQuery tags:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading of the _Governance cente_r, click\nTags\n.\n(Optional) Under\nTags\n, click the funnel icon to filter tags by source type. Click\nBigQuery\nto filter for tags imported from Google BigQuery.\nIn the\nOverview\nsection, you can view a total count of synced Google BigQuery tags. To the right of\nOverview\n, click\nSynced tags\nto view additional details   -  including tag name, type, and values, description, total count of linked assets, connection name, and timestamp for last synced.\n(Optional) Click the\nLinked assets\ntab to view linked assets for your Google BigQuery tag.\n(Optional) In the top right, click the pencil icon to add a description and change the\ntag icon\n. Tags synced from Google BigQuery cannot be renamed.\nYou can now\nattach Google BigQuery tags\nto your Google BigQuery assets in Atlan! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nMine Google BigQuery\nNext\nWhat does Atlan crawl from Google BigQuery?\nPrerequisites\nImport Google BigQuery tags to Atlan\nView Google BigQuery tags in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core",
    "content": "Connect data\nETL Tools\ndbt\nReferences\nWhat does Atlan crawl from dbt Core?\nOn this page\nWhat does Atlan crawl from dbt Core?\nAtlan crawls and maps the following assets and properties from dbt Core. Atlan also supports lineage between the following:\ndbt models\ndbt seeds\ndbt sources\nSQL tables and views materialized by dbt models, dbt seeds, dbt sources\nColumn-level lineage for these entities\nOnce you've\ncrawled dbt\n, you can\nuse dbt-specific filters\nfor quick asset discovery:\nTest status\n-  filter\ndbt tests\nthat passed, failed, or have a warning or error\nAlias\n-  filter by the name of a dbt model's identifier in the dbt project\nUnique id\n-  filter by the unique node identifier of a dbt model\nProject name\n-  filter by dbt project name, only supported for\ndbt Core version 1.6+\nEnvironment name\n-  filter by dbt environment name\nJob status\n-  filter by dbt job status\nLast job run\n-  filter by the last run of the dbt job\nAtlan's dbt crawler also populates custom metadata to further enrich the assets in Atlan. The\nAtlan dbt-specific property\ncolumn in the tables below gives the name of the mapped custom metadata property in Atlan.\nDid you know?\nAtlan lets you\nsync your dbt tags\nand update your dbt assets with the synced tags. You can also\nmap other metadata to Atlan's assets through your dbt models\n.\nTables\nâ\nAtlan maps tables from dbt Core to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndescription\ndescription\nasset profile and overview sidebar\nconfig (alias)\nalias\nasset filter and properties sidebar\nstats (row_count)\nrowCount\nasset profile and filter, overview sidebar\nstats (bytes)\nsizeBytes\nasset filter and overview sidebar\nstats (last_modified)\nsourceUpdatedAt\nasset profile and properties sidebar\nproject (name)\nassetDbtProjectName\nasset filter and overview sidebar\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\nraw_sql\nor\nraw_code\ndbtRawSQL\noverview sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\npackageName\nassetDbtPackageName\nasset filter and properties sidebar\nalias\nassetDbtAlias\nasset filter and properties sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\nsourceCreatedAt\nasset profile, overview and properties sidebar\ncompiled_sql\nor\ncompiled_code\ndbtCompiledSQL\noverview sidebar\nfreshness_data (criteria)\nassetDbtSourceFreshnessCriteria\noverview sidebar\nColumns\nâ\nAtlan maps columns from dbt Core to its\nColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ndescription\ndescription\nasset profile and overview sidebar\nmeta\nassetDbtMeta\nAPI only\ntags\nassetDbtTags\nasset filter and overview sidebar\npackageName\nassetDbtPackageName\nasset filter and properties sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated_at\nsourceCreatedAt\nasset profile, overview and properties sidebar\nModels\nâ\nAtlan maps models from dbt Core to its\nModel\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nexecuteCompletedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nowner\nsourceCreatedBy\nasset profile and properties sidebar\nstatus\ndbtJobRuns.dbtModelRunStatus\noverview sidebar\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\nraw_sql\nor\nraw_code\ndbtRawSQL\noverview sidebar\ncompiled_sql\nor\ncompiled_code\ndbtCompiledSQL\noverview sidebar\nstats\ndbtStats\nAPI only\nconfig.materialized\ndbtMaterializationType\nAPI only\nSources\nâ\nAtlan maps sources from dbt Core to its\nDbtSource\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nowner\nsourceCreatedBy\nasset profile and properties sidebar\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\ncriteria\nassetDbtSourceFreshnessCriteria\noverview sidebar\nstats\ndbtStats\nAPI only\nstate\ndbtState\nAPI only\nTests\nâ\nwarning\nFor dbt Core, upload the\nrun_results.json\nfile to crawl dbt tests. It's recommended to place the file in the same folder as the\nmanifest.json\nfile.\nAtlan maps tests from dbt Core to its\nTest\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nname\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\ntags\nassetDbtTags\nasset filter and overview sidebar\nstatus\ndbtTestStatus\nasset profile\nstate\ndbtTestState\nAPI only\nerror\ndbtTestError\nasset profile and overview sidebar\nraw_code\ndbtTestRawCode\noverview sidebar\nraw_sql\ndbtTestRawSQL\noverview sidebar\ncompiled_code\ndbtTestCompiledCode\nAPI only\ncompiled_sql\ndbtTestCompiledSQL\nAPI only\nlanguage\ndbtTestLanguage\nasset profile and overview sidebar\nSeeds\nâ\nAtlan maps models from dbt Core to its\nSeed\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\nexecuteCompletedAt\nsourceUpdatedAt\nasset profile and properties sidebar\nowner\nsourceCreatedBy\nasset profile and properties sidebar\nstatus\ndbtJobRuns.dbtModelRunStatus\noverview sidebar\nalias\nassetDbtAlias\nasset filter and properties sidebar\nmeta\nassetDbtMeta\nAPI only\nuniqueId\nassetDbtUniqueId\nasset filter and overview sidebar\nstats\ndbtSeedStats\nAPI only\nfilePath\ndbtSeedfilePath\nasset profile and overview sidebar\nPrevious\nWhat does Atlan crawl from dbt Cloud?\nNext\nConnection issues\nTables\nColumns\nModels\nSources\nTests\nSeeds"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/update-column-metadata-in-google-sheets",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nHow to update column metadata in Google Sheets\nOn this page\nupdate column metadata in Google Sheets\nOnce you've\nconnected Atlan with Google Sheets\n, you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.\nAtlan currently supports importing and updating column metadata for the following asset types:\nTables\nViews\nMaterialized views\nLooker explores\nMicrosoft Power BI tables\nSalesforce objects\nTableau data sources\ndanger\nYou need to be logged into your Atlan instance before you can start importing column metadata for your Atlan assets in Google Sheets.\nImport column metadata\nâ\nYou can import column metadata for your data assets directly into Google Sheets.\nTo import column metadata for your data assets into Google Sheets:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nFrom the dropdown menu, click\nAtlan\n.Â\nFrom the list of options in the Atlan add-on, click\nEnrich metadata\nto view a list of your data assets in a sidebar.\n(Optional) To filter your assets by a specific asset type, in the Atlan sidebar, click the asset type   -  for example,\nTableau Datasources\n.\nIn the Atlan sidebar on your spreadsheet, select the data asset(s) you want to import.\nClick\nImport\nto import column metadata for your selected assets.\nThe column metadata for your selected assets are now available in Google Sheets! ð\nDid you know?\nIf any changes are made to your imported columns in Atlan, you'll need to import those columns once again to access the updated version in Google Sheets.\nUpdate column metadata\nâ\nOnce you've imported your data assets from Atlan, you can edit the metadata for your selected data asset in Google Sheets. You can make changes to the column metadata once all the columns have been successfully imported.\nYou can only make changes to the metadata in the following columns:\nDescription\nCertification Status\nCertification Message\nAnnouncement Type\nAnnouncement Title\nAnnouncement Message\nTags\nYou\ncannot\nmake the following changes:\nEdit headers for any of the columns\nEdit the metadata in the\nColumn Name\n,\nData Type\n,\nPropagated Tags\n, and\nQualified Name\ncolumns\nDelete any columns or rows\nAny of these changes will not be pushed to Atlan and you'll receive an error message.\nBulk update tags for columns\nâ\ndanger\nYou cannot make any changes to the metadata in the\nPropagated Tags\ncolumn.\nNavigate to the\nTags\ncolumn to add tags to your column assets in Google Sheets:\nWhen adding tags to columns:\nThe tag must already exist in Atlan. If the tag does not exist in Atlan, updates will not sync and you will receive an error message.\nTag match is case-sensitive, ensure that the tag is formatted exactly as it exists in Atlan. For example, if the tag in Atlan is formatted as\nMarketing Analysis\n, then columns tagged with\nmarketing analysis\nwill not sync.\nYou can add multiple tags in the\nTags\ncolumn   -  separate multiple tags with a comma\n,\n. If you are in a region that uses a separator other than a comma, you will need to\nmodify your spreadsheetâs settings\nto use commas as separators.\nIf you have added tags that exist in Atlan as well as ones that do not,\nonly\nthe existing tags will be synced. The unsupported tags will not sync and you will receive an error message.\nTag propagation\nis disabled by default in Atlan, hence tags will not be propagated.\nPush your changes to Atlan\nâ\nOnce you've made changes to the column metadata, complete these steps to push your changes:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nFrom the dropdown menu, click\nAtlan\nand then click\nPush to Atlan\n.\nA dialog box will appear once the changes have synced.\n(Optional) Click\nOpen in Atlan\nto verify the changes.\nIn Atlan, an\nUpdated using Google Sheets\nstamp will appear in the\nactivity log\nfor updated assets. (Optional) Click the\nGoogle Sheets\nlink to view the source spreadsheet from Atlan.\ndanger\nIf you do not have the\npermission\nto update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Google Sheets. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access.\nView asset profiles in Google Sheets\nâ\nOnce you've imported your data assets into Google Sheets, you can also view their\nasset profiles\n. Complete these steps:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nIn the dropdown menu, click\nAtlan\n.\nNext, click\nOpen asset in sidebar\nto view the asset profile on your Google spreadsheet.\nYou can also update components of your asset profile directly in Google Sheets. Your changes will sync automatically to Atlan.\nDid you know?\nYou can\ndownload impacted assets\nfor impact analysis in Google Sheets.\nTags:\nconnectors\ndata\nintegration\ncrawl\nPrevious\nLink your account\nNext\nHow to update column metadata in Microsoft Excel\nImport column metadata\nUpdate column metadata\nPush your changes to Atlan\nView asset profiles in Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata",
    "content": "Build governance\nCustom Metadata\nOn this page\nCustom Metadata\nOverview:\nCreate and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information. Define custom fields, relationships, and validation rules to capture additional context about your data assets.\nGet started\nâ\nFollow these steps to implement custom metadata in Atlan:\nAdd options\nGuides\nâ\nControl access to metadata and data\nDisable data access\nAdd custom metadata badges\nManage custom metadata structures\nReferences\nâ\nWhat happens when users do not have access to metadata\nConcepts\nâ\nWhat is custom metadata\nFAQ\nâ\nCan I update input type for existing custom metadata\nTags:\ncustom metadata\nattributes\ndata catalog\ngovernance\natlan\nNext\nAdd options\nGet started\nGuides\nReferences\nConcepts\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/add-options",
    "content": "Build governance\nCustom Metadata\nGet Started\nAdd options\nOn this page\nAdd options\nWho can do this?\nYou must be an admin user in Atlan to create options for custom metadata properties.\nOptions in Atlan stand for enumerations or enumerated data types. Options allow you to create your own set of predefined and related values. Once you've created your options, you can add them to your\ncustom metadata properties\nto ensure consistency of usage across the organization.\nExample\nâ\nImagine that you would like to denote values for the data quality level of your metadata in Atlan. To solve for this, you could create an option\nData quality\nand define three indicative values:\nBronze\n-  for freshly crawled metadata\nSilver\n-  for asset enrichment in progress\nGold\nÂ   -  for well-documented assets\nOnce you've created the option, you can add it as a\ncustom metadata property\n. Then you can enrich your assets with this additional context for your data teams.Â\nCreate options\nâ\nTo create an option:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nOptions\n.\nUnder the\nOptions\nheading, click\nGet started\n.\nIn the\nNew option\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for your option   -  for example,\nData quality\n.\nFor\nValues\n, enter a list of values considered valid, separate each value with a semicolon\n;\n-\nGold\n,\nSilver\n, and\nBronze\n.\nClick\nCreate\nto add your option.\nYou have just created an option! ð\nTo edit the values for your option, click on the pencil icon in the top right to make your changes and then save them.\nDid you know?\nAtlan currently only supports\ndeleting options through API\n.\nTags:\ndata\ncrawl\nPrevious\nCustom Metadata\nNext\nControl access to metadata and data?\nExample\nCreate options"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/disable-data-access",
    "content": "Build governance\nCustom Metadata\nAccess Management\nDisable data access\nOn this page\nDisable data access\nWho can do this?\nYou will need to be an admin user in Atlan to configure these options.\nWhat if you want to block access to data for your users, and only allow them to access metadata?\nThere are different ways to do this in Atlan. From the most wide-reaching to the most granular:\nBlock all querying\nâ\nTo stop all users from querying data, across all data assets:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nToggle off the\nInsights\noption. (This should also deactivate all sub-options of Insights.)\ndanger\nUsers will still be able to preview sample data, even with Insights turned off.\nBlock by source\nâ\nWhen setting up a crawler\nâ\nTo stop all users from accessing data for a source, when setting up the crawler:\nSet\nAllow SQL Query\nto\nNo\nto stop users from querying any data in the source.\nSet\nAllow Data Preview\nto\nNo\nto stop users from previewing any data in the source.\nSo to block\nall\naccess to data for that source, set both options to\nNo\n.\nWhen setting up a connection's credentials\nâ\nYou can configure the credentials for some data sources without data access permissions.\nIf the credentials cannot query data, Atlan will not be able to query or preview data.\nIf Atlan cannot query or preview data, no users in Atlan can query or preview data either.\ndanger\nThis depends on the connector   -  some connectors need a level of data access even to crawl metadata. The specific set up guide for each connector gives you the minimal set of permissions.\nBlock by asset\nâ\nWho can do this?\nIn addition to being an Admin user, you will need to be a connection admin for the source containing the assets.\nTo stop users from querying or previewing data for specific assets:\nDefine a\npersona\nwith a\ndata policy\nthat denies access to those assets.\nAdd the users to that persona.\ndanger\nTo ensure Atlan blocks data access for all users (including connection admins) the data policy must\nexplicitly deny\nquery access. A lack of data policy (implicit deny) will not prevent connection admins from querying and previewing data.\nBlock by tag\nâ\nTo stop users from querying or previewing data that has a particular\ntag\n:\nDefine a\npurpose\non that tag.\nWithin the purpose, define a\ndata policy\nthat denies query access for those users. (This will also apply to data previews.)\nEven if only a single column has the tag, Atlan will block querying and previewing of the entire asset.\nTags:\ndata\ncrawl\nPrevious\nControl access to metadata and data?\nNext\nAdd custom metadata badges\nBlock all querying\nBlock by source\nBlock by asset\nBlock by tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/add-custom-metadata-badges",
    "content": "Build governance\nCustom Metadata\nBadge Management\nAdd custom metadata badges\nOn this page\nAdd custom metadata badges\nWho can do this?\nYou must be an admin user to be able to add badges for\ncustom metadata\n.\nBringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges.\nAdmin users can use badges to highlight custom metadata right in the asset overview, ensuring greater visibility. Custom metadata badges can help users quickly get the context they need for their data assets, for example:\nIf an Airflow DAG was successful\nIf a table's data quality checks failed\nAtlan currently supports creating badges for custom metadata properties with the following input types   -  text, number, options, and boolean. Note that Atlan currently does not support creating badges for multivalued types. If\nAllow multiple values\nis toggled on\n, you will not be able to create a badge for that custom metadata property.\nCreate custom metadata badges\nâ\ndanger\nBefore creating a new badge, you'll need to have created at least one\ncustom metadata structure\n.\nTo create a custom metadata badge:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nBadges\n.\nUnder the\nBadges\nheading, click\n+ Create new\n.\nIn the\nCreate new badge\npopover, add the following details:\nFor\nCustom Metadata property\n, select the property you want to create a badge for   -  in this example, we'll select\nLast Run Status\nfor\nAirflow ETL Details\nas the property.Â\nFor\nName\n, add a name to your badge, such as\nAirflow Run Status\n.\n(Optional) For\nDescription\n, add a description.\nClick\nCreate\nto create your badge.\nTo add options to your custom metadata badge, click\n+New option\n.\nTo define the options for your custom metadata badge, add the following details:\nTo set a matching condition for your values, select\nEquals (=)\nor\nNot Equals (!=)\n.\nFor\nEnter value\n, enter a value   -  such as,\nSuccessful\n.\nClick the\ngrey box\nto choose a color for your badge.\n(Optional) Click the\neye icon\nto preview the badge.Â\n(Optional) Click\n+New option\nto add more options to your badge.Â\nClick\nSave\nto save your badge options.\nYour custom metadata badge is now live! ð\nAdd custom metadata badges to assets\nâ\nOnce you've created your custom metadata badges, you can add them to your assets.Â\nDid you know?\nIf you\nedit the custom metadata properties\nof an asset, the badges will appear automatically.\nTo add a custom metadata badge to an asset:\nFrom the left menu of any screen, click\nAssets\n.Â\nOn the\nAssets\npage, select an asset to add a badge.\nIn the asset sidebar on the right, click the custom metadata tab   -  in this example, we'll select\nAirflow ETL Details\n.\nIn the custom metadata panel in the sidebar, click\nStart editing\n.\nEnter the value(s) you want for the custom metadata properties on the asset and click\nUpdate\n.\nThe custom metadata badge will now be displayed in the asset profile and sidebar! ð\nTags:\natlan\ndocumentation\nPrevious\nDisable data access\nNext\nManage custom metadata structures\nCreate custom metadata badges\nAdd custom metadata badges to assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/manage-custom-metadata-structures",
    "content": "Build governance\nCustom Metadata\nStructure Management\nManage custom metadata structures\nOn this page\nManage custom metadata structures\nWho can do this?\nYou must be an admin user to manage custom metadata structures, including defining new ones.\nBefore users or integrations can enrich assets with custom metadata, you must first define its structure.\nCreate custom metadata structure\nâ\nTo create a new custom metadata structure:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nCustom Metadata\n.\nUnder the\nStart adding custom metadata\nheading, click the\n+\nGet started\nto add a new structure:\nFor\nName\nenter a name for the custom metadata structure. (In\nour examples\n, this would be\nIPR\nor\nETL\n.)\n(Optional) To personalize your custom metadata, to the left of the name, click the image icon. From the upper right of the corresponding dialog:\nClick\nIcons\nto add an icon to your custom metadata. Click the gray box to change the color of the icon to green, yellow, or red.\nClick\nEmoji\nto add an emoji to your custom metadata.\nClick\nUpload Image\nto upload an image for your custom metadata. The recommended size for image uploads is 24x24 pixels.\n(Optional) Add a description of the custom metadata below these.\nAt the bottom right of the dialog, click the\nCreate\nbutton.\nCreate properties in the structure\nâ\nTo create custom metadata properties within a custom metadata structure:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nCustom Metadata\n.\nUnder the\nCustom Metadata\nheading, select the custom metadata structure you want to change.\nClick the\nNew property\nbutton (no properties yet) or\nAdd property\nbutton (to add more properties):\nFor\nName\n, enter a name for one property. (In\nour examples\n, this would be one of\nLicense type\n,\nProvider\n,\nJob link\n, and so on.)\nFor\nType\n, select the type of value you expect users to use for this property:\nThe\nText\ntype allows free-form text values.\nThe\nInteger\ntype allows only whole numbers (no decimals).\nThe\nDecimal\ntype allows fractional numbers (those with decimal points).\nThe\nBoolean\ntype allows only a\nYes\nor a\nNo\nvalue.\nThe\nDate\ntype allows both date and time values in the following format   -  day, month, year, hours, minutes, and seconds.\nThe\nOptions\ntype allows you to\ndefine your own set of predefined options\nfor values that are valid.\nThe\nUsers\ntype allows only existing Atlan users as values.\nThe\nGroups\ntype allows only existing Atlan groups as values.\nThe\nURL\ntype allows only web links.\nThe\nSQL\ntype allows only SQL code.\n(Optional) For\nDescription\n, enter an explanation for how you expect users to use this property.\nIf you chose\nOptions\nas the type, either:\nUnder\nSelect Options\n, select an existing set of options to reuse.\nClick the\nCreate New\nlink to create a new set of options.\nUnder\nOption name\n, give the options a name.\nUnder\nValues\n, enter the list of values considered valid (separated by\n;\n).\n(Optional) Under\nAssets\n, you can configure the connections and asset types on which this custom metadata should be visible to:\nFor\nConnections\n, select the\nconnection\nto which you want to limit users to be able to enrich assets with this property. For example, you may want a property to only apply to a specific Snowflake connection.\nFor\nApplicable asset types\n, select the kinds of assets you want users to be able to enrich with this property. For example, you may want a property to only apply to SQL assets like tables and views, and not to BI assets.\nÂ (Optional) Under\nGlossary assets\n, you can configure the glossaries and glossary asset types on which this custom metadata should be visible to:\nFor\nGlossaries\n, select the\nglossaries\nto which you want to limit users to be able to enrich assets with this property.\nFor\nApplicable asset types\n, select the\nglossary assets\nyou want users to be able to enrich with this property. For example, you may want a property to only apply to terms within a glossary, and not to categories.\n(Optional) Under\nDomain assets\n, you can configure the data domains, subdomains, and products on which this custom metadata should be visible to:\nFor\nDomains\n, select the\ndomains or subdomains\nto which you want to limit users to be able to enrich with this property.\nFor\nApplicable asset types\n, select the\ndomains, subdomains\n, or\nproducts\nyou want users to be able to enrich with this property. For example, you may want a property to only apply to products within a specific subdomain, and not to the parent domain.\n(Optional) Under\nOther assets\n, for\nApplicable asset types\n, select assets that neither fall under the rubric of a connection or glossary   -  currently only\nfile assets\nare supported.\n(Optional) Under\nConfigurations\ntoggle any extra settings for the property:\nAllow multiple values\ncontrols whether users can enter more than a single value for this property. (Note: this is only available for some types.)\nShow in filter\ncontrols whether users can filter on this property when doing asset discovery.\nShow in overview\ncontrols whether the property will show up in the\nOverview\nsidebar tab of assets. (All properties will show in the custom metadata's own tab, but those with this\nShow in overview\nenabled will also show in the\nOverview\ntab.)\nThat's it, your users can now\nenrich assets with this custom metadata\n! ð\nDelete properties from a structure\nâ\ndanger\nDeleting a custom metadata property will remove the values for that property from any assets.\nTo delete custom metadata properties from a custom metadata structure:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nCustom Metadata\n.\nUnder the\nCustom Metadata\nheading, select the custom metadata structure you want to change.\nIn the properties table on the right, click the delete icon on the far right of the row containing the property to delete the property.\nWhen prompted for confirmation, click the\nConfirm\nbutton.\nDelete custom metadata structure\nâ\nYou can also delete an entire custom metadata structure.\ndanger\nDeleting a custom metadata structure will remove all its properties and all its custom metadata values from any assets. You might want to consider\nusing personas to hide the custom metadata\n, until you confirm it is no longer needed.\nTo delete a custom metadata structure:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nCustom Metadata\n.\nUnder the\nCustom Metadata\nheading, select the custom metadata structure you want to delete.\nIn the upper right of the custom metadata structure, click the red delete icon.\nWhen prompted for confirmation, click the\nDelete\nbutton.\nView linked assets\nâ\nOnce users in your organization have\nenriched their assets with custom metadata\n, you will be able to view the linked assets right from the governance center.\nTo view assets with custom metadata:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nMetadata\nheading, click\nCustom Metadata\n.\nUnder the\nCustom Metadata\nheading, click\nLinked Assets\nto view all the assets linked to the custom metadata.\n(Optional) Click any asset to open the asset sidebar for more details.\nTags:\ndata\nintegration\nPrevious\nAdd custom metadata badges\nNext\nWhat happens when users do not have access to metadata?\nCreate custom metadata structure\nCreate properties in the structure\nDelete properties from a structure\nDelete custom metadata structure\nView linked assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/references/what-happens-when-users-do-not-have-access-to-metadata",
    "content": "Build governance\nCustom Metadata\nReferences\nWhat happens when users do not have access to metadata?\nOn this page\nWhat happens when users do not have access to metadata?\nUsers can search and discover all assets in Atlan. However, if they have not been granted permissions to act on those assets, their access will be limited. Atlan shows this limited access with a lock icon.\nThe limitations are tied to:\nWhether or not the user is a\nconnection admin\nAccess policies\nThe limitations are in terms of the actions that you can and cannot perform. A combination of the above two factors will usually determine these limitations.\nConnection admin\nâ\nConnection admins manage connectivity to a data source. Even if you are a\nmember\nuser, as a connection admin you'll have full access to the assets from that connection.Â\nAny user with connection admin status will not see the lock icon for their assets in Atlan. However, there are exceptions   -  an access policy can override a connection admin's default full access.\nSetting access policies helps you maintain granular control over your assets in Atlan. You can define these access policies by\npersonas\nand\npurposes\n.\nAccess policies\nâ\nAccess policies often supersede the default permissions associated with connection admins and user roles. Access policies either allow or restrict access to certain assets.\nFor example, even as a\nmember\nuser, you'll be able to add\ntags\nand\nterms\nto an asset if you're part of a persona with a metadata policy that allows this action. Guest users in Atlan can only suggest changes to asset metadata if\nenabled from the admin center\n.\nIn fact, access policies can also be used to give users full access to certain assets without making them connection admins.\nUser roles\nâ\nAlthough there are\ndefault permissions\nassociated with each user role (\nadmin\n,\nmember\n, and\nguest\n), access to assets is entirely dependent on whether the user is a connection admin or part of a persona or purpose.\nFor example, a\nmember\nuser who is neither a connection admin nor part of any persona or purpose will see every single asset in Atlan with a lock icon.\nTags:\natlan\ndocumentation\nPrevious\nManage custom metadata structures\nNext\nWhat is custom metadata?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/concepts/what-is-custom-metadata",
    "content": "Build governance\nCustom Metadata\nConcepts\nWhat is custom metadata?\nOn this page\nCustom Metadata\nAtlan provides basic metadata for assets like certification, owners, and descriptions. But since every data team has their own unique needs, Atlan allows you to extend these with your own unique properties.\nCustom metadata helps users understand and use data through extra context on assets in Atlan.\nCommon, standardized context helps people quickly understand data and its background, especially your organization's business users. With custom metadata, finding and working with data has never been easier. Users can filter data and understand its context through your organization's own perspective!\nExamples\nâ\nFor example, you could add\nIPR\nas a new metadata group to capture intellectual property rights. This could include custom metadata fields like:\nLicense type\n-  to define the type of license under which the asset can be used.\nProvider\n-  to define the source of the asset, in cases where the license requires attribution.\nAs another example, you might want to integrate details from a custom-built ETL processing engine. You could create an\nETL\nmetadata group, and include fields like:\nJob link\n-  to provide a link to the job run in your ETL engine.\nLast run date\n-  to show when the job that created or changed this asset last ran.\nLast run status\n-  to show whether the last run of the job was successful or not.\nHighlights of adding custom metadata\nâ\nWith custom metadata, you can:\nOrganize custom fields in groups defined by you.\nEnsure accuracy of your metadata values by setting restrictions on values. For example, you can define the type of value (date, boolean), predefined options for values, and so on.\nMaintain standard metadata by telling the users exactly what metadata values to add for each data asset. No confusion means faster data enrichment!\nDid you know?\nCustom metadata in Atlan lets you add whatever metadata fields you need. This helps the team keep data clean with perfectly customized metadata provisions.\nTags:\natlan\ndocumentation\nPrevious\nWhat happens when users do not have access to metadata?\nNext\nUpdate input type for existing custom metadata\nExamples\nHighlights of adding custom metadata"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/faq/change-input-type-custom-metadata",
    "content": "Build governance\nCustom Metadata\nFAQ\nUpdate input type for existing custom metadata\nUpdate input type for existing custom metadata\nUpdating input type from SQL to text for existing\ncustom metadata\nis currently not supported in Atlan.\nTags:\natlan\ndocumentation\nPrevious\nWhat is custom metadata?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products",
    "content": "Configure Atlan\nData Products\nOn this page\nData Products\nâ\nAvailable via the Data Marketplace package\nOverview:\nUse Atlan's data products capabilities to organize and govern your data assets by domain. Create curated data products that help your teams discover, understand, and collaborate on data more effectively.\nGet started\nâ\nHow to create data domains\nGuides\nâ\nDomain Management\nâ\nHow to create domain policies\n: Define access and governance policies.\nHow to monitor data domains\n: Track domain usage and performance.\nProduct Management\nâ\nHow to create data products\n: Create and manage your data products.\nHow to add stakeholders\n: Assign roles and responsibilities.\nConcepts\nâ\nWhat are data products\n: Learn about data products and their components.\nWhat is a product score\n: Understand how product scores are calculated.\nWhat is business lineage\n: Learn about business lineage in data products.\nTags:\ndata-products\ndata-domains\ngovernance\ncapabilities\nNext\nCreate data domains\nGet started\nGuides\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-data-domains",
    "content": "Configure Atlan\nData Products\nGet Started\nCreate data domains\nOn this page\nCreate data domains\nWho can do this?\nBefore you can create a data domain, you will need your Atlan admin to\nenable the products module in your Atlan workspace\n. Once enabled, first review the\norder of operations\n. You will need to be a\ndomain owner or domain admin in Atlan\nor have access through\ndomain policies\nto create and manage data domains. If you do not wish to enable the products module but still want to create domains in Atlan, refer to\nHow to manage domains\ninstead.\nData domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization.\nData domains in Atlan take true meaning from:\nKey stakeholders of a specific domain and their roles\nData products within that specific domain\nCreate a data domain\nâ\nTo create a data domain:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nOn the\nProducts\npage, click\nGet started\n.\nFor\nOverview\n, enter the basic details for your domain:\n(Optional) For\nCover\n, click the\nChange\nbutton to select an image from the gallery or upload an image of your own. Click\nReposition\nto drag and reposition the cover image and then click\nSave position\nto save your preferences.\n(Optional) For\nTheme\n, choose from the available color options to add a theme to your domain.\nFor\nName\n, enter a meaningful name for your data domain   -  for example,\nCustomer Service\n. The character limit for a domain name is 80 characters.\n(Optional) Click the domain icon to change the icon for your domain.\n(Optional) For\nDescription\n, enter a description for your domain.\nFor\nOwners\n, assign additional users or groups as domain owner.\nIn the top right of the screen, click the\nCreate\nbutton to complete setup.\nCongrats on creating your data domain in Atlan! ð\n(Optional) Create a data subdomain\nâ\ndanger\nYou will first need to create a data domain before you can add a data subdomain to it.\nData subdomains help you logically segment your data domains according to business needs.\nTo create a data subdomain:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a domain or subdomain, you can either:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain or subdomain.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nView all\nbutton to view more domains.\nIn the upper right of your data domain page, click the\n+ Add button\nand then click\nNew sub-domain\nto add a data subdomain.\nFor\nOverview\n, enter the basic details for your subdomain:\n(Optional) For\nCover\n, click the\nChange\nbutton to select an image from the gallery or upload an image of your own. Click\nReposition\nto drag and reposition the cover image and then click\nSave position\nto save your preferences.\n(Optional) For\nTheme\n, choose from the available color options to add a theme to your subdomain.\nFor\nName\n, enter a meaningful name for your subdomain   -  for example,\nSocial Media\n. The character limit for a subdomain name is 80 characters.\n(Optional) Click the domain icon to change the icon for your subdomain.\n(Optional) For\nDescription\n, enter a description for your subdomain.\nFor\nOwners\n, assign additional users or groups as subdomain owner.\nIn the top right of the screen, click the\nCreate\nbutton to complete setup.\nCongrats on creating your data subdomain in Atlan! ð\nYour data producers can now\nadd data products\nto your data domain.\nUpdate a data domain\nâ\nThe domain profile includes essential details about the data domain. You can also curate what your domain users will be able to view.\nTo update a data domain:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a domain or subdomain, you can either:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain or subdomain.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nView all\nbutton to view more domains.\nOn your data domain page, the\nOverview\ntab displays important details about the domain. (Optional) From the top right, click the\n+ Add\nbutton and then:\nClick\nNew sub-domain\nto add data subdomains.\nClick\nNew product\nto add data products.\ndanger\nEven as a domain owner or admin, you will need to have\ncreate, update, and delete permissions through domain policies\nfor a specific domain or subdomain to create and manage data products.\nUnder\nSummary\n, view a total count of data products in your domain and domain description:\n(Optional) Click\n+ Add stakeholder\nto\nadd stakeholders\n.\n(Optional) Click the\nDescription\nfield to update the description.\n(Optional) For\nOwners\n, click the pencil icon to add or remove\nowners\n.\n(Optional) If\ncustom metadata properties\nare available, you can add\ncustom metadata\nto your domain.\n(Optional) Click\n+ Add resource\nto\nadd a resource\nto your domain.\nUnder\nReadme\n, click\n+ Add\nto\nadd a README\nto your data domain or\nuse Atlan AI for documentation\n.\nÂ From the top right of the data domain profile:\nClick the user avatars to view a list of recently visited users, total views on your domain, total number of unique visitors, and total views by user.\nUse the days filter to filter domain views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your domain\nand bookmark it for easy access.\nClick the\nSlack\nor\nTeams\nicon to post on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the 3-dot icon to\nadd an announcement\nor a\nresource\nto your domain.\nSwitch to the\nProducts\ntab to view\ndata products\nwithin your domain.\nSwitch to the\nStatistics\ntab to\nmonitor domain usage\n.\nSwitch to the\nLineage\ntab to view\nbusiness lineage for your domain\n.\nMove a subdomain or product\nâ\nYou can move subdomains and products within and across domains to better organize your business entity. Move data products to a different subdomain or domain, or create subdomains within the same domain or across your domains in Atlan.\nYou will need the following permissions:\nMoving a subdomain or product from one domain to another   -\nUpdate Domains\npermission\non both the source and target domains.\nMoving a subdomain or product within the same domain   -\nUpdate Domains\npermission\non the domain you want to reorganize.\nTo move an existing subdomain or product:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nIn the left menu of the\nProducts\npage, you can either:\nDrag and drop a subdomain or product into the relevant domain within the same or a different parent domain. In the popup, click\nMove\nto confirm the changes.\nTo the right of the subdomain or product name, click the three dots icon and then click\nMove to\n. In the\nMove to\ndialog, select a relevant parent domain within the same or a different domain and then click\nMove\nto confirm the changes.\nConvert a subdomain to a domain\nâ\nYou can convert subdomains into parent domains. For example, as your organization grows, some small teams may evolve into major departments. In that case, you may want your subdomains in Atlan to better reflect your organizational architecture and convert them to domains.\nYou will need the\nUpdate Domains\npermission\non the subdomain and parent domain of the subdomain you want to convert.\nTo convert a subdomain into a domain:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nIn the left menu of the\nProducts\npage, to the right of the subdomain you want to convert, click the three dots icon and then click\nConvert to domain\n.\nIn the\nConvert to domain\ndialog, click\nConvert to domain\nto confirm your changes.\nArchive a data domain\nâ\nYou can archive your data domains and subdomains when they are no longer in use. Note that Atlan does not allow archiving a domain that contains any active subdomains or products. Ensure that your domain content is inactive before you proceed.\nTo archive a data domain:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a domain or subdomain to archive, you can either:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain or subdomain.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nView all\nbutton to view more domains.\nFrom the top right of the domain or subdomain profile, click the 3-dot icon and then click\nArchive\n.\nFrom the\nArchive domain?\ndialog, click\nArchive\nto archive your data domain.\nDid you know?\nTo programmatically create, update, and manage data domains using API, refer to our\ndeveloper documentation\n.\nTags:\natlan\ndocumentation\nPrevious\nData Products\nNext\nAdd stakeholders\nCreate a data domain\n(Optional) Create a data subdomain\nUpdate a data domain\nMove a subdomain or product\nConvert a subdomain to a domain\nArchive a data domain"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/add-stakeholders",
    "content": "Configure Atlan\nData Products\nStakeholder Management\nAdd stakeholders\nOn this page\nAdd stakeholders\nWho can do this?\nBefore you can create a stakeholder, you will need your Atlan admin to\nenable the products module in your Atlan workspace\n. Once enabled, first review the\norder of operations\n. You will need to have update permissions through\ndomain policies\nfor the specific domain(s) or subdomain(s) to create and manage stakeholders.\nStakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams.\nFor example, for the data domain\nCustomer Service\n, you may want to define a\nCustomer Service Manager\nstakeholder and assign it to the people who serve that function. This way, your customer support team will know whom to contact for questions or escalate any issues.\nOnce you have created stakeholders or want to use the default options from Atlan:\nYou can add stakeholder information for any and all domains. This will help you provide additional metadata for your domain, but does not help enforce access control currently. For the latter, you will need to\ncreate domain policies\n.\nYou can assign any user as a stakeholder within a specific domain.\nEach user can be mapped to only one responsibility within a domain. However, users can have multiple stakeholder responsibilities across multiple domains.\nCreate a stakeholder\nâ\nAtlan provides you with predefined stakeholders for your data domains. You can either use the default options and assign them to your users or create new definitions for stakeholders based on your business needs.\nTo create a stakeholder:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, you can either:\nIf you have\nenabled the products module\n, click\nDomains & products\n.\nIf you have not enabled the products module,Â click\nDomains\n.\nOn the\nDomains & products\nor\nDomains\npage, change to the\nSettings\ntab.\nUnder\nStakeholders\n, you can view the default stakeholders   -  domain owner, data engineer, data product owner, and data architect.\nTo the right of each responsibility, click the right-facing arrow to view the domains it applies to and users assigned to that responsibility. You can neither delete nor make any changes to the default options.\nTo create a new stakeholder, click\nAdd\n.\nIn the\nAdd responsibility\ndialog, enter the following:\nFor\nName\n, enter a meaningful name for your stakeholder. Atlan recommends following a naming convention for responsibilities across your domains to help users find stakeholders more easily.\nFor\nApplies to\n, click the dropdown to select domains that this responsibility should apply to:\nTo select\nAll domains\n, you must have update permissions on all data domains in Atlan.\nTo select any one specific domain or multiple domains, you must have update permissions for the specific data domain(s).\nAdd a stakeholder\nâ\nYou can add stakeholders directly from the domain or subdomain profile.\nTo add a stakeholder to your data domain or subdomain:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a domain or subdomain, you can either:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain or subdomain.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nView all\nbutton to view more domains.\nFrom the domain or subdomain profile, under\nStakeholders\n, click\n+ Add stakeholder\n.\nIn the right\nStakeholders\npane, click\n+ Stakeholders\n.\nIn the\nAdd Stakeholders\ndialog, configure the following:\nFor\nUsers or groups\n, select the users or groups that the stakeholder responsibility should apply to.\nClick the stakeholder dropdown to select a default stakeholder, a custom one you created, or click\nCreate new\nto create a new one.\nClick\nAdd\nto add the stakeholder to the domain or subdomain.\nTags:\natlan\ndocumentation\nPrevious\nCreate data domains\nNext\nCreate data products\nCreate a stakeholder\nAdd a stakeholder"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-data-products",
    "content": "Configure Atlan\nData Products\nProduct Management\nCreate data products\nOn this page\nCreate data products\nâ\nAvailable via the Data Marketplace package\nWho can do this?\nBefore you can create a data product, you need your Atlan admin to\nenable the products module in your Atlan workspace\n. Once enabled, first review the\norder of operations\n. You need to have create, update, and delete permissions through\ndomain policies\nfor the specific domain or subdomain to create and manage data products.\nData products help your data consumers easily discover and work with data assets. As you get started, here are some questions to consider:\nWhat use case(s) are you trying to solve as an organization?\nHow to define a common vocabulary and approach for creating data products and ensuring interoperability across domains?\nHow to design data products to power discovery and drive usage among data consumers?\nData products in Atlan can be highly adaptable to the needs of your organization.\nCreate a data product\nâ\nYou can either create a data product from the products module or lineage graph.\nTo create a data product, complete these steps.\nFrom the products module:\nâ\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a domain or subdomain, you can either:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain or subdomain.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nSee all\nbutton to view more domains.\nIn the upper right of your data domain or subdomain page, click the\n+ Add\nbutton, and then from the dropdown, click\nNew product\nto add a new data product.\nFrom the lineage graph:\nâ\nFrom the left menu of any screen in Atlan, click\nAssets\n.\n(Optional) In the\nFilters\nmenu\non the left, expand the\nProperties\nmenu and then click\nHas lineage\nto filter for assets with data lineage.\nSelect an asset, and from the top right of the asset card, click the\nView lineage\nicon to open the lineage graph.\nOn the lineage graph, select an asset to create a data product. Atlan will only include assets that are visible on the lineage graph. To include more assets:\n(Optional) Hover over the\n+\nbutton to the right of any asset and then click the\nExpand all\nbutton to include assets further upstream or downstream horizontally in the data product.\n(Optional) Click\nShow all\nto include assets further upstream or downstream assets vertically in the data product.\nFrom the top right of the lineage graph, click the\nbox with plus icon\nfor data products to create a data product from the lineage graph.\nIn the\nNew product via lineage\nform, configure the following:\nFor\nAdd assets\n, any assets that are visible on the lineage graph will be automatically included in the data product. Note that\nprocess\n, child, and\npartial\nassets are currently not supported for data product creation from the lineage graph. You can either keep all the asset selections or deselect any assets.\n(Optional) The asset you had selected on the lineage graph will be automatically set as an output port. You can keep that selection, click\nOutput port\nto remove the current selection, or click\nMark as output port\non any other assets to set additional output ports.\nClick\nContinue\nto proceed.\nProvide details\nâ\nTo provide details:\n(Optional) For\nCover\n, click the\nChange\nbutton to select an image from the gallery or upload an image of your own. Click\nReposition\nto drag and reposition the cover image and then click\nSave position\nto save your preferences.\nFor\nName\n, enter a meaningful name for your data product   -  for example,\nSocial Media Marketing\n. The character limit for a product name is 80 characters.\nFor\nDomain\n, select a data domain from the dropdown   -  for example,\nCustomer Service\n.\n(Optional) For\nDescription\n, enter a description for your domain.\n(Optional) For\nCriticality\n, select a level of business criticality from the dropdown   -  choose from\nHigh\n,\nMedium\n, and\nLow\n.\n(Optional) For\nSensitivity\n, assign a data sensitivity level from the dropdown   -  choose from\nPublic\n,\nInternal\n, and\nConfidential\n.\n(Optional) For\nOwners\n, assign additional users or groups as data product owner.\n(Optional) For\nVisibility\n, select who can access and monitor the data product throughout its entire lifecycle:\nPrivate to members of this domain\n-  only members of a specific domain can access the data product.\nPrivate to selected members\n-  only members of a specific domain and other selected users or groups can access the data product.\nPublic\n-  everyone in the organization can access the data product.\nIf creating a data product from the products module, in the top right of the screen, click the\nContinue\nbutton. If creating a data product from the lineage graph, at the bottom of the form, click the\nContinue\nbutton and skip to the Review the data product section.\nAdd assets\nâ\nTo select assets to include in the data product, you can select via the asset browser or using filters.\nAdd via browser\nâ\nClick the checkbox to select individual assets to include in your data product.\n(Optional) Use the search bar to search for assets by the technical name of the asset.\n(Optional) Filter assets by specific asset types. Click the 3-dot icon to view more asset type filters.\n(Optional) Click the\nShow: all\ndropdown and change to\nSelected assets\nto only view your asset selection.\n(Optional) Click the\nAll filters\ndropdown, and then from the\nAll filters\npane:\nClick\nHierarchy\nto filter assets by connection, database, and schema.\nClick\nCertificate\nto filter assets by\ncertification status\n.\nClick\nOwners\nto filter assets by\nasset owners\n.\nClick\nTags\nto filter assets by your\ntags\nin Atlan, including\nimported tags\n.\nClick\nTerms\nto filter assets by\nlinked terms\n.\nClick\nProperties\nto filter assets by\ncommon asset properties\n.\nIn the top right of the screen, click the\nContinue\nbutton.\nAdd via rules\nâ\nUnder\nSelect assets\n, click\nAdd via filters\nto add assets to your data product using asset filters.\nTo set a matching condition for the filters, select\nMatch all\nor\nMatch any\n.\nMatch all\nwill logically\nAND\nthe criteria, while\nMatch any\nwill logically\nOR\nthe criteria.\nFor\nAttributes\n, select a relevant option:\nClick\nConnection\nand then select an existing connection. (Optional) To further refine your asset selection:\nClick\nAll databases\nto filter by databases in a selected connection.\nClick\nAll schemas\nto filter by schemas in a selected connection.\nClick\nConnector\nto filter assets by\nsupported connectors\n.\nClick\nAsset type\nto filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more.\nClick\nCertificate\nto filter assets by\ncertification status\n.\nClick\nOwners\nto filter assets by\nasset owners\n.\nClick\nTags\nto filter assets by your\ntags\nin Atlan, including\nimported tags\n.\nClick\nGlossary, terms, & categories\nto filter by a specific\nglossary\nor\ncategory\nto bulk update all the nested terms or by multiple glossaries and categories.\nClick\nLinked terms\nto filter assets by\nlinked terms\n.\nClick\nSchema qualified Name\nto filter assets by the qualified name of a given schema.\nClick\nDatabase qualified Name\nto filter assets by the qualified name of a given database.\nClick\ndbt\nto filter assets by dbt-specific filters and then select a\ndbt Cloud\nor\ndbt Core\nfilter.\nClick\nProperties\nto filter assets by\ncommon asset properties\n.\nClick\nUsage\nto filter assets by\nusage metrics\n.\nClick\nMonte Carlo\nto filter assets by\nMonte Carlo-specific filters\n.\nClick\nSoda\nto filter assets by\nSoda-specific filters\n.\nClick\nTable/View\nto filter tables or views by row count, column count, or size.\nClick\nColumn\nto filter columns by\ncolumn-specific filters\n, including parent asset type or name, data type, or\ncolumn keys\n.\nClick\nProcess\nto filter\nlineage processes\nby the SQL query.\nClick\nQuery\nto filter assets by associated\nvisual queries\n.\nClick\nMeasure\nto filter\nMicrosoft Power BI measures\nusing the external measures filter.\nFor\nOperator\n, select\nIs one of\nfor values to include or\nIs not\nfor values to exclude. Depending on the selected attribute(s), you can also choose from\nadditional operators\n:\nSelect\nEquals (=)\nor\nNot Equals (!=)\nto include or exclude assets through exact match search.\nSelect\nStarts With\nor\nEnds With\nto filter assets using the starting or ending sequence of values.\nSelect\nContains\nor\nDoes not contain\nto find assets with or without specified values contained within the attribute.\nSelect\nPattern\nto filter assets using supported\nElastic DSL regular expressions\n.\nSelect\nIs empty\nto filter assets with null values.\nFor\nValues\n, select the relevant values. The values will vary depending on the selected attributes.\n(Optional) To add more filters, click\nAdd filter\nand select\nFilter\nto add individual filters or\nFilter\nGroup\nto nest more filters in a group.\n(Optional) To view all the assets that match your rules, in the\nFilters\ncard, click\nView\nassets\nfor a preview.\nIn the top right of the screen, click the\nContinue\nbutton.\n(Optional) Select output ports\nâ\nDid you know?\nOutput ports determine the relationships between your data products. These relationships are visually represented as\nbusiness lineage\n.\nTo select output ports:\nFrom the list of assets, select output port(s) to allow your data consumers to consume the data product across domains. These assets will serve as the consumption layer for your data product.\nFor\nInput ports\n, Atlan displays a total count of input ports for your data product. These assets are designated as output ports in other data products, and serve as input ports for your data product. Click\nView assets\nto view all input port assets.\nIn the top right of the screen, click the\nContinue\nbutton.\nReview the data product\nâ\nOnce you have reviewed your data product, you can either:\nClick\nSave as draft\nto save your changes in a draft version and publish when ready. Only you and any other users you add as\nowners\nto the product will be able to search for, view, and edit your draft products, depending on their\npermissions\n.\nClick\nCreate and publish\nto publish it immediately.\nCongrats on creating a data product in Atlan! ð\nYou can also use\ngovernance workflows\nto govern the creation and change in status of data products.\nUpdate a data product\nâ\nDid you know?\nYou can\nmove your data products\nwithin and across subdomains or domains to reorganize them as needed.\nOnce you have created a data product, you will also need to monitor and manage it during its entire lifecycle. This helps ensure that the data product stays fresh, up-to-date, and trustworthy. The data product profile in Atlan allows you to curate how your users can use the data product.\nTo update a data product:\nFrom the left menu of any screen in Atlan, you can either:\nClick\nProducts\n. To select a data product, you can:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or expand the relevant domain or subdomain.\nIn the\nData products\nsection, select a trending or recently viewed data product. The list of\nTrending products\nis sorted by the total count of views on each product, with the most viewed product listed at the top.\nFrom the top right of any screen in Atlan, click the\nstar icon\n. From the\nStarred assets\npopup, select a starred data product.\nFrom the left navigation menu or\nProducts\nhomepage, click\nMy drafts\nto continue working on your draft products. Products in draft mode are only visible to product owners until these are published.\nIf your Atlan admin has\nenabled the\nShow products in asset discovery\ntoggle\n, click\nAssets\nto search for data products from\nasset discovery\n.\nOn your data domain page, next to the\nOverview\ntab, click the\nProducts\ntab and select your data product of interest.\n(Optional) From the top right of the product profile, click the\nPublished\ndropdown to update the status of your data product:\nDraft\n-  data product is only visible to product owners.\nPublished\n-  data product is published for users to consume.\nSunset\n-  data product is planned for retirement.\nArchived\n-  data product is archived and no longer visible to users. To restore an archived data product, click the\nRestore product\nbutton and then click\nRestore\n. Atlan will restore the archived data product and it will reappear in product and asset discovery, domain profile, and product lineage.\nUnder\nSummary\n, view details about the data domain your data product belongs to, including criticality, sensitivity, and freshness.\n(Optional) Click the pencil icon to update\nCriticality\nto signify business impact:\nHigh\n-  high business impact\n_Medium   - _ moderate business impact\nLow\n-  internal or a non-business impact\n(Optional) Click the pencil icon to update\nSensitivity\nto assign a data classification:\nPublic\n-  may be freely accessible\nInternal\n-  may only be distributed within the organization\nConfidential\n-  may only be limited to a specific domain or team within the organization\n(Optional) Click\nAdd resource\nto\nadd a resource\nto your asset.\nUnder\nProduct Score\n, view a\nscorecard\nfor your data product, calculated based on metadata completeness.\nUnder\nAt A Glance\n, view a total count of linked assets and output ports.\nUnder\nReadme\n, click\n+ Add\nto\nadd a README\nto your data product or\nuse Atlan AI for documentation\n.\nUnder\nDetails\n, you can view and update metadata for your data product   -  including visibility,\nterms\n,\nowners\n,\ntags\n,\ncertificates\n, and\ncustom metadata\n. You can also\nset up playbooks\nto update product metadata in bulk.\nSwitch to the\nAssets\ntab to view linked assets.\n(Optional) Click the\nEdit\nbutton to add or remove assets from your data product.\n(Optional) Select an asset to view its asset profile in a sidebar.\n(Optional) Filter assets by asset types   -  for example, use the\nTable\nfilter to view table assets only.\n(Optional) Click\nDisable as output port\nto remove an output port or click\nSet as output port\nto set an asset as an output port.\nSwitch to the\nLineage\ntab to view\nbusiness lineage\nfor your data product.\nHover over any data product to view the metadata popover for more context.\nClick the\nview output ports\nmenu to view output ports for your data product.\nIn the upper right of the lineage graph, click the\ndownward arrow\nto download an image of the product lineage graph.\nSwitch to the\nActivity\ntab to view the\nactivity log\nfor your data product.\nView details about changes made to the data product and\nfilter for specific types of metadata changes\n.\nView top and recent users for your data product.\nView a list of data producers for your data product.\nSwitch to the\nContracts\ntab to view any\nlinked contracts\nfor the output ports in your data product.\nFrom the top right of the data product profile:\nClick the user avatars to view a list of recently visited users, total views on your asset, total number of unique visitors, and total views by user.\nUse the days filter to filter asset views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your data product\nand bookmark it for easy access.\nClick the\nSlack\nor\nTeams\nicon to post on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the 3-dot icon to\nadd an announcement\nor a\nresource\nto your data product.\nIf you have enriched your draft products with\nterms\nor\ntags\n, your draft products will be visible to other users as linked assets when viewed from the term or tag profile, respectively. However, only a product owner with the requisite permissions to update the product can make any changes to the draft product.\nDid you know?\nTo programmatically create, update, and manage data products using API, refer to our\ndeveloper documentation\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nAdd stakeholders\nNext\nCreate domain policies\nCreate a data product\nUpdate a data product"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-domain-policies",
    "content": "Configure Atlan\nData Products\nDomain Management\nCreate domain policies\nOn this page\nCreate domain policies\nWho can do this?\nBefore you can create a domain policy, you will need your Atlan admin to\nenable the products module in your Atlan workspace\n. Once enabled, first review the\norder of operations\n. You will need to be a\ndomain admin in Atlan\nto create and manage domain policies.\nDomain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more.\nIn Atlan, you can define domain policies for data\ndomains through\npersonas\n. This framework allows you to ensure that your data products are secure and only accessible to the individuals or teams involved in managing the data.\nCreate a persona\nâ\nTo create a persona:\nFrom the left menu of any screen, clickÂ\nGovernance\n.\nUnderÂ\nAccess Control\n, clickÂ\nPersonas\n.\nIf this is the first persona, click the\nGet started\nbutton. Otherwise click the\nNew persona\nbutton.\nEnter a meaningful name for the persona, (optional) a description, and then click\nCreate\n.\nYou now have an empty persona. It won't do much until you complete the next steps, too!\nAdd users and groups\nâ\nTo add users and groups to the persona, from within the persona:\nTo the right of theÂ\nUsers and groups\nbox, click theÂ\nAdd\nbutton.\nSelect users and / or groups:\nUnder the single-user icon, select users to add to the persona.\nUnder the double-user icon, select groups to add to the persona.\nClick theÂ\nUpdate\nbutton to save the users and groups to the persona.\nNow this persona will be available to those users and groups. It still won't do much, though, without some policies...\n(Optional) Add rich documentation\nâ\nTo add rich documentation describing the persona:\nUnderÂ\nSummary\n, thenÂ\nChannels\n, add any Slack channels relevant to the persona.\nUnderÂ\nResources\n,Â add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive   -  anything that has a URL.\nUnder\nReadme\n, write a richly-formatted description of the persona.\nAdd a domain policy\nâ\nTo add a domain policy to the persona, from within the persona:\nChange to theÂ\nPolicies\ntab.\nClick theÂ\nNew Policy\nbutton and choose the type of policy.\nChooseÂ\nDomain policy\n.\nUnder\nName\n, briefly describe the policy's intention.\nUnderÂ\nSelect domains\n, choose the domain on which to apply the policy   -  for example,\nFinance\n.\n(Optional) For\nConfigure permissions\n, choose the permissions the policy will grant. By default, all permissions will be granted. To select others:\nTo the right ofÂ\nConfigure permissions\n, click theÂ\nEdit\nlink.\nSelect the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. You can enable or limit the following:\nRead\n: permission to view metadata, resources, and READMEs for data domains\nUpdate Domains\n: permission to update metadata, resources, and READMEs for data domains\nCreate Sub-domains\n: permission to create new data subdomains within a domain\nUpdate Sub-domains\n: permission to update metadata, resources, and READMEs for data subdomains\nCreate Products\n: permission to create new data products within a domain\nUpdate Products\n: permission to update metadata, resources, and READMEs for data products\nDelete Products\n: permission to delete data products within a domain\nUpdate Custom Metadata For Domains\n: permission to update\ncustom metadata\nfor data domains\nUpdate Custom Metadata For Sub-Domains\n: permission to update\ncustom metadata\nfor data subdomains\nUpdate Custom Metadata For Products\n: permission to update\ncustom metadata\nfor data products\nAt the bottom of the list, click\nSave\n.\nAt the bottom of theÂ\nDomain policy\nsidebar, clickÂ\nSave\n.\n(Optional) Set preferences\nâ\nDid you know?\nYou can also personalize the details users will see in the sidebar or filters menu when in a persona. This is great to limit information overload, by showing only what is relevant to a given set of users.\nTo set preferences for the persona:\nChange to theÂ\nPreferences\ntab of the persona. From the left menu, configure the following:\nTo set the default landing page forÂ the persona:\nClick\nNavigation\nto view landing page preferences.\nFor\nSet default landing page\n, click the dropdown and then click\nProducts\nso that your users land on the data products page when they log into Atlan.\nÂ To limit the\nout-of-the-box tabs\nthat should be visible to the persona:\nClick\nAsset sidebar\nto view asset sidebar preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona.\nTo limit the\nasset filters\nthat should be visible to the persona:\nClickÂ\nAsset filters\nto view asset filter preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona.\nTo limit the\ncustom metadata\nthat should be visible to the persona:\nClick\nCustom metadata\nto view custom metadata preferences.\nClick the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the persona.\nTo personalize your data products, click\nProducts\nand then click the\nAdd cover image\nbutton to set a cover image for data products within your selected domain.\nTags:\natlan\ndocumentation\nPrevious\nCreate data products\nNext\nMonitor data domains\nCreate a persona\nAdd users and groups\n(Optional) Add rich documentation\nAdd a domain policy\n(Optional) Set preferences"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/concepts/what-is-business-lineage",
    "content": "Configure Atlan\nData Products\nConcepts\nWhat is business lineage?\nOn this page\nWhat is business lineage?\nWho can do this?\nBefore you can explore business lineage, you will need your Atlan admin to\nenable the products module in your Atlan workspace\n.\nBusiness lineage is a representation of relationships between your\ndata products\nin Atlan. This simplified view of product lineage can help you quickly grasp the provenance and relationships of your data products and draw links in the chain of your asset universe.\nAs a map to your data estate, business lineage can help:\nData producers understand how their data products are used within the organization.\nData consumers gain visibility into the provenance of the data products they use.\nBoth producers and consumers better understand the business relevance and impact of data.\nView product lineage\nâ\ndanger\nAtlan currently does not support generating lineage for products with more than 30,000 assets. This is to ensure that the lineage generation workflows are optimized to run efficiently.\nOutput ports determine the relationships between your data products. For example, if your data product includes assets that are designated as output ports in another data product:\nwithin the same domain or subdomain,\nacross different subdomains within the same domain,\nor across entirely different domains,\n...this relationship will be visualized as business lineage.\nThe assets designated as output ports in other data products serve as\ninput ports for your data product\n, automatically linking them based on your asset selection. Note that you cannot set input ports manually. You can, however, designate output ports while\ncreating a data product\n.\nProcessing lineage between data products involves running cron jobs every hour. For example:\nFor products with\n<10k\nassets, lineage appears in 15-20 minutes.\nFor products with\n>10k\nassets, lineage may take up to 1 hour to be processed.\nSupported scenarios\nâ\nIn addition to the scenarios listed above, product lineage will be captured when:\nCreating products with shared assets:\nProduct 1 â Product 2: If Product 1 is created with Asset 1 as an output port, and Product 2 is consequently created with Asset 1 as a regular asset, lineage will be generated from Product 1 to Product 2 after processing has been completed.\nProduct 2 â Product 1: If Product 1 is created with Asset 1 as a regular asset, and Product 2 is later created with Asset 1 marked as an output port, lineage will be generated from Product 2 to Product 1.\nLineage will be generated in either scenario whether products are created via the\nbrowse option or rules\n.\nRemoving or modifying assets:\nIf Asset 1 is removed from Product 2, lineage between Products 1 and 2 will be removed after processing.\nIf Asset 1 is unmarked as an output port in Product 1, lineage will be removed between the two products.\nIf either Product 1 or 2 is archived, lineage will be removed.\nDynamic asset listing: For example, if Product 1 is created with Asset 1 (output port) and Product 2 is created with Asset 2 (regular asset) using a tag-based rule (for example, all assets with the\nPII\ntag), then adding a\nPII\ntag to Asset 1 will generate lineage from Product 1 to Product 2. This is also applicable to data product creation with term-based rules and more.\nTo view product lineage:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a data product, you can either:\nFrom the left navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain and then select a data product.\nIn the\nExplore products\nsection on the\nProducts\nhomepage, select a recently viewed, starred, or new data product.\nFrom the tabs along the top of your product page, click the\nLineage\ntab.\nOn the lineage graph, the home icon indicates the base product. (Optional) Hover over any data product to view a metadata popover for more context   -  including domain name,\ncriticality and sensitivity levels\n, owner, and\nproduct score\n. You can also open lineage in a new tab.\n(Optional) For any applicable product, click the\nview output port\nmenu to view output ports:\nThe default view shows 10 output ports. Click\nShow more\nto view the full list of output ports.\nHover over an output port in the list and then click the upward arrow to view lineage for that output port asset in a new tab.\n(Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow.\n(Optional) From the top right of the lineage graph:\nClick the\nFind in canvas\nsearch bar to search for any specific products on the lineage graph.\nClick the downward arrow to download product lineage as an image.\nClick the eye icon to set preferences for the lineage graph:\nFor\nAdditional metadata\n, show or hide the following context for your assets   -  domain name,\nannouncements\n, or sensitivity and criticality levels.\nFor\nLine arrows\n, show or hide the arrows that indicate data flows on the lineage graph.\nClick the question mark icon to share feedback.\n(Optional) From the bottom right of the lineage graph:\nClick the minimap icon to view an abridged version of the lineage graph.\nClick the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base product.\nClick the fullscreen icon to expand the lineage view to fullscreen mode.\nClick the minus or plus icons to zoom out or zoom in on the lineage graph, respectively.\nView domain lineage\nâ\nDomain lineage is a visual representation of the relationships between your domains and subdomains connected through data products. It can serve as a map to your business domain, helping you understand how data products are used within a specific domain.\nFor example, if there are subdomains that do not contain any products, this comprehensive view will help you target those subdomains for deprecation. Or, if a product is used across multiple subdomains, this may signal the need for more robust measures for change management.\nWhen viewing domain lineage, you can view the hierarchy of subdomains and products within a specific domain.\nTo view domain lineage:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nTo select a\ndata domain\n, you can either:\nFrom the navigation menu on the\nProducts\nhomepage, use the search bar or select the relevant domain or subdomain.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nView all\nbutton to view more domains.\nFrom the tabs along the top of your domain page, click the\nLineage\ntab.\nOn the lineage graph, the home icon indicates the base domain.\n(Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow.\n(Optional) From the top right of the lineage graph:\nClick the\nFind in canvas\nsearch bar to search for any specific products on the lineage graph.\nClick the downward arrow to download product lineage as an image.\nClick the plus icon to expand all domain nodes to view data products.Â Hover over a data product in the list and then click the upward arrow to view lineage for that product in a new tab.\nClick the eye icon to set preferences for the lineage graph. For\nAdditional metadata\n, show or hide the following context for your assets   -  domain type and\nannouncements\n.\nClick the question mark icon to share feedback.\n(Optional) From the bottom right of the lineage graph:\nClick the minimap icon to view an abridged version of the lineage graph.\nClick the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base product.\nClick the fullscreen icon to expand the lineage view to fullscreen mode.\nClick the minus or plus icons to zoom out or zoom in on the lineage graph, respectively.\nDid you know?\nArchived products and domains are removed from business lineage. However, for any archived asset that may have been designated as an output port in active data products, the lineage links created by that asset will be visible on the lineage graph.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nMonitor data domains\nNext\nWhat is a product score?\nView product lineage\nView domain lineage"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/concepts/what-is-a-product-score",
    "content": "Configure Atlan\nData Products\nConcepts\nWhat is a product score?\nOn this page\nWhat is a product score?\nHow can Atlan help you build trust in your data products? Product scores!\nBased on the principles of data as a product, product scores can help you signal the accuracy and completeness of your\ndata products\n, helping build trust in them. The product scorecard in Atlan enables you to:\nIdentify and prioritize data quality issues\nQuantify the business impact of data quality\nDrill down for detailed analysis and take action\nShare insights across stakeholders\nThe product score is updated daily at 4:00 AM UTC. You can hover over the relative time in the product scorecard to view the timestamp for when any metadata attribute of the product was last updated in Atlan.\nDid you know?\nYou can also choose to hide the product score from your data products. Contact your Atlan admin to\nturn off the product score\nfrom the admin center.\nComponents of a product score\nâ\nAtlan calculates and assigns a product score to your\ndata products\nbased on a preset criteria of metadata completeness. Atlan evaluates metadata enrichment on the data product, output ports, or a combination of both. Using a weighted scoring method, values are automatically assigned on the basis of how well your data product satisfies the six principles of data as a product. Atlan scores your data product on a scale of 0-5, with 0 being the lowest and 5 being the highest score.\nThe six principles of data as a product are:\nDiscoverable\n: The discoverability of a data product is based on the availability of aggregate metadata. Atlan quantifies discoverability in the form of\nglossary terms\nlinked to the data product and output ports.\nUnderstandable\n: A data product is considered to be understandable when it has defined and curated contextual metadata to help your data consumers understand the product better. Atlan quantifies how understandable a product is in the form of:\nA\ndescription\nor\nREADME\nadded to the data product, or\nREADMEs\nadded to output ports.\nAddressable\n: A data product is considered to be addressable when it has well-documented owners and points of contact. Atlan quantifies how addressable a data product is in terms of\nowners\nassigned to the data product and its output ports.\nSecure\n: A secure data product will clearly signify the sensitivity of data. Atlan quantifies how secure a data product is on the basis of\nsensitivity classifications on the data product\nand\ntags attached\nto its output ports.\nInteroperable\n: Interoperability of a data product is determined on the basis of the visibility and completeness of\ntechnical lineage between data assets\n.\nTrustworthy\n: Trustworthiness of a data product is determined on the basis of\ncertificates\nand\ndata contracts\nattached to the data product.\nA product score is not set in stone, and will change depending on the completeness of metadata enrichment. The score can help you understand how to improve your data product to make it more useful for your data consumers.\nAtlan currently does not allow you to either configure the score or define your own criteria.\nScoring rubric\nâ\nAtlan adheres to the following scoring rubric to assign product scores:\nPrinciple\n0\n1\n2\n3\n4\n5\nDiscoverable\nNo terms linked to product\n-\n-\n-\n-\nAt least 1 term linked to product\nUnderstandable\nNo description on product or output ports AND no README on product\nDescription on product AND â¤ 10% of output ports have descriptions but no READMEs OR No description on product or output ports AND product has a README OR No description or README on product AND 11%-40% output ports have descriptions\nDescription on product AND 11%-40% output ports have descriptions but no READMEs OR Description on product AND < 10% of output ports have descriptions AND product has a README OR No description or README on product AND 41%-70% output ports have descriptions\nDescription on product AND 41%-70% output ports have descriptions but no READMEs OR Description on product AND 11%-40% output ports have descriptions AND product has a README OR No description or README on product AND 91%-90% output ports have descriptions\nDescription on product AND 71%-90% output ports have descriptions but no READMEs OR Description on product AND 41%-70% output ports have descriptions AND product has a README OR No description or README on product AND >90% output ports have descriptions\nDescription on product AND >90% output ports have descriptions but no READMEs OR Description on product AND 71%-90% output ports have descriptions AND product has a README\nAddressable\nNo owners on product and output ports OR Product does not have an owner AND â¤ 10% of output ports have owners\nProduct has an owner AND â¤ 10% of output ports have owners OR Product does not have an owner AND 11%-40% output ports have owners\nProduct has owners AND 11%-40% output ports have owners OR Product does not have an owner AND 41%-70% output ports have owners\nProduct has owners AND 41%-70% output ports have owners OR Product does not have an owner AND 71%-90% output ports have owners\nProduct has owners AND 71%-90% output ports have owners OR Product does not have an owner AND >90% output ports have owners\nProduct has owners AND >90% output ports have owners\nSecure\nProduct does not have sensitivity classification AND no tags attached to output ports OR Product does not have sensitivity classification AND â¤ 10% of output ports have tags attached\nProduct has sensitivity classification AND no tags attached to output ports OR Product does not have sensitivity classification AND 11%-40% output ports have tags attached\nProduct has sensitivity classification AND 11%-40% output ports have tags attached OR Product does not have sensitivity classification AND 41%-70% output ports have tags attached\nProduct has sensitivity classification AND 41%-70% output ports have tags attached OR Product does not have sensitivity classification AND 71%-90% output ports have tags attached\nProduct has sensitivity classification AND 71%-90% output ports have tags attached OR Product does not have sensitivity classification AND >90% output ports have tags attached\nProduct has sensitivity classification AND >90% output ports have tags attached\nInteroperable\nNone of the output ports have technical lineage\nâ¤ 10% of output ports have technical lineage\n11%-40% of output ports have technical lineage\n41%-70% of output ports have technical lineage\n71%-90% of output ports have technical lineage\n> 90% of output ports have technical lineage\nTrustworthy\nProduct neither has a certificate nor a contract\n-\n-\nProduct does not have a certificate but has a contract OR Product has a certificate but no contract\n-\nProduct has a certificate and contract\nScoring method\nâ\nTo calculate the product score, Atlan uses a weighted scoring method:\nAssign scores for each principle   -  Atlan assigns a score based on each of the principles to determine a product score within the range of 0-5.\nSet weights for each principle   -  Atlan determines the weight of each principle based on the completion rate of metadata enrichment.\nCalculate weighted scores   -  the score for each principle is multiplied by its weight. For example, if\nTrustworthy\nhas a score of 4 on a scale of 5 and a weight of 30%, the weighted score would be\n4 * 0.3\n=\n1.2\n.\nSum up the weighted scores   -  Atlan adds up the weighted scores for each principle to arrive at a total score, which is displayed on the data product.\nInterpret the score   -  you can use the total weighted score to evaluate the data product's overall alignment with the principles of data as a product. A higher score will indicate closer alignment with the principles and metadata completion.\nTags:\natlan\ndocumentation\nPrevious\nWhat is business lineage?\nNext\nWhat are data products?\nComponents of a product score\nScoring rubric\nScoring method"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/how-tos/create-a-persona",
    "content": "Configure Atlan\nAccess control\nGet started\nCreate persona\nOn this page\nCreate persona\nWho can do this?\nYou must be an admin user to create personas.\nTo create a persona:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder\nAccess Control\n, click\nPersonas\n.\nIf this is the first persona, click the\nGet started\nbutton. Otherwise click the\nNew persona\nbutton.\nEnter a meaningful name for the persona, (optional) a description, and then click\nCreate\n.\nYou now have an empty persona. It won't do much until you complete the next steps, too!\nAdd users and groups\nâ\nTo add users and groups to the persona, from within the persona:\nTo the right of the\nUsers and groups\nbox, click the\nAdd\nbutton.\nSelect users and / or groups:\nUnder the single-user icon, select users to add to the persona.\nUnder the double-user icon, select groups to add to the persona.\nClick the\nUpdate\nbutton to save the users and groups to the persona.\nNow this persona becomes available to those users and groups. It still doesn't do much, though, without some policies...\nAdd rich documentation (optional)\nâ\nTo add rich documentation describing the persona:\nUnder\nSummary\n, then\nChannels\n, add any Slack channels relevant to the persona.\nUnder\nResources\n, add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive â anything that has a URL.\nUnder\nReadme\n, write a richly-formatted description of the persona.\nAdd policies\nâ\nFor the persona to really do anything, you need to define one or more policies. Repeat the following steps for each set of assets and permissions you want to control through the persona.\nDid you know?\nThe higher level at which you can define the assets, the better. For example, if you create a policy at a database level, all\nfuture\nschemas and tables (in that database) are also covered by the policy.\nTo add policies to the persona, from within the persona:\nChange to the\nPolicies\ntab.\nClick the\nNew Policy\nbutton and choose the type of policy.\nAdd a metadata policy\nâ\nAdd a metadata policy to grant or restrict permissions to change metadata. You can also control access to data quality rules through metadata policies. For more information about data quality rules, see\ndata quality rules\n.\nFollow these steps to set up a metadata policy:\nChoose\nMetadata Policy\n.\nUnder\nName\n, briefly describe the policy's intention.\nUnder\nSelect a connection\n, choose the connection on which to apply the policy.\n(Optional) For\nAsset selector\n, choose the assets the policy controls. By default, all assets in the connection are included. To select others:\nIn the\nAll assets\nbox, click the\nx\n.\nUnder\nAsset selector\n, click the\nAdd\nlink.\nTo search for and select the assets to control with the policy, in the\nAdd Assets\ndialog:\nClick\nBrowse\nto search and select assets from all databases in the connection.\nClick\nSearch\nto search from and select individual assets in the connection.\nClick\nCustom\nto search and select assets by their qualified name.\nClick\nSave\nto confirm your selections.\n(Optional) For\nConfigure permissions\nchoose the\npermissions the policy grants\n. By default, all permissions are granted. To select others:\nTo the right of\nConfigure permissions\nclick the\nEdit\nlink.\nSelect the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one.\nAt the bottom of the list, click\nSave\n.\n(Optional) For\nDeny selected permissions\nchoose whether you want to explicitly deny these permissions.\ndanger\nIf enabled, this overrides all grants from any other policies for the same users.\nAt the bottom of the\nMetadata Policy\nsidebar, click\nSave\n.\nDid you know?\nWhen using the custom asset selector for metadata and data policies, you can add\n/*\nafter the database name to select all the schemas in that database.\nAdd a data policy\nâ\nTo grant or restrict permissions to query or preview data:\nChoose\nData Policy\n.\nUnder\nName\n, briefly describe the policy's intention.\nUnder\nSelect a connection\n, choose the connection on which to apply the policy.\n(Optional) For\nAsset selector\n, choose the assets the policy controls. By default, all assets in the connection are included. To select others:\nIn the\nAll assets\nbox, click the\nx\n.\nUnder\nAsset selector\n, click the\nAdd\nlink.\nTo search for and select the assets to control with the policy, in the\nAdd Assets\ndialog:\nClick\nBrowse\nto search and select assets from all databases in the connection.\nClick\nSearch\nto search from and select individual assets in the connection.\nClick\nCustom\nto search and select assets by their qualified name.\nClick\nSave\nto confirm your selections.\n(Optional) For\nDeny Query\nchoose whether you want to explicitly deny the ability to query and preview data on these assets.\ndanger\nIf enabled, this overrides all grants from any other policies for the same users.\nAt the bottom of the\nData Policy\nsidebar, click\nSave\n.\nAdd a glossary policy\nâ\nTo grant or restrict permissions to change glossary contents:\nChoose\nGlossary Policy\n.\nUnder\nName\n, briefly describe the policy's intention.\nUnder\nSelect glossary\n, choose the glossary or glossaries on which to apply the policy.\n(Optional) For\nConfigure permissions\nchoose the\npermissions the policy grants\n. By default, all permissions are granted. To select others:\nTo the right of\nConfigure permissions\nclick the\nEdit\nlink.\nSelect the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one.\nAt the bottom of the list, click\nSave\n.\nAt the bottom of the\nGlossary Policy\nsidebar, click\nSave\n.\nSet preferences (optional)\nâ\nDid you know?\nYou can also personalize the details users see in the sidebar or filters menu when in a persona. This is great to limit information overload, by showing only what's relevant to a given set of users.\nTo set preferences for the persona:\nChange to the\nPreferences\ntab of the persona. From the left menu, configure the following:\nTo set the default landing page for the persona:\nClick\nNavigation\nto view landing page preferences.\nFor\nSet default landing page\n, click the dropdown and then select the page where your users land when they log into Atlan. (See also\nWhat will be the default landing page for users with two or more personas?\n)\nKeep\nHome\nas the default selection for homepage.\nClick\nAssets\nto direct your users to asset search and discovery.\nClick\nGlossary\nto direct your users to glossaries.\nClick\nInsights\nto direct your users to the query editor.\nClick\nProducts\nto direct your users to the homepage for data products.\nClick\nCustom\nto set a custom path of your choice. For\nAtlan page link\n, specify a path to the page in Atlan you want your users to land on and make sure it's available to users in the persona. Click\nSave\nto save your preference.\nYou must turn off\nView \"All assets\" in Assets Discovery\nfrom\nLabs\nin the admin center to make sure that users\nonly\nland on your preselected page. Complete the steps in\nHow to restrict asset visibility\nto do so.\nTo limit the asset types that are visible to the persona:\nClick\nAsset types\nto view asset type preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona.\nTo limit the\nout-of-the-box tabs\nthat are visible to the persona:\nClick\nAsset sidebar\nto view asset sidebar preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona.\nTo limit the\nasset filters\nthat are visible to the persona:\nClick\nAsset filters\nto view asset filter preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona.\nTo set the default asset name that's visible to the persona:\nClick\nAsset name\nto set display preferences for asset name.\nClick the\nPrefer display name over technical name\ncheckbox to display the\ndisplay name\n, if added, over the technical name for the persona. Uncheck the box to display the technical name instead.\nTo limit the\ncustom metadata\nthat's visible to the persona:\nClick\nCustom metadata\nto view custom metadata preferences.\nClick the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the persona.\nTo personalize your\ndata products\n, click\nProducts\nand then click the\nAdd cover image\nbutton to set a cover image for data products within your selected domain.\nTags:\natlan\ndocumentation\nPrevious\nCreate groups\nNext\nCreate purpose\nAdd users and groups\nAdd rich documentation (optional)\nAdd policies\nSet preferences (optional)"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/monitor-data-domains",
    "content": "Configure Atlan\nData Products\nDomain Management\nMonitor data domains\nOn this page\nMonitor data domains\nWho can do this?\nBefore you can monitor domains, you will need your Atlan admin to\nenable the products module in your Atlan workspace\n. Once enabled, first review the\norder of operations\n. You will need to be a\ndomain owner or admin in Atlan\nto track domain activity and usage.\nThe\nStatistics\ntab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more.\nSummarize data products\nâ\nThe\nSummary\nsection provides a high-level overview of your domain and data products.\nTo summarize data products:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nBrowse all\nbutton to view more domains.\nFrom the tabs along the top of your data domain page, click the\nStatistics\ntab.\nThe\nSummary\nsection provides you with a high-level overview of data product metrics:\nUnder\nProduct summary\n, you can view a total count of data products and output ports as well as information about domains and subdomains.\nUnder\nProducts by status\n, you can view a total count of products grouped by status:\nActive\n-  data products active for users to consume.\nSunset\n-  data products planned for retirement.\nArchived\n-  data products archived and no longer available to users.\nReview enrichment completion\nâ\nThe\nActivity\nsection displays a total count of data products grouped by type of metadata enrichment. You can also view the total count of products that need to be updated   -  for example, products without a description.\nTo review metadata enrichment for data products:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nBrowse all\nbutton to view more domains.\nFrom the tabs along the top of your data domain page, click the\nStatistics\nÂ tab.\nUnder\nProducts by enrichment\n, navigate to the card you want to view   -  for this example, we'll select the\nWith Description\ncard.\n(Optional) In the\nWith Description\ncard, click the products remaining button to view all the products without a description in the sidebar.\nTrack product creation over time\nâ\nThe\nProducts created over time\ngraph provides visual insights into your data product creation process.\nTo track product creation over time:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nBrowse all\nbutton to view more domains.\nFrom the tabs along the top of your data domain page, click the\nStatistics\nÂ tab.\nUnder\nProducts created over time\n, view a time graph for product creation within a specific domain.\nMonitor domain usage\nâ\nThe\nUsage\ntabÂ allows you to track and monitor domain usage. You can also view top domain visitors in this section.\nTo track domain usage:\nFrom the left menu of any screen in Atlan, click\nProducts\n.\nFrom the top right of\nDiscover domains\n, select your data domain of interest. If you cannot find your data domain, click the\nBrowse all\nbutton to view more domains.\nFrom the tabs along the top of your data domain page, click the\nStatistics\nÂ tab.\nUnder\nUsage\n, you can either:\nTrack views over time for your data products   -  total views and views by date.\nView a set of top domain visitors.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nCreate domain policies\nNext\nWhat is business lineage?\nSummarize data products\nReview enrichment completion\nTrack product creation over time\nMonitor domain usage"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/disable-user-activity",
    "content": "Configure Atlan\nAdministration\nFeature Management\nDisable user activity\nOn this page\nDisable user activity\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to disable user activity on asset profiles.\nYou can\nview recently visited users\nand total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps.\nDisable asset profile visitors\nâ\nTo disable asset profile visitors for your assets in Atlan:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAssets\nheading of the\nLabs\npage, turn off\nAsset profile visitors\n.\nYour users will not be able to view user activity on your assets in Atlan! ð\nIf you'd like to enable user activity, follow the steps above and then turn it on.\nTags:\natlan\ndocumentation\nPrevious\nAllow members to view reports\nNext\nHow to enable associated terms\nDisable asset profile visitors"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/create-announcements",
    "content": "Configure Atlan\nIntegrations\nCommunication\nSMTP and Announcements\nCreate announcements\nOn this page\nCreate announcements\nAdding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions.\nWhat type of announcements would you want to share with your team? Here are a few examples:\nAdd an announcements\nâ\nTo add an announcement to an asset:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nOn the\nAssets\npage, select an asset to add an announcement. You can either:\nOpen the asset profile. From the top right of the\nOverview\ntab in the asset profile, click the\nvertical 3-dot\nÂ icon, and then from the dropdown, click\nAdd announcement\n.\nIn the\nOverview\ntab of the asset sidebar, click the\nhorizontal 3-dot\nicon, and then from the dropdown, click\nAdd announcement\n.\nIn the\nNew Announcement\ndialog, enter the following details:\nFrom the top right, click the\ndownward arrow\nÂ and choose from three announcement types:\nInformation\n,\nIssue\n, or\nWarning\n.\nFor\nAdd title\n, enter a title for your announcement.\nFor\nDescription\n, enter a description for your announcement.\n(Optional) You can include HTML hyperlinks to direct users to additional information   -  for example, wrap the text with\n<a href=\"https://my.url.com\">description text</a>\n.\n(Optional) You can use Markdown syntax to write a description. Any headings will be rendered in heading 6 (\n<h6>\n).\ndanger\nAtlan currently does not support adding images to your announcements.\n(Optional) To share your announcement on Slack or Teams, you can either:\nClick the\nShare\nbuttonÂ to integrate\nSlack\nor\nMicrosoft Teams\n.\nIf you have already integrated\nSlack\nor\nMicrosoft Teams\n, click the checkbox for\nShare on Slack\nor\nShare on Teams\n, respectively.\n(Optional) To configure the announcements channel for Slack or Teams:\nIf you have already configured a\nSlack\nor\nMicrosoft Teams\nchannel to receive announcement alerts, that channel will be preselected. You can change to a different channel, if available.\nIf you have not configured a channel for announcements, enter the channel name to receive notifications for announcements. This channel will be displayed as the\nAnnouncements channel\nÂ in yourÂ\nSlack\nor\nMicrosoft Teams\nintegration.\nClick\nAdd\nto create your announcement.\n(Optional) To edit an announcement, from the top right of the announcement box, click the 3-dot icon and then click\nEdit\n.\n(Optional) To delete an announcement, from the top right of the announcement box, click the 3-dot icon and then click\nDelete\n.\nYou just created an announcement! ð\nThis announcement will be visible to anyone who views the asset. You can also create similar announcements for other types of data assets, including\nglossaries, categories, and terms\n.\nTo create, remove, and manage announcements using API, refer to our\ndeveloper documentation\n.\nDid you know?\nYou can only create one announcement per asset. To add more information to your announcement, you can either edit the existing one to update it or delete the old one and create a new announcement.\nTags:\ndata\nintegration\nPrevious\nConfigure SMTP\nNext\nManage system announcements\nAdd an announcements"
  },
  {
    "url": "https://docs.atlan.com/tags",
    "content": "Tags\nA\nâ\naccess-control\n19\nadministration\n1\nagreements\n1\nAI\n2\nAI agents\n1\nair-gapped\n1\nairflow\n1\naiven\n1\nalerts\n9\nalteryx\n2\nalways on\n1\namazon\n5\namazon-athena\n1\namazon-s3\n4\nanalysis\n1\nanalytics\n8\nannouncements\n2\nanomalo\n1\napache\n3\napi\n72\napp\n6\narchitecture\n1\nassests\n1\nasset-profile\n5\nassets\n3\nastronomer\n1\natlan\n133\nAtlan MCP\n8\natlan-ai\n2\natlas\n1\nattributes\n1\nauthentication\n53\nauto-re-attachment\n2\nautomation\n25\naws\n4\naws lambda\n1\nazure\n4\nB\nâ\nbigid\n4\nbigquery\n1\nbrowse\n1\nbrowser-extension\n3\nbusiness intelligence\n15\nbusiness-terms\n17\nC\nâ\ncalculation-view\n1\ncapabilities\n10\ncassandra\n1\ncatalog\n7\ncategorization\n1\ncdi\n4\nclassification\n1\nClaude\n1\ncloud\n1\ncloudera\n2\ncollaboration\n4\ncommunication\n2\ncompliance\n1\ncomposer\n1\nconcepts\n1\nconfiguration\n19\nconfluent\n2\nconnect\n2\nconnections\n1\nconnectivity\n64\nconnector\n63\nconnectors\n299\ncontainer-images\n1\ncontracts\n1\ncosmosdb\n1\ncratedb\n6\ncrawl\n227\ncrawling\n1\ncredentials\n1\ncrm\n1\ncross-workspace-extraction\n1\nCursor\n1\ncustom metadata\n1\nD\nâ\ndagster\n4\ndapr\n1\ndashboards\n6\ndata\n255\ndata assets\n1\ndata factory\n1\ndata integration\n6\ndata lake\n1\ndata quality\n20\ndata transformation\n1\ndata warehouse\n4\ndata-catalog\n6\ndata-domains\n1\ndata-flow\n1\ndata-lineage\n60\ndata-modeling\n1\ndata-models\n1\ndata-products\n1\ndata-sources\n13\ndata-transfer\n1\ndatabase\n21\ndatabricks\n6\ndatastax\n1\ndbt\n1\ndefinitions\n18\ndependencies\n9\ndeployment\n4\ndimensions\n1\ndiscovery\n9\ndocker\n1\ndocumentation\n117\ndomains\n1\ndomo\n1\ndownstream-impact\n8\ndynamodb\n1\nE\nâ\necc\n1\nembedded\n1\nenrichment\n1\nerd\n1\nerp\n6\netl\n7\netl-tools\n10\nevent hubs\n1\nF\nâ\nfaq\n33\nfaq-administration\n2\nfaq-automation\n2\nfaq-connections\n6\nfaq-connectors\n1\nfaq-discovery\n1\nfaq-governance\n6\nfaq-insights\n9\nfaq-integrations\n17\nfaq-lineage\n16\nfaq-metadata\n1\nfaq-platform\n2\nfaq-security\n1\nfaq-support\n1\nfirewall\n1\nfivetran\n1\nG\nâ\ngcp\n2\ngcs\n3\nget-started\n3\nglossary\n18\nglue\n1\ngoogle\n2\ngoogle-gcs\n3\ngovernance\n18\ngraphql\n25\ngroups\n1\nguides\n1\nH\nâ\nhelm\n1\nhelp\n1\nhive\n1\nhosted\n1\nhow-to\n1\nhybrid bi\n5\nI\nâ\nibm cognos\n1\nidentity management\n3\nimpact-analysis\n58\nimpala\n2\ninformatica\n6\ninsights\n1\nintegration\n123\nintegrations\n14\ninventory-reports\n1\nJ\nâ\njira\n2\nK\nâ\nkafka\n5\nkubernetes\n1\nL\nâ\nlambda\n1\nlineage\n76\nlogic\n1\nlogs\n1\nlooker\n1\nM\nâ\nmatillion\n2\nmessaging\n6\nmetabase\n1\nmetadata\n12\nmetadata-extractor\n1\nmetrics\n2\nmicrosoft\n4\nMicrosoft Copilot Studio\n1\nmicrosoft teams\n1\nmicrostrategy\n1\nmigration\n1\nmode\n1\nmodel\n13\nmongodb\n1\nmonitoring\n9\nmonte carlo\n1\nmsk\n1\nmultiple-concatenation\n1\nmwaa\n1\nmysql\n1\nN\nâ\nn8n\n1\nnetwork\n1\nnosql\n3\nnotifications\n9\nO\nâ\noauth\n1\nobservability\n3\noffline\n1\non-premises\n1\nopenlineage\n6\nopentelemetry\n1\noperations\n1\noracle\n1\norchestration\n12\norganization\n2\notlp\n1\nP\nâ\nparsing\n1\npermissions\n20\nplaybooks\n1\npolicies\n1\npopularity\n1\npostgresql\n1\npower bi\n1\npreflight-checks\n1\nprestosql\n1\nprivacy\n4\nproject management\n3\nproperties\n2\nQ\nâ\nqlik sense\n2\nquery\n1\nquery history\n1\nquick-start\n3\nquicksight\n1\nR\nâ\nredash\n1\nredpanda\n1\nredshift\n1\nreference\n8\nrelational\n4\nreleases\n1\nremote\n6\nreporting\n1\nrequests\n1\nrest-api\n25\nroles\n2\nrules\n2\nS\nâ\ns3\n4\ns4hana\n1\nsalesforce\n10\nsap\n2\nsap-ecc\n1\nsap-hana\n1\nsap-s4hana\n1\nschema\n6\nschema registry\n1\nschema-drift\n5\nschema-monitoring\n5\nscim\n2\nscopes\n1\nsearch\n2\nsecrets\n1\nsecure-agent\n10\nsecurity\n24\nservicenow\n2\nsetup\n54\nsiem\n1\nsigma\n1\nsisense\n1\nslack\n5\nsmtp\n2\nsnowflake\n12\nsoda\n1\nspark\n1\nspreadsheets\n1\nsql\n4\nsql server\n1\nsso\n2\nstewardship\n1\nstorage\n8\nsupport\n2\nsynapse\n1\nT\nâ\ntableau\n2\ntags\n1\ntasks\n1\nteams\n1\nteradata\n1\nterminology\n1\nthoughtspot\n1\ntokens\n1\ntransformations\n2\ntrino\n1\ntroubleshooting\n8\nU\nâ\nupstream-dependencies\n13\nusage\n1\nuser groups\n2\nusers\n1\nV\nâ\nvisualization\n6\nW\nâ\nwarehouse\n1\nwebhooks\n4\nWindsurf\n1\nworkflow\n13\nworkflows\n5"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control",
    "content": "Configure Atlan\nAccess control\nOn this page\nAccess Control\nOverview:\nManage user permissions and access to data assets in Atlan for security and compliance. Control who can view, edit, and manage your data assets through granular access policies and user roles.\nGet started\nâ\nFollow these steps to implement access control in Atlan:\nCreate personas\nCreate Purpose\nConcepts\nâ\nWhat are personas?\n: Detailed explanation of personas.\nWhat are Purposes?\n: Detailed explanation of purposes.\nWhat are Sidebar Tabs?\n: Detailed explanation of purposes.\nTags:\naccess control\npermissions\nsecurity\ngovernance\natlan\nNext\nInvite new users\nGet started\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/add-users-to-groups",
    "content": "Configure Atlan\nAccess control\nManage users and groups\nAdd users to groups\nOn this page\nAdd users to groups\nWho can do this?\nYou will need to be an admin user in Atlan to manage group membership.\nAdd users to a group\nâ\nTo add many users to one group:\nFrom the left menu on any screen, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, or from the tiles, click\nGroups\n.\nTo the right of a group row, click the user button.\nCheck all users to add to the group.\nClick theÂ\nSave\nbutton.\nAdd groups to a user\nâ\nTo add many groups to one user:\nFrom the left menu on any screen, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, or from the tiles, clickÂ\nUsers\n.\nTo the right of a user row, click the group button.\nCheck all groups to add to the user.\nClick theÂ\nAdd\nbutton.\nMap users to SSO groups\nâ\nAtlan supports configuring SSO group mappings. You will first need to\ncreate groups\nin Atlan that correspond to the groups you want to map from your SSO provider to Atlan.\nTo automatically assign users to Atlan groups based on their SSO groups, refer to the documentation for supported SSO providers:\nAzure AD\nGoogle\nJumpCloud\nOkta\nOneLogin\nSAML 2.0\nYou can also\nset default roles\nfor new users joining the Atlan workspace via SSO.\nTags:\natlan\ndocumentation\nPrevious\nManage users\nNext\nManage user authentication\nAdd users to a group\nAdd groups to a user\nMap users to SSO groups"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/additional-connectivity-to-data-sources",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nReferences\nAdditional connectivity to data sources\nOn this page\nAdditional connectivity to data sources\nIn addition to connecting to your\ndata sources\ndirectly, Atlan also supports connecting through:\nPrivate network link\nâ\nAWS PrivateLink\nâ\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS:\nAmazon Athena\nAmazon MSK\nAmazon Redshift\nDatabricks\nHive\nMicrosoft SQL Server   -\nAmazon RDS\nand\nAmazon EC2\nMySQL\nPostgreSQL\nSnowflake\nTableau\nTrino\nAzure Private Link\nâ\nAzure Private Link\ncreates a secure, private connection between services running in Azure:\nDatabricks\nSnowflake\nPrivate Service Connect\nâ\nPrivate Service Connect creates a secure, private connection between services running in Google Cloud Platform:\nGoogle BigQuery\nDocker-based offline extraction\nâ\nAtlan supports the offline extraction method for fetching metadata from supported sources. You will need to first extract the metadata yourself and then make it available in S3.\nFor offline extraction, Atlan uses the following:\nBase image   -  Ubuntu for SQL-based extraction, Alpine Linux for REST API extraction.\nProgramming language   -  Kotlin for SQL sources, Python for BI sources and event buses.\nDatabases\nâ\nAmazon Redshift\nDatabricks\nHive\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nSAP HANA\nSnowflake\nTeradata\nBI tools\nâ\nIBM Cognos Analytics\nLooker\nTableau\nThoughtSpot\nData movement tools\nâ\ndbt Core\nEvent buses\nâ\nAiven Kafka\nApache Kafka\nConfluent Kafka\nRedpanda Kafka\nMiners\nâ\nDatabricks\nTeradata\nS3 miner\nâ\nAmazon Redshift\nGoogle BigQuery\nHive\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nSnowflake\nTeradata\nKubernetes-based offline extraction\nâ\nRefer to\nHow to connect on-premises databases to Kubernetes\n, and then request sample ConfigMap and CronJob files for the following supported SQL connectors:\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nTags:\natlan\ndocumentation\nPrevious\nWhat is the crawler logic for a deprecated asset?\nNext\nConnectors and capabilities\nPrivate network link\nDocker-based offline extraction\nKubernetes-based offline extraction"
  },
  {
    "url": "https://docs.atlan.com/product/administration",
    "content": "Configure Atlan\nAdministration\nAdministration\nAtlan's administration features provide comprehensive tools for managing your data workspace. Configure user access, monitor system logs, and customize workspace settings to help your organization's data environment operate efficiently and securely while meeting your specific requirements.\nKey features\nâ\nð\nLogs\nMonitor system events and user activities through comprehensive logs. Track changes and troubleshoot issues with detailed event history.\nð§ª\nLabs\nAccess experimental features and beta functionality. Test new capabilities before they're generally available.\nð\nREADME templates\nStandardize documentation across your data assets with customizable templates. Maintain consistent information structure.\nFeatured guides\nâ\nView event logs\nLearn how to access and interpret system event logs\nEnable labs features\nDiscover how to activate and use experimental features\nCreate README templates\nSet up standardized templates for asset documentation\nTags:\natlan\ndocumentation\nNext\nAllow guests to request updates"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-guests-to-request-updates",
    "content": "Configure Atlan\nAdministration\nGet Started\nAllow guests to request updates\nOn this page\nAllow guests to request updates\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to allow guest users to request metadata updates.\nGuest users\nin Atlan can only suggest changes to asset metadata if enabled from the admin center. To allow guest users to\nrequest\nÂ updates, follow these steps.\nEnable guest users to request updates\nâ\nTo enable your guest users to request metadata updates:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder theÂ\nAccess control\nheading of the\nLabs\npage, turn on\nAllow guests to raise requests for metadata updates\n.Â\nYour guest users will now be able to\nraise requests\nfor metadata updates on assets! ð\nIf you'd like to disable this option for your guest users, follow the steps above and then turn it off.\nTags:\natlan\ndocumentation\nPrevious\nAdministration\nNext\nAllow members to view reports\nEnable guest users to request updates"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/authenticate-sso-credentials-to-query-data",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGuides\nAuthenticate SSO credentials to query data\nOn this page\nAuthenticate SSO credentials to query data\nWho can do this?\nAny\nAtlan user\nwith\ndata access to the asset\nand SSO credentials for the connection.\nOnce your connection admins have configured SSO authentication, you can query data using your SSO credentials. Atlan currently supports the following connectors for SSO authentication:\nAmazon Redshift\n-  currently only Okta is supported as the identity provider.\nGoogle BigQuery\n-  Atlan uses Google OAuth 2.0, which handles integration with all identity providers. However, Atlan has only validated the integration with Okta.\nSnowflake\n-  Atlan uses\nSnowflake External OAuth\nfor SSO authentication, thus supporting all\nSnowflake-supported identity providers\n.\nTo query data using shared credentials instead, refer to\nProvide credentials to query data\n.\nDid you know?\nConnections that require you to provide SSO credentials have a small icon next to them. If the connection you want to query has no icon, you can query it with its default shared credentials. You only need to authenticate for connections with this icon.\nSet up your SSO credentials\nâ\nAtlan supports SSO authentication for querying data from the following connections:\nAmazon Redshift\nâ\nAtlan supports Okta SSO authentication for Amazon Redshift connections. Before you can query data with SSO credentials, you will first need to\nenable Okta SSO authentication for Amazon Redshift\nin Atlan.\nTo set up your Okta SSO credentials for an Amazon Redshift connection:\nFrom the left menu of any screen, click\nInsights\n.\nUnder the\nExplorer\ntab on the left, use the dropdown to select the Amazon Redshift connection that requires SSO credentials.\nA\nSet up SSO authentication for Redshift\ndialog will appear. Click\nGet started\nto set up your SSO credentials for the connection:\nFor\nAuthentication\n,\nOkta\nis the default selection.\nFor\nUsername\n, enter your Okta username.\nFor\nPassword\n, enter the password for your Okta username.\nClick the\nTest Authentication\nbutton to confirm connectivity.\nOnce authentication is successful, click\nDone\n.\nClose the\nSSO authentication completed\ndialog to return to the query editor.\nYou can now\nrun queries\nusing your Okta SSO credentials! ð\nGoogle BigQuery\nâ\nAtlan supports Google OAuth 2.0 SSO authentication for Google BigQuery connections. Before you can query data with SSO credentials, you will first need to\nenable SSO authentication for Google BigQuery\nin Atlan.\nTo set up your SSO credentials for a Google BigQuery connection:\nFrom the left menu of any screen, click\nInsights\n.\nUnder the\nExplorer\ntab on the left, use the dropdown to select the Google BigQuery connection that requires SSO credentials.\nAn\nAuthorizing\ndialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click\nAllow\nto enable authentication.\nOnce authorization is successful, close the\nAuthorizing\ndialog to return to the query editor.\nYou can now\nrun queries\nusing your SSO credentials! ð\nSnowflake\nâ\nAtlan supports SSO authentication via Snowflake External OAuth for Snowflake connections. Before you can query data with SSO credentials, you will first need to\nenable SSO authentication for Snowflake\nin Atlan.\nTo set up your Snowflake OAuth credentials for a Snowflake connection:\nFrom the left menu of any screen, click\nInsights\n.\nUnder the\nExplorer\ntab on the left, use the dropdown to select the Snowflake connection that requires Snowflake OAuth credentials.\nAn\nAuthorizing\ndialog will appear. Once authorization is successful, close the\nAuthorizing\ndialog to return to the query editor.\n(Optional) To change roles and warehouses, click the connection name in the left menu or the\nEditor context\ntab with the connection name:\nFor\nRole\n, click the\nSelect role\ndropdown to select a granted role assigned to you in Snowflake. If no role is selected, Atlan will use the default\nPUBLIC\nrole in Snowflake for authentication.\nFor\nWarehouse\n, click the\nSelect warehouse\ndropdown to change warehouses.Â\nYou can now\nrun queries\nusing your Snowflake OAuth credentials! ð\nRemove your SSO credentials\nâ\nTo remove your SSO credentials for a connection:\nFrom the left menu of any screen, click\nInsights\n.\nUnder the\nExplorer\ntab on the left, use the dropdown to select the connection with SSO authentication enabled.\nFrom the upper right of the query editor, click the\nEditor context\ntab with the connection name.\nIn the\nEditor context\ndialog, hover over the timestamp and then click\nLog out\n.\nIn the\nLog out from Insights\ndialog, click\nLog out\nto confirm.\nDid you know?\nYou can refer to\ntroubleshooting connector-specific SSO authentication\nto troubleshoot any errors.\nTags:\natlan\ndocumentation\nPrevious\nHow to enable SAML 2.0 for SSO\nNext\nAuthenticate SSO credentials to view sample data\nSet up your SSO credentials\nRemove your SSO credentials"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/authenticate-sso-credentials-to-view-sample-data",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGuides\nAuthenticate SSO credentials to view sample data\nOn this page\nAuthenticate SSO credentials to view sample data\nWho can do this?\nAny\nAtlan user\nwith\ndata access to the asset\nand SSO credentials for the connection. Atlan will display a\n100-row sample of the data\n.\nOnce your connection admins have configured SSO authentication, you can view sample data using your SSO credentials. Atlan currently supports the following connectors for SSO authentication:\nAmazon Redshift\n-  currently only Okta is supported as the identity provider.\nGoogle BigQuery\n-  Atlan uses Google OAuth 2.0, which handles integration with all identity providers. However, Atlan has only validated the integration with Okta.\nSnowflake\n-  Atlan uses\nSnowflake External OAuth\nfor SSO authentication, thus supporting all\nSnowflake-supported identity providers\n.\nTo view sample data using shared credentials instead, refer to\nProvide credentials to view sample data\n.\nSet up your SSO credentials\nâ\nAtlan supports SSO authentication for viewing sample data from the following connections:\nAmazon Redshift\nâ\nAtlan supports Okta SSO authentication for Amazon Redshift connections. Before you can view sample data with SSO credentials, you will first need to\nenable Okta SSO authentication for Amazon Redshift\nin Atlan.\nTo set up your Okta SSO credentials for viewing sample data, for Amazon Redshift:\nOn the\nAssets\npage, click an Amazon Redshift asset to view its asset profile.\nFrom the asset profile, click\nSample data\n.\nA\nSet up SSO authentication for Redshift\ndialog will appear. Click\nGet started\nto set up your SSO credentials for the connection:\nFor\nAuthentication\n,\nOkta\nis the default selection.\nFor\nUsername\n, enter your Okta username.\nFor\nPassword\n, enter the password for your Okta username.\nClick the\nTest Authentication\nbutton to confirm connectivity.\nOnce authentication is successful, click\nDone\n.\nYou can now view sample data using your Okta SSO credentials! ð\nGoogle BigQuery\nâ\nAtlan supports Google OAuth 2.0 SSO authentication for Google BigQuery connections. Before you can view sample data with SSO credentials, you will first need to\nenable SSO authentication for Google BigQuery\nin Atlan.\nTo set up SSO credentials for viewing sample data, for Google BigQuery:\nOn the\nAssets\npage, click a Google BigQuery asset to view its asset profile.\nFrom the asset profile, click\nSample data\n.\nTo set up your SSO credentials for viewing sample data, click\nGet started\n.\nAn\nAuthorizing\ndialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click\nAllow\nto enable authentication.\nOnce authorization is successful, close the\nAuthorizing\ndialog.\nYou can now view sample data using SSO credentials! ð\nSnowflake\nâ\nAtlan supports SSO authentication via Snowflake External OAuth for Snowflake connections. Before you can view sample data with SSO credentials, you will first need to\nenable SSO authentication for Snowflake\nin Atlan.\nTo set up Snowflake OAuth credentials for viewing sample data, for Snowflake:\nOn the\nAssets\npage, click a Snowflake asset to view its asset profile.\nFrom the asset profile, click\nSample data\n.\nTo set up your Snowflake OAuth credentials for viewing sample data, click\nGet started\n.\nAn\nAuthorizing\ndialog will appear. Once authorization is successful, close the\nAuthorizing\ndialog.\nYou can now view sample data using Snowflake OAuth credentials! ð\nDid you know?\nIf your Atlan admin has enabled\nsample data download\n, you will be able to export sample data in a CSV file.\nRemove SSO credentials\nâ\nTo remove your SSO credentials for a connection:\nOn the\nAssets\npage, click an asset to view its asset profile.\nFrom the asset profile, click\nSample data\n.\nAt the bottom of the\nSample data\nscreen, hover over the timestamp and then click\nLog out\n.\nIn the\nLog out from sample data preview\ndialog, click\nLog out\nto confirm.\nDid you know?\nYou can refer to\ntroubleshooting connector-specific SSO authentication\nto troubleshoot any errors.\nTags:\natlan\ndocumentation\nPrevious\nAuthenticate SSO credentials to query data\nNext\nLimit SSO automatically creating users when they log in\nSet up your SSO credentials\nRemove SSO credentials"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/automate-policy-compliance",
    "content": "Build governance\nStewardship\nPolicy Management\nAutomate policy compliance\nOn this page\nAutomate policy compliance\nâ\nAvailable via the Advanced Policy & Compliances package\nWho can do this?\nYou must be an\nadmin user\nin Atlan to\nenable\n,\ncreate\n,\nmanage\n, and approve data governance policies.\nData governance policies form the backbone of effective data governance. These help you define how to store, manage, access, and use data within an organization. You can establish guiding principles, validation rules, and best practices for monitoring data and policy compliance.\nAn organization may have data that can be publicly available, secure and confidential, or a combination of both. Atlan can help you ensure that all your data assets are governed by data governance policies and used in compliance with applicable laws and regulations.\nThe policy center in Atlan helps you build policies to align with your organizational goals for securing and managing data. You can currently choose from six different types of policies:\nData quality\n-  maintain data accuracy, consistency, and reliability by establishing standards and processes for data validation, cleansing, and quality control.\nData privacy\n-  govern the collection, processing, and sharing of personal and/or sensitive data to ensure compliance with data protection regulations and safeguarding privacy rights.\nData security\n-  outline measures and controls to protect data from unauthorized access, breaches, and loss, often including encryption, access controls, and incident response procedures.\nData lifecycle\n-  define the stages of data from creation and usage to archival and disposal. This ensures data is retained only as long as necessary and compliant with legal and business requirements.\nData ethics\n-  set guidelines for responsible, ethical, and acceptable data use, and address issues such as bias, fairness, and responsible AI implementation.\nData definitions and models\n-  standardize data definitions, taxonomies, and models to ensure consistent and accurate understanding and usage of data across the organization.\nEnable policy center\nâ\nWho can do this?\nYou must be an\nadmin user\nin Atlan to enable the policy center module for your organization. As a prerequisite, you must have the\ngovernance workflows and inbox module enabled\n.\nThe policy center provides a single control plane to link your data governance policies to your assets in Atlan.\nTo enable the policy center for your Atlan users:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nLabs\n.\nOn the\nLabs\npage, under\nGovernance center\n, configure the following:\nEnsure that the\nGovernance Workflows and Inbox\nmodule is enabled\n.\nTurn on\nPolicy Center\nto\ncreate and enforce governance policies\non your assets in Atlan. In the\nEnable Policy Center\ndialog, click\nEnable\nto confirm.\nIf you'd like to disable the\nPolicy Center\nÂ module from your organization's Atlan workspace, follow the steps above to turn it off.\nOnce enabled, you can also temporarily disable the module and turn it on again as needed. For any policies you may have created, this will not result in any data loss.\nTags:\natlan\ndocumentation\nPrevious\nManage tasks\nNext\nCreate policies\nEnable policy center"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/multiple-azure-ad-tenants",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nFAQ\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nAtlan can currently only connect to any one Azure AD tenant.\nHowever, you can\nreach out to Atlan support\nto share more details about your organizational setup and requirements. For example, are your users currently assigned to separate tenants? This will help us better understand your use case.\nTags:\natlan\ndocumentation\nfaq-integrations\nPrevious\nMicrosoft Defender SSO error\nNext\nCan we use a Microsoft SSO login?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/restrict-querying-data-warehouse",
    "content": "Use data\nInsights\nFAQ\nCan we restrict who can query our data warehouse?\nCan we restrict who can query our data warehouse?\nYou can restrict who can query data at different levels of granularity:\nBlock all querying\nBlock all querying of a source\nBlock specific users from querying specific assets by\nasset\nor\ntag\nTags:\natlan\ndocumentation\nfaq-insights\nPrevious\nCan I turn off sample data preview for the entire organization?\nNext\nHow can I identify an Insights query in my database access log?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/configure-custom-domains-for-microsoft-excel",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nConfigure custom domains for Microsoft Excel\nOn this page\nConfigure custom domains for Microsoft Excel\nWho can do this?\nYou will need your Microsoft 365 administrator to complete these steps   -  you may not have access yourself. Before you begin, you may need to\nDetermine if Centralized Deployment of add-ins works for your organization\n.\nIf your Atlan tenant is hosted on a custom domain   -  for example,\nhttps://<your-tenant-name>.mycompany.com\nÂ   -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel.\nPrerequisites\nâ\nThe Atlan add-in must be centrally deployed from the Microsoft 365 admin center\n.\nYou must have access to a Windows machine to install Microsoft PowerShell.\nInstall PowerShell\nâ\nInstall PowerShell to deploy the Atlan add-in for custom domains:\nWindows: PowerShell comes pre-installed on most modern Windows systems. If you need to upgrade the package, you can download it from the\nMicrosoft website\n.\nmacOS: Open Terminal and run:\nbrew install --cask powershell\nLinux: Open Ubuntu and run:\nsudo apt-get update\nsudo apt-get install -y powershell\nFor other distributions, refer to Microsoft's\nPowerShell installation guide\n.\nConfigure the add-in for Microsoft Excel\nâ\nTo install the Atlan add-in directly in Microsoft Excel:\nOpen PowerShell and run the following command to install the required module:\nInstall-Module -Name O365CentralizedAddInDeployment -Scope CurrentUser\nRun the following command to import the module:\nImport-Module -Name O365CentralizedAddInDeployment\nRun the following command to connect to the organization add-in service:\nConnect-OrganizationAddInService\nIn the\nMicrosoft\nauthorization dialog, select an account to authenticate the connection.\n(Optional) Run the following command to list existing add-ins in the organization:\nGet-OrganizationAddIn\nRun the following command to set a custom domain for the Atlan add-in:\nSet-OrganizationAddInOverrides -ProductId <your-product-ID>  -AppDomains \"<your-custom-domain>\"\nReplace\n<your-product-ID>\nwith the product ID of the Atlan add-in.\nReplace\n<your-custom-domain>\nwith your organization's custom domain.\n(Optional) Troubleshooting add-in connectivity\nâ\nWho can do this?\nAny individual in your organization with access to Microsoft Excel and Atlan tenant on a custom domain.\nThis section is optional if you deployed the Atlan add-in for the first time in your organization after completing the steps above. Your users will be able to\nconnect Atlan to Microsoft Excel\nfrom your custom domain.\nHowever, if any user tried to set up the add-in prior to the configuration above, they may not able to connect Atlan to Microsoft Excel. In that case, Atlan recommends clearing the add-in cache using the following steps.\nClear Microsoft Excel Online cacheÂ\nâ\nTo clear the local storage of your Microsoft Excel online app, from Google Chrome:\nOpen a blank Microsoft Excel workbook.\nFrom the top right of your browser, click the vertical three dots icon for more menu options.\nFrom the more options menu, click\nMore Tools\nand then click\nDeveloper Tools\n.\nIn the top menu of developer tools, click\nApplication\n.\nIn the left menu of the\nApplication\ntab, under\nStorage\n, click\nLocal storage\n.\nFind an entry for\nhttps://***-excel.officeapps.live.com\n. Right-click on the entry and then click\nClear\n.\nRefresh the Microsoft Excel Online app in your browser.\nClear Microsoft Excel cache on Mac\nâ\nTo clear the Microsoft Excel cache on Mac:\nClose Microsoft Excel.\nUse the\nFinder\nto navigate to the\n/Users/<user>/Library/Containers/com.microsoft.Excel/Data/Library/Application   Support/Microsoft/Office/16.0/Wef.\nfolder.\nSearch for\na313dc5c-6ca5-4346-abfc-c84c57d4b9dc\nand delete all the files and folders returned in the search results.\nRestart your Microsoft Excel app.\nClear Microsoft Excel cache on Windows\nâ\nTo clear the Microsoft Excel cache on Windows:\nOpen Microsoft Excel.\nFrom the ribbon, navigate to\nFile\n>\nOptions\n>\nTrust Center\n>\nTrust Center Settings\n>\nTrusted Add-in Catalogs\n.\nIn\nTrusted Web Add-in Catalogs\n, select the checkbox\nNext time Office starts, clear all previously-started web add-ins cache\n.\nClose Microsoft Excel and then restart it to clear the add-in cache. Once it has restarted, the checkbox will be deselected automatically.\nTags:\natlan\ndocumentation\nPrevious\nBulk enrich metadata\nNext\nDownload impacted assets in Google Sheets\nPrerequisites\nInstall PowerShell\nConfigure the add-in for Microsoft Excel\n(Optional) Troubleshooting add-in connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/connections",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nConnectors\nAtlan's connectors enable you to integrate, catalog, and govern metadata from leading data platforms. These connectors help you discover, document, and manage your data assets within your organization's data ecosystem, providing a unified view of your data landscape.\nCore offerings\nâ\nð\nPreflight Checks\nVerify permissions and configuration before running crawlers for successful metadata extraction.\nð\nWorkflow Management\nFollow recommended sequences for running workflows to ensure proper metadata extraction and lineage construction.\nð\nConnectivity Options\nChoose from various connectivity methods including direct connections, private network links, and secure agent extraction.\nð\nMetadata Extraction\nComprehensive crawling of metadata including schemas, tables, views, and lineage information from supported sources.\nð¡\nFor optimal results, follow the recommended workflow order: crawl data stores first, then run data quality tools, mine query logs, run extract-load tools, transformation tools, and finally crawl business intelligence tools.\nTags:\nconnect\ndata integration\nconnectors\natlan\ndata sources\nNext\nManage connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-forms",
    "content": "Build governance\nStewardship\nForm Management\nCreate forms\nOn this page\nCreate forms\nWho can do this?\nYou must be an\nadmin user\nin Atlan to create and manage forms. Anyone with access to Atlan   -  admin, member, or guest user   -  can fill out forms while\nraising requests\n.\nYou can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more.\nYou can use forms to:\nStandardize request details, improving governance and automation.\nTrack responses and enable structured data collection from user requests.\nForms can currently be embedded within the following governance workflow templates only:\nAccess management\nNew entity creation\nCreate a form\nâ\nTo create a new form:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nForms\n.\nClick the\n+ New form\nbutton to create a new form.\nFor\nHeading\n, configure the following:\nFor\nHeading text\n, enter a name indicative of the form's purpose   -  for example,\nData Access Request\n.\n(Optional) For\nSubheading text\n, enter a brief description.\nClick anywhere on the screen to save the name and description.\nTo add a field to your form, in the\nConfigure\nform, enter the following details:\nFor\nType\n, select the type of value you expect users to use for this field:\nText\ntype allows free-form text values.\nDropdown\ntype allows creating a list of predefined set of values. For\nOptions\n, click\nAdd option\nto set as many values as you want for your dropdown list.\nEmail\ntype allows creating a text field for email addresses.\nNumber\ntype allows numeric values.\nDate\ntype allows date values in the UTC date format   -  year, month, day.\nCustom metadata property\ntype allows defining existing custom metadata properties. For\nProperty\nand\nOptions\n, select the associated\nproperty\nand\noptions\nfor your custom metadata structure.\nFor\nName\n, enter a name that describes the purpose of the input field.\nFor\nDescription\n, enter a description for your input field.\nFor\nPlaceholder text\n, add instructions to help users complete the field.\n(Optional) Toggle on\nRequired\nto prevent form submission if the field is empty.\n(Optional) Click the\n+ Add another field\nbutton to add more fields to your form.\n(Optional) Click an existing field to perform additional operations:\nClick the reorder icon to drag and drop the selected field to reorder your form.\nClick the trash icon to delete the selected field.\nClick the copy icon to create a duplicate copy of the selected field.\n(Optional) Once you have completed your form, in the top right of the screen, you can:\nClick\nPreview\nto view a draft of the form before publishing it.\nClick\nSave as draft\nto save your changes in a draft version and publish when ready.\nÂ Click\nPublish\nto publish your completed form immediately.\nYour form is now live! ð\nOnce published, you will be able to\nembed forms in governance workflows\nto collect more information from requesters.\nManage forms\nâ\nOnce you have created forms, you can manage and modify your forms and monitor responses from the\nForms\ndashboard. Form deletion is currently not supported.\nTo manage forms:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nForms\n.\nFrom the\nOverview\ntab, you can view the following:\nFilter your existing forms by\nPublished\n,\nDraft\n, or\nDisabled\nstatus.\nTo edit a form, click the name of your form and edit it.\nTo disable a form, select a form and then change the status from\nDraft\nor\nPublished\nto\nDisabled\n.\nChange to the\nMonitor responses\ntab to view all form responses:\nClick the\nSubmitted by\nfilter to filter responses from specific users.\nClick any response to view more details. You can also view all other responses to a specific form.\nFrequently asked questions\nâ\nCan Markdown syntax be used in the form description?\nâ\nAtlan currently does not support Markdown syntax in the form description or any other fields.\nTags:\natlan\ndocumentation\nPrevious\nRevoke data access\nNext\nTroubleshooting policies\nCreate a form\nManage forms\nFrequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/create-groups",
    "content": "Configure Atlan\nAccess control\nGet started\nCreate groups\nCreate groups\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to create groups.\nTo create a group in Atlan:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder\nWorkspace\nclick\nGroups\n.\nClick the\nCreate Group\nbutton.\nEnter details for the new group in\nCreate Group\n:\nFor\nName\nenter a descriptive name for the group.\n(Optional) ForÂ\nDescription\nenter an explanation of the group.\n(Optional) ForÂ\nSlack\nenter the name of an existing Slack channel for the group. (You do not need to include the # in the channel name.) Click\nTest your slack link\nto attempt to open the channel in your\nintegrated Slack workspace\n.\n(Optional) ForÂ\nUsers\nsearch for and add any users that should be in the group. (You can also add users later.)\n(Optional) To add this group for all new users automatically, enable\nMark as default\n.\nClick\nCreate Group\nbutton.\nYou now have a new group in Atlan! ð\nDid you know?\nOnce you've\nintegrated Slack\nand added the Slack channel for your group, simply click the Slack icon next to the group name in Atlan and you'll be redirected to the group channel on Slack. You can then post questions and share assets directly on that channel.\nTags:\natlan\ndocumentation\nPrevious\nInvite new users\nNext\nCreate persona"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-policies",
    "content": "Build governance\nStewardship\nPolicy Management\nCreate policies\nOn this page\nCreate policies\nâ\nAvailable via the Advanced Policy & Compliances package\nWho can do this?\nYou must be an\nadmin user\nin Atlan to\nenable\n,\ncreate\n,\nmanage\n, and approve data governance policies.\nYou can create a policy to document guidelines for the following:\nHow's data processed and managed within your organization?\nWho is responsible for the data under various circumstances?\nWhat can you do to reduce potential business problems from the improper use of data?\nBefore you can create a data governance policy, you must have an Atlan admin\nenable the policy center module\nin your Atlan workspace. To create a policy, complete the following steps.\nCreate a policy\nâ\nTo create a new policy:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPolicy center\n.\nFrom the\nPolicy Center\n, click the\n+ New policy\nbutton to create a new policy.\nIn the\nCreate a new policy\ndialog, enter the following details:\nFor\nPolicy name\n, enter a meaningful name for your policy.\nFor\nPolicy type\n, choose a policy type.\nFor\nOwners\n, assign individual users or groups as policy owners.\nClick\nCreate\nto get started.\nDefine the policy\nâ\nAdd a purpose\nâ\nOnce you have created a policy, you can define its purpose. This is the mission statement of your policy, where you can outline what you want the policy to accomplish.\nTo define the purpose of your policy:\nIn the\nOverview\ntab of the policy page, under\nPurpose\n, you can either:\nClick\nEdit\nto manually describe the purpose of your policy and then click\nSave\n.\nClick\nask Atlan AI\nto add an Atlan AI-generated description. In the\nGenerate purpose using Atlan AI\nform, enter the following details:\nFor\nEnter industry\n, enter the name of your industry   -  for example,\nFinance\n.\nTo define the area of impact of your policy, click the dropdown to select\nGlobal\n,\nRegional\n, or\nLocal policy\n.\nFor compliance type, select\nStandard\n,\nRegulation\n, or\nOther\n.\nFor\nEnter compliance\n, enter the compliance regulation.\nFor\nDescribe the policy\n, enter a brief description of your policy.\nClick\nGenerate Purpose\nto generate an Atlan AI-generated purpose.\n(Optional) Click\n+ Add resource\nto\nadd resources\nto your policy description.\nDescribe the policy\nâ\nYou can set out the best practices, goals, and guidelines for your policy document.\nTo describe your policy:\nSwitch to the\nPurpose\ntab of your policy page.\nIn the\nPolicy\nsection, click\nEdit\nto write your policy. You can either manually draft the policy description or use Atlan AI to do the same and then edit as needed.\nClick\nSave\nto save your changes.\n(Optional) Add policy exceptions\nâ\nA policy exception is a method for maintaining a policy but granting exceptions to authorized individuals or entities. Doing so will allow them to circumvent one or more restrictions.\nTo add a policy exception:\nIn the\nPurpose\ntab of the policy page, under\nPolicy Exceptions\n, click\nAdd policy exception\n.\nIn the\nNew policy exception\nform, enter the following details:\nFor\nException name\n, enter a meaningful name.\nFor\nPurpose\n, briefly describe the purpose of this exception.\nFor\nUsers\n, select the individual users or groups to whom this exception should apply.\nClick\nAdd exception\nto save your changes.\n(Optional) Click\n+ Add new\nto add more policy exceptions.\nDefine scope and rules\nâ\nDid you know?\nYour selected assets will not be linked to the draft policy until after it has been approved. It may also take a few hours after the policy has been approved for the assets to be linked while the linkage workflow runs in the background.\nSelect asset scope\nâ\nYou can determine the assets within the scope of your policy. Policy rules will only apply to the filtered subset of assets you select.\nTo select assets:\nSwitch to the\nScope & Rules\ntab of your policy page.\nFor\nAsset scope\n, use the asset filters to select the relevant assets. The operators and values will vary depending on the selected attributes.\n(Optional) To add more filters, click\nAdd filter\n.\n(Optional) To preview the assets included in the scope of your policy, click\nView all\n.\nClick\nSave scope\nto save asset selection.\n(Optional) To the right of any filter, click the three horizontal dots and then:\nTo remove a filter, click\nDelete\n.\nTo turn off a filter, click\nDisable\n. Click\nEnable\nto turn on any disabled filters.\nCreate compliance rules\nâ\nTo implement and enforce your policy, you can create a set of rules to specify permitted or restricted actions, enable compliance with data standards, and ensure accountability.\nIf the assets scoped to the policy do not comply with all the rules, Atlan will trigger an\nincident\nto alert you. This incident can help you understand the specific rules that have been violated by the assets, making them noncompliant with the policy.\nAtlan currently supports creating 10 rules per policy.\nTo define compliance rules:\nIn the\nScope & Rules\ntab of your policy page,\nAssets must comply with the scope defined above\nis the default rule for all policies. You must first determine your asset scope before you can create compliance rules.\nFor\nCompliance rules\n, use the attribute filters to create a rule with which scoped assets must comply. The operators and values will vary depending on the selected attributes. Atlan currently supports creating policy rules based on the following metadata attributes:\nCertificates\nOwners\nTerms\nTags\nCustom metadata\n(Optional) To add more rules, click\nAdd another rule\n.\nClick\nSave rules\nto save the rules you created for the policy. Atlan will scan scoped assets to ensure that these match all the rules. An incident will be triggered for any asset that does not comply with\nall\nthe policy rules.\n(Optional) To the right of any rule, click the three horizontal dots and then:\nTo remove a rule, click\nDelete\n.\nTo turn off a rule, click\nDisable\n. Click\nEnable\nto turn on any disabled rules.\nDefine policy validity\nâ\nTo define the validity period of your policy:\nIn the\nPolicy Details\nsidebar of the\nOverview\ntab, for\nValid till\n, click the pencil icon to set a validity period.\nFrom the calendar, set a date for when the policy will expire.\nFor\nReview period\n, click the pencil icon to set a review period. For\n...days before expiry\n, enter a numeric value for when the policy should be reviewed before its expiration date.\nBy default, Atlan will display a warning message on the policy 30 days prior to its expiration date. You can adjust the review period to set a different timeline. During the review period, you can either\nrevise the expiring policy\nor extend its validity period.\nSelect approval workflow\nâ\nTo select an approval workflow:\nSwitch to the\nRelationships\nÂ tab of your policy page.\nFrom the left menu, select\nApproval Workflows\n.\nIn the\nApproval Workflows\nsection, click\nAdd approval workflow\n.\nIn the\nSelect Approval Workflow\ndialog, click the relevant approval workflow for your policy.\n(Optional) Hover over\nApprovers\nto view a list of approvers.\nClick\nSave\nto save your selections.\n(Optional) Add terms related to this policy\nâ\nYou can add business context to your policies in Atlan.\nIn the\nOverview\ntab of the policy page, under\nLinked Terms\n, click the\n+\nbutton to add related terms.\n(Optional) Add related policies\nâ\nYou may want to group data governance policies by policy type, business function, and more. You can optionally create relationships between your policies in Atlan to build a more comprehensive framework of data governance.\nTo add related policies:\nSwitch to the\nRelationships\nÂ tab of your policy page.\nFrom the left menu, select\nRelated Policies\n.\nIn the\nRelated Policies\nÂ section, click\nAdd related policies\n.\nIn the left menu of the\nAdd policies related to\n... dialog, click the relevant policies to connect to your policy.\nClick\nAdd policies\nto save your selections.\nSubmit for approval\nâ\nOnce you have reviewed your policy, in the top right of your screen, click\nSubmit for approval\nto submit your policy for approval.\nIf the policy has been approved and the workflow linking the policy to your selected assets has run successfully, the policy you created will become active and govern linked assets.\nFor governed assets, linked policies will appear on the\nasset sidebar\n. You can hover over a linked policy in the asset sidebar to view details in a popover, including policy type, purpose, and certification status, and even navigate to the policy in the policy center.\nDid you know?\nIf you have any questions about setting up policies, head over to\nTroubleshooting policies\n.\nTags:\natlan\ndocumentation\nPrevious\nAutomate policy compliance\nNext\nManage policies\nCreate a policy\nDefine the policy\nDefine scope and rules\nDefine policy validity\nSelect approval workflow\n(Optional) Add terms related to this policy\n(Optional) Add related policies\nSubmit for approval"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/how-tos/create-a-purpose",
    "content": "Configure Atlan\nAccess control\nGet started\nCreate purpose\nOn this page\nCreate purpose\nWho can do this?\nYou will need to be an admin user to create purposes.\ndanger\nA purpose acts on at least one tag. This\ntag must be created\nbefore\ncreating the purpose.\nTo create a purpose:\nFrom the left menu of any screen, clickÂ\nGovernance\n.\nUnderÂ\nAccess Control\n, clickÂ\nPurposes\n.\nIf this is the first purpose, click the\nGet started\nbutton. Otherwise, in the top right, click the\nNew Purpose\nbutton.\nFor\nPurpose name\n, enter a meaningful name for the purpose.\n(Optional) Add a description for the purpose.\nIn the lower-left corner of the dialog, click\nSelect tag\n.\nSelect one or more tags from the list, and then click on the purpose box again to close the list.\nClickÂ\nCreate\nto create the purpose.\nYou now have an empty purpose.\nDid you know?\nThe purpose will not yet control any access. Your users can still use the purpose to quickly browse assets with any of the tags selected, though.\n(Optional) Add rich documentation\nâ\nTo add rich documentation describing the purpose:\nUnderÂ\nSummary\n, thenÂ\nChannels\n, add any Slack channels relevant to the purpose.\nUnderÂ\nResources\n,Â add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive   -  anything that has a URL.\nUnder\nReadme\n, write a richly formatted description of the purpose.\nAdd policies\nâ\nDid you know?\nFor the purpose to control access, you need to define one or more policies. Repeat the following steps for each set of users and permissions you want to control through the purpose.\nTo add policies to the purpose, from within the purpose:\nChange to theÂ\nPolicies\ntab.\nClick theÂ\nNew Policy\nbutton and choose the type of policy.\nAdd a metadata policy\nâ\nTo grant or restrict permissions to change metadata:\nChooseÂ\nMetadata policy\n.\nUnder\nName\n, briefly describe the policy's intention.\n(Optional) Under\nUsers and Groups\n, choose the users to whom to apply the policy. By default, all users will be included. To select others:\nIn theÂ\nUsers and Groups\nbox, click theÂ\nx\n.\nUnderÂ\nUsers and Groups\n, click theÂ\nAdd\nlink.\nSearch for and select the users and groups to control with the policy, and then click anywhere in theÂ\nMetadata policy\nsidebar.\n(Optional) For\nConfigure permissions\n, choose the\npermissions the policy will grant\n. By default, all permissions will be granted. To select others:\nTo the right ofÂ\nConfigure permissions\n, click theÂ\nEdit\nlink.\nSelect the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one.\nAt the bottom of the list, click\nSave\n.\n(Optional) ForÂ\nDeny selected permissions\n, choose whether you want to explicitly deny these permissions.\ndanger\nIf enabled, this will\noverride all grants\n_take_priority) of those permissions from any other policies for the same users.\nAt the bottom of theÂ\nMetadata policy\nsidebar, clickÂ\nSave\n.\nAdd a data policy\nâ\nTo\ngrant or restrict permissions\nto query or preview data:\nChooseÂ\nData policy\n.\nUnder\nName\n, briefly describe the policy's intention.\n(Optional) Under\nUsers and Groups\n, choose the users to whom to apply the policy. By default, all users will be included. To select others:\nIn theÂ\nUsers and Groups\nbox, click theÂ\nx\n.\nUnderÂ\nUsers and Groups\n, click theÂ\nAdd\nlink.\nSearch for and select the users and groups to control with the policy, and then click anywhere in theÂ\nData policy\nsidebar.\n(Optional) ForÂ\nQuerying Permissions\n, choose whether you want to explicitly deny the ability to query and preview data on these assets.\ndanger\nIf enabled, this will\noverride all grants\n_take_priority) of those permissions from any other policies for the same users. This will also deny at the table level   -  even if only 1 out of 100 columns in a table has the tag, previewing and querying will be denied for the entire table.\n(Optional) For\nConfigure permissions\n, choose the masking policy to apply. By default, no masking will be applied. To apply masking, under\nMasking (Optional)\n, select the type of masking to apply. If you are unsure what they do, hover over each one to see a more detailed description and an example.\nAt the bottom of theÂ\nData policy\nsidebar, clickÂ\nSave\n.\n(Optional) Set preferences\nâ\nDid you know?\nYou can also personalize the details users will see in the sidebar or filters menu when in a purpose. This is great to limit information overload, by showing only what is relevant to a given set of users.\nTo set preferences for the purpose:\nChange to theÂ\nPreferences\ntab of the purpose. From the left menu, configure the following:\nTo limit the asset types that should be visible to the purpose:\nClick\nAsset types\nto view asset type preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose.\nTo limit the\nout-of-the-box tabs\nthat should be visible to the purpose:\nClick\nAsset sidebar\nto view asset sidebar preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose.\nTo limit the\nasset filters\nthat should be visible to the purpose:\nClickÂ\nAsset filters\nto view asset filter preferences.\nClick the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose.\nTo limit the\ncustom metadata\nthat should be visible to the purpose:\nClick\nCustom metadata\nto view custom metadata preferences.\nClick the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the purpose.\nTags:\natlan\ndocumentation\nPrevious\nCreate persona\nNext\nManage users\n(Optional) Add rich documentation\nAdd policies\n(Optional) Set preferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-pipelines",
    "content": "Data Pipelines\nAtlan's Data Pipelines connectors enable you to integrate, catalog, and govern metadata from leading ETL tools and workflow orchestration platforms. These connectors help you document data transformations, track lineage, and manage the complete lifecycle of your data as it moves through various systems.\nKey concepts\nâ\nETL\n: Extract, transform, load processes that move and reshape data between systems\nOrchestration\n: Platforms that schedule, coordinate, and monitor data workflows\nLineage\n: Visual representation of data movement from source to destination\nTransformation\n: Rules and logic applied to data as it moves through pipelines\nCore offerings\nâ\nð\nProcess Tracking\nDocument ETL processes and monitor data movement across your organization\nð\nEnd-to-End Lineage\nVisualize complete data lineage from source through transformation to consumption\nð\nImpact Analysis\nUnderstand how changes to pipelines affect downstream systems and reports\nð\nCode Integration\nLink transformation code and business logic to your data assets in the catalog\nGet started\nâ\n1\nSelect your platform\nChoose the ETL tool or orchestration platform you want to connect from the sidebar\nâ\n2\nSet up credentials\nConfigure authentication and access permissions following the platform-specific guide\nâ\n3\nExtract metadata\nRun the crawler to capture lineage, transformation logic, and pipeline structure\nð¡\nUse the sidebar navigation to browse available connectors, including popular tools like dbt, Fivetran, Apache Airflow, Alteryx and Azure Data Factory.\nTags:\nconnect\ndata integration\netl\nworkflows\norchestration\nlineage\natlan\nconnectors\ndata"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks",
    "content": "Build governance\nData Quality Studio\nDatabricks Data Quality\nOn this page\nDatabricks Data Quality Studio\nPrivate Preview\nOverview:\nMonitor the quality of your Databricks assets in Atlan using Databricks' native data quality capabilities. This integration lets you create rules, track metrics, and view data quality insights directly within Atlan's discovery, lineage, and data products.\nGet started\nâ\nFollow these steps to set up Databricks as your data quality studio:\nSet up Databricks\nEnable data quality on a connection\nFAQ\nâ\nSetup and configuration\n: Common questions about Databricks setup and configuration.\nTags:\ndatabricks\ndata-quality\ngovernance\natlan\nNext\nSet up Databricks\nGet started\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains",
    "content": "Build governance\nDomains\nOn this page\nDomains\nOverview:\nDomains help you organize data assets in Atlan into logical, business-aligned structures. They provide a way to group related assets and establish ownership and governance boundaries across your data ecosystem.\nGet started\nâ\nFollow these steps to implement domains in Atlan:\nManage domains\nGuides\nâ\nOrganize assets\n: Learn how to organize your data assets into domains to improve discoverability and governance.\nTags:\ndomains\norganization\ngovernance\ndata assets\natlan\nNext\nManage domains\nGet started\nGuides"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-discovery-of-process-assets",
    "content": "Configure Atlan\nAdministration\nFeature Management\nHow to enable discovery of process assets\nOn this page\nEnable  discovery of process assets\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to enable discovery and tracking of process assets.\nProcesses\nrepresent the movement or transformation of assets in Atlan. By default, process assets are hidden on the\nassets page\nand\nreporting center\nto ensure an efficient asset search,\nfiltering\n, and discovery experience.\nTo create a more customizable experience for your users, you can turn on discovery and tracking of process assets.\nDid you know?\nEven if discovery of process assets is turned off, users will still be able to view process assets on the lineage graph and sidebar.\nEnable process asset discovery\nâ\nTo enable discovery and tracking of process assets:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAssets\nheading of the\nLabs\npage, turn on\nDiscover and track processes\n.Â\nYour users will now be able to search, filter, discover, and track\nprocess assets\n! ð\nIf you'd like to disable this feature, follow the steps above to turn it off.\nTags:\natlan\ndocumentation\nPrevious\nHow to enable associated terms\nNext\nHow to enable sample data download\nEnable process asset discovery"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-sample-data-download",
    "content": "Configure Atlan\nAdministration\nFeature Management\nHow to enable sample data download\nOn this page\nEnable  sample data download\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to enable sample data downloads.\nAtlan allows admin users to enable or disable downloading\nsample data\n. This can help you enforce better governance across your organization.\nTo enable sample data download, follow these steps\n.\nEnable sample data download\nâ\nTo enable sample data download:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nInsights\nheading of the\nLabs\npage, turn on\nDownload data\n.\nYour users will now be able to export\nsample data\nin a CSV file! ð\nIf you'd like to disable sample data download, follow the steps above to turn it off.\nTags:\natlan\ndocumentation\nPrevious\nHow to enable discovery of process assets\nNext\nHow to enable scheduled queries\nEnable sample data download"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-scheduled-queries",
    "content": "Configure Atlan\nAdministration\nFeature Management\nHow to enable scheduled queries\nOn this page\nEnable  scheduled queries\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to enable scheduled queries.\nTo enable scheduled queries, follow these steps.\nEnable scheduled queries\nâ\nTo enable scheduled queries:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nInsights\nheading of the\nLabs\npage, turn on\nSchedule queries\n.Â\nIn the\nTurn on scheduled queries\ndialog box, click\nYes\nto confirm.\nYour users will now be able to\nschedule queries\nin\nInsights\n! ð\nIf you'd like to disable scheduled queries, follow the steps above to turn it off.Â\nDid you know?\nOnce scheduled queries has been enabled, your users will also be able to\nexport large results via email\n.\nTags:\natlan\ndocumentation\nPrevious\nHow to enable sample data download\nNext\nRestrict asset visibility\nEnable scheduled queries"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/how-tos/enable-data-quality",
    "content": "Build governance\nData Quality Studio\nDatabricks Data Quality\nConfigure data quality\nEnable data quality on connection\nOn this page\nEnable data quality on connection\nPrivate Preview\nEnable data quality on your Databricks connection in Atlan to start monitoring data quality. This guide helps you configure the connection with the necessary credentials and permissions.\nPrerequisites\nâ\nBefore you begin, complete the following steps:\nSet up Databricks for data quality\ncompleted\nHave the service principal credentials created during Databricks setup\nIdentify the Databricks connection where you want to enable data quality\nEnable data quality\nâ\nFollow these steps to enable data quality on your Databricks connection.\nTurn on the data quality feature:\nNavigate to\nSettings\nin Atlan\nFind the\nLabs\nsection\nTurn on the\nData Quality\ntoggle\nSelect your connection and configure credentials:\nIMPORTANT\nCurrently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection,\nraise a support request\n.\nData Quality Page\nConnection Settings\nNavigate to\nGovernance\n>\nData Quality\nSelect your Databricks connection from the list\nClick\nEnable data quality\nfor your selected connection\nEnter the following credential details:\nClient ID\n: The service principal client ID created in Databricks setup\nClient Secret\n: The service principal client secret\nTenant ID\n: The tenant ID (Azure only)\nWorkspace URL\n: Your Databricks workspace URL\nSQL Warehouse\n: Your preferred SQL warehouse for DQ operations\nClick\nRun permissions check\nto verify:\nCredentials have necessary permissions in Databricks\nDatabricks setup completed correctly\nClick\nUpdate\nto save the credentials\nNavigate to\nGovernance\n>\nConnections\nSelect your Databricks connection\nOpen\nConnection settings\nfrom the sidebar\nEnter the following credential details:\nClient ID\n: The service principal client ID created in Databricks setup\nClient Secret\n: The service principal client secret\nTenant ID\n: The tenant ID (Azure only)\nWorkspace URL\n: Your Databricks workspace URL\nSQL Warehouse\n: Your preferred SQL warehouse for DQ operations\nClick\nRun permissions check\nto verify:\nCredentials have necessary permissions in Databricks\nDatabricks setup completed correctly\nClick\nUpdate\nto save the credentials\nNext steps\nâ\nAfter completing these steps:\nAtlan takes approximately 10 minutes to complete the setup in the background\nOnce finished, you'll see data quality options available on your Databricks assets\nYou can start creating data quality rules on tables and views\nNeed help\nâ\nIf you have questions or need assistance with enabling data quality on your connection, reach out to Atlan Support by\nsubmitting a support request\n.\nSee also\nâ\nData quality permissions\n- Learn about the data quality permission scopes and configuration\nConfigure alerts for data quality rules\n- Set up real-time notifications for rule failures\nTags:\ndatabricks\ndata-quality\nsetup\natlan\nPrevious\nSet up Databricks\nNext\nSetup and configuration\nPrerequisites\nEnable data quality\nNext steps\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/enable-data-quality",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nConfigure data quality\nEnable data quality on connection\nOn this page\nEnable data quality on connection\nPrivate Preview\nEnable data quality on your Snowflake connection in Atlan to start monitoring data quality. This guide helps you configure the connection with the necessary credentials and permissions.\nPrerequisites\nâ\nBefore you begin, complete the following steps:\nSet up Snowflake for data quality\ncompleted\nHave the credentials for the\natlan_dq_user\ncreated during Snowflake setup\nIdentify the Snowflake connection where you want to enable data quality\nEnable data quality\nâ\nFollow these steps to enable data quality on your Snowflake connection.\nTurn on the data quality feature:\nNavigate to\nSettings\nin Atlan\nFind the\nLabs\nsection\nTurn on the\nData Quality\ntoggle\nSelect your connection and configure credentials:\nIMPORTANT\nCurrently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection,\nraise a support request\n.\nData Quality Page\nConnection Settings\nNavigate to\nGovernance\n>\nData Quality\nSelect your Snowflake connection from the list\nClick\nEnable data quality\nfor your selected connection\nEnter the following credential details:\nUsername\n: The\natlan_dq_user\ncreated in Snowflake setup\nPassword\n: The password for your Atlan DQ user\nRole\n:\natlan_dq_service_role\nWarehouse\n: Your preferred warehouse for DQ operations\nClick\nRun permissions check\nto verify:\nCredentials have necessary permissions in Snowflake\nSnowflake setup completed correctly\nClick\nUpdate\nto save the credentials\nNavigate to\nGovernance\n>\nConnections\nSelect your Snowflake connection\nOpen\nConnection settings\nfrom the sidebar\nEnter the following credential details:\nUsername\n: The\natlan_dq_user\ncreated in Snowflake setup\nPassword\n: The password for your Atlan DQ user\nRole\n:\natlan_dq_service_role\nWarehouse\n: Your preferred warehouse for DQ operations\nClick\nRun permissions check\nto verify:\nCredentials have necessary permissions in Snowflake\nSnowflake setup completed correctly\nClick\nUpdate\nto save the credentials\nNext steps\nâ\nAfter completing these steps:\nAtlan takes approximately 10 minutes to complete the setup in the background\nOnce finished, you'll see data quality options available on your Snowflake assets\nYou can start creating data quality rules on tables and views\nNeed help\nâ\nIf you have questions or need assistance with enabling data quality on your connection, reach out to Atlan Support by\nsubmitting a support request\n.\nSee also\nâ\nOperations\n- Learn about the data quality operations and monitoring capabilities\nConfigure alerts for data quality rules\n- Set up real-time notifications for rule failures\nTags:\nsnowflake\ndata-quality\nsetup\natlan\nPrevious\nSet up Snowflake\nNext\nEnable auto re-attachment of rules\nPrerequisites\nEnable data quality\nNext steps\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary",
    "content": "Build governance\nGlossary\nOn this page\nGlossary\nOverview:\nCreate and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization. Gain visibility into business context and meaning for your data assets.\nGet started\nâ\nFollow these steps to create and manage your business glossary in Atlan:\nSet up glossaries\nGuides\nâ\nBulk upload terms in the glossary\nLink terms to assets\nConcepts\nâ\nWhat is a glossary\nTroubleshooting\nâ\nWhy am I unable to approve a glossary update request\nFAQ\nâ\nCan I add duplicate glossary terms\nWhat is the default permission for a glossary\nCan I use personas to update a term in a glossary\nCan I create backups of glossaries\nHow to fully delete glossary terms or archived items\nTags:\nglossary\nterminology\ndefinitions\ngovernance\natlan\nNext\nSet up glossaries\nGet started\nGuides\nConcepts\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/google-dashboard-login-error",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nTroubleshooting\nGoogle Dashboard login error\nOn this page\nGoogle Dashboard login error\nWhy do I get an error while logging in via Google Dashboard?\nâ\nWhen logging in from your Google Dashboard, you may receive an\nInvalid request\nerror.\nThis is a known problem with Google for which there isn't a clear solution. Here are some references about this known issue:\nreference 1\nand\nreference 2\n.\nWhile the login initiated via your Atlan URL works, the IdP initiated login unfortunately doesn't due to the problem described in the above links. We currently do not have a path to resolution from Google.\nTags:\natlan\ndocumentation\nfaq-integrations\nPrevious\nOkta first-time login authentication error\nNext\nMicrosoft Defender SSO error"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/faq/reporting-materialized-views",
    "content": "Use data\nReporting\nFAQ\nHow do I see views instead of materialized views in the reporting center?\nHow do I see views instead of materialized views in the reporting center?\nOn the\nAssets\ndashboard in the\nreporting center\n, click\nView\nin the\nAll Asset Types\ndropdown menu toÂ only see views. You can further filter by selecting a specific connector or connection as well.\nTags:\natlan\ndocumentation\nPrevious\nReport on assets\nNext\nIs there a dashboard to see how my metadata is populated?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/how-do-i-use-the-filters-menu",
    "content": "Use data\nDiscovery\nConfiguration\nHow do I use the filters menu?\nHow do I use the filters menu?\nTo learn how to use the filters menu in Atlan, follow the instructions in this\nguide\n.\nTags:\natlan\ndocumentation\nPrevious\nAdd custom metadata\nNext\nHow to interpret timestamps"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/parameterized-queries",
    "content": "Use data\nInsights\nFAQ\nHow to use parameterized queries?\nHow to use parameterized queries?\nWhen querying in Insights, you can define custom parameters to\nmake your query interactive\n. You can define values for your custom parameters, specify data types, and incorporate them into your query.\nTags:\natlan\ndocumentation\nfaq-insights\nPrevious\nMonitor for runaway queries?\nNext\nWhat controls the frequency of queries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/faq/dashboard-metadata",
    "content": "Use data\nReporting\nFAQ\nIs there a dashboard to see how my metadata is populated?\nIs there a dashboard to see how my metadata is populated?\nAtlan's\nreporting center\nprovides you with a composite view of your metadata across all the sources integrated with Atlan. You can track metrics for asset enrichment, see metadata updates over time, review personas and purposes, and much more.\nTags:\natlan\ndocumentation\nPrevious\nHow do I see views instead of materialized views in the reporting center?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/limit-sso-automatically-creating-users-when-they-log-in",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGuides\nLimit SSO automatically creating users when they log in\nLimit SSO automatically creating users when they log in\nOnly users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan.\nTo restrict access to certain users, edit the list of users configured for Atlan in your SSO provider to a limited set of users.\nEach time a new user needs to be onboarded, they will need to be added to this list in your SSO provider before they can access Atlan. Without being added to this list, no user in Atlan will be automatically created even if they attempt to log in.\nTags:\natlan\ndocumentation\nPrevious\nAuthenticate SSO credentials to view sample data\nNext\nSet default user roles for SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-policies",
    "content": "Build governance\nStewardship\nPolicy Management\nManage policies\nOn this page\nManage policies\nWho can do this?\nYou must be an\nadmin user\nin Atlan to\nenable\n,\ncreate\n,\nmanage\n, and approve data governance policies.\nOnce you have created policies, you can manage and revise your policies, monitor policy breaches, report incidents, and more from the\nPolicy\ncenter\ndashboard.\nMonitor policies\nâ\nTo monitor your policies and take action:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPolicy center\n.\nFrom the\nOverview\ntab in the\nPolicy Center\n, you can:\nReview the\nAction Required\nsection:\nClick\nOpen incidents\nto take action on any open incidents. You can filter incidents by\nClosed\n,\nOpen\n, or\nIn-progress\nstatus. Select any incident to view more details and assets linked to the incident.\nClick\nPolicy breaches\nto examine policy breaches, whichÂ occur when any one condition is breached or violated for a particular policy.\nView the total count of ungoverned assets in your Atlan workspace. Ungoverned assets refer to assets that are not governed by any policy in Atlan.\nClick\nPolicies to approve\nto review and approve policies pending your approval.\nIn the\nPolicies\nsection, view recent, draft, or starred policies.\nIn the\nAt a glance\nsection, view policies by policy type.\nUnder\nMy Policies\n, view all the policies you have either created or are designated as an owner.\nChange to the\nAll policies\ntab to view all the policies in your Atlan workspace. You can filter policies by status   -\nActive\n,\nDraft\n, or\nDeprecated\n. Select any policy to view more details.\nChange to the\nReporting\ntab to monitor activity related to all policies in Atlan.\nFor\nAll Policies\n, visualize your policies in Atlan by type or status.\nFor\nAssets governed over time\n, visualize trends in your governed assets over time.\nFor\nActive policies by type\n, view active policies by policy type.\nFor\nAssets with multiple policies\n, view assets that fall under the purview of multiple policies.\nFor\nPolicies with exceptions\n, view policies with defined exceptions.\nFor\nActivity\n, view an activity log for all your policies in Atlan.\nRevise a policy\nâ\nAtlan currently only supports revising an active policy that is within its validity period.\nIf a policy comes to the end of its validity period and you decide not to extend or revise it, the policy will be deprecated at the end of its validity period. Any assets within the scope of that expired policy will be automatically delinked and no new incidents will be generated. You will still be able to view the deprecated policy and assets within the scope of that policy in the policy center.\nIf you revise an existing policy, Atlan will create a new draft of that existing policy with the same details. You can then revise the policy details and set a new validity period. Only when the new version has been approved and becomes active, the previous version will be deprecated, and stop scanning assets and generating any new incidents.\nNote that the workflow linking the revised policy to your selected assets has to run successfully in the background for the revised policy to become active and govern linked assets.\nTo revise an existing policy:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPolicy center\n.\nSelect a policy to revise.\nFrom the top right, click the\nRevise policy\nbutton. This will create a new draft version of your existing policy. The previous policy will still be under enforcement until the new policy is approved, becomes active, and successfully linked to selected assets.\nEdit the draft policy to make any changes, such as updating the validity period.\nSubmit the revised policy for approval. Only when the new version has been approved and becomes active, the previous version will be deprecated.\nYou can only delete a draft or published policy in Atlan if you're an owner of that policy.\nReport an incident\nâ\nOnce you have created data governance policies, Atlan will scan governed assets for any incidents and report them in near real time.\nIf any changes not compliant with your policy definition and\ncompliance rules\nare detected among governed assets, Atlan will generate incidents automatically and notify the policy owners from the policy center. You can then take action on open incidents.\nIn addition to automated incident reporting, you can manually report incidents that may warrant attention.\nTo manually report an incident:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nPolicy center\n.\nUnder\nAction Required\n, click\nOpen incidents\nto open the\nIncidents\nsidebar.\nFrom the\nIncidents\nsidebar, click\nAdd new incident\nto manually report an incident.\nIn the\nReport new incident\n, enter the following details:\nFor\nBrief Description\n, enter a brief description of the issue you have detected.\nFor\nIn-depth description\n, add details about the issue, including steps to reproduce the issue.\n(Optional) For\nRelated Policy\n, select an impacted policy, if any.\n(Optional) For\nAdd assets\n, select any and all impacted assets that may apply.\nClick\nSubmit incident\nto submit your incident report.\nTags:\natlan\ndocumentation\nPrevious\nCreate policies\nNext\nRevoke data access\nMonitor policies\nRevise a policy\nReport an incident"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/manage-system-announcements",
    "content": "Configure Atlan\nIntegrations\nCommunication\nSMTP and Announcements\nManage system announcements\nOn this page\nManage system announcements\nWho can do this?\nYou will need to be an admin user in Atlan to manage system announcements.\nHave you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape.\nSystem announcements appear on the homepage of all users, in an announcement box under\nRecent updates\n.\nDid you know?\nYou can only create one system announcement per instance. To add more information to your system announcement, you can either edit the existing one to update it or delete the old one and create a new system announcement.\nAdd a system announcement\nâ\nTo add a system announcement:\nFrom the left menu of any screen, clickÂ\nAdmin\n.\nIn the upper right of the page, click the\nNew announcement\nbutton.\n(Optional) At the top of the dialog, click on\nInformation\nto change the style of announcement:\nTo give general information, in blue, choose\nInformation\n.\nTo display a problem, in red, chooseÂ\nIssue\n.\nTo give a warning, in yellow, chooseÂ\nWarning\n.\nForÂ\nAdd announcement header...\nenter a brief title for your announcement.\nForÂ\nAdd description...\nenter the detailed explanation for the announcement. (Optional) You can use Markdown syntax to write a description. Any headings will be rendered in heading 6 (\n<h6>\n).\ndanger\nAtlan currently does not support adding images to your announcements.\nAt the bottom of the dialog, clickÂ\nSave\n.\nYour announcement will now appear on the home page of every user that visits Atlan! ð\nRemove a system announcement\nâ\nTo remove a system announcement:\nFrom the left menu of any screen, clickÂ\nAdmin\n(or from the homepage where the announcement is displayed).\nIn the upper-right corner of the announcement box, click the 3-dot button.\nFrom the options, clickÂ\nDelete\n.\nYour announcement has now been removed from the home page of every user that visits Atlan! ð\nTags:\natlan\ndocumentation\nPrevious\nCreate announcements\nNext\nIdentity Management Integrations\nAdd a system announcement\nRemove a system announcement"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/monitor-runaway-queries",
    "content": "Use data\nInsights\nFAQ\nMonitor for runaway queries?\nMonitor for runaway queries?\nAtlan provides a default 10-minute timeout for all\nconnectors\n. For example, if a query runs for more than 10 minutes, it will be aborted to safeguard against query abuse.\nTags:\natlan\ndocumentation\nfaq-insights\nPrevious\nHow can I identify an Insights query in my database access log?\nNext\nHow to use parameterized queries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains/how-tos/organize-assets",
    "content": "Build governance\nDomains\nAsset Organization\nHow to organize assets\nOn this page\nOrganize assets\nWho can do this?\nAny\nnon-guest user with edit access to an asset's metadata\ncan add assets to domains. This only includes admin and member users.\nDomain policies\ncurrently do not have any impact outside the\nproducts module\n.\nYou can map and organize your assets into\ndomains and subdomains\n. Domains provide you with a logical structure to group and govern your assets that aligns with business needs and ensures a curated discovery experience.\nTo add assets to a domain, note the following:\nYou can link assets to domains irrespective of whether or not you use\ndata products\n.\nYou can only add assets to any one specific domain or subdomain. Assets may be used across multiple domains, but can only belong to one domain or subdomain.\nYou can filter assets by a single domain, multiple domains, or no domains.\nAtlan currently does not support adding glossaries, categories, and terms to domains.\nAtlan currently does not support\nraising a request\nto add assets to domains.\nAdmin users can bulk add assets to domains using\nplaybooks\n.\nAdd an asset to a domain\nâ\nDid you know?\nYou can also\nset up playbooks\nto bulk add assets to your domains and subdomains. You will need to be an admin user in Atlan to create playbooks.\nTo add an asset to a domain, complete the following steps.\nTo add an asset to a domain:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nOn the\nAssets\npage, click an asset to open the asset sidebar.\nIn the\nOverview\nsidebar, under\nDomains\n, click\nAdd to domain\n.\nIn the popup, check the boxes to select the domain or subdomain to which you want to add the asset. You can only select any one parent domain or nested subdomain.\n(Optional) Hover over the linked domain or subdomain to view details in a popover, including the user that added the domain. You can also:\nClick\nView domain\nto view the domain profile from the governance center.\nIf the\nproducts module is turned off\n, you will need to be an admin user in Atlan to view the domain.\nIf the\nproducts module is turned on\n,\ndomain policies\nwill determine your ability to view the domain.\nClick the unlink icon to remove the asset from the linked domain.\n(Optional) Click the pencil icon to change to a different domain or remove it from the asset.\n(Optional) In the\nFilters\nmenu\non the left, click\nDomains\nto filter assets by domains:\nCheck the boxes to select one or more domains or subdomains to filter your assets.\nClick\nNo domains\nto filter assets not mapped to any domain.\nDid you know?\nTo programmatically add assets to a domain or remove them from a linked domain, refer to our\ndeveloper documentation\n.\nTags:\natlan\ndocumentation\nPrevious\nManage domains\nAdd an asset to a domain"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/set-up-a-private-network-link-to-amazon-msk",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nGet Started\nSet up a private network link to Amazon MSK\nOn this page\nSet up a private network link to Amazon MSK\nWho can do this?\nYou will need your Amazon MSK or AWS administrator involved   -  you may not have access to run these tasks.\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS, ensuring that traffic between services remains within the AWS network. This document describes the steps to set this up between Amazon MSK and Atlan.\nPrerequisites\nâ\nBefore you can set up private network connectivity, ensure the following:\nAmazon MSK version: Apache Kafka 2.7.1 or higher.\nAuthentication type: only IAM role-based authentication is supported.\nCluster instance type: must be larger than t3.small.\nRegion alignment: both your Amazon MSK cluster and Atlan tenant must reside in the same AWS region.\nFor more information, refer to\nRequirements and Limitations for Multi-VPC Private Connectivity\n.\nRequest Atlan's details\nâ\nTo configure private network connectivity between your AWS account and Atlan,\ncontact Atlan support\nfor the following details:\nAtlan's AWS account ID\nEnable private network link\nâ\nTo verify or enable AWS PrivateLink for Amazon MSK:\nSign in to the AWS Management Console and open the\nAmazon MSK Console\n.\nFrom the left menu, click\nClusters\n.\nOn the\nClusters\npage, under\nCluster name\n, select the cluster for which you want to enable private network link.\nOn your cluster page, below the overview section, click the\nProperties\ntab.\nIn the\nProperties\ntab, navigate to the\nNetworking settings\nsection to verify or enable AWS PrivateLink connectivity:\nIf you have verified that AWS PrivateLink is turned on, skip to the next section.\nIf AWS PrivateLink is turned off, click the\nEdit\nbutton and then click\nTurn on multi-VPC connectivity\nto enable it.\nIn the\nTurn on multi-VPC connectivity\npage, for\nAuthentication type\n, click\nIAM role-based authentication\n.\nAt the bottom of the screen, click\nTurn on selection\n. The cluster will undergo a rolling update, which may take several minutes to a few hours to complete.\nGrant access to Atlan\nâ\nOnce AWS PrivateLink is enabled for your Amazon MSK cluster, you will need to update the cluster policy to grant access to Atlan.\nTo update your Amazon MSK cluster policy:\nSign in to the AWS Management Console and open the\nAmazon MSK Console\n.\nFrom the left menu, click\nClusters\n.\nOn the\nClusters\npage, under\nCluster name\n, select the cluster for which you enabled private network link.\nOn your cluster page, below the overview section, click the\nProperties\ntab.\nIn the\nProperties\ntab, navigate to the\nSecurity settings\nsection and then click\nEdit cluster policy\n.\nIn the\nEdit cluster policy\npage, under\nCluster policy\n, configure the following:\nClick\nBasic\nas the new cluster policy.\nFor\nAccount ID(s) that need cluster access\n, enter Atlan's AWS account ID.\nClick the\nInclude Kafka service principal\ncheckbox to allow Atlan access to Kafka services only.\nClick\nSave changes\nto save your selections.\nNotify Atlan support team\nâ\nOnce you've completed the steps above,\ncontact the Atlan support team again\nand provide the following details for your Amazon MSK cluster:\nAmazon MSK Cluster ARN   -  the unique identifier of your cluster\nAtlan will create a\nmanaged VPC connection to your Amazon MSK cluster\n. Once completed, Atlan support will send you the cluster connection string (bootstrap servers) required for accessing Amazon MSK via AWS PrivateLink.\nYou can now enter the cluster connection string for the\nBootstrap servers\nfield to\ncrawl Amazon MSK\n. Atlan will securely connect to your Amazon MSK cluster using AWS PrivateLink.\nTags:\natlan\ndocumentation\nPrevious\nSet up Amazon MSK\nNext\nCrawl Amazon MSK\nPrerequisites\nRequest Atlan's details\nEnable private network link\nGrant access to Atlan\nNotify Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-a-private-network-link-to-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nGet Started\nSet up a private network link to Amazon Redshift\nOn this page\nSet up a private network link to Amazon Redshift\nWho can do this?\nYou will need your Amazon Redshift administrator or AWS administrator involved   -  you may not have access to run these tasks.\nRedshift-managed VPC endpoints create a secure, private connection between services running in AWS. This document describes the steps to set this up between Amazon Redshift and Atlan, when you use our Single Tenant SaaS deployment.\nPrerequisites\nâ\nYour Redshift cluster must be an RA3 node type.\nYour Redshift cluster must have\ncluster relocation turned on\n.\nYour Redshift cluster must be available through port 5439.\nYou must have spare capacity in your\nVPC endpoint quota\n.\n(For all details, see\nWorking with Redshift-managed VPC endpoints in Amazon Redshift\n.)\nRequest Atlan's details\nâ\nBefore granting access to your Redshift cluster to Atlan, you will need the following:\nAtlan's AWS account ID\nAtlan's VPC ID for the connection\nRequest these from Atlan support\n.\nGrant access to Atlan\nâ\nOnce you've received the details above, to\ngrant Atlan access to your Redshift cluster\n:\nSign in to the AWS Management Console and open the\nAmazon Redshift console\n.\nFrom the navigation menu, click\nClusters\n.\nFrom the table, click the name of the cluster to which you want to grant access.\nChange to theÂ\nProperties\ntab of the cluster.\nUnder theÂ\nGranted accounts\nsection, clickÂ\nGrant access\n.\nIn theÂ\nGrantee information\nform:\nForÂ\nAWS account ID\n, enter the Atlan AWS account ID.\nForÂ\nVPC\n, choose\nGrant access to specific VPCs\nand enter the Atlan VPC ID.\nAt the bottom right, click the\nGrant access\nbutton.\nNotify Atlan support team\nâ\nOnce you've completed the steps above,\ncontact the Atlan support team again\nand provide the following details for your Redshift cluster:\nAWS account ID\nRedshift cluster identifier   -  the unique identifier of your cluster\nAtlan will\ncreate a Redshift-managed VPC endpoint\n, and then reply to you with a hostname.\nWhen you use this hostname in the configuration for\ncrawling\n, Atlan will connect to Redshift over the private network.\nTags:\natlan\ndocumentation\nPrevious\nHow to enable SSO for Amazon Redshift\nNext\nCrawl Amazon Redshift\nPrerequisites\nRequest Atlan's details\nGrant access to Atlan\nNotify Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-a-private-network-link-to-microsoft-sql-server-on-amazon-ec2",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nPrivate Network\nSet up a private network link to Microsoft SQL Server on Amazon EC2\nOn this page\nSet up a private network link to Microsoft SQL Server on Amazon EC2\nWho can do this?\nYou will need your AWS administrator to complete these tasks   -  you may not have access yourself.\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Microsoft SQL Server on Amazon EC2 and Atlan.\nPrerequisites\nâ\nYou should already have the following:\nYour own non-default VPC configured in AWS.\nA Microsoft SQL Server on Amazon EC2 instance running in AWS, linked to the non-default VPC.\nPrivate subnets defined within the non-default VPC sufficient for availability.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from Atlan support\n.\nCreate security group\nâ\nYou will need to create a security group for the following:\nMicrosoft SQL Server on Amazon EC2 instance\nNetwork Load Balancer\n(NLB)\nMicrosoft SQL Server on Amazon EC2 instance\nâ\nYou can either create a new security group or add the following rule to an existing security group already attached to your Microsoft SQL Server on Amazon EC2 instance.\nTo create a security group for your Microsoft SQL Server on Amazon EC2 instance:\nOpen the\nAmazon VPC console\n.\nFrom the left menu, under\nSecurity\n, click\nSecurity Groups\n.\nClick the\nCreate security group\nbutton.\nEnter a name and description for the new security group.\nFrom the\nVPC\nlist, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located.\nFor\nInbound rules\n, leave this blank until after you have\ncreated a security group for the Network Load Balancer\n. Return to this step once you have created it, click the\nAdd rule\nbutton, and then add the following rule:\nFor\nType\n, use\nMSSQL\nif you are using the default port (1433), or use\nCustom\nand enter your port under\nPort range\n.\nFor\nDestination\n, add the\nsecurity group you created for the NLB\n.\nClick\nCreate security group\nto finish setup.\nNetwork Load Balancer\nâ\nTo create a security group for the Network Load Balancer:\nOpen the\nAmazon VPC console\n.\nFrom the left menu, under\nSecurity\n, click\nSecurity Groups\n.\nClick the\nCreate security group\nbutton.\nEnter a name and description for the new security group.\nFrom the\nVPC\nlist, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located.\nFor\nOutbound rules\n, click the\nAdd rule\nbutton and then add the following rule:\nFor\nType\n, use\nMSSQL\nif you are using the default port (1433), or use\nCustom\nand enter your port under\nPort range\n.\nFor\nDestination\n, add the\nsecurity group you created for your Microsoft SQL Server on Amazon EC2 instance\n.\nClick\nSave\n.\nClick\nCreate security group\nto finish setup.\nCreate a target group\nâ\nTo create a target group for the NLB:\nOpen the\nAmazon EC2 console\n.\nFrom the left menu, under\nLoad Balancing\n, click\nTarget Groups\n.\nClick\nCreate target group\n.\nFor\nBasic configuration\n, enter the following details:\nFor\nChoose a target type\n, keep\nInstances\n.\nFor\nTarget group name\n, enter a unique name for the new target group.\nFor\nProtocol\n, select\nTCP\n.\nFor\nPort\n,Â enterÂ\n1433\n.\nFor\nIP address type\n, selectÂ\nIPv4\n.\nFor\nVPC\n, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located.\nIn the\nHealth checks\nsection, change the protocol to\nTCP\nand keep\nAdvanced health check settings\nas the default.\nClick\nNext\nto proceed.\nTo register your Amazon EC2 instance, on the\nRegister targets\npage:\nFor\nAvailable instances\n, select your Amazon EC2 instance running Microsoft SQL Server.\nKeep the default port\n1433\nand then choose\nInclude as pending below\n.\nAt the bottom of the form, click theÂ\nCreate target group\nbutton.\nCreate internal Network Load Balancer\nâ\nTo create an NLB:\nOpen the\nAmazon EC2 console\n.\nFrom the left menu, underÂ\nLoad Balancing\n, click\nLoad Balancers\n.\nAt the top of the screen, click theÂ\nCreate Load Balancer\nbutton.\nUnder theÂ\nNetwork Load Balancer\noption, click theÂ\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name\n, enter a unique name.\nForÂ\nScheme\n, selectÂ\nInternal\n.\nForÂ\nIP address type\n, selectÂ\nIPv4\n.\nEnter the followingÂ\nNetwork mapping\nsettings for the load balancer:\nForÂ\nVPC\n, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located.\nForÂ\nMappings\n, select the availability zones with private subnets.\nFor\nSecurity groups\n, select the security group you created for the Network Load Balancer.\ninfo\nðª\nDid you know?\nThe\nEnforce inbound rules on PrivateLink traffic\nsetting is turned on by default and cannot be modified until after the load balancer has been created. If this setting is left on, you will need to\ncontact Atlan support\nand request the CIDR range of Atlan's cluster to add as an inbound rule on the\nNLB security group\n. To turn it off, follow\nthese instructions\n.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nProtocol\n, select\nTCP\n.\nFor\nPort\n, enter\n1433\n.\nFor\nTarget group\n, select the target group you created.\nReview your configuration, and click\nCreate load balancer\n.\nVerify target group is healthy\nâ\nTo verify that the target group is healthy:\nFrom the EC2 menu on the left, underÂ\nLoad Balancing\n, clickÂ\nTarget Groups\n.\nFrom theÂ\nTarget groups\ntable, click the link to the target group you created above.\nAt the bottom of the screen, under theÂ\nDetails\ntab, check that there is a 1 under bothÂ\nTotal targets\nandÂ\nHealthy\n. (Note: This number could be more than 1 if you have a multi-node deployment.)\nCreate endpoint service\nâ\nTo create an endpoint service:\nOpen the\nAmazon VPC console\n.\nFrom the left menu, under\nVirtual private cloud\n, click\nEndpoint services\n.\nAt the top of the page, click theÂ\nCreate endpoint service\nbutton.\nEnter the followingÂ\nEndpoint service\nsettings\n:\nForÂ\nName\n, enter a meaningful name.\nForÂ\nLoad balancer type\n, chooseÂ\nNetwork\n.\nForÂ\nAvailable load balancers\n, select the load balancer you created above.\nEnter the followingÂ\nAdditional settings\n:\nForÂ\nRequire acceptance for endpoint\n,Â enableÂ\nAcceptance required\nto require manual acceptance of connection requests to your endpoint service. Otherwise, these requests will be accepted automatically.\nFor\nEnable private DNS name\n, leave unchecked.\nForÂ\nSupported IP address types\n, enableÂ\nIPv4\n.\nAt the bottom of the form, click theÂ\nCreate\nbutton.\nOnce the endpoint service has been created, navigate to the\nDetails\npage. From the\nDetails\npage:\nUnder\nService Name\n, copy the value to send to Atlan.\nUnder\nAvailability Zones\n, copy the zones to send to Atlan.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of theÂ\nAllow principals\ntable, click theÂ\nAllow principals\nbutton.\nUnderÂ\nPrincipals to add\nandÂ\nARN\n, enter the Atlan account ID and root principal   -  for example,\narn:aws:iam::<account_id>:root\n.\nAt the bottom of the form, click theÂ\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all of the above steps have been completed,\ncontact Atlan support\nand provide the following details:\nService name of the endpoint service\nAvailability zones for the endpoint service\nThere are additional steps Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within\nAWS\n:\nNavigate toÂ\nServices\n, thenÂ\nNetworking & Content Delivery\n, thenÂ\nVPC\n.\nFrom the menu on the left, underÂ\nVirtual private cloud\n, clickÂ\nEndpoint services\n.\nFrom theÂ\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to theÂ\nEndpoint connections\ntab.\nYou should see a row in theÂ\nEndpoint connections\ntable with aÂ\nState\nofÂ\nPending acceptance\n.\nSelect this row, and click theÂ\nActions\nbutton and thenÂ\nAccept endpoint connection request\n.\nWait for this to complete, it could take about 30 seconds.\nRequest DNS name from Atlan\nâ\nContact Atlan support\nto request the regional DNS name of the VPC endpoint that Atlan created in the following format   -\nvpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com\n. This is the hostname you will need to use to connect to your Microsoft SQL Server on Amazon EC2 instance from within Atlan.\nð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to\ncrawl Microsoft SQL Server\nin Atlan! ð\nTags:\natlan\ndocumentation\nPrevious\nCrawl Microsoft SQL Server\nNext\nSet up a private network link to Microsoft SQL Server on Amazon RDS\nPrerequisites\nCreate security group\nCreate a target group\nCreate internal Network Load Balancer\nVerify target group is healthy\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request\nRequest DNS name from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-a-private-network-link-to-microsoft-sql-server-on-amazon-rds",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nPrivate Network\nSet up a private network link to Microsoft SQL Server on Amazon RDS\nOn this page\nSet up a private network link to Microsoft SQL Server on Amazon RDS\nWho can do this?\nYou will need your AWS administrator to complete these tasks   -  you may not have access yourself.\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Microsoft SQL Server on Amazon RDS and Atlan.\nPrerequisites\nâ\nYou should already have the following:\nYour own non-default VPC configured in AWS.\nA Microsoft SQL Server on Amazon RDS instance running in AWS, linked to the non-default VPC.\nPrivate subnets defined within the non-default VPC sufficient for availability.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from Atlan support\n.\nSet up network to RDS (in AWS)\nâ\nTo set up the private network of your Microsoft SQL Server instance, from within\nAWS\n:\nCopy network settings\nâ\nNavigate to\nServices\n, then\nDatabase\n, and then\nRDS\n.\nOn the left, under\nAmazon RDS\n, click on\nDatabases\n.\nFrom the\nDatabases\ntable, click your instance's name under the\nDB identifier\ncolumn.\nUnder the\nConnectivity & security\ntab, copy the following values:\nEndpoint\nand\nPort\nvalues\nVPC\nvalue\nSubnet group\nvalue\nOn the left, click\nSubnet groups\n.\nFrom the table, click the row whose\nName\nmatches the subnet group copied above.\nFrom the\nSubnets\ntable, copy each value under the\nCIDR block\ncolumn for private subnets.\nCreate inbound rule\nâ\nTo create an inbound rule allowing your private subnet access to your RDS instance:\nOn the left, under\nAmazon RDS\n, click on\nDatabases\n.\nFrom the\nDatabases\ntable, click your instance's name under the\nDB identifier\ncolumn.\nUnder the\nConnectivity & security\ntab, under the\nSecurity\ncolumn and the\nVPC security groups\nheading, click the link to your security group.\nAt the bottom of the screen, change to the\nInbound rules\ntab and then click the\nEdit inbound rules\nbutton.\nAt the bottom of the table, click the\nAdd rule\nbutton and create the following rule:\nFor\nType\n, use\nSQL Server\nif you are using the default port (1433), or use\nCustom\nand enter your port under\nPort range\n.\nFor\nSource\n, use\nCustom\nand enter your CIDR range (see\nCopy network settings\n).\nRepeat these sub-steps for each of your CIDR ranges.\nBelow the table, click the\nSave rules\nbutton.\n(Optional) Create RDS proxy\nâ\nBefore you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This\nmethod\nuses a username and password to connect to the RDS database.\nTo create an RDS proxy for your RDS instance:\nOn the left, under\nAmazon RDS\n, click on\nProxies\n.\nIn the upper right of the\nProxies\ntable, click the\nCreate proxy\nbutton.\nUnder\nProxy configuration\n, enter the following details:\nFor\nEngine family\n, select\nSQL Server\n.\nFor\nProxy identifier\n, enter a meaningful name for your proxy.\nUnder\nTarget group configuration\nfor\nDatabase\n, choose your RDS instance.\nUnder\nAuthentication\nfor the\nSecrets Manager secrets\n:\nIf you have an existing secret for your RDS instance's database credentials, select it from the dropdown.\nIf not, click the\nCreate a new secret\nlink and enter these details in the new tab:\nFor\nSecret type\n, select\nCredentials for Amazon RDS database\n.\nFor\nCredentials\n, enter the\nUsername\nand\nPassword\nof the database user.\nUnder\nDatabase\n, select your RDS instance.\nAt the bottom of the form, click the\nNext\nbutton.\nFor\nSecret name\n, enter a name for the secret.\nAt the bottom of the form, click the\nNext\nbutton.\nLeave the automatic secret rotation off and click the\nNext\nbutton.\nReview the secret definition and click the\nStore\nbutton.\nReturn to the tab where you started creating the RDS proxy.\nUnder\nAuthentication\nfor\nIAM authentication\n:\nIf IAM authentication is set to\nRequired\n, Atlan will use an IAM role to connect to the RDS proxy.\nIf IAM authentication is set to\nNot Allowed\n, basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy.\nUnder\nConnectivity\n, expand the\nAdditional connectivity configuration\n:\nFor\nVPC security group\n, select\nChoose existing\n.\nFor\nExisting VPC security groups\n, select the security group you edited with the inbound rules above.\nAt the bottom right of the form, click the\nCreate proxy\nbutton.\nFrom the\nProxies\ntable, click the link for the proxy you just created.\nUnder\nProxy endpoints\nsection, copy the hostname in the\nEndpoint\ncolumn.\nCreate internal Network Load Balancer\nâ\nRetrieve IP address of the RDS\nâ\nFrom an EC2 instance in your AWS account, run the following command:\nnslookup <endpoint>\nReplace\n<endpoint>\nwith the fully-qualified endpoint hostname copied from the RDS endpoint or\nRDS proxy\ncreated above.\nCopy the IP address that comes back from the command, under\nNon-authoritative answer\nand to the right of\nAddress\n.\nStart creating NLB\nâ\nTo create an NLB, from within\nAWS\n:\nNavigate to\nServices\n, then\nCompute\n, and then\nEC2\n.\nOn the left, under\nLoad Balancing\n, click on\nLoad Balancers\n.\nAt the top of the screen, click the\nCreate Load Balancer\nbutton.\nUnder the\nNetwork Load Balancer\noption, click the\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name\n, enter a unique name.\nFor\nScheme\n, select\nInternal\n.\nFor\nIP address type\n, select\nIPv4\n.\nEnter the following\nNetwork mapping\nsettings for the load balancer:\nFor\nVPC\n, select the VPC where the RDS instance is located (see\nCopy network settings\n).\nFor\nMappings\n, select the availability zones with private subnets.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nPort\n, enter\n1433\n(or the non-default port value from\nCopy network settings\n).\nFor\nDefault action\n, click the\nCreate target group\nlink. This will open the target group creation in a new browser tab.\nCreate target group\nâ\nTo create a target group for the NLB:\nEnter the following\nBasic configuration\nsettings for the target group:\nFor\nChoose target type\n, select\nIP addresses\n.\nFor\nTarget group name\n, enter a name.\nFor\nPort\n, enter\n1433\n(or the non-default port value from\nCopy network settings\n).\nFor\nIP address type\n, select\nIPv4\n.\nFor\nVPC\n, select the VPC where the RDS instance is located (see\nCopy network settings\n).\nAt the bottom of the form, click the\nNext\nbutton.\nEnter the following\nIP addresses\nsettings for the target group:\nFor\nNetwork\n, select the VPC where the RDS instance is located (see\nCopy network settings\n).\nFor\nIPv4 address\n, enter the IP address returned by the\nnslookup\ncommand (see\nRetrieve IP address of the RDS\n).\nFor\nPorts\n, enter\n1433\n(or the non-default port value from\nCopy network settings\n).\nAt the bottom of the\nIP addresses\nsection, click the\nInclude as pending below\nbutton.\nConfirm the following\nReview targets\nsettings for the target group:\nConfirm\nIP address\nmatches the IP address returned by the\nnslookup\ncommand.\nConfirm\nPort\nis 1433 (or the non-default port value used by your RDS instance).\nAt the bottom of the form, click the\nCreate target group\nbutton.\nFinish creating NLB\nâ\nReturn to the browser tab where you started the NLB creation, and continue:\nUnder\nListeners and routing\n, click the refresh arrow to the far right of the\nDefault action\ndropdown box.\nSelect the target group you created above in the\nDefault action\ndrop-down.\nAt the bottom of the form click the\nCreate load balancer\nbutton.\nIn the resulting screen, click the\nView load balancer\nbutton.\nVerify target group is healthy\nâ\nTo verify that the target group is healthy:\nFrom the EC2 menu on the left, under\nLoad Balancing\n, click\nTarget Groups\n.\nFrom the\nTarget groups\ntable, click the link to the target group you created above.\nAt the bottom of the screen, under the\nDetails\ntab, check that there is a 1 under both\nTotal targets\nand\nHealthy\n.\nCreate endpoint service\nâ\nTo create an endpoint service, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, and then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\n, click\nEndpoint services\n.\nAt the top of the page, click the\nCreate endpoint service\nbutton.\nEnter the following\nEndpoint service\nsettings\n:\nFor\nName\n, enter a meaningful name.\nFor\nLoad balancer type\n, choose\nNetwork\n.\nFor\nAvailable load balancers\n, select the load balancer you created above in\nCreate internal Network Load Balancer\n.\nEnter the following\nAdditional settings\n:\nFor\nRequire acceptance for endpoint\n, enable\nAcceptance required\n.\nFor\nSupported IP address types\n, enable\nIPv4\n.\nAt the bottom of the form, click the\nCreate\nbutton.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of the\nAllow principals\ntable, click the\nAllow principals\nbutton.\nUnder\nPrincipals to add\nand\nARN\n, enter the Atlan account ID and root principal -  for example,\narn:aws:iam::<account_id>:root\n.\nAt the bottom of the form, click the\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all of the above steps are complete,\ncontact Atlan support\n. You will need to provide Atlan support:\nThe RDS proxy or RDS endpoint DNS -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively.\nOnce this is done, there are additional steps that Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nFrom the\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to the\nEndpoint connections\ntab.\nYou should see a row in the\nEndpoint connections\ntable with a\nState\nof\nPending acceptance\n.\nSelect this row, and click the\nActions\nbutton and then\nAccept endpoint connection request\n.\nWait for this to complete, it could take about 30 seconds.\nRequest DNS name from Atlan\nâ\nContact Atlan support\nto request the regional DNS name of the VPC endpoint that Atlan created in the following format -\nvpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com\n. This is the hostname you will need to use to connect to your Microsoft SQL Server on Amazon RDS instance from within Atlan.\nð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to\ncrawl Microsoft SQL Server\nin Atlan! ð\nTags:\natlan\ndocumentation\nPrevious\nSet up a private network link to Microsoft SQL Server on Amazon EC2\nNext\nWhat does Atlan crawl from Microsoft SQL Server?\nPrerequisites\nSet up network to RDS (in AWS)\n(Optional) Create RDS proxy\nCreate internal Network Load Balancer\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request\nRequest DNS name from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nGet Started\nSet up a private network link to MySQL\nOn this page\nSet up a private network link to MySQL\nWho can do this?\nYou will need your AWS administrator to complete these tasks   -  you may not have access yourself.\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between MySQL (RDS) and Atlan.\nPrerequisites\nâ\nYou should already have the following:\nYour own non-default VPC configured in AWS.\nA MySQL RDS instance running in AWS, linked to the non-default VPC.\nPrivate subnets defined within the non-default VPC sufficient for availability.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from support\n.\nSetup network to RDS (in AWS)\nâ\nTo setup the private network of your MySQL instance, from within\nAWS\n:\nCopy network settings\nâ\nNavigate to\nServices\n, then\nDatabase\n, then\nRDS\n.\nOn the left, under\nAmazon RDS\n, click on\nDatabases\n.\nFrom the\nDatabases\ntable, click your instance's name under the\nDB identifier\ncolumn.\nUnder the\nConnectivity & security\ntab, copy the following values:\nEndpoint\nand\nPort\nvalues\nVPC\nvalue\nSubnet group\nvalue\nOn the left, click\nSubnet groups\n.\nFrom the table, click the row whose\nName\nmatches the subnet group copied above.\nFrom the\nSubnets\ntable, copy each value under the\nCIDR block\ncolumn for private subnets.\nCreate inbound rule\nâ\nTo create an inbound rule allowing your private subnet access to your RDS instance:\nOn the left, under\nAmazon RDS\n, click on\nDatabases\n.\nFrom the\nDatabases\ntable, click your instance's name under the\nDB identifier\ncolumn.\nUnder the\nConnectivity & security\ntab, under the\nSecurity\ncolumn and the\nVPC security groups\nheading click the link to your security group.\nAt the bottom of the screen, change to the\nInbound rules\ntab and click the\nEdit inbound rules\nbutton.\nAt the bottom of the table, click the\nAdd rule\nbutton and create the following rule:\nFor\nType\nuse\nMySQL/Aurora\nif you are using the default port (3306), or use\nCustom\nand enter your port under\nPort range\n.\nFor\nSource\nuse\nCustom\nand enter your CIDR range (see\nCopy network settings\n).\nRepeat these sub-steps for each of your CIDR ranges.\nBelow the table, click the\nSave rules\nbutton.\n(Optional) Create RDS proxy\nâ\nBefore you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This\nmethod\nuses a username and password to connect to the RDS database.\nTo create an RDS proxy for your RDS instance:\nOn the left, under\nAmazon RDS\n, click on\nProxies\n.\nIn the upper right of the\nProxies\ntable, click the\nCreate proxy\nbutton.\nUnder\nProxy configuration\nenter the following details:\nFor\nEngine family\nselect\nMySQL\n.\nFor\nProxy identifier\nenter a meaningful name for your proxy.\nUnder\nTarget group configuration\nfor\nDatabase\nchoose your RDS instance.\nUnder\nAuthentication\nfor the\nSecrets Manager secrets\n:\nIf you have an existing secret for your RDS instance's database credentials, select it from the drop-down.\nIf not, click the\nCreate a new secret\nlink and enter these details in the new tab:\nFor\nSecret type\nselect\nCredentials for Amazon RDS database\n.\nFor\nCredentials\nenter the\nUsername\nand\nPassword\nof the database user.\nUnder\nDatabase\nselect your RDS instance.\nAt the bottom of the form, click the\nNext\nbutton.\nFor\nSecret name\nenter a name for the secret.\nAt the bottom of the form, click the\nNext\nbutton.\nLeave the automatic secret rotation off and click the\nNext\nbutton.\nReview the secret definition and click the\nStore\nbutton.\nReturn to the tab where you started creating the RDS proxy.\nUnder\nAuthentication\nfor\nIAM authentication\n:\nIf IAM authentication is set to\nRequired\n, Atlan will use an IAM role to connect to the RDS proxy.\nIf IAM authentication is set to\nNot Allowed\n, basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy.\nUnder\nConnectivity\nexpand the\nAdditional connectivity configuration\n:\nFor\nVPC security group\nselect\nChoose existing\n.\nFor\nExisting VPC security groups\nselect the security group you edited with the inbound rules above.\nAt the bottom right of the form, click the\nCreate proxy\nbutton.\nFrom the\nProxies\ntable, click the link for the proxy you just created.\nUnder\nProxy endpoints\nsection, copy the hostname in the\nEndpoint\ncolumn.\nCreate internal Network Load Balancer\nâ\nRetrieve IP address of the RDS\nâ\nFrom an EC2 instance in your AWS account, run the following command:\nnslookup <endpoint>\nReplace\n<endpoint>\nwith the fully-qualified endpoint hostname copied from the RDS endpoint or RDS proxy created above.\nCopy the IP address that comes back from the command, under\nNon-authoritative answer\nand to the right of\nAddress\n.\nStart creating NLB\nâ\nTo create an NLB, from within\nAWS\n:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nLoad Balancing\n, click on\nLoad Balancers\n.\nAt the top of the screen, click the\nCreate Load Balancer\nbutton.\nUnder the\nNetwork Load Balancer\noption, click the\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name\nenter a unique name.\nFor\nScheme\nselect\nInternal\n.\nFor\nIP address type\nselect\nIPv4\n.\nEnter the following\nNetwork mapping\nsettings for the load balancer:\nFor\nVPC\nselect the VPC where the RDS instance is located (see\nCopy network settings\n).\nFor\nMappings\nselect the availability zones with private subnets.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nPort\nenter\n3306\n(or the non-default port value from\nCopy network settings\n).\nFor\nDefault action\nclick the\nCreate target group\nlink. This will open the target group creation in a new browser tab.\nCreate target group\nâ\nTo create a target group for the NLB:\nEnter the following\nBasic configuration\nsettings for the target group:\nFor\nChoose target type\nselect\nIP addresses\n.\nFor\nTarget group name\nenter a name.\nFor\nPort\nenter\n3306\n(or the non-default port value from\nCopy network settings\n).\nFor\nIP address type\nselect\nIPv4\n.\nFor\nVPC\nselect the VPC where the RDS instance is located (see\nCopy network settings\n).\nAt the bottom of the form, click the\nNext\nbutton.\nEnter the following\nIP addresses\nsettings for the target group:\nFor\nNetwork\nselect the VPC where the RDS instance is located (see\nCopy network settings\n).\nFor\nIPv4 address\nenter the IP address returned by the\nnslookup\ncommand (see Retrieve IP address of the RDS).\nFor\nPorts\nenter\n3306\n(or the non-default port value from\nCopy network settings\n).\nAt the bottom of the\nIP addresses\nsection, click the\nInclude as pending below\nbutton.\nConfirm the following\nReview targets\nsettings for the target group:\nConfirm\nIP address\nmatches the IP address returned by the\nnslookup\ncommand.\nConfirm\nPort\nis 3306 (or the non-default port value used by your RDS instance).\nAt the bottom of the form, click the\nCreate target group\nbutton.\nFinish creating NLB\nâ\nReturn to the browser tab where you started the NLB creation, and continue:\nUnder\nListeners and routing\n, click the refresh arrow to the far right of the\nDefault action\ndrop-down box.\nSelect the target group you created above in the\nDefault action\ndrop-down.\nAt the bottom of the form click the\nCreate load balancer\nbutton.\nIn the resulting screen, click the\nView load balancer\nbutton.\nVerify target group is healthy\nâ\nTo verify the target group is healthy:\nFrom the EC2 menu on the left, under\nLoad Balancing\nclick\nTarget Groups\n.\nFrom the\nTarget groups\ntable, click the link to the target group you created above.\nAt the bottom of the screen, under the\nDetails\ntab, check that there is a 1 under both\nTotal targets\nand\nHealthy\n.\nCreate endpoint service\nâ\nTo create an endpoint service, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nAt the top of the page, click the\nCreate endpoint service\nbutton.\nEnter the following\nEndpoint service\nsettings\n:\nFor\nName\nenter a meaningful name.\nFor\nLoad balancer type\nchoose\nNetwork\n.\nFor\nAvailable load balancers\nselect the load balancer you created above in\nCreate internal Network Load Balancer\n.\nEnter the following\nAdditional settings\n:\nFor\nRequire acceptance for endpoint\nenable\nAcceptance required\n.\nFor\nSupported IP address types\nenable\nIPv4\n.\nAt the bottom of the form, click the\nCreate\nbutton.\nDid you know?\nUnder the\nDetails\nof the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format   -\nvpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com\n. This is the hostname you will need to use to connect to the RDS instance from within Atlan.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of the\nAllow principals\ntable, click the\nAllow principals\nbutton.\nUnder\nPrincipals to add\nand\nARN\nenter the Atlan account ID and specified principal.\nAt the bottom of the form, click the\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all of the above steps are complete,\ncontact Atlan support\n.\nYou will need to provide Atlan support:\nThe RDS proxy or RDS endpoint DNS   -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively.\nOnce this is done, there are additional steps that Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nFrom the\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to the\nEndpoint connections\ntab.\nYou should see a row in the\nEndpoint connections\ntable with a\nState\nof\nPending acceptance\n.\nSelect this row, and click the\nActions\nbutton and then\nAccept endpoint connection request\n.\nWait for this to complete, it could take about 30 seconds.\nð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to\ncrawl MySQL\nin Atlan! ð\nTags:\natlan\ndocumentation\nPrevious\nSet up MySQL\nNext\nCrawl MySQL\nPrerequisites\nSetup network to RDS (in AWS)\n(Optional) Create RDS proxy\nCreate internal Network Load Balancer\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
    "content": "On this page\nSet up a private network link to PostgreSQL\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between PostgreSQL (RDS) and Atlan, when you use our Single Tenant SaaS deployment.\nWho can do this?\nYou will need your AWS administrator involved   -  you may not have access to run these tasks yourself.\nPrerequisites\nâ\nYou should already have the following:\nYour own non-default VPC configured in AWS.\nA PostgreSQL RDS instance running in AWS, linked to the non-default VPC.\nPrivate subnets defined within the non-default VPC sufficient for availability.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from support\n.\nSetup network to RDS (in AWS)\nâ\nTo setup the private network of your PostgreSQL instance, from within\nAWS\n:\nCopy network settings\nâ\nNavigate to\nServices\n, then\nDatabase\n, then\nRDS\n.\nOn the left, under\nAmazon RDS\n, click on\nDatabases\n.\nFrom the\nDatabases\ntable, click your instance's name under the\nDB identifier\ncolumn.\nUnder the\nConnectivity & security\ntab, copy the following values:\nEndpoint\nand\nPort\nvalues\nVPC\nvalue\nSubnet group\nvalue\nOn the left, click\nSubnet groups\n.\nFrom the table, click the row whose\nName\nmatches the subnet group copied above.\nFrom the\nSubnets\ntable, copy each value under the\nCIDR block\ncolumn for private subnets.\nCreate inbound rule\nâ\nTo create an inbound rule allowing your private subnet access to your RDS instance:\nOn the left, under\nAmazon RDS\n, click on\nDatabases\n.\nFrom the\nDatabases\ntable, click your instance's name under the\nDB identifier\ncolumn.\nUnder the\nConnectivity & security\ntab, under the\nSecurity\ncolumn and the\nVPC security groups\nheading click the link to your security group.\nAt the bottom of the screen, change to the\nInbound rules\ntab and click the\nEdit inbound rules\nbutton.\nAt the bottom of the table, click the\nAdd rule\nbutton and create the following rule:\nFor\nType\nuse\nPostgreSQL\nif you are using the default port (5432), or use\nCustom\nand enter your port under\nPort range\n.\nFor\nSource\nuse\nCustom\nand enter your CIDR range (see\nCopy network settings\n).\nRepeat these sub-steps for each of your CIDR ranges.\nBelow the table, click the\nSave rules\nbutton.\n(Optional) Create RDS proxy\nâ\nBefore you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This\nmethod\nuses a username and password to connect to the RDS database.\nTo create an RDS proxy for your RDS instance:\nOn the left, under\nAmazon RDS\n, click on\nProxies\n.\nIn the upper right of the\nProxies\ntable, click the\nCreate proxy\nbutton.\nUnder\nProxy configuration\nenter the following details:\nFor\nEngine family\nselect\nPostgreSQL\n.\nFor\nProxy identifier\nenter a meaningful name for your proxy.\nUnder\nTarget group configuration\nfor\nDatabase\nchoose your RDS instance.\nUnder\nAuthentication\nfor the\nSecrets Manager secrets\n:\nIf you have an existing secret for your RDS instance's database credentials, select it from the drop-down.\nIf not, click the\nCreate a new secret\nlink and enter these details in the new tab:\nFor\nSecret type\nselect\nCredentials for Amazon RDS database\n.\nFor\nCredentials\nenter the\nUsername\nand\nPassword\nof the database user.\nUnder\nDatabase\nselect your RDS instance.\nAt the bottom of the form, click the\nNext\nbutton.\nFor\nSecret name\nenter a name for the secret.\nAt the bottom of the form, click the\nNext\nbutton.\nLeave the automatic secret rotation off and click the\nNext\nbutton.\nReview the secret definition and click the\nStore\nbutton.\nReturn to the tab where you started creating the RDS proxy.\nUnder\nAuthentication\nfor\nIAM authentication\n:\nIf IAM authentication is set to\nRequired\n, Atlan will use an IAM role to connect to the RDS proxy.\nIf IAM authentication is set to\nNot Allowed\n, basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy.\nUnder\nConnectivity\nexpand the\nAdditional connectivity configuration\n:\nFor\nVPC security group\nselect\nChoose existing\n.\nFor\nExisting VPC security groups\nselect the security group you edited with the inbound rules above.\nAt the bottom right of the form, click the\nCreate proxy\nbutton.\nFrom the\nProxies\ntable, click the link for the proxy you just created.\nUnder\nProxy endpoints\nsection, copy the hostname in the\nEndpoint\ncolumn.\nCreate internal Network Load Balancer\nâ\nRetrieve IP address of the RDS\nâ\nFrom an EC2 instance in your AWS account, run the following command:\nnslookup <endpoint>\nReplace\n<endpoint>\nwith the fully-qualified endpoint hostname copied from the RDS proxy created above.\nCopy the IP address that comes back from the command, under\nNon-authoritative answer\nand to the right of\nAddress\n.\nStart creating NLB\nâ\nTo create an NLB, from within\nAWS\n:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nLoad Balancing\n, click on\nLoad Balancers\n.\nAt the top of the screen, click the\nCreate Load Balancer\nbutton.\nUnder the\nNetwork Load Balancer\noption, click the\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name\nenter a unique name.\nFor\nScheme\nselect\nInternal\n.\nFor\nIP address type\nselect\nIPv4\n.\nEnter the following\nNetwork mapping\nsettings for the load balancer:\nFor\nVPC\nselect the VPC where the RDS instance is located (see\nCopy network settings\n).\nFor\nMappings\nselect the availability zones with private subnets.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nPort\nenter\n5432\n(or the non-default port value from\nCopy network settings\n).\nFor\nDefault action\nclick the\nCreate target group\nlink. This will open the target group creation in a new browser tab.\nCreate target group\nâ\nTo create a target group for the NLB:\nEnter the following\nBasic configuration\nsettings for the target group:\nFor\nChoose target type\nselect\nIP addresses\n.\nFor\nTarget group name\nenter a name.\nFor\nPort\nenter\n5432\n(or the non-default port value from\nCopy network settings\n).\nFor\nIP address type\nselect\nIPv4\n.\nFor\nVPC\nselect the VPC where the RDS instance is located (see\nCopy network settings\n).\nAt the bottom of the form, click the\nNext\nbutton.\nEnter the following\nIP addresses\nsettings for the target group:\nFor\nNetwork\nselect the VPC where the RDS instance is located (see\nCopy network settings\n).\nFor\nIPv4 address\nenter the IP address returned by the\nnslookup\ncommand (see\nRetrieve IP address of the RDS\n).\nFor\nPorts\nenter\n5432\n(or the non-default port value from\nCopy network settings\n).\nAt the bottom of the\nIP addresses\nsection, click the\nInclude as pending below\nbutton.\nConfirm the following\nReview targets\nsettings for the target group:\nConfirm\nIP address\nmatches the IP address returned by the\nnslookup\ncommand.\nConfirm\nPort\nis 5432 (or the non-default port value used by your RDS instance).\nAt the bottom of the form, click the\nCreate target group\nbutton.\nFinish creating NLB\nâ\nReturn to the browser tab where you started the NLB creation, and continue:\nUnder\nListeners and routing\n, click the refresh arrow to the far right of the\nDefault action\ndrop-down box.\nSelect the target group you created above in the\nDefault action\ndrop-down.\nAt the bottom of the form click the\nCreate load balancer\nbutton.\nIn the resulting screen, click the\nView load balancer\nbutton.\nVerify target group is healthy\nâ\nTo verify the target group is healthy:\nFrom the EC2 menu on the left, under\nLoad Balancing\nclick\nTarget Groups\n.\nFrom the\nTarget groups\ntable, click the link to the target group you created above.\nAt the bottom of the screen, under the\nDetails\ntab, check that there is a 1 under both\nTotal targets\nand\nHealthy\n.\nCreate endpoint service\nâ\nTo create an endpoint service, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nAt the top of the page, click the\nCreate endpoint service\nbutton.\nEnter the following\nEndpoint service\nsettings\n:\nFor\nName\nenter a meaningful name.\nFor\nLoad balancer type\nchoose\nNetwork\n.\nFor\nAvailable load balancers\nselect the load balancer you created above in\nCreate internal Network Load Balancer\n.\nEnter the following\nAdditional settings\n:\nFor\nRequire acceptance for endpoint\nenable\nAcceptance required\n.\nFor\nSupported IP address types\nenable\nIPv4\n.\nAt the bottom of the form, click the\nCreate\nbutton.\nDid you know?\nUnder the\nDetails\nof the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format   -\nvpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com\n. This is the hostname you will need to use to connect to the RDS instance from within Atlan.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of the\nAllow principals\ntable, click the\nAllow principals\nbutton.\nUnder\nPrincipals to add\nand\nARN\nenter the Atlan account ID.\nAt the bottom of the form, click the\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all of the above steps are complete,\ncontact Atlan support\n. You will need to provide Atlan support:\nThe name of the endpoint service created in the\nprevious step\n. Go to\nEndpoint Services\nand copy the \"\nService Name\n\".\nThe RDS proxy or RDS endpoint DNS   -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively.\nOnce this is done, there are additional steps that Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nFrom the\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to the\nEndpoint connections\ntab.\nYou should see a row in the\nEndpoint connections\ntable with a\nState\nof\nPending acceptance\n.\nSelect this row, and click the\nActions\nbutton and then\nAccept endpoint connection request\n.\nWait for this to complete, it could take about 30 seconds.\nð The connection is now established. You can now use the\nDNS name of the Atlan VPC endpoint\nas the hostname to\ncrawl PostgreSQL\nin Atlan! ð\nTags:\natlan\ndocumentation\nPrerequisites\nSetup network to RDS (in AWS)\n(Optional) Create RDS proxy\nCreate internal Network Load Balancer\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-a-private-network-link-to-tableau-server",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nGet Started\nSet up a private network link to Tableau server\nOn this page\nSet up a private network link to Tableau server\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Tableau serverÂ and Atlan, when you use our Single Tenant SaaS deployment.\nWho can do this?\nYou will need your AWS administrator involved   -  you may not have access to run these tasks yourself.\nPrerequisites\nâ\nYou should already have the following:\nTableau instance running in AWS (private EC2 instance).\nAtlan hosted in the same region as the Tableau instance.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from support\n.\nSetup network to EC2 instance\nâ\nTo setup the private network of your Tableau EC2 instance, from within\nAWS\n:\nCopy network settings\nâ\nTo copy the network settings of your EC2 instance:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nInstances\n, click\nInstances\n.\nIn the\nInstances\ntable, click on your Tableau EC2 instance.\nUnder the instance's\nDetails\ntab:\nUnder\nVPC ID\ncopy the VPC identifier.\nUnder\nSubnet ID\nclick the subnet for the instance.\nIn the\nSubnets\ntable, copy the value under the\nIPv4 CIDR\ncolumn.\nCreate inbound rule\nâ\nTo create an inbound rule allowing your private subnet access to your EC2 instance:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nInstances\n, click\nInstances\n.\nIn the\nInstances\ntable, click on your Tableau EC2 instance.\nUnder the instance's details, change to the\nSecurity\ntab.\nUnder\nSecurity groups\nclick the security group for the instance.\nUnder the\nInbound rules\ntab, click the\nEdit inbound rules\nbutton.\nAt the bottom left of the\nInbound rules\ntable, click the\nAdd rule\nbutton.\nFor\nType\n, select\nCustom TCP\n.\nFor\nPort range\n, enter the port on which Tableau is accessible (for example, default port\n80\nand TLS port\n443\n).\nFor\nSource\n, choose\nCustom\nand enter the CIDR range for your Tableau instance (see\nCopy network settings\n).\nBelow the bottom right of the\nInbound rules\ntable, click the\nSave rules\nbutton.\nCreate internal Network Load Balancer\nâ\nStart creating NLB\nâ\nTo create an NLB, from within AWS:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nLoad Balancing\n, click on\nLoad Balancers\n.\nAt the top of the screen, click the\nCreate Load Balancer\nbutton.\nUnder the\nNetwork Load Balancer\noption, click the\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name\nenter a unique name.\nFor\nScheme\nselect\nInternal\n.\nFor\nIP address type\nselect\nIPv4\n.\nEnter the following\nNetwork mapping\nsettings for the load balancer:\nFor\nVPC\nselect the VPC where the Tableau instance is located (see\nCopy network settings\n).\nFor\nMappings\nselect the availability zones with private subnets.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nPort\nenter\n80\n(or the non-default port value used in\nCreated inbound rule\n).\nFor\nDefault action\nclick the\nCreate target group\nlink. This will open the target group creation in a new browser tab.\nCreate target group\nâ\nTo create a target group for the NLB:\nEnter the following\nBasic configuration\nsettings for the target group:\nFor\nChoose target type\nselect\nInstances\n.\nFor\nTarget group name\nenter a name.\nFor\nPort\nenter\n80\n(or the non-default port value used in\nCreate inbound rule\n).\nFor\nVPC\nselect the VPC where the Tableau instance is located (see\nCopy network settings\n).\nAt the bottom of the form, click the\nNext\nbutton.\nFrom the\nAvailable instances\ntable:\nClick the checkbox next to your Tableau instance.\nEnter the port for the instance (80 or non-default value used in steps above).\nClick the\nInclude as pending below\nbutton.\nAt the bottom right of the form, click the\nCreate target group\nbutton.\nFinish creating NLB\nâ\nReturn to the browser tab where you started the NLB creation, and continue:\nUnder\nListeners and routing\n, click the refresh arrow to the far right of the\nDefault action\ndrop-down box.\nSelect the target group you created above in the\nDefault action\ndrop-down.\nAt the bottom right of the form click the\nCreate load balancer\nbutton.\nIn the resulting screen, click the\nView load balancer\nbutton.\nVerify target group is healthy\nâ\ndanger\nAs a prerequisite for TLS configuration on Tableau Server only, ensure that the health check\nProtocol\nof the target group is set to\nHTTPS\nor\nmodify the health check settings\nas required.\nTo verify the target group is healthy:\nFrom the EC2 menu on the left, under\nLoad Balancing\n, click\nTarget Groups\n.\nFrom the\nTarget groups\ntable, click the row for the target group you created above.\nAt the bottom of the screen, under the\nDetails\ntab, check that there is a 1 under both\nTotal targets\nand\nHealthy\n.\nCreate endpoint service\nâ\nTo create an endpoint service, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nAt the top of the page, click the\nCreate endpoint service\nbutton.\nEnter the following\nEndpoint service settings\n:\nFor\nName\nenter a meaningful name.\nFor\nLoad balancer type\nchoose\nNetwork\n.\nFor\nAvailable load balancers\nselect the load balancer you created above in\nCreate internal Network Load Balancer\n.\nEnter the following\nAdditional settings\n:\nFor\nRequire acceptance for endpoint\nenable\nAcceptance required\n.\nFor\nSupported IP address types\nenable\nIPv4\n.\nAt the bottom right of the form, click the\nCreate\nbutton.\nUnder the\nDetails\nof the endpoint service, copy the hostname under\nService name\n.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of the\nAllow principals\ntable, click the\nAllow principals\nbutton.\nUnder\nPrincipals to add\nand\nARN\nenter the Atlan account ID.\nAt the bottom right of the form, click the\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all the above steps are complete,\nprovide Atlan support\nwith the following information:\nThe hostname for the endpoint service created above.\nThe port number for the Tableau instance.\nFor\nSSL certificates\nonly, the private DNS name for which you have issued an SSL certificate on your Tableau Server instance.\nThere are additional steps Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nFor\nSSL certificates\nonly, creating a DNS CNAME record pointing the private DNS name shared above to the VPC endpoint URL. This will allow Atlan to use your private DNS name with the SSL certificate.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within AWS:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nFrom the\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to the\nEndpoint connections\ntab.\nYou should see a row in the\nEndpoint connections\ntable with a\nState\nof\nPending\n.\nSelect this row, and click the\nActions\nbutton and then\nAccept endpoint connection request\n.\nIf prompted to confirm, type\naccept\ninto the field and click the\nAccept\nbutton.\nWait for this to complete, it could take about 30 seconds.\nð The connection is now established. You can now use the service endpoint provided by Atlan support (or the private DNS name for\nSSL certificates\n) as the hostname to\ncrawl Tableau\nin Atlan! ð\nTags:\natlan\ndocumentation\nPrevious\nSet up on-premises Tableau access\nNext\nCrawl Tableau\nPrerequisites\nSetup network to EC2 instance\nCreate internal Network Load Balancer\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/set-up-amazon-athena",
    "content": "Connect data\nDatabases\nQuery Engines\nAmazon Athena\nGet Started\nSet up Amazon Athena\nOn this page\nSet up Amazon Athena\nwarning\nð¤ Who can do this?\nYou will probably need your Amazon Athena administrator to run these commands   -  you may not have access yourself.\nDid you know?\nPrefixing all resources created for Atlan with\natlan-\nwill help you better identify them. You should also add AWS tags and descriptions to these resources for later reference.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"VisualEditor0\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:GetBucketLocation\"\n,\n\"s3:ListBucket\"\n,\n\"s3:GetObject\"\n,\n\"glue:GetTables\"\n,\n\"glue:GetDatabases\"\n,\n\"glue:GetTable\"\n,\n\"glue:GetDatabase\"\n,\n\"glue:SearchTables\"\n,\n\"glue:GetTableVersions\"\n,\n\"glue:GetTableVersion\"\n,\n\"glue:GetPartition\"\n,\n\"glue:GetPartitions\"\n,\n\"glue:GetUserDefinedFunctions\"\n,\n\"glue:GetUserDefinedFunction\"\n,\n\"athena:GetTableMetadata\"\n,\n\"athena:StartQueryExecution\"\n,\n\"athena:GetQueryResults\"\n,\n\"athena:GetDatabase\"\n,\n\"athena:GetDataCatalog\"\n,\n\"athena:ListQueryExecutions\"\n,\n\"athena:GetWorkGroup\"\n,\n\"athena:StopQueryExecution\"\n,\n\"athena:GetQueryResultsStream\"\n,\n\"athena:ListDatabases\"\n,\n\"athena:GetQueryExecution\"\n,\n\"athena:ListTableMetadata\"\n,\n\"athena:BatchGetQueryExecution\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:glue:<region>:<account_id>:tableVersion/*/*/*\"\n,\n\"arn:aws:glue:<region>:<account_id>:table/*/*\"\n,\n\"arn:aws:glue:<region>:<account_id>:catalog\"\n,\n\"arn:aws:glue:<region>:<account_id>:database/*\"\n,\n\"arn:aws:athena:<region>:<account_id>:datacatalog/*\"\n,\n\"arn:aws:athena:<region>:<account_id>:workgroup/*\"\n,\n\"arn:aws:s3:::<data_bucket>\"\n,\n\"arn:aws:s3:::<data_bucket>/*\"\n]\n}\n,\n{\n\"Sid\"\n:\n\"VisualEditor1\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:PutObject\"\n,\n\"s3:GetObject\"\n,\n\"s3:ListBucketMultipartUploads\"\n,\n\"s3:AbortMultipartUpload\"\n,\n\"s3:ListBucket\"\n,\n\"s3:GetBucketLocation\"\n,\n\"s3:ListMultipartUploadParts\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::<s3_bucket>/*\"\n,\n\"arn:aws:s3:::<s3_bucket>\"\n]\n}\n,\n{\n\"Sid\"\n:\n\"VisualEditor2\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n\"athena:ListDataCatalogs\"\n,\n\"Resource\"\n:\n\"*\"\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Athena instance.\nReplace\n<account_id>\nÂ with your account ID.\nReplace\n<data_bucket>\nwith the S3 bucket where your actual data resides, such as your Glue tables.\nReplace\n<s3_bucket>\nwith the S3 bucket where Athena can store temporary Athena query results.\ninfo\nðª\nDid you know?\nWe recommend using Atlan's deployment bucket to store these results. This ensures all Atlan assets are managed in a single bucket.\nIf you have an external Hive metastore connected to Athena, also add these policies:\n{\n\"Sid\"\n:\n\"VisualEditor3\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"lambda:InvokeFunction\"\n,\n\"lambda:GetFunction\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:lambda:<region>:<account_id>:function:<lambda_fn_name>\"\n]\n}\nReplace\n<region>\nwith the AWS region of your Athena instance.\nReplace\n<account_id>\nÂ with your account ID.\nReplace\n<lambda_fn_name>\nwith the name of the Lambda function used to configure the catalog.\nThese allow Atlan to trigger the Lambda function.\ndanger\nIf you're using AWS Lake Formation to manage access to your AWS resources, you will need to\ngrant permissions in AWS Lake Formation\nas well as to the objects you want to crawl.\nChoose authentication mechanism\nâ\nUsing the policy created above, configure one of the following options for authentication.\nUser-based authentication\nâ\nTo configure user-based authentication:\nCreate an AWS IAM user by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn theÂ\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nRole delegation-based authentication\nâ\nTo configure role delegation-based authentication:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new role in your AWS account by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies, attach the policy created in the previous step to this role.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n}\n}\n]\n}\n(Optional) To use an\nexternal ID\nfor additional security:\nGenerate the external ID within Atlan\n.\nPaste the external ID into the policy (replace\n<atlan_generated_external_id>\nwith it):\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n\"StringEquals\"\n:\n{\n\"sts:ExternalId\"\n:\n\"<atlan_generated_external_id>\"\n}\n}\n}\n]\n}\nNow,\nreach out to Atlan support\nwith:\nThe name of the role you created above.\nThe ID of the AWS account where the role was created.\ndanger\nWait until the support team confirms the account is allowlisted to assume the role before running the crawler.\nTags:\natlan\ndocumentation\nPrevious\nAmazon Athena\nNext\nSet up a private network link to Amazon Athena\nCreate IAM policy\nChoose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/how-tos/set-up-apache-kafka",
    "content": "Connect data\nEvent/Messaging\nApache Kafka\nGet Started\nSet up Apache Kafka\nOn this page\nSet up Apache Kafka\nWho can do this?\nYou will probably need your Apache Kafka administrator to run these commands   -  you may not have access yourself.\nAtlan supports different authentication mechanisms to securely access your Apache Kafka cluster. If the cluster is configured with \"No Auth\" (authentication not needed), Atlan connects directly. If the cluster requires authentication, you must configure it for Atlan to access your Apache Kafka cluster.\nAtlan supports the following authentication flows:\nBasic authentication using a username and password with\nSASL_PLAIN\nmechanism\nSCRAM authentication using a username and password with\nSASL_SCRAM\nmechanism\nUse basic authentication with SASL_PLAIN mechanism\nâ\nWith basic authentication using\nSASL_PLAIN\n, Atlan authenticates with Kafka using a username and password. To authenticate Atlan with Kafka using\nSASL_PLAIN\n, complete the following steps on each broker:\nCreate user by defining the user credentials in a `JAAS` login configuration file:\nKafkaServer {\norg.apache.kafka.common.security.plain.PlainLoginModule required\nusername=\"<kafka admin username>\"\npassword=\"<kafka admin password>\"\nuser_<username> = \"<password>\";\n};\nReplace\n<kafka admin username>\nand\n<kafka admin password>\nwith the administrator credentials for Kafka.\nReplace\n<username>\nwith the username you want to use in Atlan.\nReplace\n<password>\nwith the password you want to use in Atlan.\nPass the JAAS file as a JVM configuration option when running the broker:\nexport KAFKA_OPTS=\"-Djava.security.auth.login.config=<path-to-jaas-file>/jaas-kafka-server.conf\"\nAtlan does not make any API requests or queries that update the resources in your Kafka cluster. Set the minimum necessary permissions for resources:\nGrant topic permissions to read and describe topics with the following command:\n./bin/kafka-acls.sh --topic '*' --add --allow-principals user:atlan --operations Read,Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/server.properties\nGrant consumer group permissions to read and describe consumer groups with the following command:\n./bin/kafka-acls.sh --consumer-group '*' --add --allow-principals user:atlan --operations Read,Describe --allow-host '*' --config /<broker-config-path>/config/server.properties\nGrant cluster permissions to describe cluster configurations with the following command:\n./bin/kafka-acls.sh --cluster --add --allow-principals user:atlan --operations Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/config/server.properties\nOnce you have configured the Apache Kafka brokers, restart your brokers to pick up the new configuration.\nUse SCRAM authentication with SASL_SCRAM mechanism\nâ\nSCRAM (Salted Challenge Response Authentication Mechanism) provides more security than\nSASL_PLAIN\n. To use SCRAM authentication, complete the following steps on each broker:\nCreate the SCRAM user using SCRAM-SHA-256 or SCRAM-SHA-512 mechanism based on the mechanism set up on your Apache Kafka cluster:\nUse SCRAM-SHA-256 mechanism:\nbin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-256=[iterations=4096,password=<password>]\"\nReplace\n<username>\nwith the username you want to use in Atlan.\nReplace\n<password>\nwith the password you want to use in Atlan.\nUse SCRAM-SHA-512 mechanism:\nkafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-512=[iterations=4096,password=<password>]\"\nReplace\n<username>\nwith the username you want to use in Atlan.\nReplace\n<password>\nwith the password you want to use in Atlan.\nUse both SCRAM-SHA-256 and SCRAM-SHA-512 mechanisms:\nkafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-256=[iterations=4096,password=<password>],SCRAM-SHA-512=[iterations=4096,password=<password>]\"\nReplace\n<username>\nwith the username you want to use in Atlan.\nReplace\n<password>\nwith the password you want to use in Atlan.\nVerify the user configuration:\nkafka-configs.sh --zookeeper localhost:2181 --describe --entity-type users --entity-name <username>\nReplace\n<username>\nwith the username you want to use in Atlan.\nThe SCRAM authentication needs a\nJAAS\nfile. If the file doesn't exist, create one with the following content:\nKafkaServer {\norg.apache.kafka.common.security.plain.PlainLoginModule required\nusername=\"<kafka admin username>\"\npassword=\"<kafka admin password>\"\n};\nPass in the\nJAAS\nfile as a JVM configuration option when running the broker:\nexport KAFKA_OPTS=\"-Djava.security.auth.login.config=<path-to-jaas-file>/jaas-kafka-server.conf\"\nAtlan does not make any API requests or queries that update the resources in your Kafka cluster. Set the minimum necessary permissions for resources:\nGrant topic permissions to read and describe topics with the following command:\n./bin/kafka-acls.sh --topic '*' --add --allow-principals user:atlan --operations Read,Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/server.properties\nGrant consumer group permissions to read and describe consumer groups with the following command:\n./bin/kafka-acls.sh --consumer-group '*' --add --allow-principals user:atlan --operations Read,Describe --allow-host '*' --config /<broker-config-path>/config/server.properties\nGrant cluster permissions to describe cluster configurations with the following command:\n./bin/kafka-acls.sh --cluster --add --allow-principals user:atlan --operations Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/config/server.properties\nOnce you have configured the Apache Kafka brokers, restart your brokers to pick up the new configuration.\nTags:\natlan\ndocumentation\nPrevious\nApache Kafka\nNext\nCrawl Apache Kafka\nUse basic authentication with SASL_PLAIN mechanism\nUse SCRAM authentication with SASL_SCRAM mechanism"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAWS Lambda\nSet up AWS Lambda\nOn this page\nSet up AWS Lambda\nwarning\nð¤ Who can do this?\nYou will probably need your AWS Lambda administrator to run these commands   -  you may not have access yourself.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions, follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"VisualEditor0\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"lambda:InvokeFunction\"\n,\n\"lambda:InvokeAsync\"\n\"lambda:ListFunctions\"\n]\n,\n\"Resource\"\n:\n\"*\"\n}\n]\n}\nChoose authentication mechanism\nâ\nUsing the policy created above, configure one of the following options for authentication.\nUser-based authentication\nâ\nTo configure user-based authentication:\nCreate an AWS IAM user by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn theÂ\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nRole-based authentication\nâ\nTo configure role-based authentication, attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster. Please\nraise a support ticket\nto use this option.\nRole delegation-based authentication\nâ\nTo configure role delegation-based authentication:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new role in your AWS account by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies, attach the policy created in the previous step to this role.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n}\n}\n]\n}\nNow,\nreach out to Atlan support\nwith:\nThe name of the role you created above.\nThe ID of the AWS account where the role was created.\ndanger\nWait until the support team confirms the account is allowlisted to assume the role before running the Lambda function.\nTags:\natlan\ndocumentation\nPrevious\nAWS Lambda\nNext\nCreate an AWS Lambda trigger\nCreate IAM policy\nChoose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/how-tos/set-up-cloudera-impala",
    "content": "Connect data\nDatabases\nQuery Engines\nCloudera Impala\nGet Started\nSet up Cloudera Impala\nOn this page\nSet up Cloudera Impala\nWho can do this?\nYou will probably need your Cloudera Impala instance administrator to complete these steps â you may not have access yourself.\nThis guide provides step-by-step instructions to configure user access and grant the required permissions in Cloudera Impala so that Atlan can crawl metadata.\nCreate user\nâ\nCreate a user in your LDAP system for Atlan to authenticate with Impala. You can use identity providers like OpenLDAP, Active Directory, or any other service your organization uses to create this user.\nBased on the authorization service your organization uses with Impala, sync the created user with either Ranger or Sentry.\nFor Ranger, follow the\nRanger Authentication and User Sync documentation\n.\nFor Sentry, refer to the\nSentry Overview documentation\n.\nConnect to Impala using the admin user from either Ranger or Sentry to manage permissions.\nGrant permissions to assets\nâ\nThere are three ways in which you can grant permissions to assets, depending on your requirements for crawling assets.\nWho can do this?\nThe Impala or Ranger administrator likely needs to complete these steps, as you may not have the required access.\nGrant permission to crawl schema\nâ\nTo provide the SELECT privilege for the entire schema, run the following command:\nGRANT SELECT ON SCHEMA <schema_name> TO USER <atlan-user>;\nRepeat the above command for each schema you want to crawl.\nGrant permission to crawl specific tables\nâ\nTo grant access to a specific table, run the following command:\nGRANT SELECT ON TABLE <table_name> TO USER <atlan-user>;\nReplace\n<table_name>\nwith the name of the table.\nGrant permission to crawl specific columns\nâ\nTo grant column-level access, use the following command:\nGRANT SELECT(column1, column2) ON TABLE <table_name> TO USER <atlan-user>;\nReplace\ncolumn1\n,\ncolumn2\nwith the relevant column names.\nReplace\n<table_name>\nthe relevant table name.\n(Optional) Grant permission to calculate specific attributes\nâ\nRun the following SQL commands:\nGRANT ALTER ON TABLE <table_name> TO USER <atlan-user>;\nGRANT SELECT ON TABLE <table_name> TO USER <atlan-user>;\nReplace\n<table_name>\nwith the name of the table.\nThese permissions are needed to calculate attributes like\nrowCount\nand\nsizeBytes\nfor the tables.\nTags:\natlan\ndocumentation\nPrevious\nCloudera Impala\nNext\nCrawl Cloudera Impala\nCreate user\nGrant permissions to assets\n(Optional) Grant permission to calculate specific attributes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core",
    "content": "Connect data\nETL Tools\ndbt\nGet Started\nSet up dbt Core\nOn this page\nSet up dbt Core\nThis guide explains how to set up dbt Core in Atlan, including configuring access, organizing your storage bucket, and uploading the necessary metadata files so Atlan can process and analyze your dbt project data.\nSetup and access management\nâ\nIn this section, learn how to configure access for dbt Core so Atlan can connect to your storage location and read the required metadata. Choose between using your own cloud storage bucket or an Atlan-managed bucket.\nUse your own bucket (recommended)\nUse Atlan bucket\nDepending on the cloud provider in use, go to Marketplace â search for dbt â click to set up dbt â select Object Storage, and then choose the desired cloud provider. Atlan supports reading from AWS, Azure, and GCP. The setup process prompts for the information required for each cloud provider. For authentication, refer to the following:\nAmazon S3\nâ\nPlease follow the instructions below in order to\ncreate the right IAM Role with the right permissions\nAzure ADLS\nâ\nPlease follow the instructions below in order to\ncreate the right Service principle with the right permissions\nGoogle GCS\nâ\nPlease follow the instructions below in order to\ncreate the right Service account with the right permissions\nTo avoid access issues, Atlan can help you uploading the required files to the same bucket where your tenant is hosted.\nAmazon S3\nâ\nRaise a support request\nto get the details of your Atlan S3 bucket and include the ARN value of the IAM user or IAM role that Atlan can provision access to. You need to create an IAM policy and attach it to the IAM user or role to upload the required files to your Atlan bucket. To create an IAM policy with the necessary permissions, follow\nthe steps below\nGoogle Cloud Storage\nâ\nTo use Atlan's Google Cloud Storage (GCS) bucket, first you have to\ncreate a new service account\n. Then\nRaise a support request\nto share the username of the service account with Atlan. The username is in the following format:\n[email protected]\n. The Atlan support team provides you with read and write access to a particular folder in the Atlan GCS bucket. Once Atlan has granted access, you can use the service account to upload the required files.\nStructure the bucket\nâ\nOnce you have configured access, the next step is to organize your storage bucket so that Atlan can correctly identify and process uploaded files.\ninfo\nAtlan uses the\nmetadata.invocation_id\nand\nmetadata.project_id\nattributes to uniquely identify and link the uploaded files. Atlan doesn't use the file paths to identify a project or job that the file belongs to. The following directory structure is provided as a guideline\nAtlan supports extracting dbt metadata from multiple or single dbt projects. The\nmain-prefix\nhas the following format\ngcs|s3://<BUCKET_NAME>/<PATH_PREFIX>\nor\nabfss://<CONTAINER>/<PATH>\n, if you used Atlan's bucket, the Atlan support team provides it after setting up access policies on your bucket.\nYou need to use the following directory structure, even if you have a single dbt project:\nmain-prefix\n- project1\n- job1\n- manifest.json\n- other files\n- job2\n- manifest.json\n- other files\n- job4\n- manifest.json\n- other files\n- project3\n- job5\n- manifest.json\n- other files\nUpload project files\nâ\nTo load correct metadata, Atlan processes the manifest.json and run_results.json files for each job. There are many ways to load the metadata, below are suggested approaches from Atlan. You need to upload the files from the target directory of the dbt project into distinct folders. Upload the run artifacts generated from the following commands:\n(Required) Compilation results:\ndbt compile --full-refresh\nThis command generates files that contain a full representation of your dbt project's resources, including models, tests, macros, node configurations, resource properties, and more.\nFiles to upload:\nmanifest.json\nand\nrun_results.json\nAlternatively, you can upload the same files by running the\ndbt run --full-refresh\ncommand.\n(Optional) Test results:\ndbt test\nThis command executes all dbt tests in a dbt project and generates files that contain the test results.\nFiles to upload:\nmanifest.json\nand\nrun_results.json\n(Optional) Catalog:\ndbt docs generate\nThis command generates metadata about the tables and views produced by the models in your dbt project, for example, column data types and table statistics.\nFiles to upload:\nmanifest.json\nand\ncatalog.json\nTags:\natlan\ndocumentation\nPrevious\nSet up dbt Cloud\nNext\nCrawl dbt\nSetup and access management\nStructure the bucket\nUpload project files"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMetabase\nGet Started\nSet up Metabase\nOn this page\nSet up Metabase\nWho can do this?\nYou will probably need your Metabase administrator to follow the below steps   -  you may not have access yourself.\nCreate a user\nâ\nTo\ncreate a user\nfor Atlan to use when integrating with Metabase:\nFrom the upper right corner of your Metabase instance, click the gear icon and then\nAdmin Settings\n.\nAt the top of the page, change to the\nPeople\ntab.\nTo the upper right of the table, click the\nInvite someone\nbutton and enter their details:\nFor\nFirst name\nenter the user's first name, for example\nAtlan\n.\nFor\nLast name\nenter the user's last name, for example\nUser\n.\nFor\nEmail\nenter the user's email address, for example a service account email address.\nAt the bottom of the dialog, click the\nCreate\nbutton.\nWhen prompted, click\nDone\n.\nCreate a group\nâ\nYou can only attach Metabase permissions to\ngroups\n.\nTo\ncreate a group\nfor Atlan to use when integrating with Metabase:\nFrom the upper right corner of your Metabase instance, click the gear icon and then\nAdmin Settings\n.\nFrom the top menu bar, change to the\nPeople\ntab.\nFrom the left of the page, open the\nGroups\ntab.\nAt the top right, click the\nCreate a group\nbutton.\nFor\nGroup name\nenter\nAtlan\n.\nOn the right of the row click the\nAdd\nbutton.\nTo add the user to the group:\nClick the\nAtlan\ngroup you created.\nTo the upper right of the table, click the\nAdd members\nbutton.\nUnder\nMembers\nstart typing the name used above (for example,\nAtlan User\n) and select it.\nOn the right of the row, click the\nAdd\nbutton.\nSet permissions\nâ\nDid you know?\nWe do\nnot\nmake any API requests or queries that will update the\ndashboards\n,\ncollections\nor\nquestions\nin your Metabase instance.\nMinimum permissions\nâ\nTo set the minimum permissions required to\ncrawl Metabase\n:\nFrom the upper right corner of your Metabase instance, click the gear icon and then\nAdmin Settings\n.\nFrom the top menu bar, change to the\nPermissions\ntab.\nFrom the top of the page, change to the\nCollection permissions\ntab.\nFor each collection you want to crawl in Atlan:\nUnder the\nCollections\nheading on the left, click the collection.\nUnder\nPermissions for\n<collection name>\n, for the\nAtlan\ngroup, under\nCollection access\nclick the\nNo access\ndrop-down.\n(Optional) To crawl sub-collections, toggle the\nAlso change sub-collections\noption.\nSelect the\nView\npermission.\nIn the upper-right of the page, click the\nSave changes\nbutton.\nWhen prompted with\nSave permissions?\nclick the\nYes\nbutton to confirm.\nPartial lineage permissions\nâ\ndanger\nWhen a Metabase question uses native queries, these permissions cannot capture lineage to source tables and columns.\nTo set the minimal permissions for extracting lineage from Metabase:\nFrom the upper right corner of your Metabase instance, click the gear icon and then\nAdmin Settings\n.\nFrom the top menu bar, change to the\nPermissions\ntab.\nFrom the top of the page, change to the\nData permissions\ntab.\nBelow the tab, click the\nGroups\npill.\nBelow the pill, select the\nAtlan\ngroup.\nUnder\nPermissions for the Atlan group\n, for each database:\nUnder\nData access\nchange the drop-down value to\nUnrestricted\n. Although Atlan does not query data, this permission is necessary to enable the next option.\nUnder\nNative query editing\nchange the drop-down value to\nYes\n. This permission is necessary for Atlan to\nparse the queries\nthat power your Metabase questions, to generate lineage.\nIn the upper-right of the page, click the\nSave changes\nbutton.\nWhen prompted with\nSave permissions?\nclick the\nYes\nbutton to confirm.\nComplete lineage permissions\nâ\nTo set permissions for extracting lineage from all your Metabase questions:\nFrom the upper right corner of your Metabase instance, click the gear icon and then\nAdmin Settings\n.\nFrom the top menu bar, change to the\nPeople\ntab.\nOn the row for the Atlan user you created above, under\nGroups\nchange the drop-down value to\nAdministrators\n.\nDid you know?\nAdministrative access is necessary to get the default source database name used for queries. This is only available to Administrators. The\nunrestricted data access\nand\nnative query editing\npermissions above are insufficient.\n(Optional) Allowlist the Atlan IP\nâ\nIf you are using the IP allowlist in your Metabase instance, you must add your Atlan IP to the allowlist. Please\nraise a support ticket\nto learn your Atlan IP.\nTags:\natlan\ndocumentation\nPrevious\nMetabase\nNext\nCrawl Metabase\nCreate a user\nCreate a group\nSet permissions\n(Optional) Allowlist the Atlan IP"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nOn this page\nSnowflake Data Quality Studio\nPrivate Preview\nOverview:\nMonitor the quality of your Snowflake assets in Atlan using Snowflake's native\ndata metric functions\n. This integration lets you create rules, track metrics, and view data quality insights directly within Atlan's discovery, lineage, and data products.\nGet started\nâ\nFollow these steps to set up Snowflake as your data quality studio:\nSet up Snowflake\nEnable data quality on a connection\nGuides\nâ\nEnable auto re-attachment for data quality rules\n: Configure automatic re-attachment of data quality rules to assets.\nUpgrade Snowflake Data Quality Setup\n: Update existing Snowflake data quality integration to the latest version.\nReferences\nâ\nOperations\n: Technical reference for data quality operations between Atlan and Snowflake.\nFAQ\nâ\nRoles and permissions\n: Common questions about data quality roles and permissions.\nTags:\nsnowflake\ndata-quality\ngovernance\natlan\nNext\nSet up Snowflake\nGet started\nGuides\nReferences\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship",
    "content": "Build governance\nStewardship\nOn this page\nStewardship\nOverview:\nImplement data stewardship in Atlan through automated workflows, policies, and task management. Enable data teams to efficiently govern and maintain data assets through structured processes and compliance measures.\nGet started\nâ\nFollow these steps to implement stewardship in Atlan:\nAutomate data governance\nGuides\nâ\nCreate governance workflows\nManage governance workflows\nManage tasks\nAutomate policy compliance\nCreate policies\nManage policies\nRevoke data access\nCreate forms\nTroubleshooting\nâ\nTroubleshooting policies\nTags:\nstewardship\ngovernance\nworkflows\npolicies\nautomation\natlan\nNext\nAutomate data governance\nGet started\nGuides\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/always-on/references/suggestions-from-similar-assets",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAlways On\nSuggestions from similar assets\nOn this page\nSuggestions from similar assets\nAs a data team ourselves, we know that metadata curation can be time-consuming. To simplify that process, each time you fill in a metadata gap, Atlan looks for other places to reuse that information.\nFor example:\nYou add the description\nInformation about customers\nto a table called\nCUSTOMER\n.\nAtlan will look for any other tables called\nCUSTOMER\nwithout a description.\nAtlan suggests\nInformation about customers\nas the description for these other\nCUSTOMER\ntables.\nMetadata suggestions are currently available for:\nDescriptions\nTerms\nTags\nOwners\nAtlan is always working behind the scenes to help\nfill in the gaps! ð\nOur goals:\nSimplify metadata curation\nReduce human error\nImprove consistency across the data stack\nDid you know?\nEven if a field is blank, everyone can still see potential information for an asset from these suggestions. However, only users with the necessary edit permissions for that asset will be able to apply the metadata suggestions.\nFrequently asked questions\nâ\nWhere can I see suggestions?\nâ\nOn the asset sidebar, where the user generally visits the widgets for metadata updates.\nAre suggestions available for everything in my instance?\nâ\nSuggestions are only available for assets of the same type with the same name, with one exception. Metadata suggestions can be propagated across tables and views with the same name.Â\nFor example:\nYou are on a\nCUSTOMER\ntable asset with no description.\nYou will only see a suggestion if there is another\nCUSTOMER\ntable or view asset with a description already in place.\nIf there is a\ncolumn\ncalled\nCUSTOMER\nwith a description, you will not see its description as a suggestion on a table named\nCUSTOMER\n.\nIf my asset already has that field populated, will there be a suggestion to update it?\nâ\nNo, suggestions are currently only available for assets without that field populated.\nDo suggestions work across asset types (for example, from database to BI dashboard)?\nâ\nNo, currently Atlan only provides suggestions for the same asset type.\nWill Atlan make suggestions for similar but not identical assets?\nâ\nNot exactly. Currently, Atlan makes suggestions for assets that have exactly the same name.\nFor example:\nTwo tables named\nCUSTOMER\nand\nCUSTOMERS\nwill not share suggestions.\nBut two tables named\nCUSTOMER\n, even if one is in Snowflake and the other is in BigQuery, will share suggestions.\nDoes Atlan come up with its own suggestions?\nâ\nNo, Atlan only suggests what has already been filled in for some other asset. Atlan does not create its own suggestions.\nHow are suggestions provided for owners?\nâ\nWhen you update an owner for an asset type, for example, a table asset:\nAtlan automatically checks other table assets with the same name and provides the owner you updated as a suggestion.\nThis works even if the owner has been updated via a workflow or API-based bulk updates.\nTags:\natlan\ndocumentation\nPrevious\nAlways On\nNext\nTag propagation"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/troubleshooting/troubleshooting-datastax-enterprise-connectivity",
    "content": "Connect data\nDatabases\nNoSQL Databases\nDatastax Enterprise\nTroubleshooting\nTroubleshoot permission issues\nOn this page\nTroubleshoot permission issues\nThis guide helps you resolve common permission-related errors that may occur while setting up the DataStax connector.\nUser doesn't have permission to access any keyspaces\nâ\nError message\nUser does not have permission to access any keyspaces in the DataStax cluster. Please ensure the user has the necessary permissions to access the keyspaces.\nPossible causes\nThe user doesn't have the necessary permissions to access keyspaces.\nThe keyspaces don't exist or aren't available.\nResolution\nVerify that the user has the required permissions to access the keyspaces. For more information, see\npermissions required to set up the DataStax connector\n.\nConfirm that the relevant keyspaces exist and are reachable from the connector environment.\nUser doesn't have permission to fetch the cluster name\nâ\nError message\nUser does not have permission to fetch the cluster name from the DataStax cluster. Please ensure the user has the necessary permissions to access the keyspaces.\nPossible causes\nThe user lacks permissions to read system-level metadata.\nThe user is missing\nDESCRIBE\npermission on system tables or the cluster.\nResolution\nVerify that the user has access to system tables in the cluster.\nConfirm that the user has\nDESCRIBE\npermission on the cluster.\nContact your Cassandra administrator to grant necessary system-level permissions if required.\nTags:\natlan\ndocumentation\nPrevious\nPreflight checks for DataStax Enterprise\nUser doesn't have permission to access any keyspaces\nUser doesn't have permission to fetch the cluster name"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/troubleshooting/troubleshooting-exporting-large-query-results",
    "content": "Use data\nInsights\nTroubleshooting\nTroubleshooting exporting large query results\nOn this page\nTroubleshooting exporting large query results\nIf you'd like to\nexport large query results\nof more than 100,000 rows, here are a few things to keep in mind:\nIs there a compute cost for executing long-running queries?\nâ\nLong-running queries usually involve scanning a large number of rows, which can result in incurring cost against source compute. This is especially true when the query limit is increased significantly. Once the query limit has been increased, it will be applicable for all users who have access to execute such queries. To avoid incurring a high compute cost, it is recommended that you exercise caution while\nincreasing the query row limit\n.\nWhat is the maximum time limit for long-running queries?\nâ\nAtlan will issue a hard stop for queries running longer than 24 hours.\nIs there an expiration time for the download URL for results?\nâ\nThe generated download link in your email inbox has an expiry limit of 24 hours.\nWhat is the email attachment limit for CSV files?\nâ\nEmail providers typically set a limit of 25 MB on the attached files. If the file size crosses the limit, please use the download option in the email to download the results.\nHow long can it take for results to appear in the inbox?\nâ\nDepending on the file size of the query results, it may take a few minutes for the file to be available for download. If clicking on the download URL displays a file key error in the browser, please check the query status in Atlan at a later time.\nTags:\natlan\ndocumentation\nPrevious\nTroubleshooting bring your own credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/troubleshooting/troubleshooting-google-bigquery-connectivity",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nTroubleshooting\nTroubleshooting Google BigQuery connectivity\nOn this page\nTroubleshooting Google BigQuery connectivity\nDoes Atlan support nested columns beyond level 1?\nâ\nAtlan gets the raw structure for nested columns beyond level 1 from JSON files, which are then parsed and rendered on the table asset sidebar. However, nested columns beyond level 1 can only be viewed and not enriched with metadata. For example, if you\nattach a tag\nto a level 1 column, the tag will not be propagated to the nested columns.\nWhat are the known limitations of generating lineage for nested columns?\nâ\nThe following examples illustrate the known limitations of generating lineage for Google BigQuery nested columns in Atlan:\nLineage will not be generated if you use the nested column of a\nRECORD\nin the source table or view to create another nested column for a\nRECORD REPEATED\ncolumn in the target table or view. For example:\nCREATE\nVIEW\norders_view\nAS\nSELECT\nARRAY\n(\nSELECT\nAS\nSTRUCT\ncustomer\n.\nname\nAS\ncustomer_name\n,\norder_id\nAS\norder_id\n)\nAS\norders\nFROM\norders_raw\n;\nLineage will be generated for:\norders_raw.order_id\nâ\norders_view.orders.order_id\nLineage will not be generated for:\norders_raw.customer.name\nâ\norders_view.orders.customer_name\nLineage will not be generated for a nested column if you use the\nRECORD\ncolumn as well as a level 1 nested column in the source table or view to create a target column in another table or view. For example:\nCREATE\nVIEW\norders_view\nAS\nSELECT\ncustomer\nAS\ncustomer\n;\ncustomer\n.\nname\nAS\ncustomer_name\n,\norder_id\nAS\norder_id\nFROM\norders_raw\n;\nLineage will be generated for:\norders_raw.customer\nâ\norders_view.customer\norders_raw.customer.address.city\nâ\norders_view.city\nLineage will not be generated for:\norders_raw.customer.name\nâ\norders_view.customer_name\n. Instead, lineage will be generated between\norders_raw.customer\nâ\norders_view.customer_name\n.\nLineage will not be generated for a nested column if you use a table alias to refer to nested columns in a source table or view to create a target column in another table or view. For example:\nCREATE\nVIEW\norders_view\nAS\nSELECT\no\n.\norder_id\nAS\norder_id\no\n.\ncustomer\n.\nname\nas\ncustomer_name\no\n.\ncustomer\n.\naddress\n.\ncity\nas\ncity\nFROM\norders_raw o\n;\nLineage will be generated for:\norders_raw.order_id\nâ\norders_view.order_id\nLineage will not be generated for:\norders_raw.customer.name\nâ\norders_view.customer_name\norders_raw.customer.address.city\nâ\norders_view.city\nLineage will not be generated for a nested column if you use a source table or view\nCROSS JOIN\nwith\nUNNEST\nto create a target table or view. For example:\nCREATE\nVIEW\norders_view\nAS\nSELECT\norder_id\n,\ncustomer\n.\nname\nas\ncustomer_name\n,\ncustomer\n.\naddress\n.\ncity\nas\ncity\n,\ni\n.\nitem_name\nAS\nitem_name\nFROM\norders_raw\nCROSS\nJOIN\nUNNEST\n(\nitems\n)\nAS\ni\n;\nLineage will be generated for:\norders_raw.order_id\nâ\norders_view.order_id\nLineage will not be generated for:\norders_raw.customer.name\nâ\norders_view.customer_name\norders_raw.customer.address.city\nâ\norders_view.city\norders_raw.items.item_name\nâ\norders_view.item_name\nHow to debug test authentication and preflight check errors?\nâ\nInvalid project ID\nProvided GCP project ID is invalid, please check and try again.\nEnsure that the project ID is non-empty and matches the expected project in your Google Cloud console.\nInvalid service account JSON\nFollowing are the possible error messages for this issue:\nProvidedÂ Service account JSON is invalid, please check and try again.\nPrivate key in the service account JSON is invalid, please check and try again.\nFailed to sign service account access token request with the provided private key. Check the service account JSON and try again.\nMalformed JSON, please check and try again.\nThese indicate issues with the service account JSON key, possibly invalid or malformed data, or incorrect private key or formatting.\nVerify that the service account JSON is correctly formatted, and the private key is correctly specified.\nRegenerate the service account key\nif needed, and ensure that all required fields are included.\nEnsure that the file is not corrupted and follows a proper JSON structure.\nInsufficient permissions\nService account doesn't have permission to create jobs, please ensure that the service account has the 'bigquery.jobs.create' permission.\nEnsure that you have assigned the\nbigquery.jobs.create\npermission\nto the service account.\nReview the\nroles and permissions\nassigned to the service account in your Google Cloud IAM settings.\nCloud Resource Manager API disabled\nCloud Resource Manager API has not been used in the configured project before or it is disabled. Please enable it and try again after some time.\nOpen your Google Cloud console and\nenable the Cloud Resource Manager API\nfor the project.\nWait for the API to be fully activated before retrying the operation.\nInvalid grant, service account not found\nUnable to get access token for the provided service account, ensure the service account is active and try again.\nThe service account is either inactive or does not exist.\nEnsure that the\nservice account\nyou created still exists in your Google Cloud console and has neither been disabled nor deleted.\nGeneral connection failure\nFollowing are the possible error messages for this issue:\nUnable to connect to the configured BigQuery instance, please check your credentials and configs and then try again.\nCannot create poolable connection factory.\nThese indicate a general connection failure to your Google BigQuery instance, possibly due to misconfigured credentials or network issues.\nVerify that your credentials are correctly configured.\nEnsure that there are no network issues blocking the connection.\nIf the problem still persists after verifying all of the above,\ncontact Atlan support\n.\nTags:\natlan\ndocumentation\nPrevious\nPreflight checks for Google BigQuery"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/troubleshooting/troubleshooting-hive-connectivity",
    "content": "On this page\nTroubleshooting Hive connectivity\nCan the Hive crawler connect to an independent Hive metastore?\nâ\nAtlan does not support using only independent Hive metastore components for the\nHive crawler\n. Atlan also does not support crawling the metastore database specifically   -  for example, using the PostgreSQL crawler. Doing so would bring in metadata for the database itself and not Hive's metadata.\nTags:\natlan\ndocumentation"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/troubleshooting/troubleshooting-microstrategy-connectivity",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicroStrategy\nTroubleshooting\nTroubleshooting MicroStrategy connectivity\nOn this page\nTroubleshooting MicroStrategy connectivity\nIs the certified status in MicroStrategy mapped to the certificates field in Atlan?\nâ\nNo, the\ncertified status\nin MicroStrategy is not mapped to the certificates field in Atlan. While the MicroStrategy certified field only supports a\nyes\nor\nno\nstatus, the\ncertification field in Atlan\nsupports multiple values. However, if your assets have been certified in MicroStrategy, you can view that information in the\nProperties\ntab of the\nasset sidebar\nin Atlan.\nIs the owner field in MicroStrategy mapped to the owners field in Atlan?\nâ\nNo, the asset owner in MicroStrategy is displayed as the source owner in the\nOverview\nsection of the\nasset sidebar\nin Atlan.\nHow is the upstream lineage for source tables calculated for cubes and reports?\nâ\nCubes or reports do not directly reference SQL tables or columns   -  they import attributes and metrics. Attributes can directly reference columns of SQL tables while metrics may reference an attribute, a fact, or another metric. Upstream lineage for cubes and reports is created by aggregating all the directly referenced SQL tables of the attributes and metrics thatÂ cubes and reports are sourced from.\nWhy am I not seeing upstream lineage for attributes, facts, and metrics?\nâ\nAtlan currently does not support upstream lineage for schema objects like attributes, facts, and metrics.\nWhy are datasets created in a dossier missing from lineage?\nâ\nAtlan currently does not support lineage for datasets directly created in a dossier due to limitations of the MicroStrategy REST APIs. The complete metadata required for creating lineage is unavailable.\nDoes Atlan support document visualizations?\nâ\nAtlan currently does not support document visualizations due to limitations of the MicroStrategy REST APIs. The complete metadata required for supporting document visualizations is unavailable.\nTags:\natlan\ndocumentation\nPrevious\nPreflight checks for MicroStrategy"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/troubleshooting/troubleshooting-mysql-connectivity",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nTroubleshooting\nTroubleshooting MySQL connectivity\nOn this page\nTroubleshooting MySQL connectivity\nIs using RDS Proxy recommended while connecting to RDS?\nâ\nAmazon RDS Proxy\nis a fully-managed, highly-available database proxy for\nAmazon Relational Database Service (RDS)\n. It makes applications more scalable, secure, and resilient to database failures. Amazon RDS Proxy sits between your application and relational database to efficiently manage connections to the database and improve scalability of the application.\nWhen setting up a\nprivate network link to MySQL\n, Amazon RDS Proxy can help with:\nConnection pooling   -  RDS Proxy helps manage database connections by pooling them and reusing them across multiple application connections. This reduces the overhead of establishing new connections for each database request and improves performance.\nScalability   -  it automatically scales connection capacity based on demand, allowing applications to handle a high number of concurrent database connections without overwhelming the database instance. It also helps distribute the workload across multiple database instances, enabling horizontal scalability.\nConnection multiplexing   -  RDS Proxy multiplexes multiple database connections over a single secure connection. This reduces the number of connections to the database, conserving system resources and reducing the chance of hitting connection limits.\nSecurity   -  it integrates with\nAWS Identity and Access Management (IAM)\n, allowing fine-grained control over who can access the proxy and underlying database. It also supports Amazon virtual private cloud (VPC) endpoints, enhancing security through private connectivity.\nFor more questions, head over to\nAmazon RDS Proxy FAQs\n.\nTags:\natlan\ndocumentation\nPrevious\nPreflight checks for MySQL"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/troubleshooting/troubleshooting-on-premises-database-connectivity",
    "content": "Connect data\nDatabases\nOn-premises\nOn-premises Databases\nTroubleshooting\nTroubleshooting on-premises database connectivity\nOn this page\nTroubleshooting on-premises database connectivity\nNo internet access on server running metadata-extractor\nâ\nThe metadata-extractor will attempt to download missing JDBC drivers. If there is no internet connection available on the server running Docker Compose, the metadata-extractor will not be able to get the required drivers.\nTo make these drivers available without an internet connection:\nDownload the latest JDBC driver from the database vendor.\nCopy the JDBC driver files to the server running Docker Compose into a dedicated directory. This should be a sub-directory under where your compose file exists (for example,\n./jdbc/\n).\nExtract the files in that sub-directory:\ntar -zxvf postgres.tar.gz mysql.tar.gz\nChange your compose file to mount this sub-directory for each service you've defined.\nFor example, the compose file would look something like this:\nservices:\nmy-database:\n# ...\nvolumes:\n- <LOCAL-PATH-TO-DRIVERS>:/jars\n- ./output/my-database:/output\n(Replace\n<LOCAL-PATH-TO-DRIVERS>\nwith the sub-directory where you extracted the drivers:\n./jdbc\nin our example.)\nOnce you've followed these steps you should be able to run\nsudo docker-compose up\nas usual. The metadata-extractor now use these local drivers, so internet access is no longer necessary.\nI need local DNS to resolve my server addresses\nâ\nTo specify a DNS server for the metadata-extractor to use in resolving server names:\nAdd a\ndns\nelement to your service definitions in the compose file.\nFor example, the compose file would look something like this:\nservices:\nmy-database:\n<<: *extract\nenvironment:\n# ...\nvolumes:\n# ...\ndns:\n- <DNS-SERVER-ADDRESS>\n(Replace\n<DNS-SERVER-ADDRESS>\nwith the IP address of your local DNS server.)\nIn need to connect to localhost\nâ\nTo allow metadata-extractor to access a service running on the same server as Docker Compose:\nOn Linux, use\nlocalhost\nas the\nHOST\nvalue.\nOn Windows and Mac, use\nhost.docker.internal\nas the\nHOST\nvalue.\nMore details are available in the Docker article:\nI want to connect from a container to a service on the host\nTags:\natlan\ndocumentation\nPrevious\nSupported connections for on-premises databases"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/troubleshooting/troubleshooting-policies",
    "content": "Build governance\nStewardship\nTroubleshooting\nTroubleshooting policies\nOn this page\nTroubleshooting policies\nâ\nAvailable via the Advanced Policy & Compliances package\nHere are a few things to know when setting up\ndata governance policies\nin the policy center:\nWhat is the maximum number of assets that can be scoped to a policy?\nâ\nThere is currently no upper limit on the total number of assets that can be scoped to a policy. However, bear in mind that a higher asset volume can slow down the workflows for linking assets to the policy and completing compliance checks. It may also adversely impact the overall performance of Atlan, including search and discovery, workflow runtime, playbook execution, and more.\nWhat is the maximum number of rules supported in a policy?\nâ\nAtlan currently supports creating 10\nrules\nper policy. However, as with a higher volume of assets, more rules can also slow down the completion of compliance checks. Note that checks for each rule run one at a time during the scanning process.\nHow many active policies can Atlan support?\nâ\nThere is no upper limit on the total number of active policies in Atlan. However, Atlan recommends that you proceed with caution. Multiple active or overlapping policies may delay the completion of compliance checks or adversely impact overall performance.\nCan running playbooks on governed assets affect compliance checks?\nâ\nYes, compliance checks must operate in isolation from playbook runs. For any overlapping assets, compliance checks should\nnot\ncoincide with any ongoing or scheduled playbook runs. Failure to heed this warning may result in false incidents.\nCan I use newly created metadata attributes to define the asset scope or compliance rules?\nâ\nFor any newly created glossary, category, term, tag, or custom metadata, you must wait a minimum of three hours before you can use them to define the asset scope or compliance rules in a policy.\nFor any changes to a linked asset that violate a rule, how long will it take to be reported as an incident?\nâ\nThe violation check operates in near real time. However, if there is a high volume of updates and the queue becomes congested, new updates will have to wait until existing ones have been processed.\nTags:\natlan\ndocumentation\nPrevious\nCreate forms"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/troubleshooting/troubleshooting-postgresql-connectivity",
    "content": "Connect data\nDatabases\nSQL Databases\nPostgreSQL\nTroubleshooting\nTroubleshooting PostgreSQL connectivity\nOn this page\nTroubleshooting PostgreSQL connectivity\nCan Atlan crawl future tables created by any user?\nâ\nPostgreSQL does not provide a single command to grant access to future tables created by any user on a global level.\nYou will have to alter the default privileges of every current and future user that creates tables. This is to ensure that the\ndatabase role you created for integrating with Atlan\nhas access to the tables created by those users by default. For example:\nALTER\nDEFAULT\nPRIVILEGES\nFOR\nUSER\n<\nUSER_CREATING_TABLES\n>\nIN\nSCHEMA\n<\nSCHEMA\n>\nGRANT\nSELECT\n,\nREFERENCES\nON\nTABLES\nTO\natlan_user_role\n;\nHowever, altering the default privileges of every current and future user may not be sustainable or controlled from a single place. To automate the granting of permissions, you can:\nGrant function\nYou can automate the granting of privileges to the\ndatabase role you created\nfor integrating with Atlan. Note that the function below is located in the public schema. You can use any schema you want to store this function:\nSet custom conditions using PL/pgSQL to skip or allow only certain schemas or tables:\nCREATE\nOR\nREPLACE\nFUNCTION\npublic\n.\ngrant_permissions_on_all_schemas\n(\n)\nRETURNS\nvoid\nAS\n$$\nDECLARE\nschema_name\ntext\n;\nBEGIN\nFOR\nschema_name\nIN\n(\nSELECT\nnspname\nFROM\npg_namespace\nWHERE\nnspname\nNOT\nLIKE\n'pg_%'\nAND\nnspname\n!=\n'information_schema'\n)\nLOOP\nEXECUTE\nformat\n(\n'GRANT USAGE ON SCHEMA %I TO atlan_user_role'\n,\nschema_name\n)\n;\n-- grant access to all tables, including views, materialized views\nEXECUTE\nformat\n(\n'GRANT SELECT, REFERENCES ON ALL TABLES IN SCHEMA %I TO atlan_user_role'\n,\nschema_name\n)\n;\nEND\nLOOP\n;\nEND\n;\n$$\nLANGUAGE\nplpgsql\n;\nNext, set up a periodic schedule and execute this function on a daily or hourly basis to ensure that the\ndatabase role\nhas access to all new schemas or tables:\nselect\npublic\n.\ngrant_permissions_on_all_schemas\n(\n)\n;\nEvent triggers\nYou can create an event trigger on any\nCREATE SCHEMA\nor\nCREATE TABLE\ncommand. This automation will ensure minimal lag, and you will not have to set up a schedule to run the above grant function.\nNote that the event trigger only listens to new create event triggers. You will still need to run the grant function above to ensure that the database role has access to all current schemas or tables.\n-- Function to grant permissions on a specific schema\nCREATE\nOR\nREPLACE\nFUNCTION\npublic\n.\ngrant_permissions_on_schema\n(\n)\nRETURNS\nevent_trigger\nAS\n$$\nDECLARE\nobj record\n;\nBEGIN\nFOR\nobj\nIN\nSELECT\n*\nFROM\npg_event_trigger_ddl_commands\n(\n)\nWHERE\ncommand_tag\n=\n'CREATE SCHEMA'\nLOOP\nEXECUTE\nformat\n(\n'GRANT USAGE ON SCHEMA %I TO atlan_user_role'\n,\nobj\n.\nobject_identity\n)\n;\nRAISE NOTICE\n'Granted USAGE on schema % to atlan_user_role'\n,\nobj\n.\nobject_identity\n;\nEND\nLOOP\n;\nEND\n;\n$$\nLANGUAGE\nplpgsql\n;\n-- Event trigger for new schemas\nCREATE\nEVENT\nTRIGGER\ngrant_permissions_on_new_schema\nON\nddl_command_end\nWHEN\nTAG\nIN\n(\n'CREATE SCHEMA'\n)\nEXECUTE\nFUNCTION\npublic\n.\ngrant_permissions_on_schema\n(\n)\n;\n-- Function to grant permissions on a specific table\nCREATE\nOR\nREPLACE\nFUNCTION\npublic\n.\ngrant_permissions_on_table\n(\n)\nRETURNS\nevent_trigger\nAS\n$$\nDECLARE\nobj record\n;\nBEGIN\nFOR\nobj\nIN\nSELECT\n*\nFROM\npg_event_trigger_ddl_commands\n(\n)\nWHERE\ncommand_tag\n=\n'CREATE TABLE'\nor\ncommand_tag\n=\n'CREATE VIEW'\nor\ncommand_tag\n=\n'CREATE TABLE AS'\nor\ncommand_tag\n=\n'CREATE MATERIALIZED VIEW'\nLOOP\nEXECUTE\nformat\n(\n'GRANT SELECT, REFERENCES ON TABLE %s TO atlan_user_role'\n,\nobj\n.\nobject_identity\n)\n;\nRAISE NOTICE\n'Granted SELECT, REFERENCES on table % to atlan_user_role'\n,\nobj\n.\nobject_identity\n;\nEND\nLOOP\n;\nEND\n;\n$$\nLANGUAGE\nplpgsql\n;\n-- Event trigger for new tables, views, mat views\nCREATE\nEVENT\nTRIGGER\ngrant_permissions_on_new_table\nON\nddl_command_end\nWHEN\nTAG\nIN\n(\n'CREATE TABLE'\n,\n'CREATE VIEW'\n,\n'CREATE TABLE AS'\n,\n'CREATE MATERIALIZED VIEW'\n)\nEXECUTE\nFUNCTION\npublic\n.\ngrant_permissions_on_table\n(\n)\n;\nTags:\natlan\ndocumentation\nPrevious\nPreflight checks for PostgreSQL"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-tag-management",
    "content": "Connect data\nData Warehouses\nSnowflake\nTroubleshooting\nTroubleshooting Snowflake tag management\nOn this page\nTroubleshooting Snowflake tag management\nHere are a few things to know about\nmanaging Snowflake tags\nin Atlan:\nIf a Snowflake tag is not attached to an asset, will enabling reverse sync trigger any updates?\nâ\nWhen reverse tag sync is enabled, updates in Snowflake will only be triggered if an imported tag is attached to a Snowflake asset in Atlan or an existing Atlan tag for an asset is updated with an imported Snowflake tag.\nWill Snowflake tags show up as native assets in Atlan?\nâ\nNo, Atlan currently does not support Snowflake tags as native assets.\nDo corresponding tags in Atlan and Snowflake need to be of the same case for the sync to work?\nâ\nTag sync happens through case-insensitive name match. Even if the Snowflake tag and the corresponding Atlan tag have the same name but are of a different case, the tags will be synced and remain consistent between Atlan and Snowflake.\nWhen reverse tag sync is enabled, will all Snowflake assets with tags in Atlan be updated in Snowflake?\nâ\nEnabling reverse tag sync will not trigger any updates unless tag updates are made for the Snowflake assets either in Atlan or Snowflake.\nIf a tag is deleted in Snowflake, will the corresponding tag in Atlan also be deleted?\nâ\nOnly the imported Snowflake tag associated with the corresponding Atlan tag and its association with linked assets will be removed. The Atlan tag and the Snowflake assets to which the Atlan tag is attached will remain unaffected.\nHow will tag propagation work for Snowflake tags?\nâ\nAtlan supports\ntag propagation\nbased on asset hierarchy and lineage. This existing functionality will remain unaffected by imported Snowflake tags. To learn about tag propagation in Snowflake, see\nhere\n.\nHow can I find out if reverse tag sync was successful?\nâ\nIf you're using the\naccount usage method\n, expect latency of data for up to 3 hours. You can also use a\ntable function\nlike\nTAG_REFERENCES\nto confirm reverse tag sync.\nWhat happens if a tag is deleted in Atlan but remains in Snowflake?\nâ\nThe tag will be recreated during the next Snowflake crawler run and linked to assets where the relationship still exists in Snowflake.\nDoes enabling reverse sync automatically push all updates to Snowflake?\nâ\nNo. However, you can\nset up playbooks\nin Atlan to update your Snowflake assets with imported tags at scale. If\nreverse sync is enabled\nfor the Snowflake tags, updates will be pushed to Snowflake as well.\nWhat is the default propagation setting for synced Snowflake tags and can it be changed?\nâ\nWhen imported Snowflake tags are attached to assets after a\ncrawler run\n,\ntag propagation is turned off\nby default in Atlan.\nCan I restrict who can push asset-specific tag syncs to Snowflake?\nâ\nNo, this functionality is currently unavailable. However, stay tuned for more updates.\nWill reverse sync remove a tag from an asset in Snowflake if it is removed in Atlan?\nâ\nIf a tag is removed from a Snowflake asset in Atlan, it will not delete the tag in Snowflake. Instead, the asset will no longer be linked to the Snowflake tag in Atlan only.\nIf tag propagation and reverse sync are enabled, will my assets in Snowflake display multiple values of the same tag?\nâ\nIf both tag propagation and reverse sync are enabled, and multiple values of the same tag are being propagated via different paths to a Snowflake asset:\nAtlan will preserve all values of the attached tag on the asset.\nSnowflake will only preserve the last modified value of the attached tag on the asset.\nIs it possible to filter and select specific tags to import in Atlan?\nâ\nNo, Atlan currently does not support filtering and selecting specific Snowflake tags to import. Atlan will crawl all the tags associated with an asset in Snowflake.\nCan Atlan reverse sync tags to dropped tables in Snowflake?\nâ\nIf tables are dropped in Snowflake, then the corresponding link between the table and tag is also removed. If new tables are created in their place, you will need to recreate the link.\nYou can either:\nApply tags to the new tables in Snowflake\nCrawl the new Snowflake tables in Atlan\n,\nrun a playbook for tag attachment\n, and then\npush tag updates to Snowflake\nIf you're also using dbt, you can use pre-hooks and post-hooks to preserve metadata like attached tags while building your dbt models. Refer to\ndbt documentation\nto learn more about hooks.\nTags:\natlan\ndocumentation\nPrevious\nTroubleshooting Snowflake connectivity\nNext\nSnowflake warehouse configuration"
  },
  {
    "url": "https://docs.atlan.com/faq/user-management-and-access-control",
    "content": "Configure Atlan\nFrequently Asked Questions\nUser Management and Access Control\nOn this page\nUser Management and Access Control\nComplete guide to managing users, configuring access controls, and understanding permissions in Atlan.\nHow can I find all users in Atlan?\nâ\nYou must be an\nadmin user\nto see the full user list. In the\nAdmin\ncentre open\nUsers & Groups â Users\nto view, filter, and export all users in your workspace.\nCan I change my access level as an admin to test member and guest permissions?\nâ\nChanging your own admin role is risky because any background workflows or playbooks you own might fail. Instead, create separate member and guest test accounts (for example, with disposable email addresses) and use an incognito browser window to verify permissions.\nHow can a user with a member role query tables?\nâ\nGive the member a\npersona\nor\npurpose\nthat contains the necessary\ndata access policies\nfor the tables they need to query.\nHow do you become an admin for a connection?\nâ\nOnly an existing connection admin (or a workspace admin) can assign additional connection admins. Ask a current connection admin to add you via the connection sidebar or by editing the connector workflow.\nWhy do some assets have a lock or a request to get approval from an admin?\nâ\nA member sees a lock icon when they're neither a\nconnection admin\nnor part of any persona or purpose that permits asset changes. To gain edit rights, become a connection admin or be added to a suitable persona or purpose.\nWhich permissions take priority when policies have overlapping assets?\nâ\nWhen more than one policy affects the same asset, Atlan applies the most restrictive rule. An explicit\ndeny\nalways overrides a permit. See\nHow do I control access to metadata and data?\nfor details.\nCan I restrict business users to only view verified assets?\nâ\nNo. Access policies can currently target connections and\ntags\nbut not certification status.\nWhat's the default landing page for users with two or more personas?\nâ\nAfter you set\nlanding page preferences\n:\nIf exactly one persona applies to the user, they land on that persona's configured page.\nIf multiple personas apply and the \"All assets\" view is\ndisabled\n, Atlan chooses the first persona alphabetically.\nIf the \"All assets\" view is available, the default landing page is\nAll assets\n.\nCan I restrict metadata access to a subset of columns?\nâ\nYes. Create a\nmetadata policy\nand, in the asset selector, drill down to choose only the columns that should be visible.\nHow's the atlan_user id used for authentication in Redshift?\nâ\nAtlan connects via the Amazon Redshift JDBC driver, using the IAM credentials you configure. The\natlan_user\nidentifier is not used to log in; instead, Atlan attributes each query to the actual user in Redshift's logs.\nIs the data for queries or sample preview masked in memory?\nâ\nYes. Atlan rewrites the SQL at run-time so that masked values (for example, via\nREPLACE\n) are returned directly from the source database; the unmasked data never reaches Atlan's memory.\nCan users link terms to assets without glossary access?\nâ\nYesâprovided their metadata policy allows them to add or remove terms. They can link terms from the asset sidebar even if they don't have edit rights on the glossary itself.\nWhy do I get permission denied when running an API request?\nâ\nAdd one or more\npersonas\nto the API token so it can access the connection's assets. You can assign personas to a token in\nAdmin â API authentication\n. For steps, see\nAPI authentication\n.\nDoes Atlan have a password policy?\nâ\nYes. Atlan enforces these minimum requirements:\nMinimum length:\n12 characters\nMust include\nat least 1 digit\nMust include\nboth lowercase and uppercase letters\nMust include\nat least 1 special character\nGenerated passwords expire after\n90 days\n, and you can't reuse your last\n5\npasswords.\nWhat assets can be transferred from a removed user and how?\nâ\nRemoving a user from Atlan and transferring ownership of their assets may entail one of the following actions or a combination thereof:\nRemove the user from a list of owners.\nDelete the associated asset.\nTransfer ownership of assets to a new user.\nCategory\nCondition\nAction\nPersona\nUser is present\nRemove user from the persona\nPurpose\nUser is present\nRemove user from the purpose\nOwner metadata\nSole owner\nTransfer ownership to transferee\nMultiple owners\nRemove user from owner metadata attribute\nConnection admin\nSole connection admin\nTransfer role to transferee\nMultiple connection admins\nRemove user from list of connection admins\nQuery collection owner\nIf query collection is private\nDelete query collection along with its folders and queries\nIf query collection is shared and user has view permissions\nRemove user from query collection\nIf query collection is shared and user is sole owner\nTransfer query collection to transferee\nIf query collection is shared and has multiple owners\nRemove user from list of owners\nQuery owner\nIf parent collection of the query is to be deleted\nDelete the query\nIf parent collection of the query is not to be deleted and user is sole owner\nTransfer ownership to transferee\nIf parent collection of the query is not to be deleted and query has multiple owners\nRemove user from owner metadata of the query\nStarred assets\nUser is present in the\nstarredBy\nattribute of an asset\nRemove user from\nstarredBy\nattribute\nAPI tokens\nUser has created API tokens\nDelete all API tokens created by user\nSCIM tokens\nUser has created SCIM tokens\nDelete all SCIM tokens created by user\nUser-level integrations\nUser has created an integration with Jira, Slack, Teams, or more\nDelete all user-level integrations\nRequests\nUser has submitted requests\nDelete all requests from user\nPlaybooks\nOne-time playbooks\nNo action\nScheduled playbooks\nIf user is the creator of the playbook and playbook schedule, transfer playbook and schedule to transferee\nWorkflows\nOne-time workflows\nNo action\nScheduled workflows\nIf user is the creator of the workflow and workflow schedule, transfer workflow and cron to transferee\nScheduled queries\nIf results are shared with other users\nRemove user from the list of query result recipients, transfer the workflow, cron, and parent collection to transferee, and remove deleted user from owner metadata in queries\nIf results are not shared with other users\nDelete the workflow\nCan I remove users if SSO or SCIM is enforced?\nâ\nYes, you can remove users irrespective of whether you're using basic authentication, SSO, or SCIM provisioning in Atlan.\nWill the activity log include metadata updates made by a removed user?\nâ\nThe activity log will retain historical information on any metadata updates made by a removed user, logged under their username. This is crucial to maintain data integrity for auditing purposes.\nIs it possible to reactivate a removed user?\nâ\nNo, it is not possible to reactivate a removed user. Since the user will be hard-deleted from Atlan, there will be no trace of the user in the identity system. Atlan maintains historical records of removed users for auditing purposes only. Whether you're using basic authentication, SSO, or SCIM provisioning, any returning user with the same username will be treated as a new user in Atlan.\nTags:\natlan\ndocumentation\nfaq-administration\nPrevious\nTags and Metadata Management\nNext\nWorkflows and Data Processing"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups",
    "content": "On this page\nUsers and groups\nOverview:\nManage users and groups in Atlan to control access and organize your data team. Set up user roles, create groups, and delegate administration to maintain a secure and efficient data platform.\nGet started\nâ\nFollow these steps to implement user and group management in Atlan:\nInvite new users\nGuides\nâ\nManage users\nManage user authentication\nCreate groups\nAdd users to groups\nDelegate administration\nConcepts\nâ\nWhat are groups\nWhat are user roles\nTags:\nusers\ngroups\nauthentication\nadministration\ngovernance\natlan\nGet started\nGuides\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/connections/concepts/what-are-preflight-checks",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nConcepts\nWhat are preflight checks?\nOn this page\nWhat are preflight checks?\nPreflight checks makes sure that all the required grants and permissions are in place while setting up a new workflow in Atlan. The framework of these comprehensive preflight checks may vary by connection source.\nWhen setting up a new workflow, you can run preflight checks to:\nPerform the necessary technical validations during setup\nHelp improve your workflow success rate\nSupported sources\nâ\nAtlan currently supports preflight checks for the following connection sources:\nAirflow, Amazon MWAA, Astronomer, and Google Cloud Composer\nAiven Kafka\nAmazon MSK\nAmazon Redshift\nAmazon QuickSight\nAnomalo\nDatabricks\ndbt\nDomo\nConfluent Schema Registry\nFivetran\nGoogle BigQuery\nHive\nLooker\nMetabase\nMicrosoft Azure Data Factory\nMicrosoft Azure Synapse Analytics\nMicrosoft Power BI\nMicrosoft SQL Server\nMicroStrategy\nMode\nMonte Carlo\nMySQL\nOracle\nPostgreSQL\nPrestoSQL\nQlik Sense Cloud\nRedash\nRedpanda Kafka\nSalesforce\nSAP HANA\nSigma\nSisense\nSnowflake\nTableau\nTeradata\nTrino\nTags:\natlan\ndocumentation\nPrevious\nHow to provide SSL certificates\nNext\nWhat is the crawler logic for a deprecated asset?\nSupported sources"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/concepts/what-are-the-sidebar-tabs",
    "content": "Configure Atlan\nAccess control\nReferences\nWhat are the sidebar tabs?\nOn this page\nWhat are the sidebar tabs?\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to personalize the asset sidebar for personas and purposes.\nPersonas\nand\npurposes\nhelp you manage data access and curate assets for your users. You can also limit the details in the asset sidebar to only what is relevant to the persona or purpose.Â\nPersonalize the asset sidebar\nâ\nOnce you've created a\npersona\nor\npurpose\n, you can set preferences for each sidebar tab. If enabled, the tab will display relevant information for a supported asset in the sidebar. You can personalize the asset sidebar for the following asset types and more:\nAll assets\nâ\nLineage\nâ\nThe\nLineage\ntab allows users to:\nView the upstream sources and downstream transformations of an asset.\nSearch and filter by upstream and downstream directions or asset types.\nOpen the\nlineage graph\n.\nRelations\nâ\nThe\nRelations\ntab allows users to:\nView\nassets related\nto each other beyond a parent-child relationship.\nSearch and filter related assets by asset type.\nActivity\nâ\nThe\nActivity\nÂ tab allows users to:\nView the\nchangelog\nfor an asset.\nSearch and filter activities by type of metadata update.\nResources\nâ\nThe\nResources\ntab allows users to:\nView\ninternal or external URLs\nfor contextual information on an asset.\nRequests\nâ\nThe\nRequests\nÂ tab allows users to:\nView\nrequests\nfor metadata updates on an asset.\nSearch and filter requests by request status.\nProperties\nâ\nThe\nProperties\ntab allows users to:\nView\nessential properties\nof an asset, including unique identifier, qualified name, and more.\nSQL assets\nâ\nColumns\nâ\nThe\nColumns\ntab allows users to:\nView columns for table assets.\nSearch and filter columns by data type.\nView\nusage and popularity metrics\n,\ncolumn keys\n, and data lineage.\nOpen the column sidebar for a selected column.\nUsage\nâ\nThe\nUsage\ntab allows users to:\nView\nusage metadata\nfor Snowflake and Google BigQuery assets   -  including top users, top five queries by context,\ncompute cost\n, and more.\nFact-Dim Relations\nâ\nThe\nFact-Dim Relations\ntab allows users to:\nView assets associated with each other by\nfact-dimension relationships\n.\nSearch and filter related assets by asset type.\nProfile\nâ\nThe\nProfile\ntab allows users to:\nView data profiling metrics for\nprofiled column assets\n-  including column quality, summary statistics, and timestamp for last profiled.\nQueries\nâ\nThe\nQueries\ntab allows users to:\nView\nsaved queries\nfor an asset.\nUse the search bar for quick search.\nBI assets\nâ\nFields\nâ\nThe\nFields\ntab allows users to:\nView data type fields for supported asset types   -  data source fields and calculated fields for\nTableau\ndata sources, columns for\nMicrosoft Power BI\ntables, fields for\nLooker\nexplores and views,\nSigma\ndatasets and data elements,\ndbt\nmodels, and\nSalesforce\nobjects.\nSearch and filter fields by lineage.\nVisuals\nâ\nThe\nVisuals\ntab allows users to:\nView visualization assets for\nRedash queries\n.\nUse the search bar for quick search and view lineage information.\nVisualizations\nâ\nThe\nVisualizations\ntab allows users to:\nView visualization assets for\nThoughtSpot liveboards\n.\nUse the search bar for quick search and view lineage information.\nSchema Objects\nâ\nThe\nSchema Objects\ntab allows users to:\nView schema objects for MicroStrategy\nprojects\n,\nreports\n, and\ncubes.\nSearch and filter schema objects by asset type.\nETL assets\nâ\nRuns\nâ\nThe\nRuns\ntab allows users to:\nView run history for task and Airflow DAG assets.\nFilter runs by run status and type.\nCopy run IDs and view runs directly in Airflow.\nTasks\nâ\nThe\nTasks\ntab allows users to:\nView task assets for Airflow DAGs or ETL processes.\nObject storage assets\nâ\nObjects\nâ\nThe\nObjects\ntab allows users to:\nView bucket assets for object storage connectors like\nGoogle Cloud Storage\nor\nS3\n.\nUse the search bar for quick search.\nData quality indicators\nâ\nIncidents\nâ\nThe\nIncidents\ntab allows users to:\nView alerts for\nMonte Carlo monitors\nwhen anomalies are detected.\nExamine incidents by total count of incidents and timestamp for last synced.\nFilter incidents by incident type.\nOpen incidents directly in Monte Carlo.\nMonte Carlo\nâ\nThe\nMonte Carlo\ntab allows users to:\nView timestamp for last synced and total count of incidents and custom monitors for table assets monitored by\nMonte Carlo\n.\nExpand the\nIncidents\nand\nCustom monitors\ntabs to view more details.\nSoda\nâ\nThe\nSoda\ntab allows users to:\nView timestamp for last synced and scanned and a total count of\nSoda\nchecks on the asset.\nExamine Soda checks in detail and open them directly in Soda.\ndbt Test\nâ\nThe\ndbt Test\ntab allows users to:\nView a total count of dbt tests on the asset and also grouped by test status.\nOpen the dbt tests in a sidebar or view directly in dbt.\nGlossary assets\nâ\nAssets\nâ\nThe\nAssets\ntab allows users to:\nView\nlinked assets\nfor glossary\nterms\n.\nSearch and filter linked assets by asset type.\nQuery assets\nâ\nSchedules\nâ\nThe\nSchedules\ntab allows users to:\nView a list of\nschedules\nfor a saved query, if any.\nTags:\natlan\ndocumentation\nPrevious\nWhat are user roles?\nNext\nUser Role Sync\nPersonalize the asset sidebar\nAll assets\nSQL assets\nBI assets\nETL assets\nObject storage assets\nData quality indicators\nGlossary assets\nQuery assets"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/column-keys-crawled",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nFAQ\nWhat column keys does Atlan crawl?\nOn this page\nWhat column keys does Atlan crawl?\nIf the following column keys are defined in the SQL database at source, Atlan will crawl and display them as attributes for your assets:\nPrimary key\n-  uniquely identifies each row in a table.\nForeign key\n-  links together two tables.\nPartition key\n-  determines logical partitions in a table.\nSort key\n-  determines the order in which rows are stored in a table.\nIndex key\n-  defines the order for an index in the database.\nCluster key\n-  determines the order in which the database is partitioned.\nDistributed key\n-  determines where data is stored in a database.\nView column keys\nâ\nNavigate to the left menu of any screen in Atlan and click\nAssets\nto begin:\nFrom the asset preview\nâ\nTo view column keys in the asset preview:\nFrom the\nAssets\npage, navigate to the asset preview section. The asset preview for column assets will display available column keys.\nFrom the asset profile\nâ\nTo view column keys in the asset profile:\nFrom the\nAssets\npage, right-click a table or a column asset and select\nOpen profile\n.\nNavigate to\nColumn preview\nto view available column keys.\nFrom the sidebar\nâ\nTo view column keys in the sidebar:\nFrom the\nAssets\npage, click a table or a column asset.\nIn the sidebar to the right, click the\nColumns\ntab to view available column keys in the sidebar.\nCreate foreign key relationships\nâ\nYou can create foreign key relationships only through APIs for your column assets in Atlan. You can use the\nforeignKeyTo\nand\nforeignKeyFrom\nstatements to create column references for your foreign keys and maintain the referential integrity of your assets. Refer to our\ndeveloper documentation\nto assign foreign key relationships.\nOnce you have created foreign key relationships, your users will be able to view the column references and better understand the relationships between assets.\nFilter assets by column keys\nâ\nYou can filter your asset search results by\ntype-specific property\nfilters, such as column keys for your column assets.Â\nTo filter column assets by column keys:\nFrom the left menu on any screen in Atlan, click\nAssets\n.\nUnder the search bar on the\nAssets\npage, click the\nColumn\ntab to filter for column assets.\nIn the\nFilters\nmenu on the left, click the\nColumn\nfilter.\nTo add a\ntype-specific property\nfilter to refine your search, select a column-type property filter   -  for example,\nPrimary key\n.\nIn the filter dialog, click\nYes\nto view column assets with a primary key or click\nNo\nto only view column assets without a primary key.\nSupported sources\nâ\nAtlan supports column keys for the following connectors:\nAmazon Athena\nAmazon Redshift\nAWS Glue\nDatabricks\nGoogle BigQuery\nHive\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nSAP HANA\nSnowflake\nTeradata\nTags:\natlan\ndocumentation\nfaq-connections\nPrevious\nHow often does Atlan crawl Snowflake?\nNext\nWhat's the difference between connecting to Athena and Glue?\nView column keys\nCreate foreign key relationships\nFilter assets by column keys\nSupported sources"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/query-frequency",
    "content": "Use data\nInsights\nFAQ\nWhat controls the frequency of queries?\nWhat controls the frequency of queries?\nAccess to querying assets can be controlled through Atlan's\npersona-based data policies\n. In order to prevent query abuse, there is a default limit of 100 on the number of rows in each query.\nTags:\natlan\ndocumentation\nfaq-insights\nPrevious\nHow to use parameterized queries?\nNext\nWhy do I only see tables from the same schema to join from in a visual query?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
    "content": "Connect data\nData Warehouses\nSnowflake\nManage Snowflake in Atlan\nConfigure Snowflake data metric functions\nOn this page\nConfigure Snowflake data metric functions\nPrivate Preview\nTo use system data metric functions (DMFs) from Snowflake, you need to configure your Snowflake setup.\nPrerequisites\nâ\nBefore proceeding with the configuration, make sure the following prerequisites are met:\nSnowflake editions\n-  Atlan data quality for Snowflake is only supported for Enterprise and Business Critical editions of Snowflake.\nAdministrative access   -  the user configuring Snowflake must have an\nACCOUNTADMIN\nrole or equivalent administrative privileges.\nDedicated warehouse   -  your organization must have a dedicated Snowflake warehouse for running data quality-related queries.\nCreate roles in Snowflake\nâ\nYou will need to create the following two roles in Snowflake for the Atlan data quality integration:\nData quality admin role (\ndq_admin\n)   -  a high-privilege role responsible for managing DMF associations on tables and views. This role is used to create and manage the database objects required for data quality operations.\nAtlan data quality service role (\natlan_dq_service_role\n)   -  a service role that Atlan will use to interact with Snowflake objects related to data quality operations. This role will be assigned to the Atlan service user.\nCreate data quality admin role\nâ\nRun the following commands to create the\ndq_admin\nrole and grant access to the Snowflake warehouse:\nCREATE\nOR\nREPLACE\nROLE dq_admin\n;\nGRANT\nOPERATE\n,\nUSAGE\nON\nWAREHOUSE\n\"<warehouse-name>\"\nTO\nROLE dq_admin\n;\nReplace\n<warehouse-name>\nwith the name of your dedicated Snowflake warehouse for running data quality-related queries.\nCreate Atlan data quality service role\nâ\nRun the following commands to create the\natlan_dq_service_role\nand grant access to the Snowflake warehouse:\nCREATE\nOR\nREPLACE\nROLE atlan_dq_service_role\n;\nGRANT\nOPERATE\n,\nUSAGE\nON\nWAREHOUSE\n\"<warehouse-name>\"\nTO\nROLE atlan_dq_service_role\n;\nReplace\n<warehouse-name>\nwith the name of your dedicated Snowflake warehouse for running data quality-related queries.\nCreate a user in Snowflake\nâ\nA dedicated Snowflake user is required for Atlan to connect to your Snowflake instance. You will need to create this integration user and assign the Atlan data quality service role to this user.\nRefer to the\nCreate a user\ndocumentation to create the new user.\nAfter creating the user, grant the Atlan data quality service role to the new user you created in Snowflake:\nGRANT\nROLE atlan_dq_service_role\nTO\nUSER\n<\natlan_dq_user\n>\n;\nGrant privileges\nâ\nThe following privileges must be granted to the roles created in Snowflake for the Atlan data quality integration:\nPrivileges for data quality admin\nâ\nGrant the\ndq_admin\nrole the ability to create databases and access system DMFs in Snowflake:\nGRANT\nCREATE\nDATABASE\nON\nACCOUNT\nTO\nROLE dq_admin\n;\nGRANT\nDATABASE\nROLE SNOWFLAKE\n.\nDATA_METRIC_USER\nTO\nROLE dq_admin\n;\nPrivileges for table owners\nâ\nFor each role that owns tables in your Snowflake environment, grant the following privileges:\nGRANT\nROLE\n<\ntable_owner\n>\nTO\nROLE dq_admin\n;\nGRANT\nDATABASE\nROLE SNOWFLAKE\n.\nDATA_METRIC_USER\nTO\nROLE\n<\ntable_owner\n>\n;\nGRANT\nEXECUTE\nDATA\nMETRIC\nFUNCTION\nON\nACCOUNT\nTO\nROLE\n<\ntable_owner\n>\n;\nReplace\n<table_owner>\nwith the role that owns Snowflake tables.\nPrivileges for Atlan data quality service role\nâ\nGrant the following privileges to the\natlan_dq_service_role\n:\nGRANT\nAPPLICATION ROLE SNOWFLAKE\n.\nDATA_QUALITY_MONITORING_VIEWER\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nDATABASE\nROLE SNOWFLAKE\n.\nDATA_METRIC_USER\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nEXECUTE\nTASK\nON\nACCOUNT\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nEXECUTE\nMANAGED TASK\nON\nACCOUNT\nTO\nROLE atlan_dq_service_role\n;\nSet up required objects\nâ\nOnce you have created roles and granted the required privileges, you will need to create the necessary objects such as a dedicated database, schema, and stored procedure to be used for managing DMF operations.\nChange to the\ndq_admin\nrole:\nUSE\nROLE dq_admin\n;\nCreate the database and schema in Snowflake for the Atlan data quality integration:\nCREATE\nDATABASE\nATLAN_DQ\n;\nCREATE\nSCHEMA\nATLAN_DQ\n.\nSHARED\n;\nThe\nATLAN_DQ\ndatabase serves as a container for all objects related to the Atlan data quality integration.\nThe\nATLAN_DQ.SHARED\nschema provides a separate namespace for shared procedures and functions.\nCreate the store procedure in Snowflake to manage DMFs:\n/**\n* Manages Data Metric Functions (DMF) attachment operations for Snowflake tabular entities. Runs with the privileges of the procedure owner.\n* @param {string} ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE)\n* @param {string} ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE)\n* @param {string} ENTITY_NAME - Fully qualified name of the entity (database.schema.name)\n* @param {string} [DMF_NAME=null] - Fully qualified name of the DMF (database.schema.name)\n* @param {string} [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations\n* @param {string} [SCHEDULE_TYPE=null] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED)\n* @param {string} [SCHEDULE_VALUE=null] - Schedule value based on type\n* @returns {string} - JSON string with operation status and result message\n*/\nCREATE\nOR\nREPLACE\nSECURE\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nACTION\nSTRING\n,\nENTITY_TYPE STRING\n,\nENTITY_NAME STRING\n,\nDMF_NAME STRING\nDEFAULT\nNULL\n,\nDMF_ARGUMENTS_JSON STRING\nDEFAULT\n'[]'\n,\nSCHEDULE_TYPE STRING\nDEFAULT\nNULL\n,\nSCHEDULE_VALUE STRING\nDEFAULT\nNULL\n)\nRETURNS\nSTRING\nLANGUAGE\nJAVASCRIPT\nEXECUTE\nAS\nOWNER\nAS\n$$\n// -----------------------------------------------------UTILITY FUNCTIONS-----------------------------------------------------\n/**\n* Executes a SQL query with parameters\n* @param {string} sqlText - SQL statement to execute\n* @param {Array} [binds=[]] - Array of bind parameters for the query\n* @param {boolean} [returnFirstRow=false] - Whether to return only the first row\n* @returns {Object} Object containing execution result or error information\n*/\nfunction\nexecuteQuery\n(\nsqlText\n,\nbinds\n=\n[\n]\n,\nreturnFirstRow\n=\nfalse\n)\n{\ntry {\nif\n(\n!\nsqlText\n)\nreturn\n{\nisErrored:\ntrue\n,\nmessage:\n\"SQL Text is required\"\n,\nresult:\nnull\n,\n}\n;\nconst statement\n=\nsnowflake\n.\ncreateStatement\n(\n{ sqlText\n,\nbinds }\n)\n;\nconst result\n=\nstatement\n.\nexecute\n(\n)\n;\nconst response\n=\n{\nisErrored:\nfalse\n,\nmessage:\n\"\"\n,\nresult:\nnull\n,\n}\n;\nif\n(\nreturnFirstRow\n)\n{\nresponse\n.\nresult\n=\nresult\n.\nnext\n(\n)\n? result :\nnull\n;\nreturn\nresponse\n;\n}\nresponse\n.\nresult\n=\nresult\n;\nreturn\nresponse\n;\n} catch\n(\nerr\n)\n{\nreturn\n{\nisErrored:\ntrue\n,\nmessage:\n`\n${err.code} - ${err.message} - ${sqlText} with binds: ${binds.join(\", \")}\n`\n,\nresult:\nnull\n,\n}\n;\n}\n}\n/**\n* Safely parses a JSON string\n* @param {string} jsonString - JSON string to parse\n* @returns {Object} Parsed JSON object or null if invalid\n*/\nfunction\nsafelyParseJSON\n(\njsonString\n)\n{\ntry {\nreturn\nJSON\n.\nparse\n(\njsonString\n)\n;\n} catch\n(\nerr\n)\n{\nreturn\nnull\n;\n}\n}\n/**\n* Validates a number within a range\n* @param {string} value - Number to validate\n* @param {number} min - Minimum value\n* @param {number} max - Maximum value\n* @returns {boolean} True if number is valid\n* @returns {boolean} False if number is invalid\n*/\nfunction\nisNumberValid\n(\nvalue\n,\nmin\n,\nmax\n)\n{\nconst num\n=\nparseInt\n(\nvalue\n,\n10\n)\n;\nreturn\n!\nisNaN\n(\nnum\n)\n&&\nnum\n=\nmin\n&&\nnum\n&\nle\n;\nmax\n;\n=\n\"max;\"\n}\n=\n\"}\"\n*\n*\n=\n\"**\"\n*\n=\n\"*\"\nescapes\n=\n\"Escapes\"\nand\n=\n\"and\"\nquotes\n=\n\"quotes\"\na\n=\n\"a\"\nsnowflake\n=\n\"Snowflake\"\nidentifier\n=\n\"identifier\"\n@param\n=\n\"@param\"\n{string}\n=\n\"{string}\"\n-\n=\n\"-\"\nraw\n=\n\"Raw\"\nto\n=\n\"to\"\nnormalize\n=\n\"normalize\"\n@returns\n=\n\"@returns\"\nproperly\n=\n\"Properly\"\nquoted\n=\n\"quoted\"\nsafe\n=\n\"safe\"\nfor\n=\n\"for\"\nsql\n=\n\"SQL\"\nfunction\n=\n\"FUNCTION\"\nnormalizeidentifier\n(\nidentifier\n)\n=\n\"normalizeIdentifier(identifier)\"\n{\n=\n\"{\"\nreturn\n=\n\"return\"\n`\n=\"\n`\n\" ${identifier\n.\nreplace\n(\n=\n\"${identifier.replace(\"\ng\n,\n=\n\"g,\"\n)\n}\n=\n\")}\"\n`\n;=\"\n`\n;\n\" retrieves=\"\nRetrieves\n\" all=\"\nall\n\" columns=\"\ncolumns\n\" given=\"\ngiven\n\" entity.=\"\nentity\n.\n\" validates=\"\nValidates\n\" that=\"\nthat\n\" the=\"\nthe\n\" entityexists=\"\nentityexists\n\" procedure=\"\nprocedure\n\" owner=\"\nowner\n\" has=\"\nhas\n\" access=\"\naccess\n\" it.=\"\nit\n.\n\" entityname=\"\nentityName\n\" fully=\"\nFully\n\" qualified=\"\nqualified\n\" entity=\"\nentity\n\" name=\"\nname\n\" {array}=\"\n{Array}\n\" array=\"\nArray\n\" of=\"\nof\n\" column=\"\ncolumn\n\" objects=\"\nobjects\n\" with=\"\nwith\n\" datatype=\"\ndataType\n\" properties=\"\nproperties\n\" @throws=\"\n@throws\n\" {error}\n=\n\"{Error}\"\nif\n=\n\"if\"\ndoesn\n=\n\"doesn\"\nt\n=\n\"t\"\nexist\n=\n\"exist\"\nor\n=\n\"or\"\nis\n=\n\"is\"\ninaccessible\n=\n\"inaccessible\"\ngetallcolumnsforentity\n(\nentityname\n)\n=\n\"getAllColumnsForEntity(entityName)\"\nconst\n=\n\"const\"\nsqltext\n=\n\"SHOW COLUMNS IN IDENTIFIER(?)\"\n;\n=\n\";\"\nbinds\n=\n\"[entityName];\"\nresult\n,\n=\n\"result,\"\niserrored\n,\n=\n\"isErrored,\"\nmessage\n=\n\"message\"\nbinds\n)\n;\n=\n\"binds);\"\n(\niserrored\n)\n=\n\"(isErrored)\"\nexists\n=\n\"exists\"\nit\n=\n\"it\"\nthrow\n=\n\"throw\"\nnew\n=\n\"new\"\nerror\n(\nmessage\n)\n;\n=\n\"Error(message);\"\nwhile\n=\n\"while\"\n(\nresult\n.\nnext\n(\n)\n)\n=\n\"(result.next())\"\nname:\n=\n\"name:\"\nresult\n.\ngetcolumnvalue\n(\n=\n\"result.getColumnValue(\"\ncolumn_name\n=\n\"column_name\"\n)\n,\n=\n\"),\"\ndatatype:\n=\n\"dataType:\"\njson\n.\nparse\n(\nresult\n.\ngetcolumnvalue\n(\n=\n\"JSON.parse(result.getColumnValue(\"\ndata_type\n=\n\"data_type\"\n)\n)\n.\ntype\n,\n=\n\")).type,\"\n}\n;\n=\n\"};\"\n(\ncolumn\n.\ndatatype\n=\n\"==\"\nfixed\n=\n\"FIXED\"\n)\n=\n\")\"\ncolumn\n.\ndatatype\n=\n\"NUMBER\"\ncolumns\n.\npush\n(\ncolumn\n)\n;\n=\n\"columns.push(column);\"\ncolumns\n;\n=\n\"columns;\"\ndmf\n=\n\"DMF\"\nvalid\n=\n\"valid\"\ndmfname\n=\n\"dmfName\"\ndmfarguments\n=\n\"dmfArguments\"\narguments\n=\n\"arguments\"\n{\nboolean\n}\n=\n\"{boolean}\"\nwhether\n=\n\"Whether\"\ninvalid\n=\n\"invalid\"\nisdmfvalid\n(\ndmfname\n,\n=\n\"isDMFValid(dmfName,\"\ndmfarguments\n)\n=\n\"dmfArguments)\"\nidentifier\n(\n?\n)\n(\n${dmfarguments}\n)\n`\n,=\"IDENTIFIER(?)(${dmfArguments})\n`\n,\n\" [dmfname],=\"\n[\ndmfName\n]\n,\n\" true);=\"\ntrue\n)\n;\n\" true;=\"\ntrue\n;\n\" checks=\"\nChecks\n\" timezone=\"\nTimezone\n\" validate=\"\nvalidate\n\" true=\"\nTrue\n\" false=\"\nFalse\n\" istimezonevalid(timezone)=\"\nisTimezoneValid\n(\ntimezone\n)\n\" result=\"\nexecuteQuery\n(\n`\nSELECT\" convert_timezone(?,=\"CONVERT_TIMEZONE(?,\" current_timestamp())\n`\n,\n=\n\"CURRENT_TIMESTAMP())`,\"\n[\ntimezone\n]\n,\n=\n\"[timezone],\"\n!\nresult\n.\niserrored\n;\n=\n\"!result.isErrored;\"\ngenerates\n=\n\"Generates\"\ntype\n=\n\"type\"\nsignature\n=\n\"signature\"\nbased\n=\n\"based\"\non\n=\n\"on\"\n{object}\n=\n\"{Object}\"\nentitycolumnsmap\n=\n\"entityColumnsMap\"\nmap\n=\n\"Map\"\nnames\n=\n\"names\"\nin\n=\n\"in\"\nformat\n=\n\"format\"\n>\n:\n[\n{ name:\n,\ndataType:  }\n]\n}\n*\n@param\n{string} baseEntityName\n-\nName\nof\nthe base entity\n*\n@returns\n{string} DMF\ntype\nsignature\n*\n@throws\n{Error}\nIf\nentity\nnot\nfound\nin\nthe cache\n*\n/\nfunction\ngenerateDMFTypeSignature\n(\ndmfArguments\n,\nentityColumnsMap\n,\nbaseEntityName\n)\n{\nif\n(\n!\ndmfArguments\n||\n!\ndmfArguments\n.\nlength\n)\nreturn\n\"\"\n;\nconst baseEntityColumns\n=\nentityColumnsMap\n[\nbaseEntityName\n]\n;\nif\n(\n!\nbaseEntityColumns\n)\n{\nthrow new Error\n(\n`\nEntity ${baseEntityName} not found in the cache\n`\n)\n;\n}\nconst baseEntityColumnArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=\nparam\n.\ntype\n=\n=\n=\n\"COLUMN\"\n)\n.\nmap\n(\nparam\n=\n{\nconst\ncolumn\n=\nbaseEntityColumns\n.\nfind\n(\ncol\n=\ncol\n.\nname\n=\n=\n=\nparam\n.\nname\n)\n;\nreturn\ncolumn\n?\ncolumn\n.\ndataType :\nnull\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\nconst baseEntityArguments\n=\n`\nTABLE(${baseEntityColumnArguments})\n`\n;\nconst referencedEntityArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=\nparam\n.\ntype\n=\n=\n=\n\"TABLE\"\n)\n.\nmap\n(\nentityParam\n=\n{\nconst entityName\n=\nentityParam\n.\nname\n;\nconst entityColumns\n=\nentityColumnsMap\n[\nentityName\n]\n;\nif\n(\n!\nentityColumns\n)\n{\nthrow new Error\n(\n`\nEntity ${entityName} not found in the cache\n`\n)\n;\n}\nconst columnTypes\n=\nentityParam\n.\nnested\n.\nmap\n(\nnestedParam\n=\n{\nconst\ncolumn\n=\nentityColumns\n.\nfind\n(\ncol\n=\ncol\n.\nname\n=\n=\n=\nnestedParam\n.\nname\n)\n;\nreturn\ncolumn\n?\ncolumn\n.\ndataType :\nnull\n;\n}\n)\n.\nfilter\n(\nBoolean\n)\n.\njoin\n(\n\", \"\n)\n;\nreturn\n`\nTABLE(${columnTypes})\n`\n;\n}\n)\n;\nconst arguments\n=\n[\nbaseEntityArguments\n,\n.\n.\n.\nreferencedEntityArguments\n]\n.\njoin\n(\n\", \"\n)\n;\nreturn\narguments\n;\n}\n/**\n* Generates DMF arguments for SQL statements\n* @param {string} dmfArguments - Array of DMF arguments\n* @returns {string} Formatted DMF arguments for SQL statements\n*/\nfunction\ngenerateDMFColumnArguments\n(\ndmfArguments\n)\n{\nreturn\ndmfArguments\n.\nmap\n(\nparam\n=\n{\nif\n(\nparam\n.\ntype\n=\n=\n=\n\"COLUMN\"\n)\n{\nreturn\nnormalizeIdentifier\n(\nparam\n.\nname\n)\n;\n}\n// Handle TABLE type with nested columns\nreturn\n`\n${normalizeIdentifier(param.name)}(${\nparam.nested\n.map(nested = normalizeIdentifier(nested.name))\n.join(\", \")\n})\n`\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\n}\n// -----------------------------------------------------VALIDATION FUNCTIONS-----------------------------------------------------\n/**\n* Validates that mandatory arguments are provided and valid\n* @throws {Error} If any mandatory argument is missing or invalid\n*/\nfunction\nvalidateMandatoryArguments\n(\n)\n{\nconst VALID_ACTIONS\n=\nnew\nSet\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n,\n\"UPDATE_SCHEDULE\"\n]\n)\n;\nconst VALID_ENTITY_TYPES\n=\nnew\nSet\n(\n[\n\"TABLE\"\n,\n\"VIEW\"\n,\n\"MATERIALIZED VIEW\"\n,\n\"EXTERNAL TABLE\"\n,\n\"ICEBERG TABLE\"\n]\n)\n;\nconst DMF_OPS\n=\nnew\nSet\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n]\n)\n;\nconst VALID_SCHEDULE_TYPES\n=\nnew\nSet\n(\n[\n\"MINUTES\"\n,\n\"CRON\"\n,\n\"ON_DATA_CHANGE\"\n,\n\"NOT_SCHEDULED\"\n]\n)\n;\nconst SCHEDULE_TYPES_THAT_REQUIRE_VALUE\n=\nnew\nSet\n(\n[\n\"MINUTES\"\n,\n\"CRON\"\n]\n)\n;\nif\n(\n!\nVALID_ACTIONS\n.\nhas\n(\nACTION\n)\n)\nthrow new Error\n(\n`\nInvalid ACTION: \"${ACTION}\". Valid options are ${Array.from(VALID_ACTIONS).join(\", \")}\n`\n)\n;\nif\n(\n!\nVALID_ENTITY_TYPES\n.\nhas\n(\nENTITY_TYPE\n)\n)\nthrow new Error\n(\n`\nInvalid ENTITY_TYPE: \"${ENTITY_TYPE}\". Valid options are ${Array.from(VALID_ENTITY_TYPES).join(\", \")}\n`\n)\n;\nif\n(\nDMF_OPS\n.\nhas\n(\nACTION\n)\n&&\n!\nDMF_NAME\n)\nthrow new Error\n(\n\"DMF_NAME is required for DMF related actions\"\n)\n;\nif\n(\nACTION\n=\n=\n=\n\"UPDATE_SCHEDULE\"\n)\n{\nif\n(\n!\nSCHEDULE_TYPE\n)\nthrow new Error\n(\n\"SCHEDULE_TYPE is required for SCHEDULE action\"\n)\n;\nif\n(\n!\nVALID_SCHEDULE_TYPES\n.\nhas\n(\nSCHEDULE_TYPE\n)\n)\nthrow new Error\n(\n`\nInvalid schedule type: \"${SCHEDULE_TYPE}\". Valid options are ${Array.from(VALID_SCHEDULE_TYPES).join(\", \")}\n`\n)\n;\nif\n(\nSCHEDULE_TYPES_THAT_REQUIRE_VALUE\n.\nhas\n(\nSCHEDULE_TYPE\n)\n&&\n!\nSCHEDULE_VALUE\n)\nthrow new Error\n(\n\"SCHEDULE_VALUE is required for SCHEDULE action\"\n)\n;\n}\n}\n/**\n* Parses a fully qualified name into its components\n* @param {string} fullyQualifiedName - Fully qualified name to parse\n* @returns {Object} Object with database, schema, and name properties\n* @throws {Error} If invalid fully qualified name\n*/\nfunction\nvalidateFullyQualifiedName\n(\nfullyQualifiedName\n)\n{\nconst parts\n=\nfullyQualifiedName\n.\nsplit\n(\n\".\"\n)\n.\nmap\n(\npart\n=\npart\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n;\nif\n(\nparts\n.\nlength\n!=\n=\n3\n)\nthrow new Error\n(\n`\nInvalid fully qualified name: ${fullyQualifiedName}. Expected format: database.schema.name\n`\n)\n;\n}\n/**\n* Validates the structure of DMF arguments JSON\n* @param {string} rawDMFArguments - Raw JSON string of DMF arguments\n* @throws {Error} If DMF arguments structure is invalid\n*/\nfunction\nvalidateDMFArgumentsStructure\n(\nrawDMFArguments\n)\n{\nconst parsedStructure\n=\nsafelyParseJSON\n(\nrawDMFArguments\n)\n;\nif\n(\n!\nparsedStructure\n)\nthrow new Error\n(\n\"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\"\n)\n;\nif\n(\n!\nArray\n.\nisArray\n(\nparsedStructure\n)\n)\nthrow new Error\n(\n\"DMF_ARGUMENTS_JSON must be an array\"\n)\n;\nconst referencedEntities\n=\nparsedStructure\n.\nfilter\n(\n(\nparam\n)\n=\nparam\n.\ntype\n=\n=\n=\n\"TABLE\"\n)\n;\nif\n(\nreferencedEntities\n.\nlength\n1\n)\nthrow new Error\n(\n\"Only one referenced entity is allowed\"\n)\n;\nconst validationFunctions\n=\n{\narrayItem:\n(\nparam\n)\n=\n[\n\"COLUMN\"\n,\n\"TABLE\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n&&\nparam\n.\nname\n,\nnestedItem:\n(\nparam\n)\n=\n[\n\"COLUMN\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n&&\nparam\n.\nname\n,\n}\n;\nif\n(\n!\nparsedStructure\n.\nevery\n(\nvalidationFunctions\n.\narrayItem\n)\n)\nthrow new Error\n(\n\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"\n)\n;\nif\n(\nreferencedEntities\n.\nlength\n0\n)\n{\nfor\n(\nconst referencedEntity\nof\nreferencedEntities\n)\n{\nif\n(\n!\nArray\n.\nisArray\n(\nreferencedEntity\n.\nnested\n)\n||\n!\nreferencedEntity\n.\nnested\n.\nevery\n(\nvalidationFunctions\n.\nnestedItem\n)\n)\nthrow new Error\n(\n\"Invalid nested parameters\"\n)\n;\n}\n}\n}\n/**\n* Validates that all specified columns exist in an entity\n* @param {Array} columnsToCheck - Array of column names to validate\n* @param {Array} entityColumns - Array of column metadata from the entity\n* @param {string} entityName - Name of the entity for error message\n* @throws {Error} If any column doesn't exist in the entity\n*/\nfunction\nvalidateColumnsExistInEntity\n(\nentityName\n,\nallColumnsInEntity\n,\ncolumnsToCheck\n)\n{\nfor\n(\nconst\ncolumn\nof\ncolumnsToCheck\n)\n{\nif\n(\n!\nallColumnsInEntity\n.\nsome\n(\ncol\n=\ncol\n.\nname\n=\n=\n=\ncolumn\n)\n)\nthrow new Error\n(\n`\nColumn ${column} not found in entity ${entityName}\n`\n)\n;\n}\n}\n/**\n* Validates that all provided identifiers exist and are accessible\n* Checks entity names, column names, and DMF compatibility\n* @param {string} entityName - Fully qualified name of the entity\n* @param {string} dmfName - Fully qualified name of the DMF\n* @param {Array} dmfArguments - Array of DMF arguments\n* @throws {Error} If any identifier doesn't exist or is inaccessible\n*/\nfunction\nvalidateProvidedIdentifiers\n(\nentityName\n,\ndmfName\n=\n\"\"\n,\ndmfArguments\n=\n[\n]\n)\n{\nvalidateFullyQualifiedName\n(\nentityName\n)\n;\n// Validate the provided entity names and store all the columns for each entity in a map\nconst baseEntityName\n=\nentityName\n;\nconst baseEntityAllColumns\n=\ngetAllColumnsForEntity\n(\nentityName\n)\n;\nconst entityColumnsMap\n=\n{\n[\nbaseEntityName\n]\n: baseEntityAllColumns }\n;\nconst allReferencedEntities\n=\ndmfArguments\n.\nfilter\n(\nparam\n=\nparam\n.\ntype\n=\n=\n=\n\"TABLE\"\n)\n;\nfor\n(\nconst referencedEntity\nof\nallReferencedEntities\n)\n{\nconst\ncolumns\n=\ngetAllColumnsForEntity\n(\nreferencedEntity\n.\nname\n)\n;\nentityColumnsMap\n[\nreferencedEntity\n.\nname\n]\n=\ncolumns\n;\n}\n// Valite all of the provided columns are valid and exist in their respective entities\nconst allBaseEntityColumnsInArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=\nparam\n.\ntype\n=\n=\n=\n\"COLUMN\"\n)\n.\nmap\n(\nparam\n=\nparam\n.\nname\n)\n;\nvalidateColumnsExistInEntity\n(\nbaseEntityName\n,\nbaseEntityAllColumns\n,\nallBaseEntityColumnsInArguments\n)\n;\nfor\n(\nconst referencedEntity\nof\nallReferencedEntities\n)\n{\nconst columnsInArguments\n=\nreferencedEntity\n.\nnested\n.\nmap\n(\nnestedParam\n=\nnestedParam\n.\nname\n)\n;\nvalidateColumnsExistInEntity\n(\nreferencedEntity\n.\nname\n,\nentityColumnsMap\n[\nreferencedEntity\n.\nname\n]\n,\ncolumnsInArguments\n)\n;\n}\nif\n(\ndmfName\n)\n{\n// Validate that the DMF is valid and exists\nconst generatedTypeSignature\n=\ngenerateDMFTypeSignature\n(\ndmfArguments\n,\nentityColumnsMap\n,\nbaseEntityName\n)\n;\nisDMFValid\n(\ndmfName\n,\ngeneratedTypeSignature\n)\n;\n}\n// All provided identifiers are valid, actually exist and are accessible to the procedure owner\n}\n/**\n* Validates CRON expression syntax\n* Performs detailed validation of all CRON components and timezones to protect against SQL injection\n* @param {string} cronExpression - CRON expression to validate\n* @throws {Error} If CRON expression is invalid\n*/\nfunction\nvalidateCronExpression\n(\ncronExpression\n)\n{\nif\n(\ncronExpression\n.\nlength\n100\n)\nthrow new Error\n(\n\"Cron expression is too long\"\n)\n;\nconst cronFields\n=\ncronExpression\n.\ntrim\n(\n)\n.\nsplit\n(\n/\n\\s\n+\n/\n)\n;\nif\n(\ncronFields\n.\nlength\n!=\n=\n6\n)\nthrow new Error\n(\n\"Invalid cron expression. Expected 6 fields\"\n)\n;\nconst\n[\nminute\n,\nhour\n,\ndayOfMonth\n,\nmonth\n,\ndayOfWeek\n,\ntimezone\n]\n=\ncronFields\n;\nconst isTimezoneValidResult\n=\nisTimezoneValid\n(\ntimezone\n)\n;\nif\n(\n!\nisTimezoneValidResult\n)\nthrow new Error\n(\n\"Invalid timezone provided in the cron expression\"\n)\n;\nconst regexPatterns\n=\n{\nminute\n:\n/\n^\n(\n\\\n*\n|\n\\d\n+\n|\n\\\n*\n\\\n/\n\\d\n+\n|\n\\d\n+\n\\\n-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\nhour\n:\n/\n^\n(\n\\\n*\n|\n\\d\n+\n|\n\\\n*\n\\\n/\n\\d\n+\n|\n\\d\n+\n\\\n-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\ndayOfMonth:\n/\n^\n(\n\\\n*\n|\nL\n|\n\\d\n+\n|\n\\\n*\n\\\n/\n\\d\n+\n|\n\\d\n+\n\\\n-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\nmonth\n:\n/\n^\n(\n\\\n*\n|\n\\d\n+\n|\nJAN\n|\nFEB\n|\nMAR\n|\nAPR\n|\nMAY\n|\nJUN\n|\nJUL\n|\nAUG\n|\nSEP\n|\nOCT\n|\nNOV\n|\nDEC\n|\n\\\n*\n\\\n/\n\\d\n+\n|\n\\d\n+\n\\\n-\n\\d\n+\n|\n[\nA\n-\nZ\n]\n{\n3\n}\\\n-\n[\nA\n-\nZ\n]\n{\n3\n}\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n|\n(\n[\nA\n-\nZ\n]\n{\n3\n}\n(\n,\n[\nA\n-\nZ\n]\n{\n3\n}\n)\n*\n)\n)\n$\n/\ni\n,\ndayOfWeek:\n/\n^\n(\n\\\n*\n|\n\\d\n+\n|\nSUN\n|\nMON\n|\nTUE\n|\nWED\n|\nTHU\n|\nFRI\n|\nSAT\n|\n\\d\n+\nL\n|\n[\nA\n-\nZ\n]\n{\n3\n}L\n|\n\\\n*\n\\\n/\n\\d\n+\n|\n\\d\n+\n\\\n-\n\\d\n+\n|\n[\nA\n-\nZ\n]\n{\n3\n}\\\n-\n[\nA\n-\nZ\n]\n{\n3\n}\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n|\n(\n[\nA\n-\nZ\n]\n{\n3\n}\n(\n,\n[\nA\n-\nZ\n]\n{\n3\n}\n)\n*\n)\n)\n$\n/\ni\n,\n}\n;\nif\n(\nminute\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nminute\n,\n0\n,\n59\n)\n)\nthrow new Error\n(\n\"Invalid minute value\"\n)\n;\nif\n(\nhour\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nhour\n,\n0\n,\n23\n)\n)\nthrow new Error\n(\n\"Invalid hour value\"\n)\n;\nif\n(\ndayOfMonth\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\ndayOfMonth\n,\n1\n,\n31\n)\n)\nthrow new Error\n(\n\"Invalid day of month value\"\n)\n;\nif\n(\nmonth\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nmonth\n,\n1\n,\n12\n)\n)\nthrow new Error\n(\n\"Invalid month value\"\n)\n;\nif\n(\ndayOfWeek\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\ndayOfWeek\n,\n0\n,\n6\n)\n)\nthrow new Error\n(\n\"Invalid day of week value\"\n)\n;\nif\n(\n!\nregexPatterns\n.\nminute\n.\ntest\n(\nminute\n)\n||\n!\nregexPatterns\n.\nhour\n.\ntest\n(\nhour\n)\n||\n!\nregexPatterns\n.\ndayOfMonth\n.\ntest\n(\ndayOfMonth\n)\n||\n!\nregexPatterns\n.\nmonth\n.\ntest\n(\nmonth\n)\n||\n!\nregexPatterns\n.\ndayOfWeek\n.\ntest\n(\ndayOfWeek\n)\n)\nthrow new Error\n(\n\"Invalid cron expression\"\n)\n;\n}\n/**\n* Validates schedule-specific arguments\n* Ensures schedule type and value are compatible and valid\n* @throws {Error} If schedule configuration is invalid\n*/\nfunction\nvalidateProvidedArgumentsForSchedule\n(\n)\n{\nconst VALID_MINUTES\n=\nnew\nSet\n(\n[\n\"5\"\n,\n\"15\"\n,\n\"30\"\n,\n\"60\"\n,\n\"720\"\n,\n\"1440\"\n]\n)\n;\nif\n(\nSCHEDULE_TYPE\n=\n=\n=\n\"MINUTES\"\n&&\n!\nVALID_MINUTES\n.\nhas\n(\nSCHEDULE_VALUE\n)\n)\nthrow new Error\n(\n`\nInvalid SCHEDULE_VALUE for MINUTES. Valid options are ${Array.from(VALID_MINUTES).join(\", \")}\n`\n)\n;\nif\n(\nSCHEDULE_TYPE\n=\n=\n=\n\"CRON\"\n)\nvalidateCronExpression\n(\nSCHEDULE_VALUE\n)\n;\n// SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE\n}\n/**\n* Validates all provided arguments\n* Performs comprehensive validation on input parameters\n* @throws {Error} If any validation fails\n*/\nfunction\nvalidateAllArguments\n(\n)\n{\nvalidateMandatoryArguments\n(\n)\n;\n// Validates all mandatory arguments are provided in the correct format\nif\n(\nACTION\n=\n=\n=\n\"UPDATE_SCHEDULE\"\n)\nvalidateProvidedArgumentsForSchedule\n(\n)\n;\n// Validates the provided schedule type and value\nelse\n{\nvalidateDMFArgumentsStructure\n(\nDMF_ARGUMENTS_JSON\n)\n;\n}\nvalidateProvidedIdentifiers\n(\nENTITY_NAME\n,\nDMF_NAME\n,\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n)\n;\n// All provided arguments are valid and legal\n}\n// -----------------------------------------------------MAIN FUNCTION-----------------------------------------------------\n/**\n* Main function to manage DMF operations\n* Validates all arguments and executes the main logic\n* @returns {string} JSON string with operation status and result message\n*/\nfunction\nmain\n(\n)\n{\nvalidateAllArguments\n(\n)\n;\n// Validate all provided arguments\n// If the provided arguments are valid, proceed with the main logic\nconst dmfArguments\n=\ngenerateDMFColumnArguments\n(\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n)\n;\nconst SQL_TEMPLATES\n=\n{\nATTACH_DMF:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} ADD DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments})\n`\n,\nDETACH_DMF:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} DROP DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments})\n`\n,\nSUSPEND_DMF:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) SUSPEND\n`\n,\nRESUME_DMF:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) RESUME\n`\n,\nUPDATE_SCHEDULE: {\nMINUTES:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = '${SCHEDULE_VALUE} MINUTE'\n`\n,\nCRON:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'USING CRON ${SCHEDULE_VALUE}'\n`\n,\nON_DATA_CHANGE:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES'\n`\n,\nNOT_SCHEDULED:\n`\nALTER ${ENTITY_TYPE} ${ENTITY_NAME} UNSET DATA_METRIC_SCHEDULE\n`\n,\n}\n,\n}\n;\nlet sqlText\n=\n\"\"\n;\nlet returnMessage\n=\n\"\"\n;\nif\n(\nACTION\n=\n=\n=\n\"UPDATE_SCHEDULE\"\n)\n{\nsqlText\n=\nSQL_TEMPLATES\n[\nACTION\n]\n[\nSCHEDULE_TYPE\n]\n;\nreturnMessage\n=\n`\nSuccessfully updated schedule for ${ENTITY_NAME} to ${SCHEDULE_TYPE} ${SCHEDULE_VALUE}\n`\n;\n}\nelse\n{\nsqlText\n=\nSQL_TEMPLATES\n[\nACTION\n]\n;\nreturnMessage\n=\n`\nACTION: ${ACTION} performed successfully on ${ENTITY_NAME} with DMF: ${DMF_NAME} and DMF Arguments: ${dmfArguments}\n`\n;\n}\nconst result\n=\nexecuteQuery\n(\nsqlText\n)\n;\nreturn\nJSON\n.\nstringify\n(\n{\nisSuccessful:\n!\nresult\n.\nisErrored\n,\nmessage: result\n.\nisErrored ? result\n.\nmessage : returnMessage\n,\n}\n)\n;\n}\n// Execute the main function and return the result\ntry {\nreturn\nmain\n(\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\nJSON\n.\nstringify\n(\n{\nisSuccessful:\nfalse\n,\nmessage: err\n.\nmessage\n,\n}\n)\n;\n}\n$$\n;\nGrant access to Atlan data quality service role\nâ\nFinally, grant permissions to the Atlan data quality service role to access the database, schema, and stored procedure you created in Snowflake:\nUSE\nROLE dq_admin\n;\nGRANT\nUSAGE\nON\nDATABASE\nATLAN_DQ\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\nATLAN_DQ\n.\nSHARED\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n)\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nCREATE\nSCHEMA\nON\nDATABASE\nATLAN_DQ\nTO\nROLE atlan_dq_service_role\n;\nTags:\ndata\nintegration\nsetup\nconfiguration\nPrevious\nManage Snowflake tags\nNext\nMultiple tag values and concatenation\nPrerequisites\nCreate roles in Snowflake\nCreate a user in Snowflake\nGrant privileges\nSet up required objects\nGrant access to Atlan data quality service role"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/preflight-checks-for-snowflake",
    "content": "Connect data\nData Warehouses\nSnowflake\nReferences\nPreflight checks for Snowflake\nOn this page\nPreflight checks for Snowflake\nBefore\nrunning the Snowflake crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nDatabase and schema check\nâ\nInformation schema\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nAccount usage\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nWarehouse access\nâ\nâ\nCheck successful\nâ\nOperation cannot be performed\n/\nUser is not authorized to perform this action\n/\nCannot be resumed because resource monitor has exceeded its quota\nMiner\nâ\nDid you know?\nOnce you have crawled assets from Snowflake, you can\nmine query history\n.\nQuery history view\nâ\nâ\nCheck successful\nâ\nCannot access query history table. Please run the command in your Snowflake instance: GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role;\nAccess history view\nâ\nâ\nCheck successful\nâ\nCheck failed. Something went wrong with your request.\nSessions view\nâ\nâ\nCheck successful\nâ\nCheck failed. Something went wrong with your request.\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Snowflake?\nNext\nTroubleshooting Snowflake connectivity\nDatabase and schema check\nWarehouse access\nMiner"
  },
  {
    "url": "https://docs.atlan.com/tags/connectivity",
    "content": "64 docs tagged with \"connectivity\"\nView all tags\nAiven Kafka\nIntegrate, catalog, and govern Aiven Kafka assets in Atlan.\nAmazon Athena\nIntegrate, catalog, and govern Amazon Athena assets in Atlan.\nAmazon DynamoDB\nIntegrate, catalog, and govern Amazon DynamoDB assets in Atlan.\nAmazon MSK\nIntegrate, catalog, and govern Amazon MSK assets in Atlan.\nAmazon MWAA OpenLineage\nIntegrate, catalog, and visualize Amazon MWAA lineage in Atlan.\nAmazon QuickSight\nIntegrate, catalog, and govern Amazon QuickSight assets in Atlan.\nAmazon Redshift\nIntegrate, catalog, and govern Amazon Redshift assets in Atlan.\nAmazon S3\nIntegrate, catalog, and govern Amazon S3 assets in Atlan.\nAnomalo\nIntegrate, catalog, and govern Anomalo assets in Atlan.\nApache Airflow OpenLineage\nIntegrate, catalog, and visualize Apache Airflow lineage in Atlan.\nApache Kafka\nIntegrate, catalog, and govern Apache Kafka assets in Atlan.\nApache Spark OpenLineage\nIntegrate, catalog, and visualize Apache Spark lineage in Atlan.\nAstronomer OpenLineage\nIntegrate, catalog, and visualize Astronomer lineage in Atlan.\nAWS Glue\nIntegrate, catalog, and govern AWS Glue assets in Atlan.\nBigID\nIntegrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata.\nCloudera Impala\nIntegrate, catalog, and govern Cloudera Impala assets in Atlan.\nConfluent Kafka\nIntegrate, catalog, and govern Confluent Kafka assets in Atlan.\nConfluent Schema Registry\nIntegrate, catalog, and govern Confluent Schema Registry assets in Atlan.\nCrawl Cloudera Impala\nLearn how to crawl metadata from Cloudera Impala into Atlan.\nDagster\nIntegrate, catalog, and visualize Dagster lineage in Atlan.\nDatabricks\nIntegrate, catalog, and govern Databricks assets in Atlan.\nDataStax Enterprise\nCatalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data.\ndbt\nIntegrate, catalog, and govern dbt assets in Atlan.\nDomo\nIntegrate, catalog, and govern Domo assets in Atlan.\nFivetran\nIntegrate, catalog, and govern Fivetran assets in Atlan.\nGoogle BigQuery\nIntegrate, catalog, and govern Google BigQuery assets in Atlan.\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan.\nHive\nCatalog and govern Hive assets in Atlan for discovery and governance.\nIBM Cognos Analytics\nIntegrate, catalog, and govern IBM Cognos Analytics assets in Atlan.\nInformatica CDI\nIntegrate, catalog, and govern Informatica Cloud Data Integration assets in Atlan.\nLooker\nIntegrate, catalog, and govern Looker assets in Atlan.\nMatillion\nIntegrate, catalog, and govern Matillion assets in Atlan.\nMetabase\nIntegrate, catalog, and govern Metabase assets in Atlan.\nMicrosoft Azure Cosmos DB\nCatalog and govern Cosmos DB assets in Atlan for discovery and governance.\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan.\nMicrosoft Azure Event Hubs\nIntegrate, catalog, and govern Azure Event Hubs assets in Atlan.\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan.\nMicrosoft Power BI\nIntegrate, catalog, and govern Power BI assets in Atlan.\nMicrosoft SQL Server\nIntegrate, catalog, and govern Microsoft SQL Server assets in Atlan.\nMicroStrategy\nIntegrate, catalog, and govern MicroStrategy assets in Atlan.\nMode\nIntegrate, catalog, and govern Mode assets in Atlan.\nMongoDB\nCatalog and govern MongoDB assets in Atlan for discovery and governance.\nMonte Carlo\nIntegrate, catalog, and govern Monte Carlo assets in Atlan.\nMySQL\nIntegrate, catalog, and govern MySQL assets in Atlan.\nOn-Premises Databases\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required.\nOracle\nIntegrate, catalog, and govern Oracle assets in Atlan.\nPostgreSQL\nIntegrate, catalog, and govern PostgreSQL assets in Atlan.\nPrestoSQL\nIntegrate, catalog, and govern PrestoSQL assets in Atlan.\nQlik Sense Cloud\nIntegrate, catalog, and govern Qlik Sense Cloud assets in Atlan.\nQlik Sense Enterprise (Windows)\nIntegrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan.\nRedash\nIntegrate, catalog, and govern Redash assets in Atlan.\nRedpanda Kafka\nIntegrate, catalog, and govern Redpanda Kafka assets in Atlan.\nSalesforce\nIntegrate, catalog, and govern Salesforce assets in Atlan.\nSAP ECC\nIntegrate, catalog, and govern SAP ECC assets in Atlan.\nSAP HANA\nCatalog and govern SAP HANA assets in Atlan for discovery and governance.\nSAP S/4HANA\nIntegrate, catalog, and govern SAP S/4HANA assets in Atlan.\nSigma\nIntegrate, catalog, and govern Sigma assets in Atlan.\nSisense\nIntegrate, catalog, and govern Sisense assets in Atlan.\nSnowflake\nIntegrate, catalog, and govern Snowflake assets in Atlan.\nSoda\nIntegrate, catalog, and govern Soda assets in Atlan.\nTableau\nIntegrate, catalog, and govern Tableau assets in Atlan.\nTeradata\nCatalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage.\nThoughtSpot\nIntegrate, catalog, and govern ThoughtSpot assets in Atlan.\nTrino\nIntegrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/snowflake",
    "content": "12 docs tagged with \"snowflake\"\nView all tags\nData quality permissions\nReference for data quality permission scopes and configuration in Atlan.\nEnable auto re-attachment of rules\nLearn how to enable automatic re-attachment of data quality rules to Snowflake tables and views.\nEnable data quality on connection\nEnable and configure data quality for your Snowflake connection in Atlan.\nMultiple tag values and concatenation\nLearn how Atlan handles multiple tag values for Snowflake objects, including concatenation, sorting, and reverse synchronization.\nOperations\nAtlan crawls and manages the following data quality operations and results from Snowflake.\nRoles and permissions\nExplanation of Snowflake's security model and role requirements for data quality operations.\nRules and dimensions\nReference for available data quality rules and classification dimensions in Snowflake data quality.\nSet up Snowflake\nConfigure Snowflake to enable data quality monitoring through Atlan.\nSnowflake\nIntegrate, catalog, and govern Snowflake assets in Atlan.\nSnowflake Data Quality Studio\nSet up and configure Snowflake for data quality monitoring through Atlan.\nSnowflake warehouse configuration\nRecommended Snowflake warehouse configuration to enable reliable Atlan workflow execution.\nUpgrade to Snowflake data quality studio\nUpdate existing Snowflake data quality integration to the latest version"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/troubleshooting/troubleshooting-connector-specific-sso-authentication",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nTroubleshooting\nTroubleshooting connector-specific SSO authentication\nOn this page\nTroubleshooting connector-specific SSO authentication\nAtlan currently supports the following connectors for SSO authentication to\nquery data\nand\npreview sample data\n:\nAmazon Redshift\nGoogle BigQuery\nSnowflake\nGeneral\nâ\nHow will SSO authentication interact with any data policies in Atlan?\nâ\nAtlan supports data policies mandated at source if using SSO authentication. Explicit restrictions will take precedence, unless otherwise configured.\nLet's examine two scenarios using the example of a masking policy:\nIf you have a\ndata policy\nin Atlan to mask sensitive data and are also using SSO authentication with no masking policy at source, the data will be masked in Atlan. However, if you have toggled on\nEnable data policies created at source to apply for querying in Atlan\nwhile configuring SSO authentication in Atlan, only source policies will take effect and the data will not be masked in Atlan as per the source policy.\nIf you do not have any\ndata policy\nin Atlan but are using SSO authentication with a masking policy at source for sensitive data, the data will be masked in Atlan.\nSnowflake\nâ\nWhy am I getting an incorrect username or password error message?\nâ\nIf you receive the following error message:\nCannot create PoolableConnectionFactory (Incorrect username or password was specified.)\nThe\nsecurity integration in Snowflake\nmaps Atlan email addresses to Snowflake login names. First, check if a user with an Atlan email address exists in Snowflake.\nIf a user exists and the Snowflake login name is not an email address, your Snowflake administrator will have to manually update the user-mapping in the security integration to use email addresses instead. To do so, add the following command to the\nsecurity integration in Snowflake\n:\nEXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE\n=\n'EMAIL_ADDRESS'\nRefer to\nSnowflake documentation\n.\nWhy am I getting a role error message?\nâ\nIf you receive the following error message:\nCannot create PoolableConnectionFactory (Role <'ACCOUNTADMIN'/'ORGADMIN'/'SECURITYADMIN'> specified in the connect string is not granted to this user. Contact your local system administrator, or attempt to login with another role, e.g. PUBLIC\nBy default, Snowflake blocks the\nACCOUNTADMIN\n,\nORGADMIN\n, and\nSECURITYADMIN\nroles from being assumed in the\nsecurity integration\n. Therefore, a user with any of these Snowflake roles will not be able to run queries with\nSnowflake OAuth-based authentication\n.\nTo allow users with the\nACCOUNTADMIN\n,\nORGADMIN\n, or\nSECURITYADMIN\nrole to query with Snowflake OAuth-based authentication, you will need to add the following command to set account-level permissions for the\nsecurity integration in Snowflake\n:\nALTER\nACCOUNT\nSET\nEXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST\n=\nFALSE\n;\nRefer to\nSnowflake documentation\n.\nTags:\nintegration\nconnectors\nPrevious\nTroubleshooting SSO\nNext\nPingFederate SSO 404 error"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/order-workflows",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nHow-tos\nHow to order workflows\nOn this page\norder workflows\nThe\norder of operations\nyou run in Atlan is important. Follow the specific workflow sequence outlined below when crawling\ndata tools\n. The right order particularly ensures that lineage is constructed without needing to rerun crawlers.\nOrder of operations\nâ\nTo have lineage across tools, you need to:\nCrawl data stores first\n-  for example,\nSQL data sources\n,\nNoSQL data sources\n,\nevent buses\n, and\nschema registries\n.\nRun data quality tools\n-  for example,\nMonte Carlo\nand\nSoda\n.\nMine query logs\n-\nmine queries through S3\nor run miner packages for supported sources.\nRun extract-load tools\n-  for example,\nFivetran\n,\nAirflow/OpenLineage\nand\nother supported distributions\n, and data processing tools like\nApache Spark/OpenLineage\n,\nAlteryx\n.\nRun transformation tools\n-  for example,\ndbt\nand\nMatillion\n.\nCrawl business intelligence tools last\n-  for example,\nsupported BI tools\nlike\nLooker\n,\nMicrosoft Power BI\n,\nTableau\n, and more.\nIf you use a different order, the upstream assets (data stores) might not yet exist when you load the BI metadata. In that case, you may see lineage within the BI metadata, but not between the BI metadata and data sources. If this happens, no worriesâjust rerun your existing workflows following the recommended order and Atlan can resolve it.\nDid you know?\nAs a general rule of thumb, start by crawling the data sourceâincluding BI toolsâbefore mining query logs. For example, when aiming to mine Microsoft Power BI, begin with a crawl of Microsoft Power BI.\nWorkflow recommendations\nâ\nThe following are general guidelines and best practices for running workflows in Atlan:\nSchedule your workflows based on how often you want your metadata in Atlan to be updated   -  weekly, monthly, and so on. To configure custom cron schedules, learn more\nhere\n.\nAvoid any overlaps between workflow schedules to ensure consistent workflow run times.\nRemember that the first workflow run can typically take much longer than subsequent runs. The first run establishes the connection, queries the source, extracts and transforms the metadata, and then publishes your assets for the first time in Atlan.\nIf running a miner for the first time, set a start date around 3 days prior to the current date and then schedule it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause workflows to time out or hit resource consumption errors.\nFor all subsequent miner runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic\nhere\n.\nRun\npreflight checks\nbefore running the crawler to check for any permissions or other configuration issues, including testing authentication.\nTroubleshooting tips\nâ\nHere are a few tips to help you troubleshoot workflow failures in Atlan:\nIf test authentication or\npreflight checks\nfail, check the source to ensure that your credentials are correct and you have requisite access to crawl the metadata.\nIf you're connecting to Atlan via private link and experience any network-related errors or timeouts during test authentication, it may mean that there is a network connectivity issue between the source and Atlan.\nReach out to Atlan support\nto help you investigate further.\nIf both test authentication and preflight checks fail and succeed intermittently when tried multiple times, this may mean that your cluster is in an unstable state and needs to be restarted.\nNotify Atlan support\nto restart your cluster.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nMine queries through S3\nNext\nHow to provide SSL certificates\nOrder of operations\nWorkflow recommendations\nTroubleshooting tips"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access",
    "content": "Connect data\nDatabases\nOn-premises\nOn-premises Databases\nGet Started\nSet up on-premises database access\nOn this page\nSet up on-premises database access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your database access details, including credentials.\nIn some cases you won't be able to expose your databases for Atlan to crawl and ingest metadata. This may happen for various reasons:\nTransactional databases may have high-load mechanisms. That could make direct connection problematic.\nSecurity requirements may restrict accessing sensitive, mission critical transactional databases from outside.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your on-premises databases you will need to use Atlan's metadata-extractor tool.\nDid you know?\nAtlan uses exactly the same metadata-extractor behind the scenes when it connects to cloud databases.\ndanger\nIf you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the metadata-extractor tool\nâ\nTo get the metadata-extractor tool:\nRaise a support ticket\nto get a link to the latest version.\nDownload the image using the link provided by support.\nTo load the image:\nFor Docker Image, load the image to the server you'll use to crawl databases:\nsudo docker load -i /path/to/jdbc-metadata-extractor-master.tar\nFor OCI Image:\nDocker:\nInstall\nSkopeo\n.\nLoad the image to the server you'll use to crawl databases:\nskopeo copy oci-archive:/path/to/jdbc-metadata-extractor-master-oci.tar docker-daemon:jdbc-metadata-extractor-master:latest\nPodman:\nLoad the image to the server you'll use to crawl databases:\npodman load -i /path/to/jdbc-metadata-extractor-master-oci.tar\npodman tag <loaded image hash> jdbc-metadata-extractor-master:latest\nGet the compose file\nâ\nAtlan provides you a configuration file for the metadata-extractor tool. This is a\nDocker compose file\n.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises databases.\nThe file is\ndocker-compose.yml\n.\nDefine database connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your database connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises database, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nCONNECTION-NAME:\n<<: *extract\nenvironment:\n<<: *CONNECTION-TYPE\n# Credentials\n# Database address\nvolumes:\n# Output folder\nReplace\nCONNECTION-NAME\nwith the name of your connection.\n<<: *extract\ntells the metadata-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\n<<: *CONNECTION-TYPE\napplies default arguments for the corresponding connection type.\nRefer to\nSupported connections for on-premises databases\nfor full details of each connection type.\nExample\nâ\nLet's explain in detail using an example:\nservices:\ninventory:                        # 1. Call this connection \"inventory\"\n<<: *extract\nenvironment:\n<<: *psql                     # 2. Connect to PostgreSQL using basic authentication\nUSERNAME: some-username       # 3. Credentials\nPASSWORD: some-password\nHOST: inventory.local         # 4. Database address\nPORT: 5432\nDATABASE: inventory\nvolumes:\n- *shared-jdbc-drivers\n- ./output/inventory:/output  # 5. Store results in ./output/inventory\nThe name of this service is\ninventory\n. You can use any meaningful name you want. In this example, we are using the same name as the database we're going to crawl.\nThe\n<<: *psql\nsets the connection type to Postgres using basic authentication.\nUSERNAME\nand\nPASSWORD\nspecify the credentials required for the\npsql\nconnection.\nHOST\n,\nPORT\nand\nDATABASE\nspecify the database address. The\nPORT\nis\n5432\nby default, so you can omit it most of the time.\nThe\n./output/inventory:/output\nline specifies where to store results. You will need to replace\ninventory\nwith the name of your connection. We recommend you to output metadata for different databases in separate folders.\nYou can add as many\ndatabase connections\nas you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nSecure credentials\nâ\ndanger\nIf you decide to keep database credentials in the compose file, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo create and use Docker secrets:\nCreate a JSON file and add the credentials that you want to use in Docker secrets. For example:\n{\n\"USERNAME\"\n:\n\"my-secret-user\"\n,\n\"PASSWORD\"\n:\n\"my-secret-password\"\n}\ninfo\nðª\nDid you know?\nThe keys here will be the environment variable names, hence consider migrating them from the compose file to secrets. Once set to secrets, the environment variables in secrets will take precedence over the ones in the compose file. If not provided in secrets, the values will be parsed from the compose file instead.\nCreate a new Docker secret:\ndocker secret create my_database_credentials credentials.json\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets\n:\nmy_database_credentials\n:\nexternal\n:\ntrue\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify\nCREDENTIAL_SECRET_PATH\nto use it as credentials.\ndanger\nIf you have added database credentials directly to the compose file, Atlan recommends that you leave\nCREDENTIAL_SECRET_PATH\nas blank.\nFor example, your compose file would now look something like this:\nsecrets\n:\nmy_database_password\n:\nexternal\n:\ntrue\nx-templates\n:\n# ...\nservices\n:\nmy-database\n:\n<<\n:\n*extract\nenvironment\n:\n<<\n:\n*psql\nCREDENTIAL_SECRET_PATH\n:\n\"/run/secrets/my_database_credentials\"\n# ...\nvolumes\n:\n# ...\nsecrets\n:\n-\nmy_database_password\nvolumes\n:\njars\n:\nTroubleshooting secure credentials\nâ\nAtlan recommends the following troubleshooting measures:\nIf you're unable to create Docker secrets, ensure that\nSwarm mode\nis enabled. Secrets are encrypted during transit and at rest in a Docker swarm. Run the following command to enable Swarm mode:\ndocker swarm init\nIf running the compose file after providing the credentials secret results in\nUnsupported external secret <secret_name>\n, complete the following steps:\nModify the compose file as follows:\nsecrets\n:\nmy_database_password\n:\nexternal\n:\ntrue\nx-templates\n:\n# ...\nservices\n:\nmy-database\n:\n<<\n:\n*extract\nenvironment\n:\n<<\n:\n*psql\nCREDENTIAL_SECRET_PATH\n:\n\"/run/secrets/my_database_credentials\"\n# ...\nvolumes\n:\n# ...\nsecrets\n:\n-\nmy_database_password\ndeploy\n:\nreplicas\n:\n1\nrestart_policy\n:\ncondition\n:\nnone\nvolumes\n:\njars\n:\nRun the compose file using the following command:\ndocker stack deploy -c docker-compose.yml <stack_name>\nReplace the\n<stack_name>\nwith the name you provided while deploying the stack.\nVerify deployment status using the following command:\ndocker stack ps --no-trunc <stack_name>\nReplace the\n<stack_name>\nwith the name you provided while deploying the stack.\nIf stack deployment has been successfully completed, monitor the\ndocker service logs\nusing the following command:\ndocker service logs <stack_name>_<service_name> --follow\nReplace the\n<stack_name>\nwith the name you provided while deploying the stack.\nReplace the\n<service_name>\nwith the service name in Docker.\ndanger\nThe\ndocker stack deploy\ncommand will run all the services in the\ndocker-compose.yml\nfile, so ensure that the\ndocker-compose.yml\nonly contains the service you intend to run.\nTags:\ndata\ncrawl\nPrevious\nOn-Premises Databases\nNext\nCrawl on-premises databases\nPrerequisites\nGet the compose file\nDefine database connections\nSecure credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/crawl-on-premises-databases",
    "content": "Connect data\nDatabases\nOn-premises\nOn-premises Databases\nCrawl On-premises Assets\nCrawl on-premises databases\nOn this page\nCrawl on-premises databases\nOnce you have\nset up the metadata-extractor tool\n, you can extract metadata from your on-premises databases using the following steps.\nRun metadata-extractor\nâ\nCrawl all databases\nâ\nTo crawl all databases using the metadata-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\n.\nCrawl a specific database\nâ\nTo crawl a specific database using the metadata-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nSave the compose file and use the command\nsudo docker-compose up <CONNECTION-NAME>\nwithin the local folder where the compose file is stored.\n(Replace\n<CONNECTION-NAME>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe metadata-extractor tool will generate the following JSON files for each\nservice\n:\ncolumns-<DATABASE>.json\ndatabases.json\nextras-procedures-<DATABASE>.json\nprocedures-<DATABASE>.json\nschemas-<DATABASE>.json\ntable-<DATABASE>.json\nview-<DATABASE>.json\nYou can inspect the metadata and make sure it is acceptable to provide the metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata, you need to upload the metadata to an S3 bucket.\nDid you know?\nTo avoid access issues, upload to the same S3 bucket that Atlan uses.\nRaise a support request\nto get your Atlan bucket details, and include the ARN of the IAM user or IAM role so access can be provisioned. To create a separate bucket, see\nOption 1: Use your own bucket\nin the dbt documentation (the steps are the same).\nTo upload the metadata to S3:\nConfirm that all the files for a particular service have the same prefix. For example,\nmetadata/inventory/columns-inventory.json\n,\nmetadata/inventory/databases.json\n, etc.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all the files using the\nAWS CLI\n:\naws s3 cp output/inventory s3://my-bucket/metadata/inventory --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nAmazon Redshift\nHive\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nSAP HANA\nSnowflake\nTeradata\nFor all of the above cases, select\nOffline\nfor the extraction method.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up on-premises database access\nNext\nSupported connections for on-premises databases\nRun metadata-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases",
    "content": "Connect data\nDatabases\nOn-premises\nOn-premises Databases\nReferences\nSupported connections for on-premises databases\nOn this page\nSupported connections for on-premises databases\nThe metadata-extractor tool supports the following connection types.\nThese describe the details required when\nsetting up on-premises database access\n.\nAmazon Redshift with basic authentication\nâ\nUse\n<<: *redshift\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 5439\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database username (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-redshift-database:\n<<: *extract\nenvironment:\n<<: *redshift\nUSERNAME: my-database-username\nPASSWORD: my-database-password\nHOST: redshift-host\nPORT: redshift-database-port\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/redshift-example:/output\nAmazon Redshift with IAM user authentication\nâ\nUse\n<<: *redshift\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 5439\n)\nDATABASE\n-  database name (\nrequired\n)\nDATABASE_USER\n-  database username of the IAM user (\nrequired\n)\nAWS_ACCESS_KEY_ID\n-  AWS access key ID (\nrequired\n)\nAWS_SECRET_ACCESS_KEY\n-  AWS secret access key (\nrequired\n)\nCLUSTER_ID\n-  cluster identifier of your private Amazon Redshift cluster (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nredshift-iam-user-example:\n<<: *extract\nenvironment:\n<<: *redshift-iam-user\nAWS_ACCESS_KEY_ID: aws-access-key-id\nAWS_SECRET_ACCESS_KEY: aws-secret-access-key\nHOST: redshift-host\nDATABASE: my-database-name\nDATABASE_USER: my-database-user\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nCLUSTER_ID: private-cluster-id\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/redshift-iam-user-example:/output\nHive with basic authentication\nâ\nUse\n<<: *hive\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 10000\n)\nDEFAULT_SCHEMA\n-  default schema name (\noptional, default is\ndefault\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nSCHEMA_EXCLUDE_REGEX\n-  regex to exclude schemas (\noptional\n)\nSCHEMA_INCLUDE_REGEX\n-  regex to include schemas (\noptional\n)\nTEMP_TABLE_REGEX\n-  regex to exclude tables (\noptional\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nhive-example:\n<<: *hive-extract\nenvironment:\n<<: *hive\nHOST: hive-host\nPORT: hive-port\nDEFAULT_SCHEMA: default\nUSERNAME: my-hive-username\nPASSWORD: my-hive-password\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/hive-example:/output\nMicrosoft SQL Server with basic authentication\nâ\nUse\n<<: *mssql\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-mssql-database:\n<<: *extract\nenvironment:\n<<: *mssql\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: mssql-database-host\nDATABASE: northwind\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/mssql-database:/output\nMySQL with basic authentication\nâ\nUse\n<<: *mysql\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 3306\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-mysql-database:\n<<: *extract\nenvironment:\n<<: *mysql\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: mysql-database-host.internal\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-mysql-database:/output\nMySQL with IAM authentication\nâ\nUse\n<<: *mysql-iam\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 3306\n)\nUSERNAME\n-  database user name (\nrequired\n)\nAWS_ACCESS_KEY_ID\n-  AWS access key id (\nrequired\n)\nAWS_SECRET_ACCESS_KEY\n-  AWS secret access key (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-mysql-database:\n<<: *extract\nenvironment:\n<<: *mysql-iam\nAWS_ACCESS_KEY_ID: my-access-key-id\nAWS_SECRET_ACCESS_KEY: my-secret-access-key\nUSERNAME: db-user\nHOST: mysql-database-host.internal\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-mysql-database:/output\nOracle with basic authentication\nâ\nUse\n<<: *oracle\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 1521\n)\nSERVICE_NAME\n-  database service name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-oracle-database:\n<<: *extract\nenvironment:\n<<: *oracle\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: oracle-database-host.internal\nSERVICE_NAME: my-service-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-oracle-database:/output\nPostgreSQL with basic authentication\nâ\nUse\n<<: *postgresql\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 5432\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-postgresql-database:\n<<: *extract\nenvironment:\n<<: *postgresql\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: postgresql-database-host.internal\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-postgresql-database:/output\nPostgreSQL with IAM authentication\nâ\nUse\n<<: *postgresql-iam\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 5432\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nAWS_ACCESS_KEY_ID\n-  AWS access key id (\nrequired\n)\nAWS_SECRET_ACCESS_KEY\n-  AWS secret access key (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-postgresql-database:\n<<: *extract\nenvironment:\n<<: *postgresql-iam\nAWS_ACCESS_KEY_ID: my-access-key-id\nAWS_SECRET_ACCESS_KEY: my-secret-access-key\nUSERNAME: db-user\nHOST: postgresql-database-host.internal\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-postgresql-database:/output\nSAP HANA with basic authentication\nâ\nUse\n<<: *sap-hana\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 39015\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-sap-hana-database:\n<<: *extract\nenvironment:\n<<: *sap-hana\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: sap-hana-database-host.internal\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-sap-hana-database:/output\nSnowflake with basic authentication\nâ\nUse\n<<: *snowflake\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 443\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-snowflake-database:\n<<: *extract\nenvironment:\n<<: *snowflake\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: snowflake-database-host.internal\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-snowflake-database:/output\nSnowflake with OAuth authentication\nâ\nUse\n<<: *snowflake-oauth\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 443\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nCLIENT_ID\n-  OAuth client ID (\nrequired\n)\nCLIENT_SECRET\n-  OAuth client secret (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-snowflake-database:\n<<: *extract\nenvironment:\n<<: *snowflake-oauth\nUSERNAME: db-user\nCLIENT_ID: my-client-id\nCLIENT_SECRET: my-client-secret\nHOST: snowflake-database-host.internal\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-snowflake-database:/output\nTeradata with basic authentication\nâ\nUse\n<<: *teradata\nunder the\nenvironment\nsection to use this connection type.\nAvailable attributes:\nHOST\n-  database host name or IP address (\nrequired\n)\nPORT\n-  database port (\noptional, default is 1025\n)\nDATABASE\n-  database name (\nrequired\n)\nUSERNAME\n-  database user name (\nrequired\n)\nPASSWORD\n-  database user password (\nrequired\n)\nUSE_SOURCE_SCHEMA_FILTERING\n-  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns (\noptional\n)\nEXCLUDE_FILTER_TEMPLATE\n-  exclude filter pattern (\noptional\n)\nINCLUDE_FILTER_TEMPLATE\n-  include filter pattern (\noptional\n)\nUSE_JDBC_INTERNAL_METHODS\n- Boolean to specify if JDBC internal methods need to be used as part of the extraction (\noptional\n)\nExample\nservices:\nmy-teradata-database:\n<<: *extract\nenvironment:\n<<: *teradata\nUSERNAME: db-user\nPASSWORD: db-user-password\nHOST: teradata-database-host.internal\nDATABASE: my-database-name\n# If using Docker Swarm mode for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'\n# If using Docker Compose for offline extraction, format the filters as follows:\nINCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nEXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}'\nUSE_SOURCE_SCHEMA_FILTERING: \"false\"\nUSE_JDBC_INTERNAL_METHODS: \"false\"\nvolumes:\n- *shared-jdbc-drivers\n- ./output/my-teradata-database:/output\nTags:\nconnectors\ndata\nauthentication\nPrevious\nCrawl on-premises databases\nNext\nTroubleshooting on-premises database connectivity\nAmazon Redshift with basic authentication\nAmazon Redshift with IAM user authentication\nHive with basic authentication\nMicrosoft SQL Server with basic authentication\nMySQL with basic authentication\nMySQL with IAM authentication\nOracle with basic authentication\nPostgreSQL with basic authentication\nPostgreSQL with IAM authentication\nSAP HANA with basic authentication\nSnowflake with basic authentication\nSnowflake with OAuth authentication\nTeradata with basic authentication"
  },
  {
    "url": "https://docs.atlan.com/tags/multiple-concatenation",
    "content": "One doc tagged with \"multiple-concatenation\"\nView all tags\nMultiple tag values and concatenation\nLearn how Atlan handles multiple tag values for Snowflake objects, including concatenation, sorting, and reverse synchronization."
  },
  {
    "url": "https://docs.atlan.com/tags/warehouse",
    "content": "One doc tagged with \"warehouse\"\nView all tags\nSnowflake warehouse configuration\nRecommended Snowflake warehouse configuration to enable reliable Atlan workflow execution."
  },
  {
    "url": "https://docs.atlan.com/tags/configuration",
    "content": "19 docs tagged with \"configuration\"\nView all tags\nAdministration and Configuration\nComplete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization.\nConfigure SMTP\nAtlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and [scheduled queries](/product/capabilities/insights/how-tos/schedule-a-query). We provide an embedded SMTP server to do this, out-of-the-box.\nConfigure Snowflake data metric functions\nConfigure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nCrawl Fivetran\nLearn about crawl fivetran.\nIntegrate Amazon MWAA/OpenLineage\nTo learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Apache Airflow/OpenLineage\nTo integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Apache Spark/OpenLineage\nAtlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIntegrate Astronomer/OpenLineage\nTo integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/.\nIntegrate Slack\nTo integrate Slack and Atlan, follow these steps.\nPreflight checks for Fivetran\nLearn about preflight checks for fivetran.\nSchedule a query\nYou must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients.\nSet up an Azure private network link to Databricks\nFor all details, see [Databricks documentation](https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/private-link-simplified?source=recommendations#create-the-workspace-and-private-endpoints-in-the-azure-portal-ui).\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.\nSet up Fivetran\nLearn about set up fivetran.\nSnowflake warehouse configuration\nRecommended Snowflake warehouse configuration to enable reliable Atlan workflow execution.\nWhat does Atlan crawl from Fivetran?\nLearn about what does atlan crawl from fivetran?.\nWhat does Atlan crawl from SAP ECC?\nWhat does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from SAP S/4HANA?\nWhat does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWorkflows and Data Processing\nEverything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan."
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/monitor-connectivity",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nHow-tos\nMonitor connectivity\nOn this page\nMonitor connectivity\nAtlan runs its crawlers through an orchestrated set of automated tasks.\nTo monitor these orchestrated set of tasks follow\nthese steps.\nMonitor the crawling process\nâ\nYou can visualize the individual tasks a workflow runs as a directed acyclic graph (or \"DAG\").\nTo visualize the crawling process:\nWhen running a workflow immediately, you will be redirected to the monitoring page within 5 seconds.\nAt any other moment:\nFrom the left menu, click\nWorkflows\nto navigate to the\nWorkflow center\n:\nBy default, workflow runs from the last 24 hours will be shown.\n(Optional) Use the filters along the top to narrow down to the workflow run you want to monitor.\nFrom the\nWorkflow run history\ntable, click the workflow run you want to check.\nOn the left of the screen under the\nSummary\ntab, you can also see:\nThe current status of the workflow run.\nThe start and finish time of the workflow run.\nThe elapsed time (duration) of the workflow run.\nWho triggered the workflow run and how (manually or automatically).\nIdentify errors\nâ\nIf a crawler fails due to an error, Atlan will show where the failure occurred in the visualization.\nTo review the failure of any workflow with an error:\nOpen the workflow run visualization (using either option above).\nUnder theÂ\nSummary\ntab on the left of the screen, click theÂ\nView Failed tasks\nbutton.\nAtlan will take you to the\nFailed Tasks\ntab on the left of the screen. Here you can review details about the specific activity or activities that failed.\nReview log files\nâ\nEach task in the DAG may produce a log file containing additional details.\nTo review the log file for a specific activity:\nClick the task (activity node) in the DAG visualization.\nOpen the\nFailed Tasks\ntab of a workflow run visualization (see steps above).\nTo the right of each failed step, click theÂ\nLogs\nbutton.\nIf there are any logs available, Atlan will display them on the screen.\nDid you know?\nNot every failed activity will produce a log. Look at the\nMessage\nfield of failed tasks for ideas about what went wrong when there is no log file available.\nManage all workflows\nâ\nYou can monitor and manage all your workflows in Atlan from the workflow center.\nTo manage all your workflows:\nFrom the left menu of any screen in Atlan, click\nWorkflows\n.\nFrom the tabs along the top in the\nWorkflow center\n, click\nManage\n.\nSearch for a specific workflow from the search bar or click\nSelect package\nto filter by\nsupported connectors\n.\n(Optional) In the\nFilters\nmenu on the left, select a filter to drill down further:\nClick\nCreated by\nto filter workflows created by specific users in Atlan.\nClick\nWorkflow type\nto filter workflows by type of workflow   -  connectors, utilities, and miners. Each workflow type also displays the total count of workflow runs for that type.\nClick\nSchedule\nto filter workflows by scheduled or unscheduled runs.\nThe workflow preview includes a summary of workflow details. Navigate to the workflow sidebar on the right, from the sidebar:\nThe\nOverview\ntab displays run count, when the workflow was created and by whom, workflow schedules if applicable, and last 5 runs. (Optional) Click\nRun workflow\nto run the workflow directly from the sidebar.\nThe\nRuns\ntab displays a summary of past workflow runs. (Optional) Select a workflow run to view more details or\nmodify connectivity\n.\nSelect any workflow to open the workflow.\nTags:\nintegration\nconnectors\nPrevious\nManage connectivity\nNext\nConnect data sources for Azure-hosted Atlan instances\nMonitor the crawling process\nIdentify errors\nReview log files\nManage all workflows"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/connect-data-sources-for-azure-hosted-atlan-instances",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nHow-tos\nConnect data sources for Azure-hosted Atlan instances\nOn this page\nConnect data sources for Azure-hosted Atlan instances\nThis document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:\nData sources hosted on Microsoft Azure\nData sources hosted in data centers\nAzure-managed data sources\nâ\nTo connect your Atlan instance hosted on Microsoft Azure with a Microsoft Azure-managed data source, Atlan recommends the following method. For this purpose, we'll consider a Microsoft Azure-managed Snowflake instance:\nFor data sources like Snowflake, you can use\nAzure Private Link\n.\nAtlan will create a private endpoint in your Atlan instance to connect to your Snowflake instance using the the resource ID.\nThis will create a request in your Atlan instance. Accept the request to proceed.\nOnce the request has been accepted, Atlan will be able to access the data source using a private endpoint over the Azure backbone network, bypassing the internet.\nAtlan will also create a private DNS and add an\nA\nrecord for the private endpoint previously created in the Azure-managed Atlan instance and share the details with you.\nYou can use this DNS record to connect to the Azure-hosted Snowflake data source.\nEach data source will require a separate Azure private endpoint.\nOn-premises data sources\nâ\nTo connect your Atlan instance hosted on Microsoft Azure with an on-premises data source, Atlan recommends the following method. For this purpose, we'll consider an on-premises MySQL server hosted in a data center:\nFor an on-premises MySQL database, you can consider a combination of\nAzure Private Link\n,\nAzure Load Balancer\n, and\nAzure Virtual Machines\n.\nFor this method, the data source must be accessible from your Microsoft Azure subscription.\nYou will need to create a virtual machine in your Azure-managed Atlan instance to port forward the request to the corresponding data source.\nAdd a network load balancer to the virtual machine and create a Private Link service to the load balancer.\nAtlan will create a private endpoint in your Atlan instance to connect to the Private Link service of the load balancer.\nAtlan will also create a private DNS and add an\nA\nrecord for the private endpoint previously created in the Azure-managed Atlan instance and share the details with you.\nYou can use this DNS record to connect to the on-premises data source using the load balancer.\nOnly one private endpoint will be required to connect to all the on-premises data sources through port forwarding.\nTags:\nconnectors\ndata\nPrevious\nMonitor connectivity\nNext\nMine queries through S3\nAzure-managed data sources\nOn-premises data sources"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/provide-ssl-certificates",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nHow-tos\nHow to provide SSL certificates\nOn this page\nprovide SSL certificates\nSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for\ncrawling Tableau\n.\nThe following types of SSL certificates are supported:\nSelf-signed\nâ\nPaste the public\n.crt\nor\n.cert\npart of your TLS certificate in the Privacy Enhanced Mail (PEM) format. For example:\n----BEGIN CERTIFICATE----\nMIIDazCCAlOgAwIBAgIJAOqRDRz0BxIAMA0GCSqGSIb3DQEBCwUAMIGZMQswCQYD\n...\n...\n...\nu1Q==\n----END CERTIFICATE----\nInternal CA\nâ\nAn SSL certificate chain is a sequence of certificates consisting of three parties:\nA root certificate authority,\nOne or more intermediate certificate authorities,\nAnd the server certificate.\nPaste the root, intermediate, and server certificates in the following format:\n----BEGIN CERTIFICATE----\nABCDE......\n----END CERTIFICATE----\n----BEGIN CERTIFICATE----\nEFGHT......\n----END CERTIFICATE----\n----BEGIN CERTIFICATE----\nNAMNOP......\n----END CERTIFICATE----\n----BEGIN CERTIFICATE----\nKROPS......\n----END CERTIFICATE----\nThe top certificate is the root certificate\nFollowed by the hops in the right sequence\nEnding with server certificate\nTags:\nconnectors\ndata\ncrawl\nPrevious\nHow to order workflows\nNext\nWhat are preflight checks?\nSelf-signed\nInternal CA"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/odbc-jdbc-driver",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nFAQ\nCan I connect to any source with an ODBC/JDBC driver?\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's\nsupported connectors\nuse a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration,\ncontact Atlan support\nto share more details about your use case.\nYou can also use Atlan APIs to integrate sources for\ncrawling data assets\n,\ngenerating lineage\n, or\nenriching existing assets with additional metadata\n.\nTags:\nconnectors\ndata\nintegration\ncrawl\napi\nfaq-connections\nPrevious\nSupported sources\nNext\nCan the Hive crawler connect to an independent Hive metastore?"
  },
  {
    "url": "https://docs.atlan.com/tags/workflow",
    "content": "13 docs tagged with \"workflow\"\nView all tags\nAutomate data governance\nYou can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nConfigure workflow execution\nLearn about configure workflow execution.\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nCreate governance workflows\n:::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/.\nDelegate administration\nThe workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows.\nHow often does Atlan crawl Snowflake?\nLearn about how often does atlan crawl snowflake?.\nManage connectivity\nOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.\nManage governance workflows\n:::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/.\nManage requests\nIf your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected.\nRequests\nRequests allow users to suggest changes to assets that they cannot directly change themselves.\nRevoke data access\nAs an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the [scope of governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows).\nSet up Alteryx\nSet up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run.\nTroubleshooting Anomalo connectivity\nLearn about troubleshooting anomalo connectivity."
  },
  {
    "url": "https://docs.atlan.com/tags/orchestration",
    "content": "12 docs tagged with \"orchestration\"\nView all tags\nAutomate data governance\nYou can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nConfigure workflow execution\nLearn about configure workflow execution.\nCreate governance workflows\n:::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nDelegate administration\nThe workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows.\nHow often does Atlan crawl Snowflake?\nLearn about how often does atlan crawl snowflake?.\nManage connectivity\nOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.\nManage governance workflows\n:::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/.\nManage requests\nIf your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected.\nRequests\nRequests allow users to suggest changes to assets that they cannot directly change themselves.\nRevoke data access\nAs an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the [scope of governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows).\nTroubleshooting Anomalo connectivity\nLearn about troubleshooting anomalo connectivity."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-github",
    "content": "Connect data\nETL Tools\ndbt\nImpact Analysis\nAdd impact analysis in GitHub\nOn this page\nAdd impact analysis in GitHub\ndanger\nFor existing users, the\ndbt-action\nis no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the\natlan-action\n. Refer to\nHow to migrate from dbt to Atlan action\nto learn more and complete the migration.\nIf you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard,Â Atlan provides a\nGitHub Action\nto help you out.\nThis action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request.\nPrerequisites\nâ\nBefore running the action, you will need to create an\nAtlan API token\n.\nYou will also need to assign a\npersona\nto the API token and add a\nmetadata policy\nthat provides the requisite permissions on assets for the Atlan dbt action to work. For example, you can add the following permissions:\ndbt   -\nRead\nand\nUpdate\nMaterialized layer, such as Snowflake   -\nRead\nand\nUpdate\nAny downstream connections, such as Microsoft Power BI   -\nRead\nonly\nWhen a pull request with changes to one or more dbt models is merged, the Atlan dbt action will link the pull request as a\nresource\nto the assets in Atlan. To ensure that the pull request is linked as a resource, you will need to assign the right persona with requisite permissions to the API token.\nYou will need to configure the default\nGITHUB_TOKEN\npermissions. Grant\nRead and write permissions\nto the\nGITHUB_TOKEN\nin your repository to allow the\natlan-action\nto seamlessly add or update comments on pull requests. Refer to\nGitHub documentation\nto learn more.\nConfigure the action\nâ\nTo set up the Atlan action in GitHub:\nCreate repository secrets\nin your repository:\nATLAN_INSTANCE_URL\nwith the URL of your Atlan instance.\nATLAN_API_TOKEN\nwith the value of the API token.\nAdd the GitHub Action to your workflow:\nCreate a workflow file in your repository   -\n.github/workflows/atlan-dbt.yml\n.\nAdd the following code to your workflow file:\nname\n:\nAtlan action\non\n:\npull_request\n:\ntypes\n:\n[\nopened\n,\nedited\n,\nsynchronize\n,\nreopened\n,\nclosed\n]\njobs\n:\nget-downstream-impact\n:\nname\n:\nGet Downstream Assets\nruns-on\n:\nubuntu\n-\nlatest\nsteps\n:\n-\nname\n:\nRun Action\nuses\n:\natlanhq/atlan\n-\naction@v1\nwith\n:\nGITHUB_TOKEN\n:\n$\n{\n{\nsecrets.GITHUB_TOKEN\n}\n}\nATLAN_INSTANCE_URL\n:\n$\n{\n{\nsecrets.ATLAN_INSTANCE_URL\n}\n}\nATLAN_API_TOKEN\n:\n$\n{\n{\nsecrets.ATLAN_API_TOKEN\n}\n}\nTest the action\nâ\nAfter you've completed the configuration above, create a pull request with a changed dbt model file to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request:\nThe GitHub workflow will add and update a single comment for every file change.\nThe impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type.\nThe comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms.\nView the impacted assets in Atlan or open the source URL   -  for example, view an impacted Looker dashboard directly in Looker.\nOnce you have merged the pull request, it will be added as a\nresource\nto the dbt model and its materialized assets. You can view the linked pull request from the\nResources\ntab of the asset sidebar.\nFor example:\nInputs\nâ\nName\nDescription\nRequired\nGITHUB_TOKEN\nFor\nwriting comments on PRs\nto print downstream assets\ntrue\nATLAN_INSTANCE_URL\nFor making API requests to the user's tenant\ntrue\nATLAN_API_TOKEN\nFor\nauthenticating API requests\nto the user's tenant\ntrue\nDBT_ENVIRONMENT_BRANCH_MAP\nFor mapping the GitHub branch with a specific dbt environment\nfalse\nIGNORE_MODEL_ALIAS_MATCHING\nFor turning off matching aliases using this variable\nfalse\nTroubleshooting the action\nâ\nWhy does the action fetch a model from an incorrect environment?\nâ\nIf there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitHub branch with a specific dbt environment. This will allow the Atlan GitHub action to parse lineage for that specific environment.\nFor example, you can provide the mapping in this format   -\nbranch name\n:\ndbt environment name\njobs:\nget-downstream-impact:\nname: Get Downstream Assets\nruns-on: ubuntu-latest\nsteps:\n- name: Run Action\nuses: atlanhq/atlan-action@v1\nwith:\nGITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}\nATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}}\nATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}}\n+         DBT_ENVIRONMENT_BRANCH_MAP: |\n+           main: dbt-prod\n+           beta: dbt-test\nWhy does the action fetch a model by its alias and not model name?\nâ\nBy default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the\nIGNORE_MODEL_ALIAS_MATCHING\ninput to true.\nFor example:\njobs:\nget-downstream-impact:\nname: Get Downstream Assets\nruns-on: ubuntu-latest\nsteps:\n- name: Run Action\nuses: atlanhq/atlan-action@v1\nwith:\nGITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}\nATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}}\nATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}}\n+         IGNORE_MODEL_ALIAS_MATCHING: true\nTags:\nconnectors\napi\nauthentication\nmodel\nPrevious\nMigrate from dbt to Atlan action\nNext\nAdd impact analysis in GitLab\nPrerequisites\nConfigure the action\nTest the action\nInputs\nTroubleshooting the action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-gitlab",
    "content": "Connect data\nETL Tools\ndbt\nImpact Analysis\nAdd impact analysis in GitLab\nOn this page\nAdd impact analysis in GitLab\ndanger\nFor existing users, the\ndbt-action\nis no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the\natlan-action\n. Refer to\nHow to migrate from dbt to Atlan action\nto learn more and complete the migration.\nIf you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard, Atlan provides a GitLab CI/CD pipelineÂ to help you out.\nThis pipeline places Atlan's impact analysis right into your merge request. So, you can view the potential downstream impact of your changes before merging the request.\nPrerequisites\nâ\nBefore running the action, you will need to create an\nAtlan API token\n.\nAssign a\npersona\nto the API token and add a\nmetadata policy\nthat provides requisite permissions on assets for the Atlan dbt action to work. For example, you can add the following permissions:\ndbt   -\nRead\nand\nUpdate\nMaterialized layer, such as Snowflake   -\nRead\nand\nUpdate\nAny downstream connections, such as Microsoft Power BI   -\nRead\nonly\nWhen a merge request with changes to one or more dbt models is merged, the Atlan dbt action will link the merge request as a\nresource\nto the assets in Atlan. To ensure that the merge request is linked as a resource, you will need to assign the right persona with requisite permissions to the\nAPI token\n.\nConfigure the action\nâ\nTo set up the Atlan dbt action in GitLab:\nDefine CI/CD variables\nin your repository:\nATLAN_INSTANCE_URL\nwith the URL of your Atlan instance.\nATLAN_API_TOKEN\nwith the value of the\nAPI token\n.\nGITLAB_TOKEN\nwith the value of the project access token.\nClick the checkboxes for\nMask variable\nand\nExpand variable\nonly. Leave the\nProtect variable\ncheckbox unchecked   -  merge request pipelines\ndo not have access to protected variables\n.\nAdd the GitLab pipeline to your workflow:\nCreate a workflow file in the root directory of your repository   -\n.gitlab-ci.yml\n.\nAdd the following code to your workflow file:\nstages\n:\n-\nget\n-\ndownstream\n-\nimpact\nget-downstream-impact-open\n:\nstage\n:\nget\n-\ndownstream\n-\nimpact\nimage\n:\nnode\n:\n20\nscript\n:\n-\ngit clone\n-\nbranch v1 https\n:\n//github.com/atlanhq/atlan\n-\naction.git\n-\ncd atlan\n-\naction\n-\nnpm install\n-\nnpm run build\n-\nnode ./adapters/index.js\nenvironment\n:\nname\n:\nget\n-\ndownstream\n-\nimpact\nrules\n:\n-\nif\n:\n'$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n-\nif\n:\n'$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS'\nwhen\n:\nnever\n-\nif\n:\n'$CI_COMMIT_BRANCH'\nTest the action\nâ\nAfter you've completed the configuration above, create a merge request with a changed dbt model file to test the action. You should see the Atlan GitLab CI/CD pipeline running and adding comments in your merge request:\nThe GitLab CI/CD pipeline will add and update a single comment for every file change.\nThe impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type.\nThe comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms.\nView the impacted assets in Atlan or open the source URL   -  for example, view an impacted Looker dashboard directly in Looker.\nOnce you have merged the merge request, it will be added as a\nresource\nto the dbt model and its materialized assets. You can view the linked merge request from the\nResources\ntab of the asset sidebar. For example:\nInputs\nâ\nName\nDescription\nRequired\nGITLAB_TOKEN\nFor\nwriting comments on PRs\nto print downstream assets\ntrue\nATLAN_INSTANCE_URL\nFor making API requests to the user's tenant\ntrue\nATLAN_API_TOKEN\nFor\nauthenticating API requests\nto the user's tenant\ntrue\nDBT_ENVIRONMENT_BRANCH_MAP\nFor mapping the GitLab branch with a specific dbt environment\nfalse\nIGNORE_MODEL_ALIAS_MATCHING\nFor turning off matching aliases using this variable\nfalse\nTroubleshooting the action\nâ\nWhy does the action fetch a model from an incorrect environment?\nâ\nIf there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitLab branch with a specific dbt environment. This will allow the Atlan GitLab CI/CD pipeline to parse lineage for that specific environment.\nFor example, you can provide the mapping in this format   -\nbranch name\n:\ndbt environment name\nstages:\n- get-downstream-impact\nget-downstream-impact-open:\nstage: get-downstream-impact\nimage: node:20\nvariables:\n+    DBT_ENVIRONMENT_BRANCH_MAP: |\n+      main: [Enter Your Branch name]\nscript:\n- git clone   - branch v1 https://github.com/atlanhq/atlan-action.git\n- cd atlan-action\n- npm install\n- npm run build\n- node ./adapters/index.js\nenvironment:\nname: get-downstream-impact\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n- if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS'\nwhen: never\n- if: '$CI_COMMIT_BRANCH'\nWhy does the action fetch a model by its alias and not model name?\nâ\nBy default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the\nIGNORE_MODEL_ALIAS_MATCHING\ninput to true.\nFor example:\nstages:\n- get-downstream-impact\nget-downstream-impact-open:\nstage: get-downstream-impact\nimage: node:20\nvariables:\n+    IGNORE_MODEL_ALIAS_MATCHING: \"true\"\nscript:\n- git clone   - branch v1 https://github.com/atlanhq/atlan-action.git\n- cd atlan-action\n- npm install\n- npm run build\n- node ./adapters/index.js\nenvironment:\nname: get-downstream-impact\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n- if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS'\nwhen: never\n- if: '$CI_COMMIT_BRANCH'\nTags:\nconnectors\ndata\napi\nauthentication\nmodel\nPrevious\nAdd impact analysis in GitHub\nNext\nWhat does Atlan crawl from dbt Cloud?\nPrerequisites\nConfigure the action\nTest the action\nInputs\nTroubleshooting the action"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/bulk-enrich-metadata",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nBulk enrich metadata\nOn this page\nBulk enrich metadata\nAtlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan.\nAtlan currently supports the following options for bulk metadata enrichment:\nExport assets to spreadsheets\nâ\nFor a fairly large number of assets to be documented, you can export your assets from Atlan to a supported spreadsheet tool and bulk enrich metadata directly from spreadsheets. Atlan currently supports exporting assets to:\nGoogle Sheets\nMicrosoft Excel online\nHere is a quick summary of this option:\nAn Atlan admin must integrate a supported spreadsheet tool in Atlan to export assets.\nOnce assets have been exported, you must install the Atlan extension to enrich assets and sync changes to Atlan.\nOnly supported for online versions of Google Sheets and Microsoft Excel.\nMust use an organizational email address to export assets from Atlan.\nExports basic as well as custom metadata.\nSupports\nexporting impacted assets\n.\nCreation of new terms or tags is not supported. To create new terms, admins and members with edit access can\nbulk upload glossaries\n.\nDeletion of terms is not supported.\nDoes not provide cron support, use custom packages instead.\nRefer to\nHow to export assets\nto get started.\nUse Atlan extension in spreadsheets\nâ\nFor a small number of assets to be documented, you can install the Atlan extension in a supported spreadsheet tool and bulk enrich metadata directly from spreadsheets.\nAtlan currently supports installing the Atlan extension in:\nGoogle Sheets\nMicrosoft Excel online and desktop versions\nHere is a quick summary of this option:\nMust install the Atlan extension for Google Sheets or Microsoft Excel.\nSupports both online and desktop versions of supported spreadsheet tools.\nOnly supports basic metadata   -  custom metadata is not supported, use\nexport assets\nÂ option instead.\nSupports importing and updating impacted assets.\nCreation of new terms or tags is not supported. To create new terms, admins and members with edit access can\nbulk upload glossaries\n.\nGlossary assets are not supported, use\nexport assets\noption instead.Â\nRefer to\nHow to integrate Atlan with Google Sheets\nor\nHow to integrate Atlan with Microsoft Excel\nto get started.\nUse custom packages\nâ\nAtlan's\nbasic\nand\nadvanced\nasset export custom packages can power miscellaneous use cases:\nExport to your file storage for reporting and analytics outside Atlan.\nExport to a CSV file, enrich metadata, and then use the\nasset import\npackage to update in Atlan.\nHere is a quick summary of this option:\nExports to a CSV file.\nProvides an alternative to out-of-the-box solutions if your administrative policies prohibit such integrations.\nThe basic package exports almost all backend attributes, recommended for one-off migrations.\nThe advanced package offers filtering capabilities, recommended if the scope of the export is limited.\nBoth packages are compatible with the\nasset import\npackage.\nOnly admin users can run custom package workflows.\nAll attributes except asset relationships are supported.\nRefer to\nAsset export (basic)\nor\nAsset export (advanced)\nto get started.\nTags:\nintegration\nconnectors\nPrevious\nWhat is included in the Slack integration?\nNext\nConfigure custom domains for Microsoft Excel\nExport assets to spreadsheets\nUse Atlan extension in spreadsheets\nUse custom packages"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/disable-sample-data-preview",
    "content": "Use data\nInsights\nFAQ\nCan I turn off sample data preview for the entire organization?\nCan I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the\nSnowflake crawler\nto prevent users from previewing any Snowflake data. Simply change\nAllow Data Preview\nto\nNo\n.\nThere is currently no global toggle to turn off sample data on a tenant level. Sample data preview also does not fall under the purview of the\nInsights toggle\n.\nTags:\nconnectors\ndata\ncrawl\nfaq\nfaq-insights\nPrevious\nCan I query any DW/DL?\nNext\nCan we restrict who can query our data warehouse?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/faq/site-renaming",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nFAQ\nCan site renaming affect the Jira integration?\nCan site renaming affect the Jira integration?\nRefer to our\ntroubleshooting Jira documentation\nto learn more.\nTags:\nintegration\nconnectors\nfaq-integrations\nPrevious\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nNext\nServiceNow"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/hive-metastore-connection",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nFAQ\nCan the Hive crawler connect to an independent Hive metastore?\nCan the Hive crawler connect to an independent Hive metastore?\nSee\nTroubleshooting Hive connectivity\n.\nTags:\nintegration\nconnectors\nfaq-connections\nPrevious\nCan I connect to any source with an ODBC/JDBC driver?\nNext\nHow often does Atlan crawl Snowflake?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/microsoft-sso-login",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nFAQ\nCan we use a Microsoft SSO login?\nCan we use a Microsoft SSO login?\nYes, Atlan supports\nAzure AD for SSO\n.\nTags:\nintegration\nconnectors\nfaq-integrations\nPrevious\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nNext\nWhat type of user provisioning does Atlan support for SSO integrations?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/connect-on-premises-databases-to-kubernetes",
    "content": "On this page\nConnect on-premises databases to Kubernetes\nWho can do this?\nYou will need access to a machine that can run Kubernetes on-premises. You will also need your database access details, including credentials.\nYou can configure and use\nAtlan's metadata-extractor tool\nto extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose.\nGet the metadata-extractor tool\nâ\nTo get the metadata-extractor tool:\nRaise a support ticket\nto get a link to the latest version.\nDownload the image using the link provided by support.\nLoad image to Kubernetes cluster\nâ\nYou cannot upload the extractor image directly to a Kubernetes cluster. You must upload the extractor image to a container registry that your Kubernetes cluster can access. This ensures that Kubernetes can readily deploy pods with the metadata-extractor tool.\nApply configuration maps\nâ\nConfigMaps contain essential settings that enable the metadata-extractor tool to connect to your database, including connection details and extraction parameters. This can help you customize the extraction process to fit your database environment.\nDeploy configurations to your specific database setup:\nkubectl apply -f config-maps.yml\nCreate a\nconfig-maps.yml\ncontaining your database settings, for example:\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: atlan-extractor-config-mysql\ndata:\nDOWNLOAD_JDBC: \"true\"\nDOWNLOAD_JDBC_URL: \"https://example.com/path/to/jdbc-driver.tar.gz\"\nDRIVER: \"com.example.jdbc.Driver\"\n# Add other necessary configurations as key-value pairs\nReplace example values with details of your database connection and the JDBC driver.\nDeploy extraction job\nâ\nSet up the CronJob for metadata extraction from the database:\nkubectl apply -f job.yml\nExample\nâ\nCreate a\njob.yml\nfor the extraction job with details like the following:\napiVersion: batch/v1\nkind: CronJob\nmetadata:\nname: atlan-extractor-cron-job\nspec:\nschedule: \"@weekly\"\njobTemplate:\nspec:\ntemplate:\nspec:\ncontainers:\n- name: crawler\nimage: your-registry/path-to-extractor-image:latest\n# Define environment variables and volume mounts as required\n(Optional) Configure CronJob schedule\nâ\nThe CronJob is configured to execute weekly by default.\nTo configure the CronJob schedule:\nOpen the\njob.yml\nfile.\nIn the\nspec\nsection, for\nschedule:\n, replace the\n\"@weekly\"\ncron expression with your preferred schedule. For example, use\n\"@daily\"\nfor daily executions or provide a\ncustom cron schedule\n.\nFor more information on CronJob schedules, refer to\nKubernetes documentation\n.\n(Optional) Trigger the job manually\nâ\nTo trigger an immediate metadata extraction, execute the CronJob manually:\nkubectl create job --from=cronjob/atlan-extractor-cron-job crawl-mysql-$(date '+%Y-%m-%d-%H-%M-%S')\nRequest files from Atlan\nâ\nTo get started,\ncontact Atlan support\nto request sample ConfigMap and CronJob files for supported SQL connectors:\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nTags:\nconnectors\ndata\nGet the metadata-extractor tool\nLoad image to Kubernetes cluster\nApply configuration maps\nDeploy extraction job\n(Optional) Trigger the job manually\nRequest files from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/troubleshooting/connection-issues",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nTroubleshooting\nConnection issues\nOn this page\nConnection issues\nThis guide helps you resolve common connection and authentication issues when setting up the CrateDB connector in Atlan.\nUnable to connect to CrateDB cluster\nâ\nIf Atlan can't establish a connection to your CrateDB cluster, you see connection errors during the initial setup or test connection phase.\nâ Error messages\nCheck failed - Unable to connect to CrateDB cluster\nConnection timed out\nð Cause\nIncorrect HTTP/HTTPS endpoint format\nFirewall blocking port\n4200\n(or your configured port)\nSSL/TLS certificate errors when using HTTPS\nð ï¸ How to fix\nVerify endpoint format\n: Make sure you're using the correct HTTP or HTTPS endpoint with proper format:\nhttp://your-host:4200\nhttps://your-cluster.crate.io:4200\nCheck firewall and port access\n: Port\n4200\nmust be open between Atlan and your CrateDB instance.\nValidate SSL certificate\n: If using HTTPS, make sure the certificate is valid and trusted by your network.\nAuthentication failed\nâ\nWhen credentials are incorrect or the user account has issues, authentication fails preventing Atlan from accessing your CrateDB cluster.\nâ Error messages\nCheck failed - Authentication failed for user <username>\n401 Unauthorized\nð Cause\nWrong username or password\nUser not present in CrateDB\nPassword expired or changed\nð ï¸ How to fix\nVerify credentials\n: Double-check that the username and password are correct.\nVeriy user exists\n: Confirm user exists in CrateDB with:\nSELECT\nname\nFROM\nsys\n.\nusers\nWHERE\nname\n=\n'atlan_user'\n;\nUpdate password\n: Reset user password with:\nALTER\nUSER\natlan_user\nSET\n(\npassword\n=\n'new_password'\n)\n;\nTags:\nconnectors\ncratedb\ndatabase\ntroubleshooting\nPrevious\nPreflight checks for CrateDB\nNext\nPermissions and limitations\nUnable to connect to CrateDB cluster\nAuthentication failed"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/connectors-and-capabilities",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nReferences\nConnectors and capabilities\nOn this page\nConnectors and capabilities\nThe matrix below lists the supported capabilities for Atlan's current integrations:\nConnector   -\nsupported sources\nDiscovery   -\nnative assets support\nLineage   -\nasset lineage support\nthrough SQL parsing, API crawling, orchestrator tools, or parsing JSON files from S3\nSecure agent extraction   -  Use a\nsecure agent\nto extract metadata.\nPopularity   -\nasset usage and popularity metrics support\nBrowser extension   -\nAtlan's browser extension support\nâ   -  capability supported\nâ   -  capability not supported\nâ   -  capability will be a paid addition\nData sources\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nAmazon Athena\nâ\nâ\nâ\nâ\nâ\nAmazon Redshift\nâ\nâ\nâ\nâ\nâ\nAWS Glue\nâ\nâ\nâ\nâ\nâ\nCloudera Impala\nâ\nâ\nâ\nâ\nâ\nDatabricks\nâ\nâ\nâ\nâ\nâ\nGoogle BigQuery\nâ\nâ\nâ\nâ\nâ\nHive\nâ\nâ\nâ\nâ\nâ\nMicrosoft Azure Synapse Analytics\nâ\nâ\nâ\nâ\nâ\nMicrosoft SQL Server\nâ\nâ\nâ\nâ\nâ\nMySQL\nâ\nâ\nâ\nâ\nâ\nOracle\nâ\nâ\nâ\nâ\nâ\nPostgreSQL\nâ\nâ\nâ\nâ\nâ\nPrestoSQL\nâ\nâ\nâ\nâ\nâ\nRedash\nâ\nâ\nâ\nâ\nâ\nSalesforce\nâ\nâ\nâ\nâ\nâ\nSAP ECC\nâ\nâ\nâ\nâ\nâ\nSAP HANA\nâ\nâ\nâ\nâ\nâ\nSnowflake\nâ\nâ\nâ\nâ\nâ\nTeradata\nâ\nâ\nâ\nâ\nâ\nTrino\nâ\nâ\nâ\nâ\nâ\nNoSQL data sources\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nAmazon DynamoDB\nâ\nâ\nâ\nâ\nâ\nDataStax Enterprise\nâ\nâ\nâ\nâ\nâ\nMicrosoft Azure Cosmos DB\nâ\nâ\nâ\nâ\nâ\nMongoDB\nâ\nâ\nâ\nâ\nâ\nBusiness intelligence tools\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nAmazon QuickSight\nâ\nâ\nâ\nâ\nâ\nDomo\nâ\nâ\nâ\nâ\nâ\nIBM Cognos Analytics\nâ\nâ\nâ\nâ\nâ\nLooker\nâ\nâ\nâ\nâ\nâ\nMetabase\nâ\nâ\nâ\nâ\nâ\nMicrosoft Power BI\nâ\nâ\nâ\nâ\nâ\nMicroStrategy\nâ\nâ\nâ\nâ\nâ\nMode\nâ\nâ\nâ\nâ\nâ\nQlik Sense Cloud\nâ\nâ\nâ\nâ\nâ\nQlik Sense Cloud Enterprise\nâ\nâ\nâ\nâ\nâ\nSigma\nâ\nâ\nâ\nâ\nâ\nSisense\nâ\nâ\nâ\nâ\nâ\nTableau\nâ\nâ\nâ\nâ\nâ\nThoughtSpot\nâ\nâ\nâ\nâ\nâ\nData movement tools\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\ndbt Cloud\nâ\nâ\nâ\nâ\nâ\ndbt Core\nâ\nâ\nâ\nâ\nâ\nFivetran\nâ\nâ\nâ\nâ\nâ\nMatillion\nâ\nâ\nâ\nâ\nâ\nMicrosoft Azure Data Factory\nâ\nâ\nâ\nâ\nâ\nData quality tools\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nAnomalo\nâ\nâ\nâ\nâ\nâ\nMonte Carlo\nâ\nâ\nâ\nâ\nâ\nSoda\nâ\nâ\nâ\nâ\nâ\nEvent buses\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nAiven Kafka\nâ\nâ\nâ\nâ\nâ\nAmazon MSK\nâ\nâ\nâ\nâ\nâ\nApache Kafka\nâ\nâ\nâ\nâ\nâ\nConfluent Kafka\nâ\nâ\nâ\nâ\nâ\nMicrosoft Azure Event Hubs\nâ\nâ\nâ\nâ\nâ\nRedpanda Kafka\nâ\nâ\nâ\nâ\nâ\nSchema registry\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nConfluent Schema Registry\nâ\nâ\nâ\nâ\nâ\nOrchestration tools\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nApache Airflow/OpenLineage\nâ\nâ\nâ\nâ\nâ\nAmazon MWAA\nâ\nâ\nâ\nâ\nâ\nAstronomer\nâ\nâ\nâ\nâ\nâ\nDagster\nâ\nâ\nâ\nâ\nâ\nGoogle Cloud Composer\nâ\nâ\nâ\nâ\nâ\nAlteryx\nâ\nâ\nâ\nâ\nâ\nData processing tools\nâ\nConnector\nDiscovery\nLineage\nSecure Agent extraction\nPopularity\nBrowser extension\nApache Spark\nâ\nâ\nâ\nâ\nâ\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\napi\nrest-api\ngraphql\nPrevious\nAdditional connectivity to data sources\nNext\nOpenLineage configuration and facets\nData sources\nNoSQL data sources\nBusiness intelligence tools\nData movement tools\nData quality tools\nEvent buses\nSchema registry\nOrchestration tools\nData processing tools"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka",
    "content": "Connect data\nEvent/Messaging\nAiven Kafka\nCrawl Aiven Kafka Assets\nCrawl Aiven Kafka\nOn this page\nCrawl Aiven Kafka\nOnce you have\nconfigured the Aiven Kafka permissions\n, you can establish a connection between Atlan and Aiven Kafka.\nDid you know?\nAtlan currently supports the\noffline extraction method\nfor fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nTo crawl metadata from Aiven Kafka after\nuploading the results to S3\n, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Aiven Kafka as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nAiven Kafka Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nExtraction method\n,\nOffline\nis the default selection.\nForÂ\nBucket name\n, enter the name of your S3 bucket.\nForÂ\nBucket prefix\n, enter the S3 prefix under which all the\nmetadata files\nexist. These include\ntopics.json\n,\ntopic-configs.json\n, and so on.\nBased on your cloud platform, enter the following details:\nIf using AWS, for\nRole ARN\n, enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3.\nIf using Microsoft Azure, enter the name of your\nAzure Storage Account\nand the SAS token for\nBlob SAS Token\n.\nIf using Google Cloud Platform, no further configuration is required.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Aiven Kafka connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Aiven Kafka crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, clickÂ\nExclude topics regex\n. (This will default to no assets, if none specified.)\nTo select the assets you want to include in crawling, click\nInclude topics regex\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Aiven Kafka crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nÂ button.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nÂ button.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Aiven Kafka\nNext\nWhat does Atlan crawl from Aiven Kafka?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/crawl-amazon-athena",
    "content": "Connect data\nDatabases\nQuery Engines\nAmazon Athena\nCrawl Athena Assets\nCrawl Amazon Athena\nOn this page\nCrawl Amazon Athena\nOnce you have configured the\nAmazon Athena access permissions\n, you can establish a connection between Atlan and Amazon Athena. (If you are also using a private network for Amazon Athena, you will need to\nset that up first\n, too.)\nTo crawl metadata from Amazon Athena, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Amazon Athena as your source:\nIn the top right corner of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nAthena Assets\n, and click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Amazon Athena credentials:\nForÂ\nHost\nenter the host name (or\nPrivateLink endpoint\n) for your Amazon Athena instance.\nForÂ\nAuthentication\nchoose the method you configured when\nsetting up the Amazon Athena access permissions\n:\nAt the bottom, enter the\nAWS Role ARN\nand\nS3 Output Location\nyou configured. The\nS3 Output Location\nis where you store temporary Athena query results.\nFor\nIAM User\nauthentication, enter the\nAWS Access Key\nand\nAWS Secret Key\nyou configured.\nFor\nIAM Role\nauthentication, enter the following:\nSet the\nAWS Role ARN\nto the ARN of the\nrole you created in your AWS account\n.\n(Optional) Under\nExternal ID\n, click the\nGenerate\nbutton. Click the button to the right of this field to copy the generated ID and use it in\nsetting up your trust policy\n.\n(Optional) For\nWorkgroup\n, you can override the defaultÂ\nprimary\nworkgroup\nfor tracking compute costs, granular permission controls, and more.\nClick\nTest Authentication\nto confirm connectivity to Amazon Athena.\nOnce successful, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Amazon Athena connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Amazon Athena data, change\nAllow SQL Query\ntoÂ\nNo\n.\n(Optional) To prevent users from previewing any Amazon Athena data, change\nAllow Data Preview\ntoÂ\nNo\n.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the Amazon Athena crawler, you can further configure it.\nYou can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a\nJava regular expression\nin theÂ\nExclude regex for tables & views\nfield.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nÂ to disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Amazon Athena crawler, after completing the steps above:\nTo run the crawler once, immediately, at the bottom of the screen click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly or monthly, at the bottom of the screen click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up a private network link to Amazon Athena\nNext\nWhat does Atlan crawl from Amazon Athena?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/how-tos/crawl-amazon-dynamodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nAmazon DynamoDB\nCrawl DynamoDB Assets\nCrawl Amazon DynamoDB\nOn this page\nCrawl Amazon DynamoDB\nOnce you have\nconfigured the Amazon DynamoDB permissions\n, you can establish a connection between Atlan and Amazon DynamoDB.\nTo crawl metadata from Amazon DynamoDB, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Amazon DynamoDB as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nAmazon DynamoDB Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Amazon DynamoDB credentials:\nFor\nExtraction method\n,\nDirect\nis the default extraction method.\nFor\nAuthentication,\nchoose the method you configured when\nsetting up the Amazon DynamoDB access permissions\n:\nFor\nIAM User\nauthentication, enter the\nAWS Access Key\nand\nAWS Secret Key\nyou downloaded\n.\nFor\nIAM Role\nauthentication, enter the\nAWS Role ARN\nyou configured\n. (Optional) Enter the\nAWS External ID\nonly if you have\nconfigured an external ID\nin the role definition.\nFor\nAWS Region\n, enter the AWS region of your Amazon DynamoDB instance.\nClick the\nTest Authentication\nbutton to confirm connectivity to Amazon DynamoDB.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Amazon DynamoDB connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Amazon DynamoDB crawler, you can further configure it.\nOn the\nMetadata\nÂ page, you can override the defaults for any of these options:\nTo have the crawler ignore tables based on a naming convention, specify a regular expression in the\nExclude tables regex\nfield.\nTo have the crawler include tables based on a naming convention, specify a regular expression in the\nInclude tables regex\nfield.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Amazon DynamoDB crawler, after completing the steps above:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Amazon DynamoDB\nNext\nWhat does Atlan crawl from Amazon DynamoDB?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nCrawl Amazon MSK Assets\nCrawl Amazon MSK\nOn this page\nCrawl Amazon MSK\nOnce you have\nconfigured the Amazon MSK permissions\n, you can establish a connection between Atlan and Amazon MSK. (If you are also using a private network for Amazon MSK, you will need to\nset that up first\n, too.)\nTo crawl metadata from Amazon MSK, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Amazon MSK as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nAmazon MSK Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Amazon MSK credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nBootstrap servers\n, enter the hostname(s) (or\nPrivateLink cluster connection string\n) of your Amazon MSK broker(s)   -  for multiple hostnames, separate each entry with a comma\n,\nor semicolon\n;\n.\nFor\nAuthentication\n,\nIAM Role\nis the default authentication method.\nFor\nDeployment Type\n,\nProvisioned\nis the default deployment type.\nFor\nSecurity protocol\n,\nSASL_SSL\nis the default security protocol.\nFor\nAWS Role ARN\n, enter the ARN of the\nIAM role you created in your AWS account\n.\nFor\nAWS Region\n, enter the AWS region of your Amazon MSK cluster.\nClick the\nTest Authentication\nbutton to confirm connectivity to Amazon MSK.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Amazon MSK connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Amazon MSK crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nFor\nSkip internal topics\n, keep the default option\nYes\nto skip\ninternal Apache Kafka topics\nor click\nNo\nto enable crawling them.\nTo have the crawler ignore topics based on a naming convention, specify a regular expression in the\nExclude topic regex\nfield.\nTo have the crawler include topics based on a naming convention, specify a regular expression in the\nInclude topic regex\nfield.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Amazon MSK crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nÂ button.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nÂ button.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up a private network link to Amazon MSK\nNext\nWhat does Atlan crawl from Amazon MSK?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/crawl-amazon-quicksight",
    "content": "Connect data\nBI Tools\nCloud-based BI\nAmazon QuickSight\nCrawl QuickSight Assets\nCrawl Amazon QuickSight\nOn this page\nCrawl Amazon QuickSight\nOnce you have\nconfigured the Amazon QuickSight permissions\n, you can establish a connection between Atlan and Amazon QuickSight.\nTo crawl metadata from Amazon QuickSight, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Amazon QuickSight as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nQuickSight Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Amazon QuickSight credentials:\nFor\nAuthentication,\nIAM User\nis the default authentication method.\nFor\nAWS Access Key\n, enter the\nAWS access key you downloaded\n.\nFor\nAWS Secret Key\n, enter the\nAWS secret key you downloaded\n.\nAt the bottom, enter the\nRegion\nand\nAWS Account ID\nof your Amazon QuickSight instance.\nClick the\nTest Authentication\nbutton to confirm connectivity to Amazon QuickSight.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Amazon QuickSight. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Amazon QuickSight data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the Amazon QuickSight connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Amazon QuickSight crawler, you can further configure it.\nOn the\nMetadata Filters\npage, you can override the defaults for any of these options:\nFor\nFetch all assets without folder\n, click\nYes\nto fetch assets not linked to any folders, including datasets, analyses, and dashboards, or click\nNo\nto only fetch assets linked to folders.\nTo select the folders you want to include in crawling, click\nInclude Folders\n. (This will default to all folders, if none are specified.)\nTo select the folders you want to exclude from crawling, click\nExclude Folders\n. (This will default to no folders, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Amazon QuickSight crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Amazon QuickSight\nNext\nWhat does Atlan crawl from Amazon QuickSight?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nCrawl Redshift Assets\nCrawl Amazon Redshift\nOn this page\nCrawl Amazon Redshift\nOnce you have configured the\nAmazon Redshift access permissions\n, you can establish a connection between Atlan and Amazon Redshift.\nTo crawl metadata from Amazon Redshift, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Amazon Redshift as your source:\nIn the top right corner of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nRedshift Assets\n, and click\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you will need to first\nextract metadata yourself\nand\nmake it available in S3\n.\nDirect extraction method\nâ\nTo enter your Amazon Redshift credentials:\nFor\nHost Name\n, enter the host name of your Amazon Redshift instance. From your Redshift cluster you can find the host name in the\nConfiguration\nsection as a variable called\nEndpoint\n.\nFor\nPort\n, enter the port number for your Amazon Redshift instance. You can find this next to the host name in the\nConfiguration\nsection of your Redshift cluster.\nFor\nDeployment Type\n, click\nProvisioned\nif your Amazon Redshift instance is deployed on\nprovisioned clusters\nor click\nServerless\nif deployed on a\nserverless workgroup\n.\nFor\nAuthentication\n, choose the method you configured when\nsetting up the Amazon Redshift access permissions\n:\nFor\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou configured.\nFor\nIAM User\nauthentication, enter the\nAWS Access Key\n,\nAWS Secret Key\n, and\nUsername\nyou configured for the database.\n(Optional) This is only required if you are accessing a private cluster on provisioned deployment, using a Network Load Balancer (NLB), and connecting via IAM, for\nCluster ID\n, enter the\nname of the Amazon Redshift cluster\nthat you want to connect to.\n(Optional) This is only required if you are accessing a private cluster on serverless deployment, for\nWorkgroup\n, enter the\nname of your workgroup\n.\nFor\nIAM Role\nauthentication, enter the\nUsername\nyou configured for the database only if your deployment type is\nProvisioned\n. For\nServerless\ndeployment type, you do not need to enter a username.\nSet the\nAWS Role ARN\nto the ARN of the\nrole you created in your AWS account\n.\n(Optional) For\nRegion\n, enter the AWS region of your Amazon Redshift instance.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from Amazon Redshift. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Amazon Redshift connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Amazon Redshift data, change\nAllow SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any Amazon Redshift data, change\nAllow Data Preview\nto\nNo\n.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the Amazon Redshift crawler, you can further configure it.\nYou can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in the\nExclude regex for tables & views\nfield.\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nAdvanced\nto configure the crawler:\nFor\nCross Connection\n, click\nYes\nto extract lineage across all available Amazon Redshift connections or click\nNo\nto limit lineage extraction to the current connection.\nFor\nControl Config\n, if Atlan support has provided you with a custom control configuration, select\nCustom\nand enter the configuration into the\nCustom Config\nbox. You can also:\nEnter\n{\"ignore-all-case\": true}\nto enable crawling assets with case-sensitive identifiers.\nIf you've configured a cloned schema to provide access to Atlan, add the following key-value pair to the\nCustom Config\nfield:\n{\"clonedPgCatalogSchema\": \"cloned_schema_name\"}\nReplace\ncloned_schema_name\nwith the name of your cloned schema.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Amazon Redshift crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up a private network link to Amazon Redshift\nNext\nMine Amazon Redshift\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka",
    "content": "Connect data\nEvent/Messaging\nApache Kafka\nCrawl Apache Kafka Assets\nCrawl Apache Kafka\nOn this page\nCrawl Apache Kafka\nAtlan crawls metadata from your Apache Kafka clust\ner, allowing you to discover, classify, and govern your Kafka topics and schemas. This guide walks you through the steps to configure and run the Apache Kafka crawler in Atlan.\nPrerequisites\nâ\nBefore you begin, complete the following prerequisites:\nApache Kafka setup:\nYou have\nconfigured the Apache Kafka permissions\n, you can establish a connection between Atlan and Apache Kafka.\nOrder of operations:\nReview the\norder of operations\nto understand the sequence of tasks for crawling metadata.\nAccess to Atlan workspace:\nYou must have the required permissions in Atlan to create and manage a connection.\nSelect the source\nâ\nTo select Apache Kafka as your source:\nIn Atlan, click\nNew\n, and from the menu, select\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nApache Kafka Assets\n.\nClick\nSetup Workflow\nin the right panel to proceed with configuration.\nProvide credentials\nâ\nInÂ\nDirect\nextraction, Atlan connects to Apache Kafka and crawls metadata directly.\nInÂ\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nDirect extraction method\nâ\nTo enter your Apache Kafka credentials:\nFor\nBootstrap servers\n, enter the hostname(s) of your Apache Kafka broker(s)   -  for multiple hostnames, separate each entry with a comma\n,\nor semicolon\n;\n.\nFor\nAuthentication\n, Atlan provides the following authentication methods:\nNo Authentication:\nIf your Apache Kafka cluster does not require authentication, Atlan can connect without any credentials..\nBasic Authentication (SASL/PLAIN):\nUses a\nusername and password\nwith the SASL_PLAIN mechanism for authentication.\nSCRAM Authentication (SASL/SCRAM):\nUses a\nusername and password\nwith the SASL_SCRAM mechanism (SCRAM-SHA-256 or SCRAM-SHA-512) for secure authentication.\nUsername,\nenter the username for your Apache Kafka brokers.\nPassword,\nenter the password for the username.\nFor\nSecurity protocol\n, select\nPlaintext\nor\nSSL\nfor No Auth, and\nSASL_PLAINTEXT\nor\nSASL_SSL\nfor Basic and SCRAM authentication.\nFor\nSASL Mechanism\n(optional for SCRAM authentication), choose the appropriate mechanism for your Kafka cluster.\nClick\nTest Authentication\nto confirm connectivity.\nOnce authentication is successful, click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Apache Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nForÂ\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the\nmetadata files\nexist. These include\ntopics.json\n,\ntopic-configs.json\n, and so on.\nBased on your cloud platform, enter the following details:\nIf using AWS, for\nRole ARN\n, enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3.\nIf using Microsoft Azure, enter the name of your\nAzure Storage Account\nand the SAS token for\nBlob SAS Token\n.\nIf using Google Cloud Platform, no further configuration is required.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Apache Kafka connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Apache Kafka crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nFor\nSkip internal topics\n, keep the default option\nYes\nto skip\ninternal Apache Kafka topics\nor click\nNo\nto enable crawling them.\nTo select the Apache Kafka assets you want to exclude from crawling, clickÂ\nExclude topics regex\n. (This will default to no assets, if none specified.)\nTo select the Apache Kafka assets you want to include in crawling, click\nInclude topics regex\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Apache Kafka crawler, after completing the steps above:\nClick\nPreflight checks\nto verify configuration.\nChoose one of the following options:\nTo run the crawler once immediately, click\nRun\n.\nTo schedule the crawler, click\nSchedule & Run\n.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Apache Kafka\nNext\nSet up on-premises Kafka access\nPrerequisites\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/how-tos/crawl-aws-glue",
    "content": "Connect data\nETL Tools\nAWS Glue\nCrawl AWS Glue Assets\nCrawl AWS Glue\nOn this page\nCrawl AWS Glue\nOnce you have configured the\nAWS Glue access permissions\n, you can establish a connection between Atlan and AWS Glue.\nTo crawl metadata from AWS Glue, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select AWS Glue as your source:\nIn the top right corner of any screen, navigate to\nNew\nÂ and then click\nNew Workflow\n.\nFrom the list of packages, select\nGlue Assets\n, and click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your AWS Glue credentials:\nFor\nAuthentication\n, choose the method you configured when\nsetting up the AWS Glue access permissions\n:\nAt the bottom, enter the\nRegion\nof your AWS Glue deployment.\nForÂ\nIAM User\nauthentication, enter theÂ\nAWS Access Key\nand\nAWS Secret Key\nyou configured.\nForÂ\nIAM Role\nauthentication, enter the following:\nSet the\nAWS Role ARN\nto the ARN of the\nrole you created in your AWS account\n.\n(Optional) Under\nExternal ID\n, click the\nGenerate\nbutton. Click the button to the right of this to copy the generated ID and use this in\nsetting up your trust policy\n.\nClickÂ\nTest Authentication\nto confirm connectivity to AWS Glue.\nOnce successful, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the AWS Glue connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the AWS Glue crawler, you can further configure it.\nYou can override the defaults for any of these options:\nSelect assets you want to include in crawling in the\nInclude Metadata\nfield. (This will default to all assets, if none are specified.)\nSelect assets you want to exclude from crawling in the\nExclude Metadata\nfield. (This will default to no assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the AWS Glue crawler, after completing the steps above:\nTo run the crawler once, immediately, at the bottom of the screen click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly or monthly, at the bottom of the screen click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up AWS Glue\nNext\nWhat does Atlan crawl from AWS Glue?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/how-tos/crawl-bigid",
    "content": "Connect data\nPrivacy & Security\nBigID\nCrawl BigID Metadata\nCrawl BigID\nOn this page\nCrawl BigID\nConfigure the Atlan BigID workflow to crawl metadata from your BigID instance and discover privacy-related data assets in Atlan. This guide walks through setting up the workflow, configure connection, map data sources, and running the crawler.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nSet up a BigID system user account. If not, follow the\nSet up BigID\nguide for detailed instructions.\nYour BigID domain name and API token which is needed to configure the workflow.\nTo crawl metadata from BigID, review the\norder of operations\n.\nPermissions required\nâ\nTo successfully configure the BigID workflow, make sure that your user role has below permissions:\nAtlan\n: Admin or Workflow Admin permissions\nBigID\n: System user with API access\nSet up workflow\nâ\nFollow these steps in Atlan to create a BigID workflow:\nClick\nNew\nand then click\nNew Workflow\nto set up a new workflow\nFrom the list of packages, select\nBigID\nand click on\nSetup Workflow\n. Provide below details:\nWorkflow Name\n: Enter a unique name to help recognize and manage the workflow\nHost FQDN\n: Enter the BigID domain name. For private-network setups, use the private DNS associated with the link.\nPersonal Access Token Value\n: Provide the API token created for the system user in the\nSet up BigID\nSSL certificate\n: Enter the root certificate PEM value if your BigID instance exposes a self-signed certificate\nClick\nTest Authentication\nto confirm connectivity to BigID\nWhen successful, at the bottom of the screen click\nNext\nSet up connection\nâ\nSet up the connection details and specify who can manage this connection. Follow these steps to configure the connection:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n. If you don't specify any user or group, nobody can manage the connection - not even admins.\nAt the bottom of the screen, click\nNext\nto proceed\nMap data sources\nâ\nMap BigID Data Sources to Atlan Connections to establish the relationship between your data assets. Follow these steps to map data sources:\nAtlan Connection\n: Select the Atlan Connection that houses the data assets that you're trying to bring BigID metadata for\nBigID Datasources\n: Select one or more BigID Data Sources that contain assets associated with the mapped Atlan Connection\nClick\nNext\nto configure custom metadata\nConfigure custom metadata\nâ\nSet up custom metadata to store BigID-discovered attributes in Atlan. Follow these steps to configure custom metadata:\nCustom Metadata\n: Create a new\nCustom Metadata\non Atlan named\nBigID Metadata\nwith a text-based property named\nAttributes\n. This is used to house the BigID-discovered, scan-related attributes that the workflow brings over to Atlan\nAttribute Custom Metadata\n: Enter the value as\nBigID Metadata\nAttribute Custom Metadata Property\n: Enter the value as\nAttributes\nRun crawler\nâ\nExecute the BigID crawler to discover and import metadata. Follow these steps to run crawler:\nRun immediately\n: Click the\nRun\nbutton to run the crawler once immediately\nSchedule run\n: Click the\nSchedule Run\nbutton to schedule the crawler to run hourly, daily, weekly, or monthly\nTroubleshooting\nâ\nIf you encounter issues during the BigID crawl process:\nAuthentication issues\n: Verify your API token is valid and has the correct permissions\nSSL certificate errors\n: Check that you've provided the correct root certificate PEM value\nNo metadata appears\n: Check that data source mapping is correct and BigID contains assets for the mapped connections\nNeed help\nâ\nContact Atlan support\n: For issues related to Atlan integration,\ncontact Atlan support\nSee also\nâ\nWhat does Atlan crawl from BigID\nTags:\nconnectors\ndata\ncrawl\nprivacy\nbigid\nPrevious\nSet up BigID\nNext\nWhat does Atlan crawl from BigID?\nPrerequisites\nPermissions required\nSet up workflow\nTroubleshooting\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/how-tos/crawl-confluent-kafka",
    "content": "Connect data\nEvent/Messaging\nConfluent Kafka\nCrawl Confluent Kafka Assets\nCrawl Confluent Kafka\nOn this page\nCrawl Confluent Kafka\nAtlan crawls metadata from your Confluent Kafka cl\nuster, allowing you to discover, classify, and govern your Kafka topics and schemas. This guide walks you through the steps to configure and run the Confluent Kafka crawler in Atlan.\nPrerequisites\nâ\nBefore you begin, complete the following prerequisites:\nConfluent Kafka setup:\nYou have\nconfigured the Confluent Kafka permissions\n, you can establish a connection between Atlan and Confluent Kafka.\nOrder of operations:\nReview the\norder of operations\nto understand the sequence of tasks for crawling metadata.\nAccess to Atlan workspace:\nYou must have the required permissions in Atlan to create and manage a connection.\nSelect the source\nâ\nTo select Confluent Kafka as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nConfluent Kafka Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nIn\nDirect extraction\n, Atlan connects to Confluent Kafka and crawls metadata directly.\nIn\nOffline extraction\n, you need to first extract metadata yourself and make it available in S3.\nDirect extraction method\nâ\nTo enter your Confluent Kafka credentials:\nFor\nBootstrap servers\n, enter the hostname(s) of your Confluent Kafka broker(s). Separate multiple hostnames with a comma\n,\nor semicolon\n;\n.\nFor\nAPI Key\n, enter the API key you copied.\nFor\nAPI Secret\n, enter the API secret you copied.\nFor Security protocol, click\nSASL_PLAINTEXT\nto connect to Confluent Kafka through a non-encrypted channel or click\nSASL_SSL\nto connect via a Secure Sockets Layer (SSL) channel.\nClick the\nTest Authentication\nbutton to confirm connectivity to Confluent Kafka.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Confluent Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the\nmetadata files\nexist. These include\ntopics.json\n,\ntopic-configs.json\n, and so on.\nBased on your cloud platform, enter the following details:\nIf using AWS, for\nRole ARN\n, enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3.\nIf using Microsoft Azure, enter the name of your\nAzure Storage Account\nand the SAS token for\nBlob SAS Token\n.\nIf using Google Cloud Platform, no further configuration is required.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Confluent Kafka connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who can manage this connection, update the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Confluent Kafka crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nFor\nSkip internal topics\n, keep the default option\nYes\nto skip\ninternal Kafka topics\nor click\nNo\nto enable crawling them.\nTo select the assets you want to exclude from crawling, clickÂ\nExclude topics regex\n. (This will default to no assets, if none specified.)\nTo select the assets you want to include in crawling, click\nInclude topics regex\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Confluent Kafka crawler:\nTo run the crawler once, immediately, click the\nRun\nbutton at the bottom of the screen.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, click the\nSchedule & Run\nbutton at the bottom of the screen.\nOnce the crawl completes, your assets appear in Atlan! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Confluent Kafka\nNext\nSet up on-premises Kafka access\nPrerequisites\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry",
    "content": "Connect data\nEvent/Messaging\nConfluent Schema Registry\nCrawl Schema Registry Assets\nCrawl Confluent Schema Registry\nOn this page\nCrawl Confluent Schema Registry\nOnce you have\nconfigured the Confluent Schema Registry access permissions\n, you can establish a connection between Atlan and Confluent Schema Registry.\nTo crawl metadata from Confluent Schema Registry,\nreview the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Confluent Schema Registry as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nConfluent Schema Registry Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Confluent Schema Registry credentials:\nFor\nHost\n, enter your\nschema registry endpoint\n.\nFor\nAPI Key\n, enter the\nAPI key you copied\n.\nFor\nAPI Secret\n, enter the\nAPI secret you copied\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Confluent Schema Registry.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Confluent Schema Registry connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Confluent Schema Registry crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the subjects you want to exclude from crawling, click\nExclude subjects\n. (This will default to no subjects, if none specified.)\nTo select the subjects you want to include in crawling, click\nInclude subjects\n. (This will default to all subjects, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Confluent Schema Registry crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Confluent Schema Registry\nNext\nWhat does Atlan crawl from Confluent Schema Registry?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/how-tos/crawl-cratedb",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nCrawl CrateDB Assets\nCrawl CrateDB\nOn this page\nCrawl CrateDB\nExtract metadata from your CrateDB database and make it available in Atlan for data discovery, governance, and lineage tracking. This guide walks you through setting up authentication and running your first crawl.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nSet up CrateDB\nwith proper user permissions\nNetwork connectivity between Atlan and your CrateDB instance\nYour CrateDB cluster HTTP endpoint and port information\nSet up workflow\nâ\nCreate a new\nCrateDB Assets\nworkflow to extract metadata from your database.\nSelect\nNew\n>\nNew Workflow\n.\nFrom the list of packages, select\nCrateDB Assets\n.\nClick\nSetup Workflow\n.\nConfigure extraction method\nâ\nChoose how to connect to your CrateDB environment:\nDirect extraction\nAgent extraction\nSelect\nDirect\nfor the extraction method.\nEnter your CrateDB connection details:\nHost\n: Your CrateDB cluster HTTP endpoint (for example,\nhttps://your-cluster.crate.io\n)\nPort\n: The port number of your CrateDB instance\nAuthentication\n: Choose\nBasic\nauthentication\nUsername\n: Enter the username you configured in CrateDB\nPassword\n: Enter the password you configured in CrateDB\nDatabase\n: Enter the name of the database to crawl\nClick\nTest Authentication\nto confirm connectivity to CrateDB using these details.\nWhen successful, click\nNext\n.\nSelect\nAgent\nfor the extraction method.\nAdd the secret keys for your secret store configuration.\nFollow the\nSecure Agent configuration guide\n.\nClick\nNext\n.\nConfigure connection details\nâ\nEnter a\nConnection Name\nto identify your CrateDB environment. For example,\nproduction-cratedb\n,\nanalytics-db\n,\ndata-warehouse\n.\nAssign\nConnection Admins\nto manage access. At least one admin is required.\nConfigure crawler settings\nâ\nBefore running the CrateDB crawler, you can configure additional settings:\nExclude Metadata\n: Select assets you want to exclude from crawling\nInclude Metadata\n: Select assets you want to include in crawling\nExclude regex for tables & views\n: Specify a regular expression to ignore tables and views based on naming conventions\nAdvanced Config\n:\nEnable Source Level Filtering\n: Enable schema-level filtering at source\nUse JDBC Internal Methods\n: Enable JDBC internal methods for data extraction\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun crawler\nâ\nYou can now start extracting metadata from your CrateDB database:\nRun now\n: Click\nRun\nto start a one-time crawl.\nSchedule runs\n: Click\nSchedule Run\nto automate recurring crawls (hourly, daily, weekly, or monthly).\nMonitor crawl progress in the activity log. Once complete, your CrateDB assets appear in Atlan.\nTroubleshooting\nâ\nIf you encounter connection or authentication issues, see\nConnection issues\n.\nSee also\nâ\nWhat does Atlan crawl from CrateDB?\nPreflight checks for CrateDB\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up CrateDB\nNext\nWhat does Atlan crawl from CrateDB?\nPrerequisites\nSet up workflow\nTroubleshooting\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/how-tos/crawl-dagster",
    "content": "Connect data\nOrchestration & Workflow\nDagster\nGet Started\nCrawl Dagster assets\nOn this page\nCrawl Dagster assets\nPrivate Preview\nCreate a crawler workflow in Atlan to capture lineage from your Dagster assets. This workflow connects to Dagster and begins lineage capture.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdmin access to your Atlan workspace\nConfigured Dagster for Atlan integration. For more information, see\nSet up Dagster\nCreate crawler workflow\nâ\nFollow these steps to create a workflow in Atlan that captures lineage from Dagster.\nIn Atlan, select\nNew\n>\nNew Workflow\n.\nFrom the package list, choose\nDagster Assets\n.\nSelect\nSetup Workflow\n.\nConfigure connection\nâ\nFollow these steps to configure the Dagster connection in Atlan and finalize lineage capture.\nEnter a\nConnection Name\n. For example,\nproduction\n,\ndevelopment\n, or\nanalytics\n.\nAssign at least one\nConnection Admin\n.\nSelect\nRun\nto create the connection.\nTrack the workflow's progress in the\nWorkflow center\n.\nAfter the workflow completes, the Dagster connection is ready to receive lineage events.\nSee also\nâ\nWhat does Atlan crawl from Dagster\n: Metadata available from Dagster after integration\nTags:\nconnectors\nlineage\ndagster\nPrevious\nSet up Dagster\nNext\nWhat does Atlan crawl from Dagster\nPrerequisites\nCreate crawler workflow\nConfigure connection\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/how-tos/crawl-datastax-enterprise",
    "content": "Connect data\nDatabases\nNoSQL Databases\nDatastax Enterprise\nCrawl Datastax Enterprise Assets\nCrawl DataStax Enterprise\nOn this page\nCrawl DataStax Enterprise\nOnce you have\nconfigured DataStax Enterprise\n, you can establish a connection between Atlan and DataStax Enterprise.\nTo crawl metadata from DataStax Enterprise, review\nthe\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select DataStax Enterprise as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nDataStax Enterprise Assets\nand click the\nSetup Workflow\nbutton.\nProvide credentials\nâ\nInÂ\nDirect\nextraction, Atlan connects to DataStax Enterprise and crawls metadata directly.\nInÂ\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your DataStax Enterprise credentials:\nFor\nHost Name\n, enter the host name of your DataStax Enterprise instance.\nFor\nPort\n, enter the port number of your DataStax Enterprise instance.\nFor\nAuthentication\n,\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou use to log in to DataStax Enterprise.\n(Optional) For\nSSL\n, keep the default\nEnabled\nto use HTTPS or click\nDisabled\nto use HTTP.\n(Optional) For\nSSL certificate\n, this is only required if\nyour DataStax Enterprise instance uses a self-signed or an internal CA SSL certificate\n, paste a\nsupported SSL certificate in the recommended format\n.\nAt the bottom of the form, click the\nTest Authentication\nbutton to confirm connectivity to DataStax Enterprise using these details.\nWhen successful, at the bottom of the screen click the\nNext\nbutton.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from DataStax Enterprise. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the DataStax Enterprise data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the DataStax Enterprise connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nConfigure the crawler\nâ\nBefore running the DataStax Enterprise crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the DataStax Enterprise keyspaces you want to include in crawling, click\nInclude Keyspaces\n. (This will default to all assets, if none are specified.)\nTo select the DataStax Enterprise keyspaces you want to exclude from crawling, click\nExclude Keyspaces\n. (This will default to no assets, if none are specified.)\nTo have the crawler ignore DataStax Enterprise keyspaces based on a naming convention, specify a regular expression in the\nExclude Keyspaces Regex\nfield.\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nDid you know?\nIf a keyspace appears in both the include and exclude filters, the exclude filter takes precedence. (The\nExclude Keyspace Regex\nalso takes precedence.)\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up DataStax Enterprise\nNext\nWhat does Atlan crawl from DataStax Enterprise?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/how-tos/crawl-domo",
    "content": "Connect data\nBI Tools\nCloud-based BI\nDomo\nCrawl Domo Assets\nCrawl Domo\nOn this page\nCrawl Domo\nOnce you have\nconfigured the Domo permissions\n, you can establish a connection between Atlan and Domo.\nTo crawl metadata from Domo, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Domo as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nDomo Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Domo credentials:\nFor\nHost Name\n, enter the URL for your Domo instance.\nFor\nAuthentication\n,\nBasic\nis the default selection.\nFor\nClient ID\n, enter the\nclient ID you copied\nfrom the Domo developer portal.\nFor\nClient Secret\n, enter the\nclient secret you copied\nfrom the Domo developer portal.\nFor\nAccess Token\n, enter the\naccess token you copied\nfrom your Domo instance.\nClick the\nTest Authentication\nbutton to confirm connectivity to Domo.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Domo connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Domo crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the dashboards you want to include in crawling, click\nInclude dashboards\n. (This will default to all assets, if none are specified.)\nTo select the dashboards you want to exclude from crawling, click\nExclude dashboards\n. (This will default to no assets, if none are specified.)\nFor\nDomoStats dataset ID to get cards metadata\n, enter the dataset ID for the\ndataset you created to import card metadata\non the DomoStats connector.\nFor\nDomoStats dataset ID to get card-dashboard relationship metadata\n, enter the dataset ID for the\ndataset you created to import card-dashboard relationship metadata\non the DomoStats connector.\nFor\nDomoStats dataset ID to get dataset-card relationship metadata\n, enter the dataset ID for the\ndataset you created to import dataset-card relationship metadata\non the DomoStats connector.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Domo crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Domo\nNext\nWhat does Atlan crawl from Domo?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/how-tos/crawl-fivetran",
    "content": "Connect data\nETL Tools\nFivetran\nCrawl Fivetran Assets\nCrawl Fivetran\nOn this page\nCrawl Fivetran\nOnce you have\nconfigured the Fivetran permissions\n, you can establish a connection between Atlan and Fivetran.\nTo enrich Atlan with metadata from Fivetran, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Fivetran as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nFivetran Enrichment\nand then click\nSetup Workflow\n.\nProvide credentials\nâ\nIn order to run this package, you must ensure the following:\nThe\nFivetran Platform Connector is set up\nand has run successfully in Fivetran at least once.\nFivetran logs are stored in a destination supported by Atlan.\nThe above destination has been crawled in Atlan.\nTo use the Fivetran Platform Connector:\nFor\nAtlan Connection\n, Atlan will use the credentials of your selected connection to read the Fivetran Platform Connector tables associated with that connection. You can either:\nCreate a connection in Atlan for the\ndestination warehouse you configured\nwhile setting up the Fivetran Platform Connector in Fivetran. This connection in Atlan must have access to the Fivetran tables created by the Fivetran Platform Connector. Atlan supports the following destinations:\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nPostgreSQL\nSnowflake\nIf you have already created a connection in Atlan, select the connection to extract. (To select a connection, the crawler must have already run for a supported destination.)\ndanger\nIf you have an existing connection, you\nmust\nensure that the user or other access permissions configured for that connection allow access to the Fivetran tables created by the Fivetran Platform Connector or update them accordingly.\nFor\nFivetran Platform Schema\n, select the\ndestination schema where Fivetran logs are stored\n. You can only select one schema and must ensure that the connection above has access to\nall Fivetran log tables stored in this destination schema\n.\nOnce successful, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Fivetran connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nRun the enrichment\nâ\nYou can now enrich Atlan with Fivetran metadata:\nTo check for any\npermissions or other configuration issues\nbefore running the enrichment, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nÂ button.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nÂ button.\nOnce the enrichment has completed running, you will see lineage extended upstream from your data platform, warehouse, or lake! ð\nTags:\nconnectors\ndata\ncrawl\napi\nconfiguration\nPrevious\nSet up Fivetran\nNext\nWhat does Atlan crawl from Fivetran?\nSelect the source\nProvide credentials\nConfigure the connection\nRun the enrichment"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nCrawl BigQuery Assets\nCrawl Google BigQuery\nOn this page\nCrawl Google BigQuery\nOnce you have configured the\nGoogle BigQuery user permissions\n, you can establish a connection between Atlan and Google BigQuery.\nTo crawl metadata from Google BigQuery, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Google BigQuery as your source:\nIn the top right corner of any screen, click\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nBigQuery Assets\nand click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Google BigQuery credentials:\nFor\nAuthentication\n,\nService Account\nis the default selection.\nFor\nConnectivity\n, choose how you want Atlan to connect to Google BigQuery:\nTo connect using a public endpoint from Google, click\nPublic Network\n.\nTo connect through a private endpoint, click\nPrivate Network Link\n. Next,\ncontact Atlan support\nto request the DNS name of the Private Service Connect endpoint that Atlan created for the integration:\nFor\nHost\n, enter the DNS name of the Private Service Connect endpoint received from Atlan in the following format   -\nhttps://bigquery-<privateserver>.p.googleapis.com\n. Replace\n<privateserver>\nwith the DNS name.\nFor\nPort\n,\n443\nis the default selection.\nFor\nProject Id\n, enter the value of\nproject_id\nfrom the\nJSON for the service account you created\n. This project ID is only used to authenticate the connection. You can\nconfigure the crawler\nto extract more than just the specified project.\nFor\nService Account Json\n, paste in the entire\nJSON for the service account you created\n.\nFor\nService Account Email\n, enter the value of\nclient_email\nfrom the\nJSON for the service account you created\n.\nAt the bottom of the form, click the\nTest Authentication\nbutton to confirm connectivity to Google BigQuery using these details.\nWhen successful, at the bottom of the screen click the\nNext\nbutton.\nConfigure the connection\nâ\nTo complete the Google BigQuery connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Google BigQuery data, change\nAllow SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any Google BigQuery data, change\nAllow Data Preview\nto\nNo\n.\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the Google BigQuery crawler, you can further configure it.\nYou can override the defaults for any of these options:\nFor\nFilter Sharded Tables\n, keep\nNo\nfor the default configuration or click\nYes\nto enable Atlan to catalog and display\nsharded tables\nwith the same naming prefix as a single table in asset discovery and the lineage graph.\nSelect assets you want to include in crawling in the\nInclude Metadata\nfield. (This will default to all assets, if none are specified.)\nSelect assets you want to exclude from crawling in the\nExclude Metadata\nfield. (This will default to no assets, if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in the\nExclude regex for tables & views\nfield.\nTo\nimport existing tags from Google BigQuery to Atlan\n, for\nImport Tags\n, click\nYes\n.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nif Atlan support has provided you with a custom control configuration.\nEnter the configuration into the\nCustom Config\nbox. You can also enter\n{âignore-all-caseâ: true}\nto enable crawling assets with case-sensitive identifiers.\nFor\nHidden Assets\n, keep\nNo\nfor the default configuration or click\nYes\nto crawl metadata from your\nhidden datasets\nin Google BigQuery.\nDid you know?\nIf a folder or project appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Google BigQuery crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nHow to enable SSO for Google BigQuery\nNext\nMine Google BigQuery\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/crawl-hive",
    "content": "Connect data\nDatabases\nQuery Engines\nHive\nCrawl Hive Assets\nCrawl Hive\nOn this page\nCrawl Hive\nOnce you have\nconfigured the Hive permissions\n, you can establish a connection between Atlan and Hive. (If you are also using a private network for Hive, you will need to\nset that up first\n, too.)\nTo crawl metadata from Hive, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Hive as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nHive Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nInÂ\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nDirect extraction method\nâ\nTo enter your Hive credentials:\nFor\nHost Name\n, enter the host name (or\nPrivateLink endpoint\n) for your Hive instance.\nFor\nPort\n, enter the port number for your Hive instance.\nFor\nUsername\n, enter the username you created for that instance.\nFor\nPassword\n, enter the password for the username.\nFor\nDefault Schema\n, enter the default schema name for your Hive instance.\nClick the\nTest Authentication\nbutton to confirm connectivity to Hive.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Hive. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your bucket details:\nForÂ\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nForÂ\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen clickÂ\nNext\n.\nConfigure the connection\nâ\nTo complete the Hive connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Hive crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the Hive assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo select the Hive assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Hive crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nDid you know?\nOnce you have crawled assets from Hive, you can run the\nHive Miner\nto\nmine query history through S3\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up a private network link to Hive\nNext\nWhat does Atlan crawl from Hive?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/crawl-ibm-cognos-analytics",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nCrawl Cognos Analytics Assets\nCrawl IBM Cognos Analytics\nOn this page\nCrawl IBM Cognos Analytics\nOnce you have\nconfigured the IBM Cognos Analytics permissions\n, you can establish a connection between Atlan and IBM Cognos Analytics.\nTo crawl metadata from IBM Cognos Analytics, revie\nw the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select IBM Cognos Analytics as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nIBM Cognos Analytics Assets\n.Â\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you will need to first\nextract metadata\nyourself and\nmake it available in S3\n.\nDirect extraction method\nâ\nTo enter your IBM Cognos Analytics credentials:\nFor\nHost Name\n, enter the hostname of your IBM Cognos Analytics instance.\nFor\nPort\n, enter the port number of your IBM Cognos Analytics instance.\nFor\nAuthentication\n, select the method you configured when\nsetting up the IBM Cognos Analytics access permissions\n:\nFor\nBasic\n, enter the\nUsername\nand\nPassword\nyou configured for the new user.\nFor\nAPI Key\n, enter the\nUsername\nand\nAPI Key\nyou configured for the new user.\nFor\nOKTA\n, enter the\nUsername\nand\nPassword\nyou configured for the new user in OKTA.\nFor\nNamespace\n, enter the\nname of the namespace where you created the new user\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to IBM Cognos Analytics.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from IBM Cognos Analytics. This method uses Atlan's cognos-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your bucket details:\nFor\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\noutput/cognos-example/contents/0/result-0.json\n,\noutput/cognos-example/contents-details/0/result-0.json\n, and so on.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the IBM Cognos Analytics connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you don't specify any user or group, no one can manage the connection-not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the IBM Cognos Analytics crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Folders\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Folders\n. (This will default to no assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the IBM Cognos Analytics crawler, after completing the steps above:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up on-premises IBM Cognos Analytics access\nNext\nCrawl on-premises IBM Cognos Analytics\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/how-tos/crawl-informatica-cdi",
    "content": "Connect data\nETL Tools\nInformatica CDI\nCrawl Informatica CDI Assets\nCrawl Informatica CDI assets\nOn this page\nCrawl Informatica CDI assets\nCreate a crawler workflow to automatically discover and catalog your Informatica Cloud Data Integration assets, including projects, workflows, and data lineage.\nPrerequisites\nâ\nBefore you begin, verify you have:\nCompleted the\nSet up Informatica CDI\nguide\nAccess to your Informatica Cloud environment\nParameter files downloaded from your Secure Agent machines\nCreate crawler workflow\nâ\nCreate a new workflow and select Informatica CDI as your connector source.\nIn the top-right corner of any screen, select\nNew\n>\nNew Workflow\n.\nFrom the list of packages, select\nInformatica CDI Assets\n>\nSetup Workflow\n.\nConfigure authentication\nâ\nSet up secure access to your Informatica Cloud environment by providing connection credentials.\nIn the\nHost\nfield, enter your Informatica CDI domain without the protocol or sub-region.\nExample\nIf your full URL is:\nhttps://usw1.dmp-us.informaticacloud.com/\nEnter only:\ndmp-us.informaticacloud.com\nEnter the\nUsername\nand\nPassword\nfor the user you created in the\nSet up Informatica CDI\nguide.\nSelect\nTest Authentication\nto verify connectivity to Informatica CDI.\nAfter successful authentication, select\nNext\n.\nConfigure connection\nâ\nSet up connection management and define who can access and manage this connection.\nEnter a\nConnection Name\nthat represents your source environment. For example, use values like production, development, gold, or analytics.\nTo modify who can manage this connection, update the users or groups listed under\nConnection Admins\n. If you don't specify any user or group, no one can manage the connection, including admins.\nSelect\nNext\nto continue.\nConfigure crawler\nâ\nSet up what to crawl and configure advanced options for accurate lineage generation.\nConfigure metadata filters using the\nInclude Metadata\nand\nExclude Metadata\nfields. If an asset appears in both fields, the exclude metadata field takes precedence.\nInclude Metadata\n: Select the projects or folders you want to include in crawling. This defaults to all assets if none are specified.\nExclude Metadata\n: Select the projects or folders you want to exclude from crawling. This defaults to no assets if none are specified.\nConfigure advanced options for uploading parameter files:\nUpload parameter files used by the Informatica CDI projects or folders in a compressed format.\nMIME types\n: Windows ZIP or Linux Zip\nRun crawler\nâ\nExecute the crawler to discover and catalog your Informatica CDI assets.\nTo run the crawler immediately, select\nRun\n.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, select\nSchedule & Run\n.\nAfter the crawler completes, you can view the assets on Atlan's asset page.\nSee also\nâ\nWhat does Atlan crawl from Informatica CDI\n: Understand the metadata and assets discovered during crawling\nTags:\nconnectors\netl-tools\ninformatica\ncdi\ncrawl\nworkflow\nPrevious\nSet up Informatica CDI\nNext\nTransformations\nPrerequisites\nCreate crawler workflow\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/how-tos/crawl-matillion",
    "content": "Connect data\nETL Tools\nMatillion\nCrawl Matillion Assets\nCrawl Matillion\nOn this page\nCrawl Matillion\nOnce you have\nconfigured the Matillion user permissions\n, you can establish a connection between Atlan and Matillion.\nTo crawl metadata from Matillion, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Matillion as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nMatillion Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Matillion credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nHostname\n, enter the host name of your Matillion instance.\nFor\nAuthentication\n,\nBasic Authentication\nis the default method.\nFor\nUsername\n, enter the\nusername you created in Matillion\n.\nFor\nPassword\n, enter the\npassword you created for the username\n.\nFor\nSSL\n, keep\nEnabled\nto connect via a Secure Sockets Layer (SSL) channel or click\nDisabled\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Matillion.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Matillion connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\nTo change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n. If you don't specify any user or group, no one can manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Matillion crawler, you can further configure it.\nOn the\nMetadata Filters\npage, you can override the defaults for any of these options. If an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nTo select the assets you want to include in crawling, click\nInclude Projects\n. (This defaults to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Projects\n. (This defaults to no assets, if none specified.)\nFor\nEnable Lineage\n, keep the default option\nYes\nto crawl lineage or click\nNo\nto disable it. End-to-end lineage is currently not supported for Matillion version 1.68 LTS due to limitations of the Matillion APIs   -  only lineage for asset transformations is supported at present.\nRun the crawler\nâ\nTo run the Matillion crawler, after completing the previous steps:\nTo run the crawler once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you can see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Matillion\nNext\nWhat does Atlan crawl from Matillion?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMetabase\nCrawl Metabase Assets\nCrawl Metabase\nOn this page\nCrawl Metabase\nOnce you have\nconfigured the Metabase user permissions\n, you can establish a connection between Atlan and Metabase.\nTo crawl metadata from Metabase, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Metabase as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nMetabase Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Metabase credentials:\nFor\nHost Name\n, enter the full URL for your Metabase instance, including the\nhttps://\n.\nFor\nPort\n, enter the port number of your Metabase instance.\nFor\nAuthentication\n, enter the\nUsername\nand\nPassword\nyou configured.\nClick the\nTest Authentication\nbutton to confirm connectivity to Metabase using these details.\nOnce successful, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Metabase connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Metabase crawler, you can further configure it.\nYou can override the defaults for any of the remaining options:\nSelect collections you want to include in crawling in the\nInclude Collections\nfield. (This will default to all collections, if none are specified.)\nSelect collections you want to exclude from crawling in the\nExclude Collections\nfield. (This will default to no collections, if none are specified.)\nDid you know?\nIf a collection appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nYou can now run the Metabase crawler.\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Metabase\nNext\nWhat does Atlan crawl from Metabase?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMicrosoft Azure Cosmos DB\nCrawl Cosmos DB Assets\nCrawl Microsoft Azure Cosmos DB\nOn this page\nCrawl Microsoft Azure Cosmos DB\nOnce you have\nconfigured the Microsoft Azure Cosmos DB permissions\n, you can establish a connection between Atlan and Microsoft Azure Cosmos DB.\nTo crawl metadata from Microsoft Azure Cosmos DB,\nreview the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Microsoft Azure Cosmos DB as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nCosmos DB Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nChoose your deployment method:\nIn\nvCore\nÂ deployment, you will need the\nprimary connection string(s)\nof your vCore-based Microsoft Azure Cosmos DB account(s).\nIn\nRU\ndeployment, you will need the\nclient ID, client secret, and tenant ID\nof the service principal you created for your RU-based Microsoft Azure Cosmos DB account.\nIn\nvCore and RU\ndeployment,Â you will need the\nprimary connection string(s)\nof your vCore-based account(s) and\nclient ID, client secret, and tenant ID\nof the service principal you created for yourÂ RU-based account.\nvCore deployment\nâ\nTo enter your Microsoft Azure Cosmos DB credentials:\nFor\nDatabase API\n,\nMongoDB\nis the default selection.\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nSelect the deployment types to crawl\n, click\nvCore\n.\nFor\nConnection Strings\n, enter the\nprimary connection string(s) you copied\nfrom your Microsoft Azure Cosmos DB account(s).\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Cosmos DB.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nRU deployment\nâ\nTo enter your Microsoft Azure Cosmos DB credentials:\nFor\nDatabase API\n,\nMongoDB\nis the default selection.\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nSelect the deployment types to crawl\n, click\nRU\n.\nFor\nClient ID\n, enter the\napplication (client) ID you copied\nfor your service principal.\nFor\nClient Secret\n, enter the\nclient secret you copied\nfor your service principal.\nFor\nTenant ID\n, enter the\ndirectory (tenant) ID you copied\nfor your service principal.\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Cosmos DB.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nvCore and RU deployment\nâ\nTo enter your Microsoft Azure Cosmos DB credentials:\nFor\nDatabase API\n,\nMongoDB\nis the default selection.\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nSelect the deployment types to crawl\n, click\nvCore and RU\n.\nFor\nClient ID\n, enter the\napplication (client) ID you copied\nof the service principal for your RU-based account.\nFor\nClient Secret\n, enter the\nclient secret you copied\nof the service principal for your RU-based account.\nFor\nTenant ID\n, enter the\ndirectory (tenant) ID you copied\nof the service principal for your RU-based account.\nFor\nConnection Strings\n, enter the\nprimary connection string(s) you copied\nfrom your vCore-based account(s).\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Cosmos DB.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Microsoft Azure Cosmos DB connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Microsoft Azure Cosmos DB crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for the following:\nFor\nExtract Collection Schemas\n, change to\nYes\nto enable Atlan to extract collection schemas by reading a subset of the documents in the collection and\nmap them to column assets\n. For\nSchemaÂ extraction sample size\n, you can set a custom value of up to 1,000 for documents to be read for schema analysis.\nRun the crawler\nâ\nTo run the Microsoft Azure Cosmos DB crawler, after completing the steps above:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Microsoft Azure Cosmos DB\nNext\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-factory",
    "content": "Connect data\nETL Tools\nMicrosoft Azure Data Factory\nCrawl Microsoft Azure Data Factory Assets\nCrawl Microsoft Azure Data Factory\nOn this page\nCrawl Microsoft Azure Data Factory\nOnce you have\nconfigured the Microsoft Azure Data Factory permissions\n, you can establish a connection between Atlan and Microsoft Azure Data Factory.\nTo crawl metadata from Microsoft Azure Data Factor\ny, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Microsoft Azure Data Factory as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nAzure Data Factory Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Microsoft Azure Data Factory credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nAuthentication\n,\nService Principal\nis the default selection.\nFor\nClient ID\n, enter the\napplication (client) ID you copied\nfor the service principal.\nFor\nClient Secret\n, enter the\nclient secret you copied\nfor the service principal.\nFor\nTenant ID\n,Â enter the\ndirectory (tenant) ID you copied\nfor the service principal.\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Data Factory.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Microsoft Azure Data Factory connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nRun the crawler\nâ\nTo run the Microsoft Azure Data Factory crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Microsoft Azure Data Factory\nNext\nWhat does Atlan crawl from Microsoft Azure Data Factory?\nSelect the source\nProvide credentials\nConfigure the connection\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/crawl-microsoft-azure-event-hubs",
    "content": "Connect data\nEvent/Messaging\nMicrosoft Azure Event Hubs\nCrawl Microsoft Azure Event Hubs Assets\nCrawl Microsoft Azure Event Hubs\nOn this page\nCrawl Microsoft Azure Event Hubs\nOnce you have\nconfigured the Microsoft Azure Event Hubs permissions\n, you can establish a connection between Atlan and Microsoft Azure Event Hubs.\nTo crawl metadata from Microsoft Azure Event Hubs,\nreview the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Microsoft Azure Event Hubs as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nAzure Event Hubs Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your authentication method:\nIn\nSAS Key\n, you will need your\nevent hub namespace and a connection string-primary key\nfor authentication.\nIn\nService Principal\n, you will need your\nevent hub namespace\nand the following:\nIf only fetching metadata from Microsoft Azure Event Hubs, you will need a\nclient ID, client secret, and tenant ID\nfor authentication.\nIf fetching metadata from both Microsoft Azure Event Hubs and Apache Kafka, you will need a\nconnection string-primary key\nÂ and\nclient ID, client secret, and tenant ID\nfor authentication.\nSAS key\nâ\nTo enter your Microsoft Azure Event Hubs credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nSelect which metadata to fetch\n, click\nFrom Only Event hubs\nto only fetch metadata from Microsoft Azure Event Hubs or click\nFrom Both Kafka and Event hubs\nto fetch metadata from both Microsoft Azure Event Hubs and Apache Kafka.\nFor\nBootstrap servers\n, enter the\nevent hub namespace you copied\nfrom Microsoft Azure Event Hubs in the following format   -\n<your event hub namespace>.servicebus.windows.net:9093\n.\nFor\nConnection string-primary key\n, enter the\nconnection string-primary key you copied\nfrom Microsoft Azure Event Hubs.\nFor\nSecurity protocol\n,\nSSL\nis the default selection for connecting via a Secure Sockets Layer (SSL) channel.\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Event Hubs.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nService principal\nâ\nTo enter your Microsoft Azure Event Hubs credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nBootstrap servers\n, enter the\nevent hub namespace you copied\nfrom Microsoft Azure Event Hubs in the following format   -\n<your event hub namespace>.servicebus.windows.net:9093\n.\nFor\nSelect which metadata to fetch\n, you can either:\nClick\nFrom Only Event hubs\nto only fetch metadata from Microsoft Azure Event Hubs. If you choose not to fetch metadata from Apache Kafka, note that Atlan will not be able to display the message count for your event hubs and consumer groups. To enter your credentials:\nFor\nClient ID\n, enter the\napplication (client) ID you copied\nfor the service principal.\nFor\nClient Secret\n, enter the\nclient secret you copied\nfor the service principal.\nFor\nTenant ID\n,Â enter the\ndirectory (tenant) ID you copied\nfor the service principal.\nClick\nFrom Both Kafka and Event hubs\nto fetch metadata from both Microsoft Azure Event Hubs and Apache Kafka and then enter your credentials:\nFor\nConnection string-primary key\n, enter the\nconnection string-primary key you copied\nfrom Microsoft Azure Event Hubs.\nFor\nClient ID\n, enter the\napplication (client) ID you copied\nfor the service principal.\nFor\nClient Secret\n, enter the\nclient secret you copied\nfor the service principal.\nFor\nTenant ID\n,Â enter the\ndirectory (tenant) ID you copied\nfor the service principal.\nFor\nSecurity protocol\n,\nSSL\nis the default selection for connecting via a Secure Sockets Layer (SSL) channel.\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Event Hubs.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Microsoft Azure Event Hubs connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Microsoft Azure Event Hubs crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nFor\nSkip internal event hubs\n, keep the default option\nYes\nto skip internal event hubs or click\nNo\nto enable crawling them.\nTo select the assets you want to exclude from crawling, click\nExclude event hubs regex\n. (This will default to no assets, if none specified.)\nTo select the assets you want to include in crawling, click\nInclude event hubs regex\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Microsoft Azure Event Hubs crawler, after completing the steps above:\nTo run the crawler once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Microsoft Azure Event Hubs\nNext\nWhat does Atlan crawl from Microsoft Azure Event Hubs?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/crawl-microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nCrawl Synapse Assets\nCrawl Microsoft Azure Synapse Analytics\nOn this page\nCrawl Microsoft Azure Synapse Analytics\nOnce you have\nconfigured the Microsoft Azure Synapse Analytics permissions\n, you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.\nTo crawl metadata from Microsoft Azure Synapse Ana\nlytics, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Microsoft Azure Synapse Analytics as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nSynapse Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nChoose your authentication method:\nIn\nBasic\nauthentication, you will need a\nusername\n,\npassword\n, and database name.\nIn\nService principal\nauthentication, you will need a\nclient ID, client secret, tenant ID\n, and database name.\nBasic authentication\nâ\nTo enter your Microsoft Azure Synapse Analytics credentials:\nFor\nHost\n, enter the\nserver name of your SQL pool\n.\nFor\nPort\n, enter the port number where your SQL pool is available.\nFor\nUsername\n, enter the\nusername you created\nwhen setting up user permissions.\nFor\nPassword\n, enter the\npassword you created\nwhen setting up user permissions.\nFor\nDatabase\n, enter the name of the database you want to crawl.\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Synapse Analytics.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nService principal authentication\nâ\nDid you know?\nFor service principal authentication, Atlan fetches Synapse pipeline metadata from the\nAzure Synapse Analytics REST API\nto generate lineage. Refer to\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\nto learn more.\nTo enter your Microsoft Azure Synapse Analytics credentials:\nFor\nHost\n, enter the\nserver name of your SQL pool\n.\nFor\nPort\n, enter the port number where your SQL pool is available.\nFor\nClient ID\n, enter the\napplication (client) ID you copied\nfor your service principal.\nFor\nClient Secret\n, enter the\nclient secret you copied\nfor your service principal.\nFor\nTenant ID\n, enter the\ndirectory (tenant) ID you copied\nfor your service principal.\nFor\nDatabase\n, enter the name of the database you want to crawl.\nClick the\nTest Authentication\nbutton to confirm connectivity to Microsoft Azure Synapse Analytics.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Microsoft Azure Synapse Analytics connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Microsoft Azure Synapse Analytics crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in the\nExclude regex for tables & views\nfield.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Microsoft Azure Synapse Analytics crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up on-premises Microsoft Azure Synapse Analytics miner access\nNext\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nCrawl SQL Server Assets\nCrawl Microsoft SQL Server\nOn this page\nCrawl Microsoft SQL Server\nOnce you have configured the\nMicrosoft SQL Server user permissions\n, you can establish a connection between Atlan and Microsoft SQL Server. (If you are also using a private network for Microsoft SQL Server, you will need to set that up first, too, for your Microsoft SQL Server on\nAmazon RDS\nor\nAmazon EC2\ninstance.)\nTo crawl metadata from Microsoft SQL Server, revie\nw the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nSelect Microsoft SQL Server as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nSQL Server Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Microsoft SQL Server credentials:\nForÂ\nHost\n,Â enter the\navailability group listener\nname (or PrivateLink endpoint for your Microsoft SQL Server on\nAmazon RDS\nor\nAmazon EC2\ninstance).\nForÂ\nPort\n,Â enter the port on which Microsoft SQL Server is available (default is 1433).\nForÂ\nUsername\n, enter the\nusername\ncreated when setting up user permissions.\nForÂ\nPassword\n, enter the\npassword\ncreated when setting up user permissions.\nFor\nDatabase\n, enter the name of the database.\nClick\nTest Authentication\nto confirm connectivity to Microsoft SQL Server using these details.\nWhen successful, at the bottom of the screen click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Microsoft SQL Server. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabase.json\n,\ncolumns-<database>.json\n, and so on.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nOnce completed, navigate to the bottom of the screen and click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Microsoft SQL Server. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Microsoft SQL Server data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nComplete the Microsoft SQL Server connection configuration:\nProvide aÂ\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Microsoft SQL Server data, change\nAllow SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any Microsoft SQL Server data, change\nAllow Data Preview\nto\nNo\n.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Microsoft SQL Server crawler, you can further configure it.\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets, if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in theÂ\nExclude regex for tables & views\nfield. You can also specify the following system tables to exclude from crawling   - Â\nsys*\n,\nMSmerge*\n,\ndbo.sys*\n,\nMSrepl*\n,\nIH*\n,\nMSpeer*\n,\ncdc.*\n,\nMS*history\n,\nMS*agent\n,\nMSdist*\n,\nMSpub*\n,\nMSsubcri*\n,\nMSdbms*\n,\nMSdynamic*\n, and\nMSagent*\n. Note that this is not an exhaustive list, for more information refer to\nsource documentation\n.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nÂ to disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Microsoft SQL Server crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's assets page! ð\nDid you know?\nOnce you have crawled assets from Microsoft SQL Server, you can run the\nSQL Server Miner\nto\nmine query history through S3\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Microsoft SQL Server\nNext\nSet up a private network link to Microsoft SQL Server on Amazon EC2\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/how-tos/crawl-microstrategy",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicroStrategy\nCrawl MicroStrategy Assets\nCrawl MicroStrategy\nOn this page\nCrawl MicroStrategy\nOnce you have\nconfigured the MicroStrategy permissions\n, you can establish a connection between Atlan and MicroStrategy.\nTo crawl metadata from MicroStrategy, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select MicroStrategy as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nMicroStrategy Assets\n.Â\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your MicroStrategy credentials:\nFor\nHost\n, enter the hostname of your MicroStrategy instance.\nFor\nAuthentication\n,\nBasic Authentication\nis the default selection.\nFor\nUsername\n, enter the\nusername\nyou created for the instance.\nFor\nPassword\n, enter the\npassword\nfor the username.\nClick the\nTest Authentication\nbutton to confirm connectivity to MicroStrategy.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the MicroStrategy connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the MicroStrategy crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Projects\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Projects\n. (This will default to no assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the MicroStrategy crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nÂ button.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up MicroStrategy\nNext\nWhat does Atlan crawl from MicroStrategy?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/how-tos/crawl-mode",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMode\nCrawl Mode Assets\nCrawl Mode\nOn this page\nCrawl Mode\nOnce you have\nconfigured the Mode user permissions\n, you can establish a connection between Atlan and Mode.\nTo crawl metadata from Mode, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Mode as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nMode Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Mode credentials:\nFor\nAuthentication\n,\nBasic\nis the default selection.\nFor\nAPI Key ID\n, enter the\nAPI key ID you copied for your API token\n.\nFor\nAPI Secret\n, enter the\nAPI secret you copied for your API token\n.\nFor\nWorkspace\n, enter the name of your Mode workspace as retrieved from the API or workspace URL.\nFor\nExclude all personal collections\n, change to\nYes\nto exclude your\npersonal collections\nor keep the default selection\nNo\nto include them.\nClick the\nTest Authentication\nbutton to confirm connectivity to Mode using these details.\nOnce successful, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Mode connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\ndanger\nIf you're authenticating in Atlan using a\nmember API token\n, Atlan currently does not distinguish between collections you have access to and ones restricted at source. This is due to limitations with the Mode API. However, Atlan will only extract basic metadata for restricted collections and no underlying assets such as reports or queries.\nBefore running the Mode crawler, you can further configure it.\nYou can override the defaults for any of these options:\nTo select the collections you want to exclude from crawling, click\nExclude Collections\n. (This will default to no collections, if none are specified.)\nTo select the collections you want to include in crawling, click\nInclude Collections\n. (This will default to all collections, if none are specified.)\nTo disable crawling archived reports and associated queries and charts from Mode, for\nCrawl Archived Reports\n, change to\nNo\n.\nDid you know?\nIf a collection appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Mode crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Mode\nNext\nWhat does Atlan crawl from Mode?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/how-tos/crawl-mongodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMongoDB\nCrawl MongoDB Assets\nCrawl MongoDB\nOn this page\nCrawl MongoDB\nOnce you have\nconfigured the MongoDB permissions\n, you can establish a connection between Atlan and MongoDB.\nTo crawl metadata from MongoDB, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select MongoDB as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nMongoDB Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nFor\nExtraction method\n,\nDirect\nis the default selection. To enter your MongoDB credentials:\nFor\nSQL interface host name\n, enter the\nhost name of the SQL (or JDBC) endpoint you copied\nfrom your MongoDB database.\nFor\nAuthentication\n,\nBasic\nis the default method.\nFor\nUsername\n, enter the\nusername you created\nin your MongoDB database.\nFor\nPassword\n, enter the\npassword you created for the username\n.\nFor\nMongoDB native host\n, enter the\nhost name of your MongoDB database\nyou copied.\nFor\nDefault database\n, enter the\nname of the default database\nyou copied from your MongoDB database.\nFor\nAuthentication database\n, enter the\nname of the authentication database\nyou copied from your MongoDB database.\nadmin\nis the default selection   -  learn more about\nauthentication databases in MongoDB\n.\nFor\nSSL\n, keep\nYes\nto connect via a Secure Sockets Layer (SSL) channel or click\nNo\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to MongoDB.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from MongoDB. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the MongoDB data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the MongoDB connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the MongoDB crawler, you can further configure it.\nOn the\nMetadata Filters\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets, if none specified.)\nTo have the crawler ignore collections based on a naming convention, specify a regular expression in the\nExclude regex for collections\nfield.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the MongoDB crawler, after completing the steps above:\nTo run the crawler once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up MongoDB\nNext\nWhat does Atlan crawl from MongoDB?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/crawl-mysql",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nCrawl MySQL Assets\nCrawl MySQL\nOn this page\nCrawl MySQL\nOnce you have configured the\nMySQL user permissions\n, you can establish a connection between Atlan and MySQL. (If you are also using a private network for MySQL, you will need to\nset that up first\n, too.)\nTo crawl metadata from MySQL, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select MySQL as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nMySQL Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your MySQL credentials:\nFor\nHost Name\nenter the host for your MySQL instance.\nFor\nPort\nenter the port number of your MySQL instance.\nFor\nAuthentication\nchoose the method you configured when\nsetting up the MySQL user\n:\nFor\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou configured in MySQL.\nFor\nIAM User\nauthentication, enter the\nAWS Access Key\n,\nAWS Secret Key\n, and database\nUsername\nyou configured.\nFor\nIAM Role\nauthentication, enter the\nAWS Role ARN\nof the\nnew role you created\nand database\nUsername\nyou configured. (Optional) Enter the\nAWS External ID\nonly if you have not configured an\nexternal ID\nin the role definition.\nClick\nTest Authentication\nto confirm connectivity to MySQL using these details.\ninfo\nðª\nDid you know?\nIf you get an\nError: 1129: Host ... is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts'\n, ask your database admin to run the\nFLUSH HOSTS;\ncommand in the RDS instance, and then try again.\nWhen successful, at the bottom of the screen click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from MySQL. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from MySQL. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the MySQL data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the MySQL connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any MySQL data, change\nAllow SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any MySQL data, change\nAllow Data Preview\nto\nNo\n.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the MySQL crawler, you can further configure it. (Some of the options may only be available when using the\ndirect extraction method\n.)\nYou can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in the\nExclude regex for tables & views\nfield.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nto disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the MySQL crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up a private network link to MySQL\nNext\nWhat does Atlan crawl from MySQL?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-on-premises-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nOn-premises Setup\nCrawl on-premises Databricks\nOn this page\nCrawl on-premises Databricks\nOnce you have\nset up the databricks-extractor tool\n, you can extract metadata from your on-premises Databricks instances by completing the following steps.\nRun databricks-extractor\nâ\nCrawl all Databricks connections\nâ\nTo crawl all Databricks connections using the databricks-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\nCrawl a specific connection\nâ\nTo crawl a specific Databricks connection using the databricks-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <connection-name>\n(Replace\n<connection-name>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe databricks-extractor tool will generate many folders with JSON files for each\nservice\n. For example:\ncatalogs\nschemas\ntables\nYou can inspect the metadata and make sure it is acceptable for providing metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket.\nDid you know?\nWe recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the\nCreate your own S3 bucket\nsection of the dbt documentation. (The steps will be exactly the same.)\nTo upload the metadata to S3:\nEnsure that all files for a particular connection have the same prefix. For example,\noutput/databricks-example/catalogs/success/result-0.json\n,\noutput/databricks-example/schemas/{{catalog_name}}/success/result-0.json\n,\noutput/databricks-example/tables/{{catalog_name}}/success/result-0.json\n, and so on.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/databricks-example s3://my-bucket/metadata/databricks-example --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nHow to crawl Databricks\nBe sure to select\nOffline\nfor the\nExtraction method\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up on-premises Databricks access\nNext\nSet up on-premises Databricks lineage extraction\nRun databricks-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/crawl-on-premises-ibm-cognos-analytics",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nCrawl Cognos Analytics Assets\nCrawl on-premises IBM Cognos Analytics\nOn this page\nCrawl on-premises IBM Cognos Analytics\nOnce you have\nset up the cognos-extractor tool\n, you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.\nRun cognos-extractor\nâ\nCrawl all IBM Cognos Analytics connections\nâ\nTo crawl all IBM Cognos Analytics connections using the cognos-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\nCrawl a specific connection\nâ\nTo crawl a specific IBM Cognos Analytics connection using the cognos-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <connection-name>\n(Replace\n<connection-name>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe cognos-extractor tool will generate many folders with JSON files for each\nservice\n. For example:\nfolders\nexplorations\nreports\nmodules\ndata sources\nand many others\nYou can inspect the metadata and make sure it is acceptable for providing metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket.\nDid you know?\nWe recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the\nCreate your own S3 bucket\nsection of the dbt documentation. (The steps will be exactly the same.)\nTo upload the metadata to S3:\nEnsure that all files for a particular connection have the same prefix. For example,\noutput/cognos-example/contents/result-0.json\n,\noutput/cognos-example/contents-details/result-0.json\n, and so on.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/cognos-example s3://my-bucket/metadata/cognos-example --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nHow to crawl IBM Cognos Analytics\nBe sure you select\nOffline\nfor the\nExtraction method\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl IBM Cognos Analytics\nNext\nWhat does Atlan crawl from IBM Cognos Analytics?\nRun cognos-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/on-premises-event-buses/how-tos/crawl-on-premises-kafka",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nGuides\nCrawl on-premises Kafka\nOn this page\nCrawl on-premises Kafka\nOnce you have\nset up the kafka-extractor tool\n, you can extract metadata from your on-premises Kafka instances by completing the following steps.\nRun kafka-extractor\nâ\nCrawl all Kafka connections\nâ\nTo crawl all Kafka connections using the kafka-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\nCrawl a specific connection\nâ\nTo crawl a specific Kafka connection using the kafka-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <connection-name>\n(Replace\n<connection-name>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe kafka-extractor tool will generate many folders with JSON files for each\nservice\n. For example:\ntopics\ntopic-configs\nconsumer-groups\nconsumer-groups-members\nand many others\nYou can inspect the metadata and make sure it is acceptable for providing metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket.Â\nTo upload the metadata to S3:\nEnsure that all files for a particular connection have the same prefix.\nUpload the files to the S3 bucket\nusing your preferred method   -  include all the files from the output folder generated after running Docker Compose.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/kafka-example s3://my-bucket/metadata/kafka-example --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nHow to crawl Apache Kafka\nHow to crawl Confluent Kafka\nHow to crawl Aiven Kafka\nHow to crawl Redpanda Kafka\nBe sure you select\nS3\nfor the\nExtraction method\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up on-premises Kafka access\nNext\nWhat does Atlan crawl from Redpanda Kafka?\nRun kafka-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/crawl-on-premises-looker",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nCrawl Looker Assets\nCrawl on-premises Looker\nOn this page\nCrawl on-premises Looker\nOnce you have\nset up the looker-extractor tool\n, you can extract metadata from your on-premises Looker instances using the following steps.\nRun looker-extractor\nâ\nCrawl all Looker connections\nâ\nTo crawl all Looker connections using the looker-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\nCrawl a specific connection\nâ\nTo crawl a specific Looker connection using the looker-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <CONNECTION-NAME>\n(Replace\n<CONNECTION-NAME>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe looker-extractor tool will generate many JSON files for each\nservice\n. For example:\nprojects.json\ndashboards.json\ndashboard_tiles.json\nlooks.json\nand many others\nYou can inspect the metadata and make sure it is acceptable to provide the metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata you will need to upload the metadata to an S3 bucket.\nDid you know?\nWe recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the\nCreate your own S3 bucket\nsection of the dbt documentation. (The steps will be exactly the same.)\nTo upload the metadata to S3:\nEnsure all files for a particular database have the same prefix. For example,\nmetadata/looker/projects.json\n,\nmetadata/looker/dashboards.json\n, etc.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/looker-example s3://my-bucket/metadata/looker-example --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nHow to crawl Looker\nBe sure you select\nOffline\nfor the\nExtraction method\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl Looker\nNext\nWhat does Atlan crawl from Looker?\nRun looker-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-on-premises-tableau",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nCrawl Tableau Assets\nCrawl on-premises Tableau\nOn this page\nCrawl on-premises Tableau\nOnce you have\nset up the tableau-extractor tool\n, you can extract metadata from your on-premises Tableau instances by completing the following steps.\nRun tableau-extractor\nâ\nCrawl all Tableau connections\nâ\nTo crawl all Tableau connections using the tableau-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\nCrawl a specific connection\nâ\nTo crawl a specific Tableau connection using the tableau-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <connection-name>\n(Replace\n<connection-name>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe tableau-extractor tool will generate many folders with JSON files for each\nservice\n. For example:\ncalculated_fields\ndashboards\ndatasources\nworkbooks\nand many others\nYou can inspect the metadata and make sure it is acceptable for providing metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket.\nDid you know?\nWe recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the\nCreate your own S3 bucket\nsection of the dbt documentation. (The steps will be exactly the same.)\nTo upload the metadata to S3:\nEnsure that all files for a particular connection have the same prefix. For example,\noutput/tableau-example/dashboards/result-0.json\n,\noutput/tableau-example/workbooks/result-0.json\n, and so on.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/tableau-example s3://my-bucket/metadata/tableau-example --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nHow to crawl Tableau\nBe sure you select\nOffline\nfor the\nExtraction method\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl Tableau\nNext\nWhat does Atlan crawl from Tableau?\nRun tableau-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-on-premises-thoughtspot",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nCrawl ThoughtSpot Assets\nCrawl on-premises ThoughtSpot\nOn this page\nCrawl on-premises ThoughtSpot\nOnce you have\nset up the thoughtspot-extractor tool\n, you can extract metadata from your on-premises ThoughtSpot instances by completing the following steps.\nRun thoughtspot-extractor\nâ\nCrawl all ThoughtSpot connections\nâ\nTo crawl all ThoughtSpot connections using the thoughtspot-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up\nCrawl a specific connection\nâ\nTo crawl a specific ThoughtSpot connection using the thoughtspot-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <connection-name>\n(Replace\n<connection-name>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe thoughtspot-extractor tool will generate many folders with JSON files for each\nservice\n. For example:\ntags\nanswers\nliveboards\nanswer-sql-queries\nliveboard-sql-queries\nYou can inspect the metadata and make sure it is acceptable for providing metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket.\nDid you know?\nWe recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the\nCreate your own S3 bucket\nsection of the dbt documentation. (The steps will be exactly the same.)\nTo upload the metadata to S3:\nEnsure that all files for a particular connection have the same prefix. For example,\noutput/thoughtspot-example/filter/answers/result-0.json\n,\noutput/thoughtspot-example/filter/liveboards/result-0.json\n,\noutput/thoughtspot-example/filter/answer-sql-queries/result-0.json\n, and so on.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/thoughtspot-example/filter s3://my-bucket/metadata/thoughtspot-example --recursive\nCrawl metadata in Atlan\nâ\nOnce you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan:\nHow to crawl ThoughtSpot\nBe sure to select\nOffline\nÂ for the\nExtraction method\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl ThoughtSpot\nNext\nWhat does Atlan crawl from ThoughtSpot?\nRun thoughtspot-extractor\n(Optional) Review generated files\nUpload generated files to S3\nCrawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/how-tos/crawl-oracle",
    "content": "Connect data\nDatabases\nSQL Databases\nOracle\nCrawl Oracle Assets\nCrawl Oracle\nOn this page\nCrawl Oracle\nOnce you have configured the\nOracle user permissions\n, you can establish a connection between Atlan and Oracle.\nTo crawl metadata from Oracle, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Oracle as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nOracle Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Oracle credentials:\nFor\nHost Name\n, enter the host for your Oracle instance.\nFor\nPort\n, enter the port number of your Oracle instance.\nFor\nUsername\nand\nPassword\n, enter the\ncredentials you created when configuring the permissions\n.\nFor\nSID\n, enter the Oracle system identifier for your database.\nFor\nDefault Database Name,\nÂ enter the database name (usually the same as the SID).\nClick the\nTest Authentication\nbutton to confirm connectivity to Oracle using these details.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Oracle. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Oracle. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Oracle data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the Oracle connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Oracle data, change\nAllow SQL Query\nto\nNo\n. (Note: This option has no effect when using the S3 extraction method.)\n(Optional) To prevent users from previewing any Oracle data, change\nAllow Data Preview\nto\nNo\n. (Note: This option has no effect when using the S3 extraction method.)\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Oracle crawler, you can further configure it. (These options are only available when using the\ndirect extraction method\n.)\nYou can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in theÂ\nExclude regex for tables & views\nfield.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Oracle crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n. (This option is only available when using the S3 extraction method.)\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Oracle\nNext\nWhat does Atlan crawl from Oracle?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/crawl-postgresql",
    "content": "Connect data\nDatabases\nSQL Databases\nPostgreSQL\nCrawl PostgreSQL Assets\nCrawl PostgreSQL\nOn this page\nCrawl PostgreSQL\nOnce you have configured the\nPostgreSQL user permissions\n, you can establish a connection between Atlan and PostgreSQL. (If you are using a private network for PostgreSQL, you will need to\nset that up first\n, too.)\nTo crawl metadata from PostgreSQL, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select PostgreSQL as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nPostgres Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your PostgreSQL credentials:\nFor\nHost\nenter the host for your PostgreSQL instance.\nFor\nPort\nenter the port number of your PostgreSQL instance.\nFor\nAuthentication\nchoose the method you configured when\nsetting up the PostgreSQL user\n:\nFor\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou configured in PostgreSQL.\nFor\nIAM User\nauthentication, enter the\nAWS Access Key\n,\nAWS Secret Key\n, and database\nUsername\nyou configured.\nFor\nIAM Role\nauthentication, enter the\nAWS Role ARN\nof the\nnew role you created\nand database\nUsername\nyou configured. (Optional) Enter the\nAWS External ID\nonly if you have not configured an\nexternal ID\nin the role definition.\nFor\nDatabase\nenter the name of the database to crawl.\nClick\nTest Authentication\nto confirm connectivity to PostgreSQL using these details.\nWhen successful, at the bottom of the screen click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from PostgreSQL. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabase.json\n,\ncolumns-<database>.json\n, and so on.\nWhen complete, at the bottom of the screen click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from PostgreSQL. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the PostgreSQL data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the PostgreSQL connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the PostgreSQL crawler, you can further configure it. (These options are only available when using the direct extraction method.)\nYou can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in the\nExclude regex for tables & views\nfield.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nto disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the PostgreSQL crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up PostgreSQL\nNext\nWhat does Atlan crawl from PostgreSQL?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/how-tos/crawl-prestosql",
    "content": "Connect data\nDatabases\nQuery Engines\nPrestoSQL\nCrawl PrestoSQL Assets\nCrawl PrestoSQL\nOn this page\nCrawl PrestoSQL\nOnce you have configured the\nPrestoSQL user permissions\n, you can establish a connection between Atlan and PrestoSQL.\nDid you know?\nAtlan currently only supports PrestoSQL until version 349. PrestoDB is not supported at present.\nTo crawl metadata from PrestoSQL, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select PrestoSQL as your source:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the list of packages, select\nPrestoSQLÂ Assets\nand click onÂ\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your PrestoSQL credentials:\nForÂ\nHost\nenter the host for your PrestoSQL instance.\nForÂ\nPort\nenter the port of your PrestoSQL instance.\nForÂ\nUsername\nenter the\nname of the user you created\n.\nForÂ\nPassword\nenter the\npassword for the user you created\n.\nClickÂ\nTest Authentication\nto confirm connectivity to PrestoSQL using these details.\nWhen successful, at the bottom of the screen clickÂ\nNext\n.\nConfigure the connection\nâ\nTo complete the PrestoSQL connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the PrestoSQL crawler, you can further configure it.\nYou can override the defaults for any of these options:\nTo select the PrestoSQL assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo select the PrestoSQL assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo enable or disable schema-level filtering at source, click\nEnable Source Level Filtering\nand select the relevant option.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the PrestoSQL crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up PrestoSQL\nNext\nWhat does Atlan crawl from PrestoSQL?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/crawl-qlik-sense-cloud",
    "content": "Connect data\nBI Tools\nCloud-based BI\nQlik Sense Cloud\nCrawl Qlik Sense Cloud Assets\nCrawl Qlik Sense Cloud\nOn this page\nCrawl Qlik Sense Cloud\nOnce you have\nconfigured the Qlik Sense Cloud permissions\n, you can establish a connection between Atlan and Qlik Sense Cloud.\nTo crawl metadata from Qlik Sense Cloud, review th\ne\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Qlik Sense Cloud as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nQlik Sense Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Qlik Sense Cloud credentials:\nFor\nHost\n, enter the tenant URL for your Qlik Sense Cloud instance.\nFor\nPort\n, enter the port number for your Qlik Sense Cloud instance.\nFor\nAuthentication\n,\nAPI key\nis the default option.\nFor\nAPI Key\n, enter the\nAPI key you copied\nfrom your Qlik Sense Cloud instance.\nClick the\nTest Authentication\nbutton to confirm connectivity to Qlik Sense Cloud.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Qlik Sense Cloud connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Qlik Sense Cloud crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, click\nExclude Spaces\n. (This will default to no assets if none are specified.)\nTo select the assets you want to include in crawling, click\nInclude Spaces\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Qlik Sense Cloud crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Qlik Sense Cloud\nNext\nWhat does Atlan crawl from Qlik Sense Cloud?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/crawl-qlik-sense-enterprise-on-windows",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nQlik Sense Enterprise on Windows\nCrawl Qlik Sense Enterprise Assets\nCrawl Qlik Sense Enterprise on Windows\nOn this page\nCrawl Qlik Sense Enterprise on Windows\nOnce you have\nconfigured the Qlik Sense Enterprise on Windows permissions\n, you can establish a connection between Atlan and Qlik Sense Enterprise on Windows.\nTo crawl metadata from Qlik Sense Enterprise on Wi\nndows, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Qlik Sense Enterprise on Windows as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nQlik Sense Enterprise Windows Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Qlik Sense Enterprise on Windows credentials:\nJWT authentication\nâ\nFor\nHost\n, enter the hostname for your Qlik Sense Enterprise on Windows instance.\nFor\nPort\n, enter the port number for your Qlik Sense Enterprise on Windows instance.\nFor\nAuthentication\n, keep\nJWT\nas the authentication method.\nFor\nJWT token\n, enter the\nJWT you generated\n.\nFor\nVirtual proxy prefix\n, enter the prefix for your\nvirtual proxy\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Qlik Sense Enterprise on Windows.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nWindows authentication\nâ\nFor\nHost\n, enter the hostname for your Qlik Sense Enterprise on Windows instance.\nFor\nPort\n, enter the port number for your Qlik Sense Enterprise on Windows instance.\nFor\nAuthentication\n, click\nWindows Auth\n.\nFor\nUsername\n, enter the\nusername\nfor Qlik Sense Enterprise on Windows in the\ndomain\\username\nformat.\nFor\nPassword\n, enter the\npassword\nfor Qlik Sense Enterprise on Windows.\nFor\nVirtual proxy prefix\n, enter the prefix for your\nvirtual proxy\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Qlik Sense Enterprise on Windows.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Qlik Sense Enterprise on Windows connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Qlik Sense Enterprise on Windows crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, click\nExclude Streams\n. (This will default to no assets if none are specified.)\nTo select the assets you want to include in crawling, click\nInclude Streams\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Qlik Sense Enterprise on Windows crawler, after completing the steps above:\nTo run the crawler once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Qlik Sense Enterprise on Windows\nNext\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/how-tos/crawl-redash",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nRedash\nCrawl Redash Assets\nCrawl Redash\nOn this page\nCrawl Redash\nOnce you have\nconfigured the Redash permissions\n, you can establish a connection between Atlan and Redash.\nTo crawl metadata from Redash, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Redash as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nRedash Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Redash credentials:\nFor\nHost Name\n, enter the URL for your Redash instance.\nFor\nAuthentication\n,\nAPI Key\nis the default selection.\nFor\nAPI Key\n, enter the\nAPI key you copied\nfrom your Redash instance.\nClick the\nTest Authentication\nbutton to confirm connectivity to Redash.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Redash connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Redash crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the queries with tags you want to include in crawling, click\nInclude queries with tags\n. (This will default to all assets, if none are specified.)\nTo select the queries with tags you want to exclude from crawling, click\nExclude queries with tags\n. (This will default to no assets, if none are specified.)\nTo select the dashboards with tags you want to include in crawling, click\nInclude dashboards with tags\n. (This will default to all assets, if none are specified.)\nTo select the dashboards with tags you want to exclude from crawling, click\nExclude dashboards with tags\n. (This will default to no assets, if none are specified.)\nFor\nAdvanced Config\n, you can either keep\nDefault\nto allow default options for asset inclusion or click\nCustom\nto further configure the crawler:\nFor\nInclude unpublished queries\n, click\nYes\nto enable crawling unpublished queries or\nNo\nto skip crawling them.\nFor\nInclude queries without tags\n, click\nYes\nto enable crawling queries without tags or\nNo\nto skip crawling them.\nFor\nInclude dashboards without tags\n, click\nYes\nto enable crawling dashboards without tags or\nNo\nto skip crawling them.\nFor\nAlternate Host URL\n, enter the protocol and host name to be used for viewing your assets directly in Redash from Atlan. If added, the\nView in Redash\noption will redirect to the alternate host URL instead of the\nhost URL used to run the crawler\n.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Redash crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Redash\nNext\nWhat does Atlan crawl from Redash?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nCrawl Redpanda Kafka Assets\nCrawl Redpanda Kafka\nOn this page\nCrawl Redpanda Kafka\nOnce you have\nconfigured the Redpanda Kafka permissions\n, you can establish a connection between Atlan and Redpanda Kafka.\nDid you know?\nAtlan currently supports the\noffline extraction method\nfor fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nTo crawl metadata from Redpanda Kafka after\nuploading the results to S3\n, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Redpanda Kafka as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nRedpanda Kafka Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nIn offline extraction, you need to first\nextract metadata yourself and make it available in S3\n.\nTo enter your S3 details:\nFor\nExtraction method\n,\nOffline\nis the default selection.\nFor\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the\nmetadata files\nexist. These include\ntopics.json\n,\ntopic-configs.json\n, and so on.\nBased on your cloud platform, enter the following details:\nIf using AWS, for\nRole ARN\n, enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3.\nIf using Microsoft Azure, enter the name of your\nAzure Storage Account\nand the SAS token for\nBlob SAS Token\n.\nIf using Google Cloud Platform, no further configuration is required.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Redpanda Kafka connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Redpanda Kafka crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, clickÂ\nExclude topics regex\n. (This will default to no assets, if none specified.)\nTo select the assets you want to include in crawling, click\nInclude topics regex\n. (This will default to all assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Redpanda Kafka crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nÂ button.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nÂ button.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Redpanda Kafka\nNext\nSet up on-premises Kafka access\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/crawl-salesforce",
    "content": "Connect data\nCRM\nSalesforce\nCrawl Salesforce Assets\nCrawl Salesforce\nOn this page\nCrawl Salesforce\nOnce you have configured the\nSalesforce user permissions\n, you can establish a connection between Atlan and Salesforce.\nTo crawl metadata from Salesforce, review the\norder of operations\nand then complete the following steps.\nSelect source\nâ\nTo select Salesforce as your source:\nIn the top right of any screen in Atlan, navigate to\nNew\nand click\nNew Workflow\n.\nFrom the list of packages, click\nSalesforce Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nEnter credentials for your Salesforce integration in Atlan. The required fields vary depending on the authentication flow you choose.\nJWT bearer flow\nClient credentials flow\nUsername-password flow\nTo enter your Salesforce credentials:\nFor\nHost Name\n, enter the full URL for your Salesforce instance, including the\nhttps://\n. For example,\nhttps://MyDomainName.my.salesforce.com\n.\nFor\nAuthentication\n, select\nOAuth 2.0 JWT Bearer\n.\nFor\nUsername\n, enter the\nintegration user's email address\n.\nFor\nIs this a Sandbox Org account?\n, change to\nYes\nif your org is a\ncopy of your production org\n.\nFor\nConsumer Key\n, enter the\nconsumer key for the connected app\n.\nFor\nEncrypted Private Key\n, enter the\nprivate key\nfrom the\nserver.key\nfile in RSA256 format:\n-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG.......\n-----END PRIVATE KEY-----\nClick\nTest Authentication\nat the bottom of the form to confirm connectivity.\nWhen successful, click\nNext\nat the bottom of the screen.\nTo enter your Salesforce credentials:\nFor\nHost Name\n, enter the full URL for your Salesforce instance, including the\nhttps://\n. For example,\nhttps://MyDomainName.my.salesforce.com\n.\nFor\nAuthentication\n, select\nOAuth 2.0 Client Credentials\n.\nFor\nConsumer Key (Client ID)\n, enter the\nConsumer Key from the external client app\n.\nFor\nConsumer Secret (Client Secret)\n, enter the\nConsumer Secret from the external client app\n.\nClick\nTest Authentication\nat the bottom of the form to confirm connectivity.\nWhen successful, click\nNext\nat the bottom of the screen.\nTo enter your Salesforce credentials:\nFor\nHost Name\n, enter the full URL for your Salesforce instance, including the\nhttps://\n. For example,\nhttps://MyDomainName.my.salesforce.com\n.\nFor\nAuthentication\n, keep the default option\nResource Owner\n.\nFor\nUsername\n, enter the integration user's email address.\nFor\nPassword\n, enter the concatenation of the user's password and the\npersonal security token\n. Entering either password or personal security token alone is insufficient. For example, password\nxyz\n+ token\n123\nâ enter\nxyz123\n.\nFor\nIs this a Sandbox Org account?\n, change to\nYes\nif your org is a\ncopy of your production org\n.\nFor\nConsumer Key\n, enter the\nconsumer key for the connected app\n.\nFor\nConsumer Secret\n, enter the\nconsumer secret for the connected app\n.\nClick\nTest Authentication\nat the bottom of the form to confirm connectivity.\nWhen successful, click\nNext\nat the bottom of the screen.\nConfigure connection\nâ\nTo complete the Salesforce connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n. If you don't specify any user or group, nobody can manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure crawler\nâ\nBefore running the Salesforce crawler, you can further configure it.\nOn the\nMetadata\npage:\nFor\nExtract reports\n, click\nYes\nif you'd like to extract\nreport metadata\nor click\nNo\n.\nFor\nExtract dashboards\n,Â click\nYes\nif you'd like to extract\ndashboard metadata\nor click\nNo\n.\nRun crawler\nâ\nTo run the Salesforce crawler, after completing the previous steps:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you can see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsalesforce\nsetup\nPrevious\nSet up username-password flow\nNext\nWhat does Atlan crawl from Salesforce?\nSelect source\nProvide credentials\nConfigure connection\nConfigure crawler\nRun crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/how-tos/crawl-sap-hana",
    "content": "Connect data\nDatabases\nSQL Databases\nSAP HANA\nCrawl SAP HANA Assets\nCrawl SAP HANA\nOn this page\nCrawl SAP HANA\nOnce you have\nconfigured the SAP HANA permissions\n, you can establish a connection between Atlan and SAP HANA.\nTo crawl metadata from SAP HANA, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select SAP HANA as your source:\nIn the top right of any screen in Atlan, navigate to\nNew\nand then click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nSAP HANA Assets\n.Â\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you will need to first\nextract metadata yourself\nand\nmake it available in S3\n.\nDirect extraction method\nâ\nTo enter your SAP HANA credentials:\nFor\nHost\n, enter the host name for your SAP HANA instance.\nFor\nPort\n, enter the port number for your SAP HANA instance.\nFor\nUsername\n, enter the username you created for the instance.\nFor\nPassword\n, enter the password for the username.\nClick the\nTest Authentication\nbutton to confirm connectivity to SAP HANA.\nOnce authentication is successful, navigate to the bottom of the screen and then click\nNext\n.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from SAP HANA. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nForÂ\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the SAP HANA connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the SAP HANA crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the SAP HANA assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo select the SAP HANA assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the SAP HANA crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nÂ button.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nÂ button.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up SAP HANA\nNext\nWhat does Atlan crawl from SAP HANA?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/how-tos/crawl-sigma",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSigma\nCrawl Sigma Assets\nCrawl Sigma\nOn this page\nCrawl Sigma\nOnce you have\nconfigured the Sigma permissions\n, you can establish a connection between Atlan and Sigma.\nTo crawl metadata from Sigma, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Sigma as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nSigma Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Sigma credentials:\nFor\nEndpoint\n, from the dropdown, select your\norganization's cloud\n.\nFor\nAuthentication\n,\nAPI Token\nis the default method.\nFor\nClient ID\n, enter the client ID associated with your\nAPI token\n.\nFor\nAPI Token\n, enter the API token that you created.\nClick the\nTest Authentication\nbutton to confirm connectivity to Sigma.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Sigma connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Sigma crawler, you can further configure it.\nOn the\nMetadata Filters\npage, you can override the defaults for any of these options:\nTo select the Sigma workbooks you want to include in crawling, click\nInclude Workbooks\n. (This will default to all workbooks, if none are specified.)\nTo select the Sigma workbooks you want to exclude from crawling, click\nExclude Workbooks\n. (This will default to no workbooks if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Sigma crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Sigma\nNext\nWhat does Atlan crawl from Sigma?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/how-tos/crawl-sisense",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSisense\nCrawl Sisense Assets\nCrawl Sisense\nOn this page\nCrawl Sisense\nOnce you have\nconfigured the Sisense permissions\n, you can establish a connection between Atlan and Sisense.\nTo crawl metadata from Sisense, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Sisense as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nSisense Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Sisense credentials:\nFor\nHost\n, enter the hostname of your Sisense instance.\nFor\nAuthentication\n,\nAPI Token\nis the default selection.\nFor\nUsername\n, enter the\nemail address you created\nfor the new user.\nFor\nPassword\n, enter the\npassword you created\nfor the username.\nClick the\nTest Authentication\nbutton to confirm connectivity to Sisense.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Sisense connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Sisense crawler, you can further configure it.\nSisense allows you to organize your Sisense assets into folders and subfolders. Atlan currently supports filtering your\ndashboards\nand\nwidgets\nat a folder level. However, if you select a parent folder to include or exclude, all the child folders will also be included or excluded, respectively. Folder-level filtering currently does not apply to\ndata-models\nand\ndata-model-tables\n, all such assets will be crawled into Atlan.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude Folders\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude Folders\n. (This will default to no assets, if none are specified.)\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Sisense crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Sisense\nNext\nWhat does Atlan crawl from Sisense?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/how-tos/crawl-soda",
    "content": "Connect data\nData Quality & Observability\nSoda\nCrawl Soda Assets\nCrawl Soda\nOn this page\nCrawl Soda\nOnce you have\nconfigured the Soda permissions\n, you can establish a connection between Atlan and Soda.\nTo crawl metadata from Soda, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Soda as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew workflow\n.\nFrom the\nMarketplace\npage, click\nSoda\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Soda credentials:\nFor\nHost Name\n, enter the hostname or base URL of your Soda instance to connect to the Soda APIs   -  for example,\ncloud.soda.io\n,\ncloud.us.soda.io\n, or\ndemo.soda.io\n.\nFor\nAuthentication\n,\nAPI Key Authentication\nis the default selection.\nFor\nAPI Key ID\n, enter the\nAPI key ID you copied\n.\nFor\nAPI Secret\n, enter the\nAPI secret you copied\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Soda.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Soda connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Soda crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click the\nInclude filter\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click the\nExclude filter\n. (This will default to no assets, if none specified.)\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the enrichment:\nTo map Soda metadata enrichment to assets from specific connections only, for\nInclude Connections\n, specify the connections in Atlan, or leave it blank to include all connections. If you have specified any connections, Atlan will map\nSoda checks\nonly to the assets included in those connections.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Soda crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Soda\nNext\nWhat does Atlan crawl from Soda?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/crawl-teradata",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nCrawl Teradata Assets\nCrawl Teradata\nOn this page\nCrawl Teradata\nOnce you have configured the\nTeradata user permissions\n, you can establish a connection between Atlan and Teradata.\nTo crawl metadata from Teradata, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Teradata as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nTeradata Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Teradata credentials:\nForÂ\nHost\n, enter hostname of your Teradata instance.\nForÂ\nPort\n, enter the port number of your Teradata instance.\nFor\nAuthentication\n,\nBasic\nis the default method.\nForÂ\nUsername\n, enter the\nusername\ncreated when setting up user permissions.\nForÂ\nPassword\n, enter the\npassword\ncreated when setting up user permissions.\nClick the\nTest Authentication\nbutton to confirm connectivity to Teradata.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nOffline extraction method\nâ\nAtlan also supports the\noffline extraction method\nfor fetching metadata from Teradata. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket or Atlan's bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabase.json\n,\ncolumns-<database>.json\n, and so on.\n(Optional) For\nBucket region\n, enter the name of the S3 region.\nOnce completed, navigate to the bottom of the screen and click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Teradata. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Teradata data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nComplete the Teradata connection configuration:\nProvide aÂ\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Teradata crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets, if none are specified.)\nTo select the assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nTo have the crawler ignore tables and views based on a naming convention, specify a regular expression in theÂ\nExclude regex for tables & views\nfield.\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nto disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Teradata crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's assets page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up Teradata\nNext\nMine Teradata\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nCrawl ThoughtSpot Assets\nCrawl ThoughtSpot\nOn this page\nCrawl ThoughtSpot\nOnce you have\nconfigured the ThoughtSpot permissions\n, you can establish a connection between Atlan and ThoughtSpot.\nTo crawl metadata from ThoughtSpot, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select ThoughtSpot as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nThoughtSpot Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you will need to first\nextract metadata yourself and make it available in S3\n.\nIn\nAgent\nextraction, Atlanâs secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your ThoughtSpot credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nHostname\n, enter the hostname of your ThoughtSpot cloud instance in the following format   -\n<company_name>.thoughtspot.cloud\n.\nFor\nAuthentication\n, select the method you configured when\nsetting up the ThoughtSpot access permissions\n:\nFor\nBasic\nAuthentication\n, enter the\nusername and password you created\n.\nFor\nTrusted Authentication\n, enter the username of your ThoughtSpot instance and the\nsecret key you created\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to ThoughtSpot.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from ThoughtSpot. This method uses Atlan's thoughtspot-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket.\nFor\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\noutput/thoughtspot-example/filter/answers/result-0.json\n,\noutput/thoughtspot-example/filter/liveboards/result-0.json\n,\noutput/thoughtspot-example/filter/answer-sql-queries/result-0.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from ThoughtSpot. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the ThoughtSpot data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the ThoughtSpot connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the ThoughtSpot crawler, you can further configure it.\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the assets you want to include in crawling, click\nInclude tags\n. (This will default to all assets, if none are specified.)\nTo select the assets you want to exclude from crawling, click\nExclude tags\n. (This will default to no assets, if none specified.)\nFor\nAssets without tags\n, keep the default option\nYes\nto skip crawling assets without\ntags\nÂ or click\nNo\nto enable crawling them.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the ThoughtSpot crawler, after completing the steps above:\nTo check for any permissions or other configuration issues before running the crawler, click\nPreflight checks\n-  currently only supported for\noffline extraction method\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nSet up on-premises ThoughtSpot access\nNext\nCrawl on-premises ThoughtSpot\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/crawl-trino",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nCrawl Trino Assets\nCrawl Trino\nOn this page\nCrawl Trino\nOnce you have configured the\nTrino user permissions\n, you can establish a connection between Atlan and Trino. (If you are also using a private network for Trino, you will need to\nset that up first\n, too.)\nTo crawl metadata from Trino, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Trino as your source:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the list of packages, selectÂ\nTrino Assets\nand click onÂ\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your Trino credentials:\nForÂ\nHost\n, enter the hostname (or\nPrivateLink endpoint\n) for your Trino instance.\nForÂ\nPort\n, enter the port of your Trino instance.\nForÂ\nUsername\n, enter the\nusername you created\n.\nForÂ\nPassword\n, enter the\npassword for the user you created\n.\nFor\nEnable TLS/HTTPS\n, change to\nTrue\nto only allow TLS or HTTPS connections or keep the default\nFalse\n.\nFor\nDisable SSL verification\n, change to\nTrue\nto disable SSL verification for self-signed certificates or keep the default\nFalse.\nClick\nTest Authentication\nto confirm connectivity to Trino using these details.\nOnce successful, at the bottom of the screen click\nNext\n.\nConfigure the connection\nâ\nTo complete the Trino connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\nAt the bottom of the screen, click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Trino crawler, you can further configure it.\nYou can override the defaults for any of these options:\nTo select the Trino assets you want to exclude from crawling, click\nExclude Metadata\n. (This will default to no assets if none are specified.)\nTo select the Trino assets you want to include in crawling, click\nInclude Metadata\n. (This will default to all assets, if none are specified.)\nFor\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto configure the crawler:\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or click\nFalse\nÂ to disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Trino crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Trino\nNext\nSet up a private network link to Trino\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/product/administration/readme-templates/how-tos/create-readme-templates",
    "content": "Configure Atlan\nAdministration\nTemplates\nCreate README templates\nOn this page\nCreate README templates\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to create and manage README templates.\nAdmin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and\nenrich their assets with READMEs\n. They will also be able to see a rich preview of each template before adding the relevant documentation.\nCreate a README template\nâ\nTo create a README template:\nIn the left menu in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading, click\nReadme templates\n.\nOn the\nReadme templates\npage, click\nGet started\n.\nIn the\nUntitled Template\ndialog box, enter the following details:\nFor template name, enter a name for your template.\n(Optional) For\nAdd description\n, add a description for your template.\n(Optional) To add an icon, click on the\nimage\nicon.\nClick\nCreate\nto proceed.Â\nIn the text editor, add your template.\nClick\nSave\n.\n(Optional) To edit the template, in the upper right of the screen, click the\npencil\nicon.Â\nYour README template is now available in the templates manager! ð\nTags:\nintegration\nconnectors\nPrevious\nHow to view query logs\nCreate a README template"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/custom-solutions",
    "content": "Get Started\nQuick Start Guides\nDevelopers\nCustom solutions\nCustom solutions\nAtlan provides\ncustom packages\ntailored to your unique use cases. These publicly-available extensions broaden the scope of our\nout-of-the-box connectivity options\n.\nReach out to your customer success manager to install any of the selected packages in your Atlan tenant:\nAsset change notification\n-  sends email notifications when assets are created, updated, and/or deleted.\nAsset export (advanced)\n-  exports a set of assets to different locations based on the filtering criteria you have applied.\nAsset export (basic)\n-  identifies and extracts assets that have been enriched in Atlan. You can modify the resulting CSV file by updating the metadata for your assets, and then load it back into Atlan using the\nasset import\npackage.\nAsset import\n-  loads metadata from a CSV file that matches the format of one extracted using either of the asset export packages (\nbasic\nor\nadvanced\n).\nLineage builder\n-  creates lineage between any source and target asset.\nLineage generator (no transformations)\n-  automatically detects assets with the same (or similar) name between two connections and creates lineage between them.\nRelational assets builder\n-  creates (and updates) net-new relational assets: connections, databases, schemas, tables, views, materialized views, and columns.\nTags:\nintegration\nconnectors\nPrevious\nAPI authentication\nNext\nSoftware development kits (SDKs)"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/faq/faq-dagster",
    "content": "Connect data\nOrchestration & Workflow\nDagster\nFAQ\nDagster integration\nOn this page\nDagster integration\nPrivate Preview\nFrequently asked questions about Dagster integration with Atlan.\nWhy is my Dagster asset linked to multiple SQL tables across different databases/schemas?\nâ\nThis happens when the\ndagster/table_name\nmetadata field doesn't include the fully qualified table name. Atlan may link to multiple tables with the same name across different databases or schemas. Use the fully qualified table name in the\ndagster/table_name\nfield with the format\ndatabase.schema.table\n. For more details, see\nAsset metadata and tags | Dagster Docs\n.\nTags:\nconnectors\nlineage\nfaq\nPrevious\nWhat does Atlan crawl from Dagster"
  },
  {
    "url": "https://docs.atlan.com/faq/data-connections-and-integration",
    "content": "Configure Atlan\nFrequently Asked Questions\nData Connections and Integration\nOn this page\nData Connections and Integration\nComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.\nHow can I set up on-prem databases if I don't use Docker Compose?\nâ\nTo use\nAtlan's metadata-extractor tool\nto extract metadata from on-premises databases with Kubernetes deployment architecture, refer to the\nHow to connect on-premises databases to Kubernetes\ndocumentation.\nDoes Atlan integrate with MongoDB?\nâ\nAtlan currently supports native integration with\nMongoDB\nand\nMicrosoft Azure Cosmos DB\nfor MongoDB deployments.\nCan Atlan work with the IBM Informix database?\nâ\nAtlan doesn't currently offer native support for IBM Informix database. Atlan is built on an Open API architecture, so you can catalog your IBM Informix assets programmatically. Refer to the\ndeveloper documentation\nto publish database, schema, table, and column objects.\nIs Atlan compatible with data quality tools?\nâ\nYes, Atlan integrates with several data quality and observability tools:\nNative integrations with tools like Great Expectations, Monte Carlo, and other data quality platforms\nYou can view data quality metrics and alerts directly within asset profiles\nCustom integrations can be built using Atlan APIs to connect additional data quality tools\nFor the most current list of supported integrations, check the\nconnections documentation\n.\nDoes Atlan integrate with Talend or Matillion?\nâ\nAtlan currently only supports native integration with\nMatillion\n.\nIf Talend uses the ELT (Extract, Load, Transform) paradigm, this means that the SQL transformation queries are pushed down into the warehouse. In that case, Atlan automatically generates the lineage for these transformations by parsing the SQL statements extracted from the query history of the data warehouse. Lineage for the extract and load steps isn't automated for Talend but is\nsupported for Fivetran\n.\nHowever, if the transformation is taking place outside the warehouse, Atlan needs access to the transformation queries to generate the lineage. This access can either be gained via a Git repository if utilized or by sharing the queries or stored procedures in a shared location, such as an S3 bucket.\nWhy does Atlan require the site administrator explorer role in Tableau?\nâ\nTo learn why Atlan requires the\nSite Administrator Explorer\nrole in Tableau, refer to this\nguide\n.\nWhy is the view button for Tableau unavailable?\nâ\nThe view button for Tableau assets may be unavailable due to:\nInsufficient permissions to access the Tableau content\nThe Tableau server being inaccessible from your current network\nAuthentication issues between Atlan and Tableau\nThe specific asset being moved or deleted in Tableau\nCheck your Tableau permissions and network connectivity, or contact your Tableau administrator.\nWho is a source owner in Microsoft Power BI?\nâ\nMicrosoft Power BI provides metadata for who generated or configured\nreports, datasets, and dataflows\n, which is then mapped to the source owner field. This has no connection to Microsoft Power BI workspace admins. You can refer to the API response schema to learn more:\ncreatedBy\nin\nWorkspaceInfoReport\nconfiguredBy\nin\nWorkspaceInfoDataset\nconfiguredBy\nin\nWorkspaceInfoDataflow\nThe API doesn't return source owner metadata for other asset types due to limitations at source, see\nMicrosoft Power BI documentation\n.\nThe value of the source owner is a string. While for sources like Looker, Atlan displays the username based on the metadata received, Microsoft Power BI APIs only return the user's email address, which is what Atlan displays for supported assets.\nIn certain cases, assets that are connected to the same Microsoft Power BI workspace and have the same source owner may display a different source owner or none at all. This is because Atlan maps it to the metadata returned by the APIs. For example, if the response is null, Atlan won't display a source owner.\nHow does Atlan work with dbt single tenant vs multi-tenant?\nâ\nThere are multiple deployment options available for\ndbt Cloud\n(see dbt documentation\nhere\nand\nhere\n).\nWhen\nintegrating with dbt Cloud\n, Atlan uses the following APIs to fetch metadata from all deployment options:\ndbt Cloud Administrative API\n- to fetch account, project, environment, job, and run metadata.\ndbt Cloud Discovery API\n- to fetch models, sources, and tests from each dbt environment.\nAre there any dbt assets that can't be viewed in dbt?\nâ\nAtlan displays the View in dbt link for newly created or synced dbt assets only â including models, sources, and tests. Atlan doesn't display this link for assets without target URLs.\nHow can I reuse my documentation from dbt?\nâ\nAtlan's\ndbt connectivity\nprovides two ways to ingest documentation from dbt:\nAutomatically load\ndetails like descriptions\n, from your existing dbt project details.\nAny other (Atlan-specific) details\nyou want to document, through dbt's meta field.\nWhat happens to asset metadata in Atlan if I switch to a new server?\nâ\nIf the assets in your new server retain the same\nqualifiedName\n, then these are recreated in Atlan with all the metadata attached. The\nqualifiedName\ndetermines asset uniqueness in Atlan (and influences the GUID of the asset). Following this logic, any asset recreated with the same name remains the same asset in Atlan - considering that the\nqualifiedName\nhas remained the same.\nBy contrast, if an asset is recreated with a new name, it becomes a new asset in Atlan - considering that the\nqualifiedName\nhas changed. This means that the attached metadata is no longer available.\nIf an asset is removed from the data source or no longer available, it'll be\narchived\nin Atlan as part of the crawler's cleanup policy.\nWhy is the metadata getting lost when migrating from Snowflake to dbt?\nâ\nMetadata loss during Snowflake to dbt migration typically occurs because:\nThe\nqualifiedName\nchanges between the source Snowflake table and the dbt model\nAsset lineage connections aren't properly maintained during migration\nCustom metadata and tags aren't transferred in the migration process\nTo preserve metadata:\nMake sure to use consistent naming conventions between Snowflake and dbt\nUse dbt's meta fields to preserve custom metadata\nRe-run Atlan crawlers after migration to rebuild lineage connections\nWhat happens when an asset is removed from Redshift?\nâ\nIf an asset such as a table or a schema is removed from Redshift or any other source, it'll also be automatically removed from Atlan during the next workflow run. Such assets are archived in Atlan (soft-deleted), so that they don't appear in search results by default. You can only\naccess archived assets\nthrough discovery.\nWhat data is Atlan actually bringing in?\nâ\nAtlan extracts metadata only, not the actual data:\nSchema information:\nTable structures, column names, data types\nUsage statistics:\nQuery patterns, user access logs (when available)\nLineage data:\nData flow and transformation logic\nCustom metadata:\nTags, descriptions, business glossary terms\nData profiles:\nexample data previews (configurable, with privacy controls)\nAtlan never stores your actual business data - it only catalogues metadata to help you understand and govern your data landscape.\nCan offline extraction fail if there are spaces in the path?\nâ\nAtlan currently doesn't support spaces in folder names for S3. The\noffline extraction\nworkflow fails if you include any spaces in the folder name in S3. To follow documented guidelines for safe characters, refer to\nAmazon S3 documentation\n.\nWhat does API only mean?\nâ\nIn the\nWhere in Atlan\ncolumn of\nWhat does Atlan crawl from (connector name)?\ndocumentation\n,\nAPI only\nindicates source properties that have been crawled in Atlan but aren't published or discoverable on the product UI.\nCan you integrate with Jupyter notebook?\nâ\nYes, Atlan supports integration with Jupyter notebooks through:\nPython SDK for programmatic access to Atlan APIs\nJupyter extensions for data discovery and cataloguing\nAbility to document and version notebook-based analyses\nIntegration with notebook-based data science workflows\nCheck the\ndeveloper documentation\nfor specific integration examples and code samples.\nCan I integrate Atlan with any web application?\nâ\nYes, Atlan provides comprehensive APIs and SDKs that enable integration with virtually any web application:\nREST APIs for all Atlan functionality\nPython, Java, and JavaScript SDKs\nWebhook support for real-time notifications\nOAuth2 and API key authentication methods\nVisit the\ndeveloper portal\nfor integration guides, API documentation, and code examples.\nWhat does the Snowflake workflow follow when gathering information?\nâ\nThe Snowflake workflow or any other connector workflow has built-in differential crawling capabilities, which means it will crawl updates that have been made across assets in the Snowflake system and sync them to Atlan. This way, any incremental metadata changes happening in Snowflake are made available in Atlan after each workflow run.\nWhat's a Snowflake process?\nâ\nSnowflake\nprocess\n(in Atlan) = Snowflake\ntransformation\nData processing in a warehouse involves a combination of these three processes:\nExtract (from source).\nLoad (to warehouse).\nTransformation (of the data between source format and a consolidated format in the warehouse).\nWhether run through a Python script, directly from Snowflake's UI, or via third-party programs like Matillion, these transformations occur within Snowflake through SQL. Regardless of what ran this transformation SQL, a Snowflake process in Atlan captures the transformation logic for use in lineage.\nTags:\nconnectors\ndata\nintegration\nfaq-connections\nPrevious\nAI and Automation Features\nNext\nSecurity and Compliance"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/connections/how-tos/delete-a-connection",
    "content": "Configure Atlan\nIntegrations\nAutomation\nConnections\nDelete a connection\nOn this page\nDelete a connection\nTo delete a connection and its assets, complete th\ne following steps.\nSelect the utility\nâ\nTo select the connection delete utility:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the filters along the top, click\nUtility\n.\nFrom the list of packages, select\nConnection Delete\nand click on\nSetup Workflow\n.\nSelect the connection\nâ\nTo select the connection to delete:\nUnder\nConnection\nselect the existing connection you want to delete.\nFor\nDelete Strategy\nchoose how you want to delete that connection and its assets:\nTo\nsoft-delete\nthe connection and its assets, choose\nArchive\n.\nTo\nfully purge\nthe connection and its assets, choose\nDelete\n.\nRun the utility\nâ\nTo delete the connection and its assets, after completing the steps above:\nAt the bottom of the page, click\nRun\n.\nOnce the utility has completed running, you will no longer see the assets in Atlan's asset page! ð\nTags:\nintegration\nconnectors\nPrevious\nConnections Integration\nNext\nWebhooks Integration\nSelect the utility\nSelect the connection\nRun the utility"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/faq/admin-user-salesforce",
    "content": "Connect data\nCRM\nSalesforce\nFAQ\nDoes Atlan require an admin user in Salesforce?\nDoes Atlan require an admin user in Salesforce?\nNo. However, it is recommended that a Salesforce administrator establishes a\nconnection between Atlan and Salesforce\n. To learn more, see\nhere\n.\nTags:\nconnectors\nsalesforce\nPrevious\nTroubleshooting Salesforce connectivity\nNext\nWhy does the description from Salesforce not show up in Atlan?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/tableau-calculated-fields-lineage",
    "content": "Use data\nLineage\nFAQ\nDoes lineage only cover calculated fields for Tableau dashboards?\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for\nTableau dashboards\n. The lineage flow can be as follows:\nUpstream sources (for example, Snowflake tables) â Tableau data sources â workbooks â worksheets â dashboards.\nTags:\nconnectors\ndata\ncrawl\nfaq\nfaq-lineage\nPrevious\nDoes Atlan support field-level lineage for BI tools?\nNext\nHow do you enable data lineage for different data sources?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/download-impacted-assets-in-google-sheets",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nDownload impacted assets in Google Sheets\nOn this page\nDownload impacted assets in Google Sheets\nOnce you've\nconnected Atlan with Google Sheets\n, you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for\nimpact analysis\n.\ndanger\nYou need to be logged into your Atlan instance before you can download impacted assets from Atlan in Google Sheets.\nDownload impacted assets in Google Sheets\nâ\nTo import lineage in Google Sheets:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nFrom the dropdown menu, click\nAtlan\n.\nFrom the list of options in the Atlan add-on, clickÂ\nImport Lineage\nto open a list of your assets in a sidebar.\n(Optional) To filter your assets by a specific asset type, in the Atlan sidebar, select the asset type.\nIn the Atlan sidebar on your spreadsheet, select the asset(s) you want to import.\nTo select the type of impacted assets you'd like to download, in the Atlan sidebar, from the\nDirection\ndropdown:\nTo download downstream assets for\nimpact analysis\n, click\nDownstream\n.\nTo download upstream assets for\nroot cause analysis\n, click\nUpstream\n.\nTo download all impacted assets, click\nBoth\n.\nTo download the impacted assets in Google Sheets:\nClick\nImport Lineage\nto download all the impacted assets in one sheet.Â\nClick the\nvertical three dots\nand then click\nImport to new sheet\nto download the assets in separate sheet tabs.\n(Optional) Once download is successful, click the asset links in your spreadsheet to view the assets in Atlan.\nThe impacted assets are now available in Google Sheets! ð\nUpdate column metadata for impacted assets\nâ\nOnce you've imported your impacted assets from Atlan, you can edit the column metadata for impacted assets in Google Sheets. You can make changes to the column metadata once all the columns have been imported successfully.\nYou can only make changes to the metadata in the following columns:\nDescription\nOwner Users\nOwner Groups\nCertification Status\nCertification Message\nAnnouncement Type\nAnnouncement Title\nAnnouncement Message\nTags\nYou\ncannot\nmake the following changes:\nEdit headers for any of the columns\nEdit the metadata in the following columns:\nSource Asset\nSource Asset Connector\nSource Asset Type\nImpacted Asset\nImpacted Asset Connector\nImpacted Asset Type\nDirection\nTerms\nPropagated Tags\nSource URL\nQualified Name\nSource Asset GUID\nImpacted Asset GUID\nImmediate Upstream\nand\nImmediate Downstream\nDelete any columns or rows\nAny of these changes will not be pushed to Atlan and you'll receive an error message.\nDid you know?\nWhen\nadding tags\nor owners for impacted assets in Google Sheets, the tag or user name for the owner user or group must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags or owners as comma-separated values.\nPush changes to Atlan\nâ\nOnce you've made changes to the column metadata, to push your changes:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nFrom the dropdown menu, click\nAtlan\nand then click\nPush to Atlan\n.\nA dialog box will appear once the changes have synced.\ndanger\nIf you do not have the\npermission\nto update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Google Sheets. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\ndownstream-impact\ndependencies\nupstream-dependencies\ndata-sources\nPrevious\nConfigure custom domains for Microsoft Excel\nNext\nDownload impacted assets in Microsoft Excel\nDownload impacted assets in Google Sheets\nUpdate column metadata for impacted assets\nPush changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nGet Started\nHow to enable SSO for Amazon Redshift\nOn this page\nEnable  SSO for Amazon Redshift\nAtlan supports SSO authentication for\nAmazon Redshift\nconnections with Okta as the identity provider. Once you've configured SSO authentication for Amazon Redshift, your users can:\nQuery data with Okta SSO credentials\nView sample data with Okta SSO credentials\nDid you know?\nIf you have already configured Okta and AWS, skip to\nconfigure SSO authentication in Atlan\n. Otherwise, complete all the steps below.\nCreate a client application in Okta\nâ\nWho can do this?\nYou will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your AWS administrator.\nYou will need to\ncreate a client application in Okta\nto use for\nconfiguring the identity provider in AWS\n.\nTo create a client application, within Okta:\nLog in to your Okta\nAdmin Console\n.\nFrom the left menu of the\nAdmin Console\n, click\nApplications\n.\nUnder\nApplications\n, click the\nBrowse App Catalog\nbutton.\nOn the\nBrowse App Integration Catalog\npage, search for and select\nAmazon Web Services Redshift\n.\nFrom the\nAmazon Web Services Redshift\npage, click the\nAdd integration\nbutton to create an integration.\nFor\nAdd\nAmazon Web Services Redshift\n, enter the following details:\nFor\nApplication label\n, enter a meaningful name for your new app integration   -  for example,\nAtlan_SSO\n.\nClick\nDone\nto proceed.\nOn your new app page, click the\nAssignments\ntab and then click the\nAssign\nbutton:\nClick\nAssign to People\nto select individual users to assign to the application.\nClick\nAssign to Groups\nto select groups to assign to the application.\nOn your new app page, click the\nSign On\ntab and then navigate to the\nSAML Signing Certificates\nsection:\nUnder\nActions\n, click\nActions\nto expand the menu, and then from the dropdown, click\nView IdP metadata\n.\nThis will open an XML file in a new tab. Save or download this file to use for\nconfiguring the identity provider in AWS\n.\nFor\nUser Authentication\n, click the\nEdit\nbutton:\nFrom the\nAuthentication policy\ndropdown, click\nOkta Dashboard\n.\nClick\nSave\nto save your changes.\nYou will need the IdP metadata XML file to configure Okta as the identity provider in AWS.\nConfigure identity provider in AWS\nâ\nWho can do this?\nYou will need your AWS administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Okta administrator.\nYou will need to establish a trust relationship between Okta as the identity provider and AWS. You will also need to create a role that Okta can use to access Amazon Redshift and assign required permissions to that role.\nCreate an identity provider\nâ\nTo\ncreate an identity provider\n, within AWS:\nSign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console.\nFrom the left menu of your AWS Identity and Access Management (IAM) console, click\nIdentity providers\nand then click the\nAdd provider\nbutton.\nIn the\nAdd an Identity provider\ndialog, enter the following details:\nFor\nProvider type\n, select\nSAML\n.\nFor\nProvider name\n, enter a name for the identity provider   -  for example,\nOkta_AtlanSSO\n.\nUnder\nMetadata document\n, click\nChoose file\nand upload the\nIdP metadata XML file you downloaded from Okta\n.\nAt the bottom of the dialog, click\nAdd provider\nto add Okta as the identity provider in AWS.\nOnce you have configured Okta as the identity provider in AWS, you will need to create a role for Okta to access Amazon Redshift.\nCreate a role\nâ\nTo\ncreate a role\n, within AWS:\nSign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console.\nFrom the left menu of your AWS Identity and Access Management (IAM) console, click\nRoles\n, and then from the top right, click the\nCreate role\nbutton.\nOn the\nCreate role\npage, enter the following details:\nFor\nSelect trusted entity\n, under\nTrusted entity type\n, click\nSAML 2.0 federation\n. Under\nSAML 2.0 federation\n, enter the following details:Â\nFor\nSAML 2.0-based provider\n, select\nthe identity provider you created in AWS\n-  for example,\nOkta_AtlanSSO\n.\nClick\nAllow programmatic access only\n.\nFor the\nAttribute\ndropdown, select\nSAML:aud\n.\nFor\nValue\n, enter\nhttps://signin.aws.amazon.com/saml\n.\nClick\nNext\nto continue.\nFor\nAdd permissions\n, click\nNext\nto proceed to the next step.\nFor\nName, review, and create\n, under\nRole details\n, enter the following details:\nFor\nRole name\n, enter a name for the role   -  for example,\nOkta_AtlanSSO_role\n.\n(Optional) For\nDescription\n, enter a description for the new role.\nClick\nCreate role\nto finish role setup. This will create a new role for Okta to access Amazon Redshift.\nOnce you have created a role for Okta to access Amazon Redshift, you will need to assign permissions to that role.\nCreate a policy\nâ\nYou will need to create an access policy and assign the\nfollowing required permissions\nto the newly created role:\nCreateClusterUser\nJoinGroup\nGetClusterCredentials\nTo create a policy, within AWS:\nSign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console.\nFrom the left menu of your AWS Identity and Access Management (IAM) console, click\nRoles\nÂ and then search for and select the\nrole you created\nin the previous step   -  for example,\nOkta_AtlanSSO_role\n.\nOn the newly created role page, to the right of\nPermission policies\n, click\nAdd permissions\n, and then from the dropdown, click\nCreate inline policy\n.\nOn the\nCreate policy\npage, you will need to assign the following permissions for Redshift   -\nGetClusterCredentials\n,\nJoinGroup\n, and\nCreateClusterUser\n. Repeat the steps below to assign each permission:\nFor\nSpecify permissions\n, under\nSelect a service\n, search for and select\nRedshift\n. Under\nRedshift\n, enter the following details:\nFor\nAllowed actions\n, search for and select a permission   -  for example,\nGetClusterCredentials\n.\nFor\nResources\n, click\nAll\n.\nClick\nNext\nto proceed.\nFor\nReview and create\n, under\nPolicy name\n, enter a name for the newly created policy   -  for example,\nOkta_AtlanSSO_rolepolicy\n.\nRetrieve identity provider and role ARN\nâ\nOnce you have configured Okta as the identity provider and created a role in AWS, you will need the identity provider ARN and role ARN for further configuration in Okta.\nTo retrieve the identity provider and role ARN, within AWS:\nSign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console.\nFrom the left menu of your AWS Identity and Access Management (IAM) console:\nClick\nIdentity providers\nand then select the\nidentity provider you created\n:\nOn the identity provider page, under\nARN\n, click the clipboard icon to copy the identity provider ARN value and store it in a secure location.\nClick\nRoles\nand then select the\nrole you created\n:\nOn the role page, under\nARN\n, click the clipboard icon to copy the role ARN value and store it in a secure location.\nConfigure the client application in Okta\nâ\nWho can do this?\nYou will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your AWS administrator.\nYou will need the identity provider ARN and role ARN from AWS for further configuration in Okta.\nTo further configure the client application in Okta:\nLog in to your Okta\nAdmin Console\n.\nFrom the left menu of the\nAdmin Console\n, click\nApplications\n.\nUnder\nApplications\n, select the client application you created in Okta.\nOn your new app page, click the\nSign On\ntab.\nOn the\nSign On\npage, next to\nSettings\n, click\nEdit\n.\nNavigate to the\nAdvanced Sign-on Settings\nsection and enter the following details:\nFor\nIdP ARN and Role ARN\n, enter the identity provider ARN and role ARN as comma-separated values   -  for example,\narn:aws:iam::403973984390:role/oktaAtlan_SSO\n,\narn:aws:iam::403976283490:saml-provider/oktaAtlan_SSO_role\n.\nFor\nAllowed DB Groups (Redshift)\n, enter the names of the Okta groups that should be provided access to Amazon Redshift.\nClick\nSave\nto confirm.\nOn your new app page, click the\nGeneral\ntab and navigate to the\nApp Embed Link\nsection.\nUnder\nEmbed Link\n, copy the link   -  for example,\nhttps://**<example>.okta.com**/home/amazon_aws_redshift/**0oa78lx856GcTMDsa697/aln1dkqcfra0piaWa0g**\n-  and store the IdP host name and app ID in a secure location to use for\nconfiguring SSO authentication in Atlan\n. For example:\nIdP host name:\n<example>.okta.com\nApp ID:\n0oa78lx856GcTMDsa697/aln1dkqcfra0piaWa0g\nConfigure SSO authentication in Atlan\nâ\nWho can do this?\nYou will need to be a\nconnection admin\nin Atlan to complete these steps. You will also need inputs and approval from your Okta and AWS administrators.\nOnce you have configured Okta and AWS, you can enable SSO authentication for your Amazon Redshift users to\nquery data\nand\nview sample data\nÂ in Atlan.\nTo configure Okta SSO on a Amazon Redshift connection, from Atlan:\nFrom the left menu of any screen, click\nAssets\n.\nFrom the\nAssets\npage, click the\nConnector\nfilter, and from the dropdown, select\nRedshift\n.\nFrom the pills below the search bar at the top of the screen, click\nConnection\n.\nFrom the list of results, select an Amazon Redshift connection to enable SSO authentication.\nFrom the sidebar on the right, next to\nConnection settings\n, click\nEdit\n.\nIn the\nConnection settings\ndialog:\nUnder\nAllow query\n, for\nAuthentication type\n, click\nOkta authentication\nto enforce SSO credentials for\nquerying data\n:\nFor\nSSO authentication\n, enter the following details:\nFor\nIDP host\n, enter the\nIdP host name you copied from Okta\n.\nFor\nApp ID\n, enter the\napp ID you copied from Okta\n.\nFor\nAWS Role ARN\n, enter the\nrole ARN retrieved from AWS\n.\nUnderÂ\nDisplay sample data\n, for\nSource preview\n, click\nOkta authentication\nto enforce SSO credentials for\nviewing sample data\n:\nIf SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data.\nIf a different authentication method is enabled for querying data, enter the\nIdP host name and app ID you copied from Okta\nand\nrole ARN retrieved from AWS\n.\n(Optional) Toggle on\nEnable data policies created at source to apply for querying in Atlan\nto apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing\ndata policies\non the connection in Atlan will be deactivated and creation of new data policies will be disabled.\nAt the bottom right of the\nConnection settings\ndialog, click\nUpdate\n.\nYour users will now be able to\nrun queries\nand\nview sample data\nusing their Okta SSO credentials! ð\nTags:\nconnectors\ndata\nintegration\nauthentication\nPrevious\nSet up Amazon Redshift\nNext\nSet up a private network link to Amazon Redshift\nCreate a client application in Okta\nConfigure identity provider in AWS\nConfigure the client application in Okta\nConfigure SSO authentication in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/enable-sso-for-google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nGet Started\nHow to enable SSO for Google BigQuery\nOn this page\nEnable  SSO for Google BigQuery\nAtlan supports SSO authentication for\nGoogle BigQuery\nconnections. Once you've configured SSO authentication for Google BigQuery, your users can:\nQuery data with SSO credentials\nView sample data with SSO credentials\nDid you know?\nWhen using OAuth 2.0 for authorization, Google displays a consent screen to the user that includes a summary of your project, policies, and scopes. If you have not configured the consent screen, complete the steps in\nconfigure OAuth consent screen\n. Otherwise, skip to\ncreate access credentials\n.\n(Optional) Configure OAuth consent screen in Google BigQuery\nâ\nWho can do this?\nYou will need your Google BigQuery administrator to complete these steps   -  you may not have access yourself.\nTo\nconfigure the OAuth consent screen\n, from Google BigQuery:\nOpen the\nGoogle Cloud console\n.\nIn the left menu of the\nGoogle Cloud\nconsole, under\nAPIs & Services\n, click\nOAuth consent screen\n.\nOn the\nOAuth consent screen\npage, under\nUser Type\n, select a preferred user type and then click\nCreate\n.\nIn the corresponding\nEdit app registration\npage, enter the following details:\nFor\nApp name\n, enter a meaningful name   -  for example,\nAtlan_SSO\n.\nFor\nUser support email\n, enter a support email for your users to troubleshoot.\nFor\nDeveloper contact information\n, enter an email address where Google can notify you about any changes to your project.\nClick\nSave and continue\nto proceed to the next step.\nOn the\nScopes\npage, complete the following steps:\nClick\nAdd or remove scopes\nto add a new scope.\nIn the\nUpdate selected scopes\ndialog, click\nBigQuery API\nto add the\n/auth/bigquery\nscope and then click\nUpdate\n.\nClick\nSave and continue\nto finish setup.\nOnce the OAuth consent screen configuration is successful, click\nGo back to dashboard\n.\nCreate access credentials in Google BigQuery\nâ\nWho can do this?\nYou will need your Google BigQuery administrator to complete these steps   -  you may not have access yourself.\nCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.\nTo\ncreate access credentials\n, from Google BigQuery:\nOpen the\nGoogle Cloud console\n.\nIn the left menu of the\nGoogle Cloud\nconsole, under\nAPIs & Services\n, click\nCredentials\n.\nFrom the upper right of the\nCredentials\npage, click\nCreate credentials\n, and from the dropdown, click\nOAuth client ID\n.Â\nIn the\nOAuth client ID\nscreen, enter the following details:\nFor\nApplication type\n, click\nWeb application\n.\nFor\nName\n, enter a meaningful name   -  for example,\nAtlan_client\n.\nUnder\nAuthorized JavaScript origins\n, click\nAdd URI\nÂ and enter your Atlan instanceÂ   -  for example,\nhttps://<company-name>.atlan.com\n.\nUnder\nAuthorized redirect URIs\n, click\nAdd URI\nÂ and enter your Atlan endpoint URI   -  for example,\nhttps://<company-name>.atlan.com/api/service/oauth\n.\nClick\nCreate\nto finish setup.\nFrom the corresponding\nOAuth client created\ndialog, copy the\nClient ID\nand\nClient secret\nand store it in a secure location.\nConfigure SSO authentication in Atlan\nâ\nWho can do this?\nYou will need to be a\nconnection admin\nin Atlan to complete these steps. You will also need inputs and approval from your Google BigQuery administrator.\nOnce you have\nconfigured access credentials in Google BigQuery\n, you can enable SSO authentication for your users to\nquery data\nand\nview sample data\nÂ in Atlan.\nTo configure SSO on a Google BigQuery connection, from Atlan:\nFrom the left menu of any screen, click\nAssets\n.\nFrom the\nAssets\npage, click the\nConnector\nfilter, and from the dropdown, select\nBigQuery\n.\nFrom the pills below the search bar at the top of the screen, click\nConnection\n.\nFrom the list of results, select a Google BigQuery connection to enable SSO authentication.\nFrom the sidebar on the right, next to\nConnection settings\n, click\nEdit\n.\nIn the\nConnection settings\ndialog:\nUnder\nAllow query\n, for\nAuthentication type\n, clickÂ\nSSO authentication\nto enforce SSO credentials for\nquerying data\n:\nFor\nSSO authentication\n, enter the following details:\nFor\nClient ID\n, enter the\nclient ID\nyou copied from Google BigQuery.\nFor\nClient secret\n, enter the\nclient secret\nÂ you copied from Google BigQuery.\nUnderÂ\nDisplay sample data\n, for\nSource preview\n, clickÂ\nSSO authentication\nto enforce SSO credentials for\nviewing sample data\n:\nIf SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data.\nIf a different authentication method is enabled for querying data, enter the\nclient ID\nandÂ\nclient secret\nyou copied from Google BigQuery.\n(Optional) Toggle on\nEnable data policies created at source to apply for querying in Atlan\nto apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing\ndata policies\non the connection in Atlan will be deactivated and creation of new data policies will be disabled.\nAt the bottom right of the\nConnection settings\ndialog, click\nUpdate\n.\nYour users will now be able to\nrun queries\nand\nview sample data\nusing their SSO credentials! ð\nTags:\nconnectors\ndata\nintegration\nauthentication\nPrevious\nSet up Google BigQuery\nNext\nCrawl Google BigQuery\n(Optional) Configure OAuth consent screen in Google BigQuery\nCreate access credentials in Google BigQuery\nConfigure SSO authentication in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/update-atlan-through-dbt",
    "content": "Connect data\nETL Tools\ndbt\nManage dbt in Atlan\nEnrich Atlan through dbt\nEnrich Atlan through dbt\nBeyond the default mapped\ndbt Cloud\nor\ndbt Core\nproperties, you can update any of Atlan's metadata attributes (except for\nname\n,\ntenantId\n, and\nqualifiedName\n) through your dbt model's\nmeta\nproperty.\nFor example, you can set:\nAnnouncements, atlan domains, certificates, custom metadata, descriptions, owners, atlan readme, tags, and terms on dbt assets and the assets that dbt materializes whenever applicable.\nFor more details onÂ\nhow\nto do these updates, including various examples, see the\ndbt\ntabs in the\nCommon asset actions\nsnippets of our developer documentation:\nCertify assets\nManage announcements\nChange descriptions\nChange owners\nTag assets\nChange custom metadata\nLink terms to assets\nLink Atlan domains to assets\nLink Readme to assets\nTags:\nconnectors\ndata\ncrawl\nenrichment\nPrevious\nManage dbt tags\nNext\nMigrate from dbt to Atlan action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools",
    "content": "On this page\nETL tools connectors\nAtlan's ETL tools connectors enable you to integrate, catalog, and govern metadata from leading ETL and data transformation platforms. These connectors help you discover, document, and manage ETL assets for analytics, governance, and business operations.\nGet started\nâ\nTo connect your ETL tool to Atlan, follow the relevant guide below. The connectors listed are currently supported.\nFor detailed setup, crawling, and troubleshooting information, use the sidebar navigation or the guides below.\nSet up guides\nâ\nConnect to AWS Glue\nConfigure the connection between Atlan and your AWS Glue environment.\nConnect to Alteryx\nConfigure the connection between Atlan and your Alteryx environment.\nConnect to dbt Cloud\nConfigure the connection between Atlan and your dbt Cloud project.\nConnect to dbt Core\nConfigure the connection between Atlan and your dbt Core project.\nConnect to Fivetran\nConfigure the connection between Atlan and your Fivetran environment.\nConnect to Matillion\nConfigure the connection between Atlan and your Matillion environment.\nConnect to Microsoft Azure Data Factory\nConfigure the connection between Atlan and your Microsoft Azure Data Factory instance.\nTags:\nconnectors\netl\nintegration\nGet started\nSet up guides"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/extract-on-premises-databricks-lineage",
    "content": "Connect data\nData Warehouses\nDatabricks\nLineage and Usage\nHow to extract on-premises Databricks lineage\nOn this page\nextract on-premises Databricks lineage\nOnce you have\nset up the databricks-extractor tool\n, you can extract lineage from your on-premises Databricks instances by completing the following steps.\nRun databricks-extractor\nâ\nTo extract lineage for a specific Databricks connection using the databricks-extractor tool:\nLog into the server with Docker Compose installed.\nChange to the directory containing the compose file.\nRun Docker Compose:\nsudo docker-compose up <connection-name>\n(Replace\n<connection-name>\nwith the name of the connection from the\nservices\nsection of the compose file.)\n(Optional) Review generated files\nâ\nThe databricks-extractor tool will generate many folders with JSON files for each\nservice\n. For example:\nextracted-lineage\nextracted-query-history\nÂ (if\nEXTRACT_QUERY_HISTORY\nis set to true)\nYou can inspect the lineage and usage metadata and make sure it is acceptable for providing metadata to Atlan.\nUpload generated files to S3\nâ\nTo provide Atlan access to the extracted lineage and usage metadata, you will need to upload the metadata to an S3 bucket.\nDid you know?\nWe recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the\nCreate your own S3 bucket\nsection of the dbt documentation. (The steps will be exactly the same.)\nTo upload the metadata to S3:\nEnsure that all files for a particular connection have the same prefix. For example,\noutput/databricks-lineage-example/extracted-lineage/result-0.json\n,\noutput/databricks-lineage-example/extracted-query-history/result-0.json\n, and so on.\nUpload the files to the S3 bucket\nusing your preferred method.\nFor example, to upload all files using the\nAWS CLI\n:\naws s3 cp output/databricks-lineage-example s3://my-bucket/metadata/databricks-lineage-example --recursive\nExtract lineage in Atlan\nâ\nOnce you have extracted lineage on-premises and uploaded the results to S3, you can extract lineage in Atlan:\nHow to extract lineage and usage from Databricks\nBe sure to select\nOffline\nfor the\nExtraction method\n.\nTags:\nconnectors\ndata\nPrevious\nHow to extract lineage and usage from Databricks\nNext\nManage Databricks tags\nRun databricks-extractor\n(Optional) Review generated files\nUpload generated files to S3\nExtract lineage in Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/how-tos/find-assets-by-usage",
    "content": "Use data\nUsage & Popularity\nGet Started\nHow to find assets by usage\nOn this page\nFind assets by usage\nData teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance.\nWith Atlan's usage and popularity metadata, you'll be able to check off all these boxes! You can view usage metrics for your assets collected over the last 30 days.\nAtlan currently supports usage and popularity metrics for the following connectors:\nAmazon Redshift\n-  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source.\nDatabricks\n-  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the\nDatabricks APIs\n.\nGoogle BigQuery\n-  tables, views, and columns\nMicrosoft Power BI\n-  reports and dashboards\nSnowflake\n-  tables, views, and columns\nFilter assets by usage\nâ\nUse the usage filters to filter your assets by usage metadata. For instance, you'll be able to filter assets with zero queries and archive them or find costly assets to better optimize your operations.\nTo filter assets by usage metadata:\nFrom the left menu in Atlan, click\nAssets\n.\nIn the\nFilters\nmenu in\nAssets\n, click\nUsage\nto expand the list of filters.\nFrom the\nUsage\nmenu:\nFor\nSQL\nassets, use the following filters:\nClick\nNumber of queries\nto filter by the number of queries at source in the last 30 days.\nClick\nNumber of users\nto filter by the number of users who queried an asset at source in the last 30 days.\nClick\nLast queried\nto filter by the last queried timestamp at source.\nClick\nLast row updated at\nto filter by the last row updated timestamp at source.\nTo filter assets by\ncompute cost\n, click\nSnowflake credits\nfor Snowflake assets or click\nBigQuery query cost\nfor Google BigQuery assets.\nFor\nBI\nassets, use the following filters:\nClick\nViews count\nto filter by the number of views at source in the last 30 days.\nClick\nNumber of users\nto filter by the number of users who viewed an asset at source in the last 30 days.\nClick\nLast viewed\nto filter by the last viewed timestamp at source.\nYour search results will now be filtered by usage metadata! ð\nSort assets by popularity\nâ\nSort your data assets by popularity metadata to view the most or least used tables, views, or columns. For example, sorting your assets by popularity can help you deprecate unused or stale data assets, helping you reduce operational costs for your organization.\nTo sort assets by popularity:\nFrom the left menu in Atlan, click\nAssets\n.\nFor\nConnector\non the\nAssets\npage, select a supported connector   -  for this example, we'll select\nSnowflake\n.\nNext to the search bar on the\nAssets\npage, click the sort button.\nFrom the\nPopularity\nsorting menu, click\nMost popular\nto view most used assets or\nLeast popular\nto view least used assets.\nYour assets in the search results will now be sorted by popularity of usage! ð\nDid you know?\nYou can also\ndeep dive into usage metrics\nfor Snowflake, Databricks, Google BigQuery, and Microsoft Power BI in Atlan.\nTags:\nconnectors\ndata\nPrevious\nUsage and Popularity\nNext\nHow to interpret usage metrics\nFilter assets by usage\nSort assets by popularity"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/identify-insights-query-db-log",
    "content": "Use data\nInsights\nFAQ\nHow can I identify an Insights query in my database access log?\nHow can I identify an Insights query in my database access log?\nAtlan appends the product name\nAtlan\nand a unique ID at the end of each query in a comment. This can help you identify\nqueries\nfrom Insights in your database access logs.\nExample query:\nSELECT\n*\nFROM\n\"WIDE_WORLD_IMPORTERS\"\n.\n\"FCT_SALES\"\n/* atlan(e7674c86-efbe-4cec-a3dd-f9b3e3a7f929) */\nYou can also search for Atlan queries in\nSnowflake\nor\nDatabricks\nusing the\nquery text\nfilter contains\natlan\n.\nTags:\nconnectors\ndata\nfaq\nfaq-insights\nPrevious\nCan we restrict who can query our data warehouse?\nNext\nMonitor for runaway queries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/spark-lineage-handling",
    "content": "Use data\nLineage\nFAQ\nHow does Atlan handle lineage from Spark jobs?\nHow does Atlan handle lineage from Spark jobs?\nAtlan currently supports\nnative integration with Apache Spark\n. Atlan crawls and catalogs\nSpark jobs\nas native assets in Atlan and reports\nOpenLineage operational metadata\nfor Spark jobs.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\ncatalog\nmetadata\ndiscovery\nfaq-lineage\nPrevious\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nNext\nHow is the Atlan lineage graph depicted using Power BI measures?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/how-tos/implement-openlineage-in-airflow-operators",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nImplement OpenLineage\nHow to implement OpenLineage in Airflow operators\nOn this page\nImplement OpenLineage in Airflow operators\nThis document helps you learn how to implement OpenLineage support for any Airflow operator. To implement OpenLineage support, consider the following types of operators:\nSupported operators\nâ\nIf you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage.\nTo install OpenLineage, refer to the documentation for supported sources:\nApache Airflow\nAmazon MWAA\nAstronomer\nGoogle Cloud Composer\nFor Airflow operators supported for OpenLineage extraction, you can refer to\nAirflow's Supported operators documentation\n. This documentation is automatically updated when OpenLineage support is added to any operator from a provider package. You have to make sure that you're using the latest version of the provider package. For more information, see the\nrecommended provider package versions for OpenLineage\n.\nCustom and unsupported operators\nâ\nIf you're using a custom or an unsupported operator, your Airflow tasks will still emit OpenLineage events but may not include task-specific metadata such as inputs and outputs, SQL query, and more. This may limit Atlan from being able to generate data lineage.\nTo implement OpenLineage support for custom and unsupported operators, refer to\nImplementing OpenLineage in Operators documentation\n. To help you understand the process, following is an example:\nSample implementation\nâ\nThis approach is recommended when working with your own operators, where you can directly implement OpenLineage methods. You can also refer to\nOpenLineage documentation\nfor more details.\nTo implement OpenLineage support for a custom or an unsupported operator:\nOpen the Operator class definition to which you want to add OpenLineage support.\nImplement at least one of the following OpenLineage methods in the Operator class:\nget_openlineage_facets_on_start()\nget_openlineage_facets_on_complete()\nThe function should return datasets in the form of inputs and outputs with OpenLineage-compliant dataset names. This allows an OpenLineage consumer such as Atlan to properly match dataset metadata collected from different sources. To learn more about naming conventions, refer to\nOpenLineage documentation\n.\nExample\nâ\nBelow is an example of a properly implemented\nget_openlineage_facets_on_complete\nmethod for the\nGCSToGCSOperator\n.\nIn this example, since there is some level of processing included in the execute method with no relevant failure data, implementing this single method was sufficient.\ndef get_openlineage_facets_on_complete(self\n,\ntask_instance)\n:\n\"\"\n\"\nImplementing _on_complete because execute method does preprocessing on internals.\nThis means we won't have to normalize self.source_object and self.source_objects\n,\ndestination bucket and so on.\n\"\"\n\"\nfrom airflow.providers.common.compat.openlineage.facet import Dataset\nfrom airflow.providers.openlineage.extractors import OperatorLineage\nreturn OperatorLineage(\ninputs=\n[\nDataset(namespace=f\n\"gs://{self.source_bucket}\"\n,\nname=source)\nfor source in sorted(self.resolved_source_objects)\n]\n,\noutputs=\n[\nDataset(namespace=f\n\"gs://{self.destination_bucket}\"\n,\nname=target)\nfor target in sorted(self.resolved_target_objects)\n]\n,\n)\nTest implementation\nâ\nAtlan recommends that you test your changes locally by running Apache Airflow on local and setting the OpenLineage transport as the \"console\". You can use\nAstronomer\non local as it is easy and quick, but feel free to use any other method.\nTo test your implementation locally:\nInstall the\nDocker Desktop application\nin your system.\nInstall\nAstro CLI\n.\nIn your root directory, create a directory for the following Astronomer files   -\nmkdir astro-airflow\nand\ncd astro-airflow\n.\nInitialize an Astronomer project with the command\nastro init\n. This will create the required files in the directory you created above.\nOpen the\n.env\nfile, add\nAIRFLOW__OPENLINEAGE__TRANSPORT='{\"type\": \"console\"}'\nto the file, and save it.\nAdd a test DAG with tasks using your custom operator with OpenLineage support to the\nastro-airflow/dags\nfolder.\nStart Astronomer Airflow with the command\nastro dev start\n.\nOpen\nhttp://localhost:8080/home\nafter Astronomer Airflow has started.\nRun the DAG that uses your custom operator with OpenLineage support.\nOpen DAG run task logs and locate the OpenLineage events in the logs.\n(Optional) Format the JSON OpenLineage events in your IDE using this\nonline tool\n.\nEnsure that the OpenLineage events contain input and output details. For example:\n{\n\"eventTime\"\n:\n\"2024-12-27T17:55:24.407459+00:00\"\n,\n\"eventType\"\n:\n\"COMPLETE\"\n,\n\"inputs\"\n:\n[\n{\n\"facets\"\n:\n{\n}\n,\n\"name\"\n:\n\"dir1/dir2/sample.csv\"\n,\n\"namespace\"\n:\n\"s3a://atlan-test-bucket\"\n}\n]\n,\n...\n...\n\"outputs\"\n:\n[\n{\n\"facets\"\n:\n{\n}\n,\n\"name\"\n:\n\"wide_world_importers.astronomer_assets.sample\"\n,\n\"namespace\"\n:\n\"databricks://dbc-8d941db8-48cd.cloud.databricks.com\"\n}\n]\n,\n...\n...\n\"schemaURL\"\n:\n\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\"\n}\nTo view other implementation examples, refer to the following documentation:\nGCSToGCSOperator\nAzureBlobStorageToGCSOperator\n(Optional) Community contribution\nâ\nIf you add OpenLineage support to an operator from the list of commonly used provider packages, consider updating the\nApache Airflow repository\n. This allows other users to implement your code and improve it over time.\nHere is an\nexample\nof a contribution to the community from a member of the Atlan team.\nFrequently asked questions\nâ\nCan Atlan extract lineage from PythonOperator or BashOperator?\nâ\nOpenLineage supports both PythonOperator and BashOperator. However, these\ncore operators\nfunction as \"black box\" operators, capable of running any code. This in turn may limit the extent of lineage extraction. If the lineage generated is incomplete, Atlan suggests that you use\nmanually annotated lineage\n(inlets and outlets).\nCan Atlan extract lineage from KubernetesPodOperator?\nâ\nOpenLineage neither supportsÂ\nKubernetesPodOperator\nnor a managed service such as\nEksPodOperator\nor\nGKEStartPodOperator\n. This is because these operators also function as \"black box\" operators, capable of running any code. Limited execution details are exposed to the operator, thus limiting Atlan's ability to extract lineage. Atlan suggests that you use\nmanually annotated lineage\n(inlets and outlets).\nAre there other methods to implement OpenLineage support for lineage generation through events?\nâ\nYes, you can use\nmanually annotated lineage\n, which requires updating the DAG code. Keep in mind that this is a fallback measure, only recommended for very specific use cases, such as when it is impossible to extract lineage from the operator itself. Manually annotated lineage is also difficult to update and prone to manual errors.\nTags:\nconnectors\ndata\nPrevious\nHow to integrate Apache Airflow/OpenLineage\nNext\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nSupported operators\nCustom and unsupported operators\nFrequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nAmazon MWAA OpenLineage\nGet Started\nHow to integrate Amazon MWAA/OpenLineage\nOn this page\nIntegrate Amazon MWAA/OpenLineage\nTo integrate Amazon Managed Workflows for Apache Airflow (MWAA) with Atlan, complete the following steps. (Alternatively, you can use the AWS Secrets Manager to store the environment variables and fetch them using the plugin, follow the steps\nhere\nto do so.)\nTo learn more about OpenLineage, refer to\nOpenLineage configuration and facets\n.\nDid you know?\nFor Apache Airflow operators supported for OpenLineage extraction, you can refer to\nAirflow's Supported operators\ndocumentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see\nHow to implement OpenLineage in Airflow operators\n. Also, check the\nrecommended provider package versions for OpenLineage\n.\nCreate an API token in Atlan\nâ\nBefore running the workflow, you need to\ncreate an API token\nin Atlan.\nConfigure the integration in Atlan\nâ\nSelect the source\nâ\nTo select Amazon MWAA/OpenLineage as your source, from within Atlan:\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the filters along the top, click\nOrchestrator\n.\nFrom the list of packages, select\nAmazon MWAA Airflow Assets\nÂ and then click\nSetup Workflow\n.\nCreate the connection\nâ\ndanger\nA single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior.\nYou will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets.\nTo configure the Amazon MWAA/OpenLineage connection, from within Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\n(Optional) For\nHost\n, enter the URL of your Apache Airflow UI   -  do\nnot\ninclude any extra paths such as\n/home\nin the URL. This will allow Atlan to help you view your assets directly in Amazon MWAA from the asset profile.\n(Optional) For\nPort\n, enter the port number for your Apache Airflow UI.\nFor\nEnable OpenLineage Events\n, click\nYes\nto enable the processing of OpenLineage events or click\nNo\nto disable it. If disabled, new events will not be processed in Atlan.\nTo create a connection, at the bottom of the screen, click the\nCreate connection\nbutton.\nConfigure the integration in Amazon MWAA\nâ\nDid you know?\nYou will need the Atlan API token and connection name to configure the integration in Amazon MWAA. This will allow Amazon MWAA to connect with the OpenLineage API and send events to Atlan.\ndanger\nAtlan does not support integrating with Apache Airflow versions older than 2.5.0.\nTo configure Amazon MWAA to send OpenLineage events to Atlan:\nBased on your Apache Airflow version on Amazon MWAA, there may be additional prerequisites for using OpenLineage:\nFor Apache Airflow versions 2.7.0 onward, update the\nrequirements.txt\nfile of your Apache Airflow instance with:\napache-airflow-providers-openlineage\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0, update the\nrequirements.txt\nfile of your Apache Airflow instance:\nopenlineage-airflow\nTo set environment variables, you will need to deploy a custom plugin to Amazon MWAA. Create an\nenv_var_plugin.py\nfile and add the following Python code in the plugin:\nFor Apache Airflow versions 2.7.0 onward:\nfrom\nairflow\n.\nplugins_manager\nimport\nAirflowPlugin\nimport\nos\nos\n.\nenviron\n[\n\"AIRFLOW__OPENLINEAGE__NAMESPACE\"\n]\n=\n\"<connection_name>\"\nos\n.\nenviron\n[\n\"AIRFLOW__OPENLINEAGE__TRANSPORT\"\n]\n=\n'''{\n\"type\": \"http\",\n\"url\": \"https://<instance>.atlan.com/events/openlineage/airflow-mwaa/\",\n\"auth\": {\n\"type\": \"api_key\",\n\"api_key\": \"<API_token>\"\n}\n}'''\nos\n.\nenviron\n[\n\"AIRFLOW__OPENLINEAGE__CONFIG_PATH\"\n]\n=\n\"\"\nos\n.\nenviron\n[\n\"AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS\"\n]\n=\n\"\"\nclass\nEnvVarPlugin\n(\nAirflowPlugin\n)\n:\nname\n=\n\"env_var_plugin\"\nAIRFLOW__OPENLINEAGE__NAMESPACE\n: replace\n<connection_name>\nwith the connection name as exactly configured in Atlan.\nAIRFLOW__OPENLINEAGE__TRANSPORT\n: specify details of where and how to send OpenLineage events.\nReplace\n<instance>\nwith the name of your Atlan instance.\nReplace\n<API_token>\nwith the API token generated in Atlan.\nAIRFLOW__OPENLINEAGE__CONFIG_PATH\n: specifies that the\napache-airflow-providers-openlineage\npackage read the OpenLineage config from environment variables instead of a config file.\nAIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS\n: specifies that OpenLineage must send events for all operators   -  only required for the\napache-airflow-providers-openlineage\npackage.\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0:\nfrom\nairflow\n.\nplugins_manager\nimport\nAirflowPlugin\nimport\nos\nos\n.\nenviron\n[\n\"OPENLINEAGE_URL\"\n]\n=\n\"https://<instance>.atlan.com/events/openlineage/airflow-mwaa/\"\nos\n.\nenviron\n[\n\"OPENLINEAGE_NAMESPACE\"\n]\n=\n\"<connection_name>\"\nos\n.\nenviron\n[\n\"OPENLINEAGE_API_KEY\"\n]\n=\n\"<API_token>\"\nclass\nEnvVarPlugin\n(\nAirflowPlugin\n)\n:\nname\n=\n\"env_var_plugin\"\nOPENLINEAGE_URL\n: points to the service that will consume OpenLineage events   -  for example,\nhttps://<instance>.atlan.com/events/openlineage/airflow-mwaa/\n.\nOPENLINEAGE_NAMESPACE\n: set the connection name as exactly configured in Atlan.\nOPENLINEAGE_API_KEY\n: set the API token generated in Atlan.\nAmazon MWAA allows you to install a plugin through a zip archive. You can either:\nUse the following code to zip your\nenv_var_plugin.py\nfile:\nzip plugins.zip env_var_plugin.py\nIf you already have a\nplugins.zip\nfile, add the\nenv_var_plugin.py\nfile to your zip file.\nUpload the\nplugins.zip\nand\nrequirements.txt\nfiles to the S3 bucket connected to your Amazon MWAA environment. Amazon MWAA requires your DAGs, plugins, and\nrequirements.txt\nfile to be in the same S3 bucket, which serves as the source location for your environment.\nYou will need to\nspecify the path\nfor the latest versions of the\nplugins.zip\nand\nrequirements.txt\nfiles in Amazon MWAA. To specify the path:\nOpen the\nEnvironments page\non the Amazon MWAA console.\nSelect an environment and then click\nEdit\n.\nIn the\nDAG code in Amazon S3\nsection, configure the following:\nFor\nPlugins file - optional\n, select the\nplugins.zip\nfile in the S3 bucket connected to your Amazon MWAA environment or choose the latest\nplugins.zip\nversion from the dropdown list.\nFor\nRequirements file - optional\n, select the latest\nrequirements.txt\nfile version from the dropdown list.\nClick\nNext, Update environment.\nor\nNext\nto save your configurations.\nVerify the Atlan connection in Amazon MWAA\nâ\nTo verify connectivity to Amazon MWAA:\nFor\nVerify connection withÂ MWAA\n, click the clipboard icon to copy and run the preflight check DAG on your Amazon MWAA instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the\npreflight checks documentation\n.\nClick\nDone\nto complete setup.\nOnce your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð\nYou can also\nview event logs\nin Atlan to track and debug events received from OpenLineage.\nTags:\nconnectors\ndata\nconfiguration\nPrevious\nAmazon MWAA OpenLineage\nNext\nWhat does Atlan crawl from Amazon MWAA/OpenLineage?\nCreate an API token in Atlan\nConfigure the integration in Atlan\nConfigure the integration in Amazon MWAA\nVerify the Atlan connection in Amazon MWAA"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/how-tos/integrate-anomalo",
    "content": "Connect data\nData Quality & Observability\nAnomalo\nGet Started\nHow to integrate Anomalo\nOn this page\nIntegrate Anomalo\nOnce you have\nconfigured the Anomalo settings\n, you can establish a connection between Atlan and Anomalo.\nTo integrate Anomalo with Atlan, review the\norder of operations\nand then complete the following steps.\nCreate an API token in Atlan\nâ\nBefore running the workflow, you will need to\ncreate an API token\nin Atlan.\nConfigure the integration in Atlan\nâ\nSelect the source\nâ\nTo select Anomalo as your source, from within Atlan:\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the list of packages, select\nAnomaloÂ Assets\nÂ and then click\nSetup Workflow\n.\nCreate the connection\nâ\nYou will only need to create a connection once to enable Atlan to receive incoming events from Anomalo. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the Anomalo events as and when your checks run in Anomalo to catalog your check metadata.\nTo configure the Anomalo connection, from within Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nFor\nHost Name\n, enter the URL of your Anomalo instance.\nFor\nAPI Key\n, enter the\nAPI key you copied\nin Anomalo.\nClick the\nTest Authentication\nbutton to confirm connectivity to Anomalo.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nOn the\nMetadata\npage, you can override the defaults for any of these options:\nTo select the warehouses you want to include in crawling, click\nInclude warehouses\n. (This will default to all warehouses, if none are specified.)\nTo select the warehouses you want to exclude from crawling, click\nExclude warehouses\n. (This will default to no warehouses, if none are specified.)\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nNavigate to the bottom of the screen and click\nNext\n.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nConfigure the integration in Anomalo\nâ\nWho can do this?\nYou will need your Anomalo\nDeployment Admin Superuser\nto complete these steps   -  you may not have access yourself.\nAs a\nDeployment Admin Superuser\nin Anomalo, you will need to create an\nOrganization Integration\nor\nDeployment Integration\nlinking a newly created Atlan integration to your Anomalo deployment. This configuration is required only after you have completed integrating Anomalo in Atlan.\nYou will need the following for this configuration:\nURL of your Atlan instance\nAPI token generated in Atlan\nTo create an Atlan integration in Anomalo:\nLog in to your Anomalo instance as a\nDeployment Admin Superuser\n.\nCreate an Atlan integration in the admin interface. Enter your Atlan hostname and the\nAPI token you generated in Atlan\n.\nCreate an\nOrganization Integration\nor a\nDeployment Integration\nlinking the new Atlan integration to your Anomalo deployment or a specific organization.\nOnce you have integrated Anomalo and your checks have completed running in Anomalo, Atlan will start receiving and processing events from Anomalo. You will see your Anomalo checks cataloged in Atlan! ð\nNote that Atlan does not fetch any historical check metadata. As new events are received from Anomalo, Atlan will process these events and catalog your Anomalo checks.\nDid you know?\nYou can also\nview event logs\nin Atlan to track and debug events received from Anomalo.\nTags:\nconnectors\nintegration\napi\nauthentication\nPrevious\nSet up Anomalo\nNext\nWhat does Atlan crawl from Anomalo?\nCreate an API token in Atlan\nConfigure the integration in Atlan\nConfigure the integration in Anomalo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nGet Started\nHow to integrate Apache Airflow/OpenLineage\nOn this page\nIntegrate Apache Airflow/OpenLineage\nTo integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to\nOpenLineage configuration and facets\n.\nAtlan also supports other Apache Airflow distributions to enhance your data management and workflow capabilities:\nAmazon MWAA\nAstronomer\nGoogle Cloud Composer\nDid you know?\nYou will need the Atlan API token and connection name to configure the integration in Apache Airflow/OpenLineage. This will allow Apache Airflow to connect with the OpenLineage API and send events to Atlan.\nCreate an API token in Atlan\nâ\nBefore running the workflow, you will need to\ncreate an API token\nin Atlan.\nConfigure the integration in Atlan\nâ\nSelect the source\nâ\nTo select Apache Airflow/OpenLineage as your source, from within Atlan:\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the filters along the top, click\nOrchestrator\n.\nFrom the list of packages, select\nAirflow Assets\nÂ and then click\nSetup Workflow\n.\nCreate the connection\nâ\ndanger\nA single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior.\nYou will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets.\nTo configure the Apache Airflow/OpenLineage connection, from within Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\n(Optional) For\nHost\nand\nPort\n, enter the URL and port number of your Apache Airflow UI, respectively. This will allow Atlan to help you view your assets directly in Apache Airflow from the asset profile.\nFor\nEnable OpenLineage Events\n, click\nYes\nto enable the processing of OpenLineage events or click\nNo\nto disable it. If disabled, new events will not be processed in Atlan.\nTo create a connection, at the bottom of the screen, click the\nCreate connection\nbutton.\nConfigure the integration in Apache Airflow/OpenLineage\nâ\nDid you know?\nFor Apache Airflow operators supported for OpenLineage extraction, you can refer to\nAirflow's Supported operators\ndocumentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see\nHow to implement OpenLineage in Airflow operators\n. Also, check the\nrecommended provider package versions for OpenLineage\n.\ndanger\nAtlan does not support integrating with Apache Airflow versions older than 2.5.0.\nTo configure Apache Airflow to send OpenLineage events to Atlan:\nBased on your Apache Airflow version, there may be additional prerequisites for using OpenLineage:\nFor Apache Airflow versions 2.7.0 onward,\ndownload\nand install the latest\napache-airflow-providers-openlineage\npackage and update the\nrequirements.txt\nfile of your Apache Airflow instance with:\napache-airflow-providers-openlineage\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0,\ndownload\nand install the latest\nopenlineage-airflow\nlibrary and update the\nrequirements.txt\nfile of your Apache Airflow instance with:\nopenlineage-airflow\nAdd the following environment variables to your project's\n.env\nfile:\ndanger\nWhen deploying Apache Airflow on Kubernetes, set these environment variables in both the Scheduler and Triggerer pods to ensure proper integration.\nFor Apache Airflow versions 2.7.0 onward:\nAIRFLOW__OPENLINEAGE__NAMESPACE\n: set the connection name as exactly configured in Atlan.\nAIRFLOW__OPENLINEAGE__TRANSPORT\n: specify details of where and how to send OpenLineage events in the following JSON string format:\n{\n\"type\"\n:\n\"http\"\n,\n\"url\"\n:\n\"https://<instance>.atlan.com/events/openlineage/airflow/\"\n,\n\"auth\"\n:\n{\n\"type\"\n:\n\"api_key\"\n,\n\"api_key\"\n:\n\"<API_token>\"\n}\n}\nReplace\n<instance>\nwith the name of your Atlan instance.\nReplace\n<API_token>\nwith the API token generated in Atlan.\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0:Â\nOPENLINEAGE_URL\n: points to the service that will consume OpenLineage events   -  for example,\nhttps://<instance>.atlan.com/events/openlineage/airflow/\n.\nOPENLINEAGE_API_KEY\n: set the API token generated in Atlan.\nOPENLINEAGE_NAMESPACE\n: set the connection name as exactly configured in Atlan.\nVerify the Atlan connection in Apache Airflow\nâ\nTo verify connectivity to Apache Airflow:\nFor\nVerify connection with Airflow\n, click the clipboard icon to copy and run the preflight check DAG on your Apache Airflow instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the\npreflight checks documentation\n.\nClick\nDone\nto complete setup.\nOnce your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð\nYou can also\nview event logs\nin Atlan to track and debug events received from OpenLineage.\nTags:\nconnectors\ndata\nconfiguration\nPrevious\nApache Airflow OpenLineage\nNext\nHow to implement OpenLineage in Airflow operators\nCreate an API token in Atlan\nConfigure the integration in Atlan\nConfigure the integration in Apache Airflow/OpenLineage\nVerify the Atlan connection in Apache Airflow"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage/how-tos/integrate-astronomer-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nAstronomer OpenLineage\nGet Started\nHow to integrate Astronomer/OpenLineage\nOn this page\nIntegrate Astronomer/OpenLineage\nTo integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to\nOpenLineage configuration and facets\n.\nDid you know?\nFor Apache Airflow operators supported for OpenLineage extraction, you can refer to\nAirflow's Supported operators\ndocumentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see\nHow to implement OpenLineage in Airflow operators\n. Also, check the\nrecommended provider package versions for OpenLineage\n.\nCreate an API token in Atlan\nâ\nBefore running the workflow, you will need to\ncreate an API token\nin Atlan.\nConfigure the integration in Atlan\nâ\nSelect the source\nâ\nTo select Astronomer/OpenLineage as your source, from within Atlan:\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the filters along the top, click\nOrchestrator\n.\nFrom the list of packages, select\nAstronomer Airflow Assets\nÂ and then click\nSetup Workflow\n.\nCreate the connection\nâ\ndanger\nA single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior.\nYou will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets.\nTo configure the Astronomer/OpenLineage connection, from within Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\n(Optional) For\nHost\n, enter the URL of your Astronomer Airflow UI. This will allow Atlan to help you view your assets directly in Astronomer from the asset profile.\n(Optional) For\nPort\n, enter the port number for your Astronomer Airflow UI.\nFor\nEnable OpenLineage Events\n, click\nYes\nto enable the processing of OpenLineage events or click\nNo\nto disable it. If disabled, new events will not be processed in Atlan.\nTo create a connection, at the bottom of the screen, click the\nCreate connection\nbutton.\nConfigure the integration in Astronomer\nâ\nDid you know?\nYou will need the Atlan API token and connection name to configure the integration in Astronomer. This will allow Astronomer to connect with the OpenLineage API and send events to Atlan.\ndanger\nAtlan does not support integrating with Apache Airflow versions older than 2.5.0.\nAstronomer has a\nbuilt-in OpenLineage integration\n-  Atlan recommends using OpenLineage version 1.2.1 or latest. You will need to use environment variables in Astronomer to set custom values for the integration with Atlan.\nTo configure Astronomer to send OpenLineage events to Atlan:\nOpen your Astronomer console and select a workspace.\nIn the left menu under\nWorkspace\n, click\nDeployments\nand then select the required deployment.\nOn your deployment page, click the\nVariables\ntab.\nOn the\nVariables\npage, click the\nEdit variables\nbutton.\nAdd the following environment variable keys and corresponding values:\nFor Apache Airflow versions 2.7.0 onward:\nAIRFLOW__OPENLINEAGE__NAMESPACE\n: set the connection name as exactly configured in Atlan.\nOPENLINEAGE_DISABLED\nand\nAIRFLOW__OPENLINEAGE__DISABLED\n: set both to\nfalse\nto enable the OpenLineage listener in Apache Airflow, if disabled by default.\nAIRFLOW__OPENLINEAGE__TRANSPORT\n: specify details of where and how to send OpenLineage events in the following JSON string format:\n{\n\"type\"\n:\n\"http\"\n,\n\"url\"\n:\n\"https://<instance>.atlan.com/events/openlineage/airflow-astronomer/\"\n,\n\"auth\"\n:\n{\n\"type\"\n:\n\"api_key\"\n,\n\"api_key\"\n:\n\"<API_token>\"\n}\n}\nReplace\n<instance>\nwith the name of your Atlan instance.\nReplace\n<API_token>\nwith the API token generated in Atlan.\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0:\nOPENLINEAGE_URL\n: points to the service that will consume OpenLineage events   -  for example,\nhttps://<instance>.atlan.com/events/openlineage/airflow-astronomer/\n.\nOPENLINEAGE_API_KEY\n: set the API token generated in Atlan.\nOPENLINEAGE_NAMESPACE\n: set the connection name as exactly configured in Atlan.\nOPENLINEAGE_DISABLED\nand\nAIRFLOW__OPENLINEAGE__DISABLED\n: set both to\nfalse\nto enable the OpenLineage listener in Apache Airflow, if disabled by default.\nÂ Click\nUpdate Environment Variables\nto save your changes. It can take up to two minutes for new variables to be applied to your deployment.\nVerify the Atlan connection in Astronomer\nâ\nTo verify connectivity to Astronomer:\nFor\nVerify connection with Astronomer\n, click the clipboard icon to copy and run the preflight check DAG on your Astronomer instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the\npreflight checks documentation\n.\nClick\nDone\nto complete setup.\nOnce your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð\nYou can also\nview event logs\nin Atlan to track and debug events received from OpenLineage.\nTags:\nconnectors\nconfiguration\nPrevious\nAstronomer OpenLineage\nNext\nWhat does Atlan crawl from Astronomer/OpenLineage?\nCreate an API token in Atlan\nConfigure the integration in Atlan\nConfigure the integration in Astronomer\nVerify the Atlan connection in Astronomer"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nHow to integrate Atlan with Google Sheets\nOn this page\nIntegrate Atlan with Google Sheets\nThe Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.Â\nIntegrating Atlan with Google Sheets allows you to:\nImport column metadata for your data assets into Google Sheets\nUpdate column metadata for your imported assets directly in Google Sheets\nView data asset profiles on Google Sheets\nDownload downstream impacted assets in Google Sheets\nWho can do this?\nAny individual in your organization with access to Atlan can install the Atlan add-on for Google Sheets. You can even install the Atlan add-on if you have a Google account with a non-gmail address. However, the Atlan add-on can also be installed at the workspace level. To install the app for users in your organization, follow this\nguide\n.\nYou'll also need the following permissions for Google Sheets within your organization:\nDisplay and run third-party web content in prompts and sidebars within Google apps\nView and manage the spreadsheets with the Atlan add-on\nInstall Atlan in Google Sheets\nâ\nTo install the Atlan add-on from the Google Workspace Marketplace, use\nthis link\n.\nTo install the Atlan add-on directly in Google Sheets, follow these steps:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nIn the dropdown menu, click\nAdd-ons\n.\nClick\nGet add-ons\nto view available add-ons in the Google Workspace Marketplace.\nIn the search bar of your Google Workspace Marketplace, type\nAtlan\nand press enter.\nClick\nInstall\nto install the Atlan add-on.\nIf you see a dialog box pop up asking for permissions, click\nAllow\nto continue.\nConnect Atlan with Google Sheets\nâ\nTo connect Atlan with your Google spreadsheets:\nIn the menu bar of your Google spreadsheet, click\nExtensions\n.\nFrom the dropdown menu, click\nAtlan\nand then click\nSetup Atlan\n.\nIn the dialog box, enter your Atlan instance URL and click\nContinue\n.\nCongrats on connecting Atlan with Google Sheets! ð\ndanger\nFor every new spreadsheet that you create using Google Sheets, you will need to follow the steps outlined above to connect Atlan with that spreadsheet. The Atlan add-on will remain connected for all tabs within an already connected spreadsheet.\nTags:\nintegration\nconnectors\ndownstream-impact\ndependencies\nPrevious\nHow to export assets\nNext\nHow to integrate Atlan with Microsoft Excel\nInstall Atlan in Google Sheets\nConnect Atlan with Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/integrate-jira-cloud",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nGet Started\nHow to integrate Jira Cloud\nOn this page\nIntegrate Jira Cloud\nWho can do this?\nYou will need to be an admin in Atlan to configure the Jira Cloud integration. You will also need inputs and approval from an administrator of your Jira Cloud workspace.\nTo integrate Jira Cloud and Atlan, follow these st\neps.\nConnect Atlan to Jira Cloud\nâ\ndanger\nYou must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work.\nAtlan uses the following\nscopes\nfor the Jira Cloud integration:\nread:jira-user\n-  view user profiles\nread:jira-work\n-  view Jira issue data\nwrite:jira-work\n-  create and manage Jira issues\nmanage:jira-configuration\n-  manage Jira global settings\noffline_access\n-  allows the Atlan app to refresh the access token\nTo connect Atlan to Jira Cloud from within Atlan:\nFrom the left menu, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, clickÂ\nIntegrations\n.\nIn theÂ\nJira\ntile, click theÂ\nConnect\nbutton.\nA new window will open, where you'll be asked to log into your Atlassian account:\nTo login with an email address, enter your email address, click\nContinue\n, and then enter your password and click\nLog in\n.\nTo login with Google, Microsoft, or Apple, click the appropriate button and follow the instructions.\nOnce you're logged in, you will be asked to allow Atlan to access your Atlassian account. Scroll to the bottom of the window and click theÂ\nAccept\nbutton.\nIf you have access to multiple Jira Cloud sites, for the\nConnecting to Jira\ndialog, click the\nSelect environment\ndropdown to select the Jira environment you want to connect to Atlan and then click\nConnect\n.\nInstall Atlan Jira app\nâ\nTo install the Atlan Jira app:\nOpen the Atlan Jira app's page in the Atlassian marketplace, through either of these ways:\nFrom theÂ\nIntegrations\npage of Atlan, in the\nJira\ntile, click the\nAdd to Jira\nbutton.\nDirectly navigate to the URL:\nhttps://marketplace.atlassian.com/apps/1225577/atlan\nIn the upper right of the page, click theÂ\nGet it now\nbutton.\nAt the bottom of the\nAdd to Jira\ndialog, click theÂ\nGet it now\nbutton.\n(Optional) Request permission from your Jira Cloud admin\nâ\nIf you are not a workspace administrator in Jira, you will be prompted to request permission to install. To request permission to install the integration:\nUnderÂ\nWant this app? Let your admin know why\nenter an explanation for installing the app.\nAt the bottom of the form, click theÂ\nSubmit request\nbutton.\nContact your Jira workspace administrator and ask them to approve the Atlan app.\nOnce approved, you'll get an email from Jira telling you that the Atlan app is installed.\nConfigure integration from Atlan to Jira Cloud\nâ\nTo configure the Jira Cloud integration from Atlan, from the\nIntegrations\nsub-menu:\nExpand theÂ\nJira\ntile. (You may need to refresh the page before the following options appear.)\nUnder theÂ\nConfigurations\ntab, you can configure the following:\nFor\nProjects and issue type\n, select the Jira projects for which your users are allowed to create tickets in Atlan. For each selected project, next to\nIssue types\n, click\nEdit\nto select the allowed issue types. (This will default to all projects and issue types, if none are specified.)\nFor\nDefault Project\n, select the Jira project to use as your default project from the allowed list of projects.\nClick\nUpdate\nto save your configuration.\n(Optional) At any future time, you can review theÂ\nOverview\ntab to see the number of linked issues between Jira and Atlan.\nAtlan is now connected to Jira Cloud! ð\nDid you know?\nThe default project is preselected when creating or linking issues to an asset in Atlan. You can change the project while creating or linking issues as needed.\n(Optional) Create a webhook for access management workflows\nâ\nIf your Atlan admin has\nenabled the governance workflows and inbox module\nin your Atlan workspace, you can either register a webhook in the Jira administration console or install it directly in Atlan to allow your requesters to view the latest status of their data access approval or revocation requests for governed assets.\nThis is only required if you:\nEnable governance workflows\n.\nWant to use the access management workflow to grant or revoke data access in a source tool using Jira\n.\nIn addition to the scopes mentioned\nhere\n, Atlan uses the following scope to manage Jira webhooks   -\nmanage:jira-webhook\n.\nFrom Atlan\nâ\nYou can directly install the webhook in Atlan if you're an admin in both your Atlan and Jira Cloud workspaces.\nTo install a webhook:\nFrom the left menu, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, clickÂ\nIntegrations\n.\nExpand theÂ\nJira\ntile. For\nGet updates on data access requests (optional)\n, click\nInstall\n. Atlan will automatically install the webhook.\nFrom Jira\nâ\nYour Jira admin will need to\nregister the webhook\nin the Jira administration console if your Atlan admin is not a Jira admin.\nTo register a webhook:\nContact Atlan support\nto provide you with the webhook secret.\nLog in to the Jira administration console with the\nAdminister Jira global permission\n.\nFrom the top right of the Jira administration console, open the settings menu, and then under _Jira Setting_s, click\nSystem\n.\nFrom the left menu, under\nAdvanced\n, click\nWebHooks\n.\nFrom the top right of your screen, click the\nCreate a WebHook\nbutton.\nIn the webhook creation form, enter the following details:\nFor\nName\n, enter a meaningful name   -  for example,\nAtlan_webhook\n.\nFor\nStatus\n, click\nEnabled\n.\nFor\nURL\n, copy and paste\nhttps://{atlan-domain}.com/api/service/jira/events\n-  replace\natlan-domain\nwith the name of your Atlan tenant.\nFor\nSecret\n, enter the webhook secret provided by Atlan support. Note that if an invalid secret is used or this field is kept blank, the webhook configuration will be automatically removed from Atlan once an event is received in Atlan.\nUnder\nEvents\n, in the text box for\nIssue related events\n, paste\nissue.property[atlan].guid is NOT EMPTY\n.\nFor\nIssue\n, click the checkboxes for\nupdated (jira\n:issue\n_updated)\nand\ndeleted (jira\n:issue\n_deleted)\n.\nSkip the rest of the fields. Click\nCreate\nto register your webhook.\nTags:\nintegration\nconnectors\nPrevious\nJira\nNext\nHow to integrate Jira Data Center\nConnect Atlan to Jira Cloud\nConfigure integration from Atlan to Jira Cloud\n(Optional) Create a webhook for access management workflows"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/netsuite-to-snowflake-lineage",
    "content": "Use data\nLineage\nFAQ\nIs there a way to build lineage from NetSuite to Snowflake?\nIs there a way to build lineage from NetSuite to Snowflake?\nWhile Atlan currently does not support native integration with NetSuite, you can create and catalog NetSuite assets using\nAtlan APIs\n. This will allow you to create NetSuite assets in Atlan and also generate lineage between these assets and any other sources (including Snowflake) using Atlan\nlineage APIs\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\napi\nrest-api\ngraphql\ncatalog\nmetadata\ndiscovery\nfaq-lineage\nPrevious\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nNext\nWhat are Power BI processes on the lineage graph?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/link-your-jira-account",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nGet Started\nLink your Jira account\nOn this page\nLink your Jira account\nTo create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that\nset up the Jira integration\n, but not for other users.\nAn Atlan admin must set up the tenant-level Jira integration in Atlan before any other user can perform a user-level integration. This is because the Atlan app installation requires inputs and approval from an administrator of your Jira workspace. Once the Jira integration has been completed, you can link your Jira account to Atlan without requiring any additional permissions from your Atlan or Jira admin.\nLink your Jira account\nâ\nTo link your Jira account:\nFrom any screen, in the upper right navigate to your name, then clickÂ\nProfile\n.\nClick the icon at the bottom of the resulting dialog to get to integrations.\nUnderÂ\nJira\nclick theÂ\nConnect\nlink.\nIn the resulting popup, log in if necessary, then scroll to the bottom and click\nAllow\n.\nUnlink your Jira account\nâ\nTo unlink your Jira account:\nFrom any screen, in the upper right navigate to your name, then clickÂ\nProfile\n.\nClick the icon at the bottom of the resulting dialog to get to integrations.\nUnderÂ\nJira\nclick theÂ\nDisconnect\nlink.\nIn the confirmation dialog, clickÂ\nConfirm\n.\nTags:\nintegration\nconnectors\nPrevious\nHow to integrate Jira Data Center\nNext\nTroubleshooting Jira\nLink your Jira account\nUnlink your Jira account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/how-tos/link-your-microsoft-teams-account",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nMicrosoft Teams\nHow-tos\nLink your Microsoft Teams account\nOn this page\nLink your Microsoft Teams account\nTo get alerts for\nstarred assets\ndirectly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that\nset up the Microsoft Teams integration\n, but not for other users.\nLink your Microsoft Teams account\nâ\nTo link your Microsoft Teams account:\nFrom any screen, in the upper right, navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnder\nTeams\n,Â click theÂ\nConnect\nlink.\nIn the resulting popup, scroll to the bottom and clickÂ\nAllow\n.\nUnlink your Microsoft Teams account\nâ\nTo unlink your Microsoft Teams account:\nFrom any screen, in the upper right, navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnder\nTeams\n, click theÂ\nDisconnect\nlink.\nIn the confirmation dialog, clickÂ\nConfirm\n.\nTags:\nintegration\nconnectors\nalerts\nmonitoring\nnotifications\nPrevious\nHow to integrate Microsoft Teams\nNext\nTroubleshooting Microsoft Teams\nLink your Microsoft Teams account\nUnlink your Microsoft Teams account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/how-tos/link-your-slack-account",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nHow-tos\nLink your Slack account\nOn this page\nLink your Slack account\nTo see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that\nset up the Slack integration\n, but not for other users.\nLink your Slack account\nâ\nTo link your Slack account:\nFrom any screen, in the upper right navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnderÂ\nSlack\n,Â click theÂ\nConnect\nlink.\nIn the resulting popup, scroll to the bottom and clickÂ\nAllow\n.\nUnlink your Slack account\nâ\nTo unlink your Slack account:\nFrom any screen, in the upper right navigate to your name, then clickÂ\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnderÂ\nSlack\n, click theÂ\nDisconnect\nlink.\nIn the confirmation dialog, clickÂ\nConfirm\n.\nTags:\nintegration\nconnectors\nPrevious\nHow to integrate Slack\nNext\nTroubleshooting Slack\nLink your Slack account\nUnlink your Slack account"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/how-tos/manage-requests",
    "content": "Use data\nRequests\nGet Started\nManage requests\nOn this page\nManage requests\nDid you know?\nAtlan supports\ngovernance workflows\n! Once you have\nenabled governance workflows and inbox\n, Atlan will channel requests and approvals for your governed assets through governance workflows and land them in your\ninbox\n.\nRequest changes to an asset\nâ\nWho can do this?\nAny\nuser\nwithout\nedit access to an asset's metadata\ncan request changes to an asset.\nTo request changes to an asset:\nNavigate to the asset you want to change. For example, use search or discovery to get there.\nClick on the part of the asset you want to change, as if you were making the change directly.\nYou'll see that you do not have permission and are instead suggesting a change.\nInstead of saving your change, Atlan will prompt you to submit it as a request.\nDid you know?\nThe lock icon and slightly transparent text show that you do not have access to an asset.\nTrack your request(s)\nâ\nFor a specific asset\nâ\nTo track your requests for a specific asset:\nNavigate to the asset you want to track. For example, use search or discovery to get there.\nFrom the right navbar of the asset, click on the\nRequest\ntab.\n(Optional) To the right of the\nRequests\nheading, change the drop down to\nAll\nto see all requests you've made on the asset.\nPending requests are those still awaiting approval (or rejection).\nApproved requests are those that someone has already accepted (and applied).\nRejected requests are those that someone has already declined (and will not be applied).\nAll your requests\nâ\nTo track all the requests you have raised, across all assets:\nClick the icon in the upper left of any page to navigate to the Atlan home screen.\nScroll to the bottom of the screen to the\nMy Requests\ncard.\n(Optional) In the upper right of the card, change the drop down to narrow requests by status.\nPending requests are those still awaiting approval (or rejection).\nApproved requests are those that someone has already accepted (and applied).\nRejected requests are those that someone has already declined (and will not be applied).\n(Optional) Hover over any pending request to see who can approve or reject it.\nGet notified on Slack\nâ\nIf your organization's\nSlack account is integrated with Atlan\n, you will receive Slack notifications when your requests are approved or rejected.\nTo receive Slack notifications on your requests:\nThe email address used for Slack and Atlan should be the same, even if you haven't personally integrated the accounts.\nThe Slack app should have been installed before August 12, 2022. If installed later, you'll need to update Slack.\nIf different email addresses were used for Slack and Atlan, you'll first need to\nlink your Slack account with Atlan\n.\nApprove or reject request(s)\nâ\nWho can do this?\nAny\nnon-guest user\nwith\nedit access to an asset's metadata\ncan approve the request. This only includes admin and member users.\nFor a specific asset\nâ\nTo approve or reject a request for a specific asset:\nNavigate to the asset you want to manage. For example, use search or discovery to get there.\nFrom the right sidebar of the asset, click on the\nRequest\ntab.\n(Optional) To the right of the\nRequests\nheading, change the drop-down to\nPending\nto see open requests on the asset.\nHover over any pending request to either approve or reject it:\nClick the arrow next to\nApprove\nto approve the change with a comment, or click\nApprove\nto approve it without a comment.\nClick the arrow next to\nReject\nto reject the change with a comment, or click\nReject\nto reject it without a comment.\nAll requests\nâ\nWho can do this?\nCurrently only admin users have access to see all requests.\nTo approve or reject requests against any asset:\nFrom the left menu of any screen, click\nGovernance\n.\nUnder the\nGovernance\nheading, click\nRequests\n.\n(Optional) In the\nRequests\ntable, click the search bar to search for requests to take action on or click the funnel icon to filter your requests:\nSelect\nConnection\nto filter by a specific\nconnector\n, or drill down further by connection, database, or schema.\nSelect\nStatus\nto filter by request status   -  pending, approved, or rejected.\nSelect\nRequestor\nto filter by specific users.\nSelect\nTags\nto filter by\ntag\nupdate requests.\nSelect\nRequest type\nto filter by type of metadata update requested   -\ndescription\n,\ntag\n,\ncertificate\n,\nterm\n, or\nowner\n.\nSelect\nAsset type\nto filter by specific asset types   -  tables, columns, queries, and more.\nSelect\nRaised in\nto filter by a predefined or custom data range.\nHover over any pending request to either approve or reject it:\nClick the arrow next to\nApprove\nto approve the change with a comment, or click\nApprove\nto approve it without a comment.\nClick the arrow next to\nReject\nto reject the change with a comment, or click\nReject\nto reject it without a comment.\nDid you know?\nYou can also\nconfigure the Slack integration\nto receive notifications for metadata update\nrequests\nraised in Atlan and\ntake action directly from Slack\n.\nUse the requests widget\nâ\nThe requests widget brings together all your requests in one location to help you track, manage, and prioritize them more effectively. You can open the requests widget from anywhere in Atlan, find all the requests that need your attention and take action immediately, and view a summary of your requests and track their statuses.\nTake action on requests\nâ\nTo take action on requests, from the requests widget:\nFrom the top right of any screen in Atlan, click the\nRequests\nicon.\nIn the\nRequests\ndialog, under\nNeeds attention\n, hover over or click any request to view actions.\nFrom the request card, you can either:\nTo reject the update request, click\nReject\nÂ or click the downward arrow to the right and then click\nReject with comment\nto add a comment as well.\nTo approve the update request, click\nApprove\nor click the downward arrow to the right and then click\nApproveÂ with comment\nto add a comment as well.\nTrack your requests\nâ\nTo track your requests, from the requests widget:\nFrom the top right of any screen in Atlan, click the\nRequests\nicon.\nIn the\nRequests\ndialog, click\nMy requests\nto track all your requests.\nTo view your requests sorted by status, click\nPending\nto view pending requests,\nApproved\nfor approved requests, or\nRejected\nfor requests that have been rejected.\nTags:\nintegration\nconnectors\nworkflow\nautomation\norchestration\nPrevious\nRequests\nNext\nWhat are requests?\nRequest changes to an asset\nTrack your request(s)\nApprove or reject request(s)\nUse the requests widget"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/migrate-from-dbt-to-atlan-action",
    "content": "Connect data\nETL Tools\ndbt\nManage dbt in Atlan\nMigrate from dbt to Atlan action\nOn this page\nMigrate from dbt to Atlan action\nThe\ndbt-action\nis a custom action designed to perform impact analysis on changes to your dbt models in a\nGitHub\nor\nGitLab\nrepository. Atlan plans to enhance this customÂ action to provide additional capabilities, such as\nimpact analysis for data contracts\n) and more.\nInstead of creating separate custom actions for each new capability, Atlan has renamed the\ndbt-action\nto\natlan-action\nto better reflect the multiple capabilities on offer and will eventually deprecate the\ndbt-action\n.\nIf you're currently using the\ndbt-action\n, Atlan strongly recommends migrating to the\natlan-action\n.\nMigration notice and timeline\nâ\nAtlan is providing you with a window of over six months to complete the migration, with a deadline set for June 2025. However, rest assured that Atlan will not archive the\ndbt-action\nuntil every organization has successfully transitioned to the\natlan-action\n.\nIf you choose not to migrate, please be aware that the\ndbt-action\nwill no longer receive any updates. This means no new fixes or features will be implemented.\nImpact of migration\nâ\nYou can expect a seamless transition   -  there will be no changes in terms of functionality. Your workflows will continue to operate as usual post-migration.\nMigrate to Atlan action\nâ\nGitHub\nâ\nTo migrate to the\natlan-action\n:\nOpen your GitHub workflow file that currently uses the\ndbt-action\n.\nReplace the\ndbt-action@v1\nwith\natlan-action@v1\nas follows:\nname: Atlan action\non:\npull_request:\ntypes: [opened, edited, synchronize, reopened, closed]\njobs:\nget-downstream-impact:\nname: Get Downstream Assets\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v4\n- name: Run Action\n-       uses: atlanhq/dbt-action@v1\n+        uses: atlanhq/atlan-action@v1\nwith:\nGITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}\nATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}}\nATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}}\nGitLab\nâ\nTo migrate to the\natlan-action\n:\nOpen your GitLab workflow file\n.gitlab-ci.yml\nthat currently uses the\ndbt-action\n.\nClone\nv1\ntag of\natlan-action\ninstead of the\nmain\nbranch of\ndbt-action\n:\nstages:\n- get-downstream-impact\nget-downstream-impact-open:\nstage: get-downstream-impact\nimage: node:20\nscript:\n-  - git clone https://github.com/atlanhq/dbt-action.git\n+   - git clone --branch v1 https://github.com/atlanhq/atlan-action.git\n-  - cd dbt-action\n+   - cd atlan-action\n- npm install\n- npm run build\n- node ./adapters/index.js\nenvironment:\nname: get-downstream-impact\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n- if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS'\nwhen: never\n- if: '$CI_COMMIT_BRANCH'\nTags:\nconnectors\ndata\nmodel\nPrevious\nEnrich Atlan through dbt\nNext\nAdd impact analysis in GitHub\nMigration notice and timeline\nImpact of migration\nMigrate to Atlan action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/mine-microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nGet Started\nMine Microsoft Azure Synapse Analytics\nOn this page\nMine Microsoft Azure Synapse Analytics\ndanger\nAtlan currently only supports mining query history for dedicated SQL pools with the Microsoft Azure Synapse Analytics miner. Mining query history for serverless SQL pools is currently not supported.\nOnce you have\ncrawled assets from Microsoft Azure Synapse Analytics\n, you can mine query history to construct lineage.\nTo mine lineage from Microsoft Azure Synapse Analy\ntics, review the\norder of operations\nand then complete the following steps.\nSelect the miner\nâ\nTo select the Microsoft Azure Synapse Analytics miner:\nIn the top right of any screen, navigate to\n+ New\nand then click\nNew workflow\n.\nUnder\nMarketplace\n, from the filters along the top, click\nMiner\n.\nFrom the list of packages, select\nSynapse Miner\nand then click\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the Microsoft Azure Synapse Analytics miner:\nFor\nConnection\n, select the connection to mine. (To select a connection,\nthe crawler must have already run\n.)\nFor\nMiner Extraction Method\n, choose your extraction method:\nIn\nQuery History\n, Atlan connects to your database and\nmines query history\ndirectly.\nIn\nOffline\n, you will need to first\nmine query history yourself\nand\nmake it available in S3\n.\nRun the miner\nâ\nTo run the Microsoft Azure Synapse Analytics miner, after completing the steps above:\nTo run the miner once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the miner has completed running, you will see lineage for Microsoft Azure Synapse Analytics assets that were created in Microsoft Azure Synapse Analytics! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Microsoft Azure Synapse Analytics\nNext\nSet up on-premises Microsoft Azure Synapse Analytics miner access\nSelect the miner\nConfigure the miner\nRun the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/mine-microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nCrawl Power BI Assets\nMine Microsoft Power BI\nOn this page\nMine Microsoft Power BI\nOnce you have\ncrawled assets from Microsoft Power BI\n, you can mine its activity events to generate\nusage metrics\n.\nTo mine activity events from Microsoft Power BI, review the\norder of operations\nand then complete the following steps.\nSelect the miner\nâ\nTo select the Microsoft Power BI miner:\nIn the top right of any screen, navigate toÂ\nNew\nand then clickÂ\nNew Workflow\n.\nFrom the filters along the top, click\nMiner\n.\nFrom the list of packages, select\nPower BI Miner\nand then click\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the Microsoft Power BI miner:\nFor\nConnection\n, select the connection to mine. (To select a connection,\nthe crawler\nmust have already run.)\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nAdvanced\nto configure the miner:\nFor\nStart time\n, choose the earliest date from which to mine activity events.\nFor\nExcluded Users\n, type the names of users to be excluded while calculating\nusage metrics\nfor Microsoft Power BI assets. Press\nenter\nafter each name to add more names.\nRun the miner\nâ\nTo run the Microsoft Power BI miner, after completing the steps above:\nTo run the miner once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the miner has completed running, you can see usage metrics for Microsoft Power BI assets that were created in Microsoft Power BI between the start time and when the miner ran! ð\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Microsoft Power BI\nNext\nWhat does Atlan crawl from Microsoft Power BI?\nSelect the miner\nConfigure the miner\nRun the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/mine-teradata",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nMine\nMine Teradata\nOn this page\nMine Teradata\nOnce you have\ncrawled assets from Teradata\n, you can mine its query history to construct lineage.\nTo mine lineage from Teradata, review the\norder of operations\nand then complete the following steps.\nSelect the miner\nâ\nTo select the Teradata miner:\nIn the top right of any screen, navigate to\n+New\nand then clickÂ\nNew workflow\n.\nUnder\nMarketplace\n, from the filters along the top, click\nMiner\n.\nFrom the list of packages, selectÂ\nTeradata Miner\nand then click\nSetup Workflow\n.\nConfigure the miner\nâ\nTo configure the Teradata miner:\nFor\nConnection\n, select the connection to mine. (To select a connection,\nthe crawler\nmust have already run.)\nFor\nMiner Extraction Method\n, choose your extraction method:\nIn\nQuery History\n, Atlan connects to your database and mines query history directly.\nIn\nOffline\n, you will need to first\nmine query history yourself\nand\nmake it available in S3\n. This method uses Atlan's\nteradata-miner tool\nto mine query history.\nFor\nStart date\n, choose the earliest date from which to mine query history.\ninfo\nðª\nDid you know?\nThe miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the\nS3 miner\nfirst. After the initial load, you can\nmodify the miner's configuration\nto use query history extraction.\n(Optional) For\nAdvanced Config\n, keep\nDefault\nfor the default configuration or click\nAdvanced\nto configure the miner:\nFor\nCross Connection\n, click\nYes\nto extract lineage across all available data source connections or click\nNo\nto only extract lineage from the selected Teradata connection.\nFor\nControl Config\n, if Atlan support has provided you with a custom control configuration, select\nCustom\nand enter the configuration into theÂ\nCustom Config\nbox. You can also:\nEnter\n{âignore-all-caseâ: true}\nto enable crawling assets with case-sensitive identifiers.\ndanger\nIf running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic\nhere\n.\nRun the miner\nâ\nTo run the TeradataÂ miner, after completing the steps above:\nTo run the miner once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the miner has completed running, you will see lineage for Teradata assets that were created in Teradata between the start date and when the miner ran! ð\nTags:\nconnectors\ndata\ncrawl\nsetup\nPrevious\nCrawl Teradata\nNext\nSet up on-premises Teradata miner access\nSelect the miner\nConfigure the miner\nRun the miner"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/okta-first-time-login-error",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nTroubleshooting\nOkta first-time login authentication error\nOn this page\nOkta first-time login authentication error\nWhy do I get an authentication error when logging in via Okta for the first time?\nâ\nIt is possible that your Okta account is not yet linked with your Atlan account.\nHere are two ways to troubleshoot:\nEnsure that the\nservice provider metadata details\nare copied accurately.\nIf you're logged into Atlan, please log out from Atlan. Once you've logged out of Atlan, clear your browser cache and then continue.\nTags:\nintegration\nconnectors\nfaq-integrations\nPrevious\nPingFederate SSO 404 error\nNext\nGoogle Dashboard login error"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/openlineage-configuration-and-facets",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nReferences\nOpenLineage configuration and facets\nOn this page\nOpenLineage configuration and facets\nOpenLineage\nis a lineage metadata extraction library that you can install in a target application such as\nApache Airflow\nor\nApache Spark\n. Once you have installed OpenLineage, you can configure the target application to integrate with Atlan. This will allow Atlan to receive OpenLineage events and catalog your assets from supported sources. You will neither have to clone a GitHub repository nor make any code changes to your DAGs.\nTo install OpenLineage, refer to the documentation for supported sources:\nApache Airflow\nAmazon MWAA\nApache Spark\nAstronomer\nGoogle Cloud Composer\nDid you know?\nTo add lineage support to sources other than the ones listed above, you can use OpenLineage's extensible specification. Refer to our\ndeveloper documentation\nto learn more.\nExample\nâ\nApache Airflow\nâ\nOnce you have configured a supported Apache Airflow distribution, you can run a sample DAG to confirm that your assets are being crawled in Atlan. Although Atlan strongly recommends running the\npreflight check DAG\nto test your Apache Airflow connection, you can also use the example DAG below to verify your setup.\nFor example:\nimport\njson\nfrom\npendulum\nimport\ndatetime\nfrom\nairflow\n.\ndecorators\nimport\n(\ndag\n,\ntask\n,\n)\n@dag\n(\ndag_id\n=\n\"example_dag_basic\"\n,\nschedule\n=\n\"@once\"\n,\nstart_date\n=\ndatetime\n(\n2023\n,\n1\n,\n1\n)\n,\ncatchup\n=\nFalse\n,\ntags\n=\n[\n\"example\"\n]\n,\n)\ndef\nexample_dag_basic\n(\n)\n:\n@task\n(\n)\ndef\nextract\n(\n)\n:\ndata_string\n=\n'{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\norder_data_dict\n=\njson\n.\nloads\n(\ndata_string\n)\nreturn\norder_data_dict\n@task\n(\nmultiple_outputs\n=\nTrue\n)\ndef\ntransform\n(\norder_data_dict\n:\ndict\n)\n:\ntotal_order_value\n=\n0\nfor\nvalue\nin\norder_data_dict\n.\nvalues\n(\n)\n:\ntotal_order_value\n+=\nvalue\nreturn\n{\n\"total_order_value\"\n:\ntotal_order_value\n}\n@task\n(\n)\ndef\nload\n(\ntotal_order_value\n:\nfloat\n)\n:\nprint\n(\nf\"Total order value is:\n{\ntotal_order_value\n:\n.2f\n}\n\"\n)\norder_data\n=\nextract\n(\n)\norder_summary\n=\ntransform\n(\norder_data\n)\nload\n(\norder_summary\n[\n\"total_order_value\"\n]\n)\nexample_dag_basic\n(\n)\nApache Spark\nâ\nOnce you have configured Apache Spark, you can run a sample Spark job to confirm that your assets are being crawled in Atlan.\nFor example:\nfrom\npyspark\n.\nsql\nimport\nSparkSession\nfrom\npyspark\n.\nsql\n.\nfunctions\nimport\ncol\n# Create a Spark session and configure the spark properties\nspark\n=\n(\nSparkSession\n.\nbuilder\n.\nmaster\n(\n'local'\n)\n.\nappName\n(\n'data_pipeline_sample'\n)\n.\ngetOrCreate\n(\n)\n)\nsnowflake_options\n=\n{\n\"sfURL\"\n:\n\".snowflakecomputing.com\"\n,\n\"sfUser\"\n:\n\"\"\n,\n\"sfPassword\"\n:\n\"\"\n,\n\"sfDatabase\"\n:\n\"\"\n,\n\"sfWarehouse\"\n:\n\"\"\n,\n\"sfSchema\"\n:\n\"\"\n,\n\"sfRole\"\n:\n\"\"\n,\n}\ninstacart_df\n=\nspark\n.\nread \\\n.\nformat\n(\n\"snowflake\"\n)\n\\\n.\noptions\n(\n**\nsnowflake_options\n)\n\\\n.\noption\n(\n\"dbtable\"\n,\n\"table1\"\n)\n\\\n.\nload\n(\n)\nfiltered_df\n=\ninstacart_df\n.\nfilter\n(\ncol\n(\n'\"order_id\"'\n)\n==\n'123456'\n)\nfiltered_df\n.\nwrite \\\n.\nformat\n(\n\"snowflake\"\n)\n\\\n.\noptions\n(\n**\nsnowflake_options\n)\n\\\n.\noption\n(\n\"dbtable\"\n,\n\"table2\"\n)\n\\\n.\nmode\n(\n\"append\"\n)\n\\\n.\nsave\n(\n)\nspark\n.\nstop\n(\n)\nSupported facets\nâ\nAn OpenLineage event will contain the following object model: dataset, job, and run entities. In addition, OpenLineage supports\nfacets\nto provide contextual metadata for events.\nAtlan currently only processes the following facets for OpenLineage events:\nApache Airflow\nâ\nOpenLineage facet\nDescription\nWhere in Atlan\njob.facets.jobType\nApache Airflow asset type (task or DAG)\nasset profile, preview, and sidebar\nrun.facets.airflow\nDAG details, including runs, tasks, owner, and task group\nasset profile, overview sidebar, and pipeline graph\nrun.facets.airflow_version\nApache Airflow version and DAG metadata\nAPI only\nrun.facets.parentRun\nparent DAG for tasks\nAPI only\nrun.facets.processing_engine\nApache Airflow and OpenLineage versions\nAPI only\noutputs.facets.columnLineage\nfetches column lineage\nlineage graph\nApache Spark\nâ\nOpenLineage facet\nDescription\nWhere in Atlan\neventType\njob run status\noverview sidebar\neventTime\njob start and end time\nasset profile\njob.namespace\nconnection name\nasset profile and overview sidebar\njob.name\nSpark job name\nasset name\nrun.runId\nSpark job name run ID\nAPI only\nrun.facets.spark_version\nSpark version\noverview sidebar\nrun.facets.spark_properties\nOpenLineage package version\nAPI only\nrun.facets.processing_engine\nSpark cluster details\nAPI only\ninputs.facets.name\nlinks input facets\nrelated assets and pipeline graph\noutputs.facets.name\nlinks output facets\nrelated assets and pipeline graph\ninputs.facets.namespace\ninput type\nrelated assets and pipeline graph\noutputs.facets.namespace\noutput type\nrelated assets and pipeline graph\ninputs.facets.symlinks\nretrieves logical entity\nAPI only\noutputs.facets.symlinks\nretrieves logical entity\nAPI only\noutputs.facets.columnLineage\nfetches column lineage\nlineage graph\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\ncatalog\nmetadata\ndiscovery\nPrevious\nConnectors and capabilities\nNext\nSupported sources\nExample\nSupported facets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/faq/permissions-and-limitations",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nFAQ\nPermissions and limitations\nOn this page\nHow do I make sure Atlan can access new tables automatically?\nâ\nCrateDB requires explicit permission configuration to access tables created after initial setup.\nGrant schema-level permissions (recommended)\n: This approach automatically includes all future tables:\nGRANT\nDQL\nON\nSCHEMA\ndoc\nTO\natlan_user\n;\nThis single command covers all current and future tables in the schema, including metadata access.\nGrant wildcard permissions\n: Use this for specific table patterns:\nGRANT\nDQL\nON\nTABLE\ndoc\n.\n*\nTO\natlan_user\n;\nRegular permission audits\n: Since CrateDB has limited stored procedure support, consider regular permission audits:\n-- Check current user permissions\nSELECT\nprivilege_type\n,\ntable_schema\n,\ntable_name\nFROM\ninformation_schema\n.\ntable_privileges\nWHERE\ngrantee\n=\n'atlan_user'\nORDER\nBY\ntable_schema\n,\ntable_name\n;\nWhat CrateDB limitations affect the Atlan connector?\nâ\nNo explicit schema creation\n: CrateDB doesn't support the\nCREATE SCHEMA\nstatement. Schemas are created implicitly when a table referencing a new schema is created.\nManual permission management\n: Since schema-level permissions aren't supported, access must be manually granted for each new table.\nNo metadata-only access\n: CrateDBâs DQL privileges apply to both metadata and data. This means the Atlan user can view and query all data in any tables they have permission to access.\nLimited automation\n: CrateDB doesn't support triggers or full stored procedure capabilities, which restricts options for automating permission management or other operational tasks.\nTags:\nconnectors\ncratedb\ndatabase\nfaq\nfaq-connectors\nPrevious\nConnection issues\nHow do I make sure Atlan can access new tables automatically?\nWhat CrateDB limitations affect the Atlan connector?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/references/preflight-checks-for-aiven-kafka",
    "content": "Connect data\nEvent/Messaging\nAiven Kafka\nReferences\nPreflight checks for Aiven Kafka\nOn this page\nPreflight checks for Aiven Kafka\nBefore\nrunning the Aiven Kafka crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nBucket credentials\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the bucket credentials passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nBucket credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ncrawl\nPrevious\nWhat does Atlan crawl from Aiven Kafka?\nBucket credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/references/preflight-checks-for-amazon-msk",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nReferences\nPreflight checks for Amazon MSK\nOn this page\nPreflight checks for Amazon MSK\nBefore\nrunning the Amazon MSK crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nCluster permission check\nâ\nâ\nCheck successful\nif the IAM role has sufficient\npermissions\nto list all the brokers for the Amazon MSK cluster.\nâ\nInternal server error\nif the IAM role is missing\nkafka-cluster:Connect\npermission.\nâ\nfailed to get list of kafka brokers\nif the IAM role has the permission to connect to the Amazon MSK cluster but is unable to fetch any broker details.\nTopic permission check\nâ\nâ\nCheck successful\nif the IAM role has sufficient\npermissions\nto list or describe topics available in the Amazon MSK cluster.\nâ\nfailed to get list of kafka topics\nif the IAM role is missing the\nkafka-cluster:DescribeTopic\npermission.\nTags:\nconnectors\ncrawl\nPrevious\nWhat does Atlan crawl from Amazon MSK?\nNext\nTroubleshooting Amazon MSK connectivity\nCluster permission check\nTopic permission check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/references/preflight-checks-for-amazon-quicksight",
    "content": "Connect data\nBI Tools\nCloud-based BI\nAmazon QuickSight\nReferences\nPreflight checks for Amazon QuickSight\nOn this page\nPreflight checks for Amazon QuickSight\nBeforeÂ\nrunning the Amazon QuickSight crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nAnalysis view permission\nâ\nThe\nListAnalyses\nREST API is used to fetch the actual list of analyses for which the user has view permission.\nâ\nCheck successful. Analysis API is accessible.\nâ\nCheck failed for listed analyses\nFolder view permission\nâ\nThe\nListFolders\nREST API is used to fetch the actual list of folders for which the user has view permission.\nâ\nCheck successful. Folder API is accessible.\nâ\nCheck failed for listed folders\nDashboard view permission\nâ\nThe\nListDashboards\nREST API is used to fetch the actual list of dashboards for which the user has view permission.\nâ\nCheck successful. Dashboard API is accessible.\nâ\nCheck failed for listed dashboards\nDataset view permission\nâ\nThe\nListDataSets\nREST API is used to fetch the actual list of datasets for which the user has view permission.\nâ\nCheck successful. Dataset API is accessible.\nâ\nCheck failed for listed datasets\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Amazon QuickSight?\nAnalysis view permission\nFolder view permission\nDashboard view permission\nDataset view permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/references/preflight-checks-for-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nReferences\nPreflight checks for Amazon Redshift\nOn this page\nPreflight checks for Amazon Redshift\nBefore\nrunning the Amazon Redshift crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nAssets\nâ\nDatabase and schema\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTables count\nâ\nâ\nCheck successful. Table count: <count>\nâ Source returned error/UI default failure message\nTables metadata\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.svv_table_info\nExternal tables metadata\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.svv_external_tables\nMiner\nâ\nDid you know?\nOnce you have crawled assets from Amazon Redshift, you can\nmine query history\n.\nQuery history\nâ\nDDL\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.stl_ddltext\nDML\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.stl_query\nSession\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.stl_connection_log\nTransaction rollback\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.stl_undone\nInsert query\nâ\nâ\nCheck successful\nâ\nCheck failed! Please grant select permission on pg_catalog.stl_insert\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Amazon Redshift?\nNext\nTroubleshooting Amazon Redshift connectivity\nAssets\nMiner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/references/preflight-checks-for-anomalo",
    "content": "Connect data\nData Quality & Observability\nAnomalo\nReferences\nPreflight checks for Anomalo\nOn this page\nPreflight checks for Anomalo\nBefore\nintegrating Anomalo\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nAnomalo instance accessibility check\nâ\nThis check tests for the validity of the\nhost name URL and API key\nyou provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid.\nâ\nCheck successful\nif the Anomalo instance is accessible.\nâ\nUnable to connect to your Anomalo instance\nPermission (read access) check\nâ\nThis check tests whether or not you have the\nrequired permissions\nfor querying the Anomalo API to fetch metadata.\nâ\nCheck successful\nâ\nUnable to fetch warehouse metadata from your Anomalo instance. Please check if the provided API key has the required permissions.\nTags:\nconnectors\ndata\napi\nPrevious\nWhat does Atlan crawl from Anomalo?\nNext\nTroubleshooting Anomalo connectivity\nAnomalo instance accessibility check\nPermission (read access) check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/references/preflight-checks-for-apache-kafka",
    "content": "Connect data\nEvent/Messaging\nApache Kafka\nReferences\nPreflight checks for Apache Kafka\nOn this page\nPreflight checks for Apache Kafka\nBefore\nrunning the Apache Kafka crawler\n, run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nCluster permission\nâ\nâ Check successful if the user has sufficient permission to list all the brokers of the Apache Kafka cluster.\nâ Failed to get list of Kafka brokers if the user has permission to connect to the Apache Kafka cluster but is unable to fetch any broker details.\nTopics permission\nâ\nâ Check successful if the user has sufficient permission to list or describe topics available in the Apache Kafka cluster.\nâ Failed to get list of Kafka topics if the user has permission to connect to the Apache Kafka cluster but is unable to fetch any topic details.\nTags:\nconnectors\ncrawl\nPrevious\nWhat does Atlan crawl from Apache Kafka?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/references/preflight-checks-for-confluent-schema-registry",
    "content": "Connect data\nEvent/Messaging\nConfluent Schema Registry\nReferences\nPreflight checks for Confluent Schema Registry\nOn this page\nPreflight checks for Confluent Schema Registry\nBefore\nrunning the ConfluentÂ Schema Registry crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflights check will be completed:\nSubjects check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName subject. Please grant at least read access to the subject\nConfigs check\nâ\nâ\nCheck successful\nâ Source returned error message\nSchemas check\nâ\nâ\nCheck successful\nâ Source returned error message\nTags:\nconnectors\ncrawl\nPrevious\nWhat does Atlan crawl from Confluent Schema Registry?\nSubjects check\nConfigs check\nSchemas check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/references/preflight-checks-for-cratedb",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nReferences\nPreflight checks for CrateDB\nOn this page\nPreflight checks for CrateDB\nBefore\nrunning the CrateDB crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks are completed:\nDatabase and schema check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTables check\nâ\nâ\nCheck successful. Table count: <count>\nâ\nCheck failed. Missing some of the required grants SELECT, SHOW VIEW, EXECUTE on tables:\nTags:\nconnectors\ncratedb\ndatabase\npreflight-checks\nPrevious\nWhat does Atlan crawl from CrateDB?\nNext\nConnection issues\nDatabase and schema check\nTables check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/references/preflight-checks-for-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nReferences\nPreflight checks for Databricks\nOn this page\nPreflight checks for Databricks\nBefore\nrunning the Databricks crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nJDBC\nâ\nSchemas\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nRest API (Unity Catalog)\nâ\nUser login/UC enabled\nâ\nâ\nCheck successful\nâ Source returned error\nFor example:\n{\"error_code\":\"403\",\"message\":\"Invalid access token.\"}\nCatalog\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName catalog\nDatabricks lineage and usage (miner)\nâ\nDid you know?\nOnce you have crawled assets from Databricks, you can\nextract lineage and usage and popularity metrics\n.\nLogin\nâ\nâ\nCheck successful\nâ\nCheck failed\nFor example:\n{\"error_code\":\"403\",\"message\":\"Invalid access token.\"}\nLineage API\nâ\nâ\nCheck successful. Lineage API is enabled.\nâ\nCheck failed\nFor example:\nLineage is not enabled for this Databricks account: 47258391-b3c8-4ff9-a0d9-5afc02443806\nQuery history API endpoint check\nâ\nâ\nCheck successful\nâ\nCheck failed\nTags:\nconnectors\ndata\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Databricks?\nNext\nTroubleshooting Databricks connectivity\nJDBC\nRest API (Unity Catalog)\nDatabricks lineage and usage (miner)"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/references/preflight-checks-for-datastax-enterprise",
    "content": "Connect data\nDatabases\nNoSQL Databases\nDatastax Enterprise\nReferences\nPreflight checks for DataStax Enterprise\nOn this page\nPreflight checks for DataStax Enterprise\nBefore running the\nDataStax Enterprise crawler\n, you can run preflight checks to perform the necessary technical validations. The following preflight checks must be completed:\nCluster permission\nâ\nâ Check successful if the connection to the DataStax Enterprise cluster is established.\nâ Check failed if unable to connect to the DataStax Enterprise cluster.\nKeyspace permission\nâ\nâ Check successful if the user has read access to\nsystem_schema.keyspaces\n.\nâ Check failed if the user does not have read access to\nsystem_schema.keyspaces\n.\nTable permission\nâ\nâ Check successful if the user has read access to\nsystem_schema.tables\n.\nâ Check failed if the user does not have read access to\nsystem_schema.tables\n.\nView permission\nâ\nâ Check successful if the user has read access to\nsystem_schema.views\n.\nâ Check failed if the user does not have read access to\nsystem_schema.views\n.\nColumn permission\nâ\nâ Check successful if the user has read access to\nsystem_schema.columns\n.\nâ Check failed if the user does not have read access to\nsystem_schema.columns\n.\nIndex permission\nâ\nâ Check successful if the user has read access to\nsystem_schema.indexes\n.\nâ Check failed if the user does not have read access to\nsystem_schema.indexes\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from DataStax Enterprise?\nNext\nTroubleshoot permission issues"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/preflight-checks-for-dbt",
    "content": "On this page\nPreflight checks for dbt\nBefore\nrunning the dbt crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\ndbt Core\nâ\nManifest file check on S3\nâ\nThis checks if manifest files are present in the provided bucket and prefix.\nâ\nCheck successful\nif Atlan can access the bucket and prefix containing the manifest files.\nâ\nCheck failed\nFor example:\nIf Atlan cannot access the bucket   -\nManifest file Check on S3: failed With error code: AccessDenied - Access Denied\nIf Atlan can access the bucket but there are no manifest files present in the bucket   -\nManifest file Check on S3: No manifest files found in the mentioned S3 Prefix\nTags:\nconnectors\ncrawl\ndbt Core"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/references/preflight-checks-for-domo",
    "content": "Connect data\nBI Tools\nCloud-based BI\nDomo\nReferences\nPreflight checks for Domo\nOn this page\nPreflight checks for Domo\nBefore\nrunning the Domo crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nDatasets API\nâ\nAtlan uses the\nDataSet API\nto fetch dataset metadata from Domo.\nâ\nCheck successful. Datasets API is working.\nâ\nCheck failed\nFor example:\n{\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}}#STATUS:403\nDashboards API\nâ\nAtlan uses the\nPage API\nto fetch dashboard metadata from Domo.\nâ\nCheck successful. Dashboards API is working.\nâ\nCheck failed\nFor example:\n{\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}}#STATUS:403\nCards API\nâ\nAtlan uses the DomoStats cards API to fetch card metadata from Domo. This checks the format of the response and tries to find the\nId\n,\nname\n, and\ndescription\ncolumns.\nâ\nCheck successful. Cards API is working.\nâ\nCheck failed. <Response received from the API>\nCard-Dashboard Relationship API\nâ\nAtlan uses the DomoStats card-dashboard relationship API to fetch card-dashboard relationship metadata from Domo. This checks the format of the response and tries to find the\ncardId\nand\npageId\ncolumns.\nâ\nCheck successful. The card-dashboard relationship API is working.\nâ\nCheck failed. <Response received from the API>\nDataset-Card Relationship API\nâ\nAtlan uses the DomoStats dataset-card relationship API to fetch dataset-card relationship metadata from Domo. This checks the format of the response and tries to find the\ndataSourceId\nand\ncardId\ncolumns.\nâ\nCheck successful. The dataset-card relationship API is working.\nâ\nCheck failed. <Response received from the API>\nTags:\nconnectors\ndata\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Domo?\nNext\nTroubleshooting Domo connectivity\nDatasets API\nDashboards API\nCards API\nCard-Dashboard Relationship API\nDataset-Card Relationship API"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/references/preflight-checks-for-fivetran",
    "content": "On this page\nPreflight checks for Fivetran\nBefore\nrunning the Fivetran enrichment\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nFivetran Platform table existence and access check\nâ\nThis check verifies if a Fivetran table named\ntable_lineage\nexists in the selected schema, and whether Atlan has been provided with read access to the metadata.\nâ\nCheck successful\nâ\nTable named table_lineage does not exist.\nor\nProvided credentials do not have read access.\nTags:\nconnectors\ndata\ncrawl\napi\nconfiguration\nFivetran Platform table existence and access check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/preflight-checks-for-google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nReferences\nPreflight checks for Google BigQuery\nOn this page\nPreflight checks for Google BigQuery\nBefore\nrunning the Google BigQuery crawler\n, you can run\npreflight checks\nto perform the necessary technical validations.Â\nDid you know?\nAll Google BigQuery resources expose the\ntestIamPermissions()\nmethod, which is used for permission testing in Atlan through REST API.\nThe following preflight checks will be completed:\nAuthorization\nâ\nEach request requires an OAuth 2.0 access token generated via the\nservice account key\n.\nAssets\nâ\nMetadata crawling permission\nâ\nâ\nCheck successful\nâ\nCheck failed. Not all permission granted. Missing permissions:\nQuery and review permission\nâ\nâ\nCheck successful\nâ\nCheck failed. Not all permission granted. Missing permissions:\nMiner\nâ\nDid you know?\nOnce you have crawled assets from Google BigQuery, you can\nmine query history\n.\nMiner policy\nâ\nQuery history\nâ\nâ\nCheck successful\nâ\nCheck failed. Not all permission granted. Missing permissions:\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nCrawler workflow\nâ\nThis checks if the selected\nconnection\nexists in Atlan.\nâ\nCheck successful\nâ\nCheck failed. Connection does not exist.\n/\nCheck failed. Workflow artifacts are missing. Please run the crawler workflow again.\nTags:\nconnectors\ndata\ncrawl\napi\nauthentication\nPrevious\nWhat does Atlan crawl from Google BigQuery?\nNext\nTroubleshooting Google BigQuery connectivity\nAuthorization\nAssets\nMiner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/references/preflight-checks-for-hive",
    "content": "Connect data\nDatabases\nQuery Engines\nHive\nReferences\nPreflight checks for Hive\nOn this page\nPreflight checks for Hive\nBefore\nrunning the Hive crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nDatabase and schema check\nâ\nInformation schema\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nMiner\nâ\nDid you know?\nOnce you have crawled assets from Hive, you can\nmine query history\n.\nS3 credentials\nâ\nâ\nCheck successful\nâ\nCheck failed\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Hive?\nDatabase and schema check\nMiner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/references/preflight-checks-for-looker",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nReferences\nPreflight checks for Looker\nOn this page\nPreflight checks for Looker\nBefore\nrunning the Looker crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nProjects view capability\nâ\nFirst, the list of projects in the\nInclude Projects\nand\nExclude Projects\nfields is determined. Next, the\nQuery Projects\nREST API is used to fetch the actual list of projects for which the user has\nview capability\n.\nâ\nCheck successful\nif all the projects from the first list are in the second one.\nâ\nCheck failed for $missingProject\nFolders view capability\nâ\nFirst, the list of folders in the\nInclude Folders\nand\nExclude Folders\nfields is determined. Next,Â the\nQuery Projects\nREST API is used to fetch the actual list of folders for which the user has\nview capability\n.\nâ\nCheck successful\nif all the folders from the first list are in the second one.\nâ\nCheck failed for $missingFolder\nGit SSH key check\nâ\nâ\nCheck successful\nif a valid SSH key is provided when field level lineage is enabled.\nâ\nField level lineage needs to clone your git repositories. Please provide your git ssh key or disable field level lineage\nor\nPlease provide your private ssh key properly. Remember to include '-----BEGIN' and '-----END' blocks also. Please remember to specify the password for private key if encrypted\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Looker?\nNext\nTroubleshooting Looker connectivity\nProjects view capability\nFolders view capability\nGit SSH key check\nS3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/references/preflight-checks-for-metabase",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMetabase\nReferences\nPreflight checks for Metabase\nOn this page\nPreflight checks for Metabase\nBefore\nrunning the Metabase crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nCollection count\nâ\nâ\nCollection count check passed. Total collections: <collection count>\nâ\nTotal collections: 0\nor source returned error\nDashboard count\nâ\nâ\nDashboard count check passed. Total dashboards: <dashboard count>\nâ\nTotal dashboards: 0\nor source returned error\nQuestion count\nâ\nâ\nQuestion count check passed. Total questions: <question count>\nâ\nTotal questions: 0\nor source returned error\nNative query editing permission\nâ\nâ\nCheck successful\nâ\nCheck failed. Missing native query editing permission on the following databases: <database list>\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Metabase?\nNext\nTroubleshooting Metabase connectivity\nCollection count\nDashboard count\nQuestion count\nNative query editing permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/references/preflight-checks-for-microsoft-azure-data-factory",
    "content": "On this page\nPreflight checks for Microsoft Azure Data Factory\nBefore\nrunning the Microsoft Azure Data Factory crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nSubscription permissions check\nâ\nâ\nCheck successful\nâ\nNo subscriptions available. This can either mean that no data factory exists or the service principal does not have access to the specified data factories. Please grant the Reader role to the service principal for at least one data factory.\nTags:\nconnectors\ndata\ncrawl\nSubscription permissions check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/references/preflight-checks-for-microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nReferences\nPreflight checks for Microsoft Azure Synapse Analytics\nOn this page\nPreflight checks for Microsoft Azure Synapse Analytics\nBefore\nrunning the Microsoft Azure Synapse Analytics crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nDatabases and schemas check\nâ\nThis check is performed for both\nbasic\nand\nservice principal\nauthentication method.\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nREST credentials check\nâ\nThis check is only performed when using the\nservice principal authentication\nmethod. Atlan authenticates the service principal credentials by attempting to fetch the access token using\nMicrosoft's authorization flow\n.\nâ\nCheck successful\nâ\nCheck failed. <Response received from the API>\nTags:\nconnectors\ndata\ncrawl\nauthentication\nPrevious\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nNext\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\nDatabases and schemas check\nREST credentials check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/preflight-checks-for-microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nReferences\nPreflight checks for Microsoft Power BI\nOn this page\nPreflight checks for Microsoft Power BI\nBefore\nrunning the Microsoft Power BI crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nCredentials scopes\nâ\nâ\nCheck successful\nÂ\nâ Source returned error\nFor example:\nCredentials Scopes: Failed with response{\"error\":\"invalid_client\",\"error_description\":\"AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app '832d86c8-cd9b-43a5-b165-ab40fb49770b'.\\r\\nTrace ID: 654ce9b2-a626-4e0b-8598-0cac69970200\\r\\nCorrelation ID: 5be327fc-93cb-4bef-ab4e-0373f11a8017\\r\\nTimestamp: 2022-10-31 09:03:41Z\",\"error_codes\":[7000215],\"timestamp\":\"2022-10-31 09:03:41Z\",\"trace_id\":\"654ce9b2-a626-4e0b-8598-0cac69970200\",\"correlation_id\":\"5be327fc-93cb-4bef-ab4e-0373f11a8017\",\"error_uri\":\"https://login.microsoftonline.com/error?code=7000215\"}#STATUS:401\nWorkspace permissions\nâ\nâ\nCheck successful\nâ\nNo workspaces available\nMetadata scan and fetch refreshables\nâ\nâ\nCheck successful\nÂ\nâ Source returned error\nFor example:\nPermissions to scan metadata and fetch refreshables: Failed with response{\"error\":{\"code\":\"PowerBINotAuthorizedException\",\"pbi.error\":{\"code\":\"PowerBINotAuthorizedException\",\"parameters\":{},\"details\":[],\"exceptionCulprit\":1}}}#STATUS:401\nTags:\nconnectors\ncrawl\nPrevious\nWhat does Atlan crawl from Microsoft Power BI?\nNext\nWhat lineage does Atlan extract from Microsoft Power BI?\nCredentials scopes\nWorkspace permissions\nMetadata scan and fetch refreshables"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/references/preflight-checks-for-microsoft-sql-server",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nReferences\nPreflight checks for Microsoft SQL Server\nOn this page\nPreflight checks for Microsoft SQL Server\nBefore\nrunning the Microsoft SQL Server crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nDatabase and schema check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nMiner\nâ\nDid you know?\nOnce you have crawled assets from Microsoft SQL Server, you can\nmine query history\n.\nS3 credentials\nâ\nâ\nCheck successful\nâ\nCheck failed\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Microsoft SQL Server?\nDatabase and schema check\nMiner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/references/preflight-checks-for-microstrategy",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicroStrategy\nReferences\nPreflight checks for MicroStrategy\nOn this page\nPreflight checks for MicroStrategy\nBefore\nrunning the MicroStrategy crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nProject permission\nâ\nFirst, the list of projects in the\nInclude Projects\nand\nExclude Projects\nfields is determined. Next,Â the\nGet Projects REST API\nis used to fetch the actual list of projects for which the user has permissions.\nâ\nCheck successful\nif all the projects from the first list are in the second one.\nâ\nCheck failed for $missingProjectId project\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from MicroStrategy?\nNext\nTroubleshooting MicroStrategy connectivity\nProject permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/references/preflight-checks-for-mode",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMode\nReferences\nPreflight checks for Mode\nOn this page\nPreflight checks for Mode\nBefore\nrunning the Mode crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nWorkspace check\nâ\nâ\nCheck successful\nâ\nAuthenticating user does not have access to specified workspace. Please grant at least Full Members access\nCollections check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName collection. Please grant at least viewer access to the collection\nQueries check\nâ\nâ\nUser has access to <count of sources> queryable data sources\nâ Access forbidden message or\nUser has access to 0 queryable data sources\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Mode?\nNext\nTroubleshooting Mode connectivity\nWorkspace check\nCollections check\nQueries check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/references/preflight-checks-for-monte-carlo",
    "content": "Connect data\nData Quality & Observability\nMonte Carlo\nReferences\nPreflight checks for Monte Carlo\nOn this page\nPreflight checks for Monte Carlo\nBefore\nrunning the Monte Carlo crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nLogin check\nâ\nâ\nCheck successful\nâ\nCheck failed for given API credential\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Monte Carlo?\nLogin check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/references/preflight-checks-for-mysql",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nReferences\nPreflight checks for MySQL\nOn this page\nPreflight checks for MySQL\nBefore\nrunning the MySQL crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nDatabase and schema check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTables check\nâ\nâ\nCheck successful. Table count: <count>\nâ\nCheck failed. Missing some of the required grants SELECT, SHOW VIEW, EXECUTE on tables:\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from MySQL?\nNext\nTroubleshooting MySQL connectivity\nDatabase and schema check\nTables check\nS3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/references/preflight-checks-for-oracle",
    "content": "Connect data\nDatabases\nSQL Databases\nOracle\nReferences\nPreflight checks for Oracle\nOn this page\nPreflight checks for Oracle\nBefore\nrunning the Oracle crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Oracle?\nS3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/references/preflight-checks-for-postgresql",
    "content": "Connect data\nDatabases\nSQL Databases\nPostgreSQL\nReferences\nPreflight checks for PostgreSQL\nOn this page\nPreflight checks for PostgreSQL\nBefore\nrunning the PostgreSQL crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nDatabase and schema check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTables check\nâ\nâ\nCheck successful. Table count: <count>\nâ\nCheck failed. Missing some of the required grants SELECT, SHOW VIEW, EXECUTE on tables:\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from PostgreSQL?\nNext\nTroubleshooting PostgreSQL connectivity\nDatabase and schema check\nTables check\nS3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/references/preflight-checks-for-prestosql",
    "content": "Connect data\nDatabases\nQuery Engines\nPrestoSQL\nReferences\nPreflight checks for PrestoSQL\nOn this page\nPreflight checks for PrestoSQL\nBefore\nrunning the PrestoSQL crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nCatalogs and schemas check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from PrestoSQL?\nCatalogs and schemas check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/references/preflight-checks-for-qlik-sense-cloud",
    "content": "Connect data\nBI Tools\nCloud-based BI\nQlik Sense Cloud\nReferences\nPreflight checks for Qlik Sense Cloud\nOn this page\nPreflight checks for Qlik Sense Cloud\nBefore\nrunning the Qlik Sense Cloud crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nSpaces\nâ\nâ\nCheck successful\nâ Source returned error   -  for example,\n401 authorization required\nApps\nâ\nâ\nCheck successful\nâ Source returned error   -  for example,\n401 authorization required\nItems\nâ\nThis check tests for access to datasets and other Qlik objects.\nâ\nCheck successful\nâ Source returned error   -  for example,\n401 authorization required\nConnections\nâ\nâ\nCheck successful\nâ Source returned error   -  for example,\n401 authorization required\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Qlik Sense Cloud?\nNext\nTroubleshooting Qlik Sense Cloud connectivity\nSpaces\nApps\nItems\nConnections"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/references/preflight-checks-for-redash",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nRedash\nReferences\nPreflight checks for Redash\nOn this page\nPreflight checks for Redash\nBefore\nrunning the Redash crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nAdministrator privileges\nâ\nâ\nCheck successful\nâ\nCheck failed. Given API key does not have admin privileges.\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Redash?\nNext\nTroubleshooting Redash connectivity\nAdministrator privileges"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/references/preflight-checks-for-redpanda-kafka",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nReferences\nPreflight checks for Redpanda Kafka\nOn this page\nPreflight checks for Redpanda Kafka\nBefore\nrunning the Redpanda Kafka crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nBucket credentials\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the bucket credentials passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nBucket credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ncrawl\nPrevious\nWhat does Atlan crawl from Redpanda Kafka?\nBucket credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/references/preflight-checks-for-salesforce",
    "content": "Connect data\nCRM\nSalesforce\nReferences\nPreflight checks for Salesforce\nOn this page\nPreflight checks for Salesforce\nBefore\nrunning the Salesforce crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nOrganization count check\nâ\nâ\nOrganization Count check passed. Organization ID:<organization ID> Organization Name: <organization name>\nâ Check failed or\nsObject type 'Organization' is not supported\nFolder count check\nâ\nâ\nFolder Count check passed. Total folders: <count>\nâ Check failed\nReport count check\nâ\nâ\nReport Count check passed. Total reports: <count>\nâ Check failed or\nMALFORMED_QUERY: Invalid SOQL query. Please check the syntax and try again.\nor\nINVALID_TYPE : sObject type 'Reportâ is not supported\nDashboard count check\nâ\nâ\nDashboard Count check passed. Total dashboards: <count>\nâ Check failed or\nINVALID_TYPE : sObject type 'Dashboardâ is not supported\nSalesforce object count check\nâ\nâ\nSalesforce Object Count check passed. Total SObjects: <count>\nâ Check failed\nField count check\nâ\nâ\nField Count check passed. Total organization fields: <count>\nâ Check failed or\nMALFORMED_QUERY: Invalid SOQL query. Please check the syntax and try again.\nor\nINVALID_TYPE : sObject type 'EntityDefinitionâ is not supported\nTags:\nconnectors\ncrawl\nsalesforce\nPrevious\nWhat does Atlan crawl from Salesforce?\nNext\nTroubleshooting Salesforce connectivity\nOrganization count check\nFolder count check\nReport count check\nDashboard count check\nSalesforce object count check\nField count check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/references/preflight-checks-for-sap-hana",
    "content": "Connect data\nDatabases\nSQL Databases\nSAP HANA\nReferences\nPreflight checks for SAP S/4HANA\nOn this page\nPreflight checks for SAP S/4HANA\nPrivate Preview\nBefore\nrunning the SAP HANA crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks must be completed:\nSchema check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from SAP HANA?\nSchema check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/references/preflight-checks-for-sigma",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSigma\nReferences\nPreflight checks for Sigma\nOn this page\nPreflight checks for Sigma\nBefore\nrunning the Sigma crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nWorkbook view permission\nâ\nFirst, the list of workbooks in the\nInclude Workbooks\nÂ and\nExclude Workbooks\nfields is determined. Next, the\nList Workbooks\nREST API is used to fetch the actual list of workbooks for which the user credentials have view permission.\nâ\nCheck successful\nif all the workbooks from the first list are in the second one.\nâ\nCheck failed for $missingWorkbookId workbook\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Sigma?\nNext\nTroubleshooting Sigma connectivity\nWorkbook view permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/references/preflight-checks-for-sisense",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSisense\nReferences\nPreflight checks for Sisense\nOn this page\nPreflight checks for Sisense\nBefore\nrunning the Sisense crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nFolders API check\nâ\nAtlan uses the\nFolders API\nto check if it's responding with a response status code 200.\nâ\nCheck successful. Folders API is working.\nâ\nCheck failed\nFor example:\n{\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}}#STATUS:403\nDashboards API check\nâ\nAtlan uses the\nDashboards API\ntoÂ check if it's responding with a response status code 200.\nâ\nCheck successful. Dashboards API is working.\nâ\nCheck failed\nFor example:\n{\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}}#STATUS:403\nDatamodels API check\nâ\nAtlan uses the\nDatamodels API\nto check if it's responding with a response status code 200. The Datamodels API check may fail with a response status code 403 if the\nuser only has\nViewer\npermissions\n.\nâ\nCheck successful. Datamodels API is working.\nâ\nCheck failed\nFor example:\n{\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}}#STATUS:403\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Sisense?\nNext\nTroubleshooting Sisense connectivity\nFolders API check\nDashboards API check\nDatamodels API check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/preflight-checks-for-tableau",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nReferences\nPreflight checks for Tableau\nOn this page\nPreflight checks for Tableau\nBefore\nrunning the Tableau crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nServer REST API version\nâ\nThe\nServer Info\nREST API is used to fetch the\nrestApiVersion\nvalue.\nâ\nCheck successful\nif the\nrestApiVersion\nis greater than or equal to 2.4.\nProjects view capability\nâ\nFirst, the list of projects in the\nInclude Projects\nand\nExclude Projects\nfields is determined. Next, the\nQuery Projects\nREST API is used to fetch the actual list of projects for which the user has\nview capability\n.\nâ\nCheck successful\nif all the projects from the first list are in the second one.\nDid you know?\nAtlan supports user credentials with\nViewer\nrole. Ensure that you grant\nView capability\nfor all the assets you want to crawl.\nMetadata API\nâ\nâ\nCheck successful\nâ\nCannot run the query because the Metadata API has not been enabled yet. Run the 'tsm maintenance metadata-services enable' command to enable the Metadata API or contact the Tableau administrator\nTags:\nconnectors\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Tableau?\nNext\nTroubleshooting Tableau connectivity\nServer REST API version\nProjects view capability\nMetadata API"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/references/preflight-checks-for-teradata",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nReferences\nPreflight checks for Teradata\nOn this page\nPreflight checks for Teradata\nBefore\nrunning the Teradata crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks will be completed:\nDatabases and schemas check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTables check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nS3\nâ\nâ\nCheck successful\nif the bucket, region, and prefix combination is valid and the S3 credential passed is accessible.\nâ\nCheck failed with error code <AWS error code> - <AWS SDK ERR message>\nFor example:\nMiner S3 credentials: failed with error code: NoSuchBucket\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Teradata?\nDatabases and schemas check\nTables check\nS3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/references/preflight-checks-for-trino",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nReferences\nPreflight checks for Trino\nOn this page\nPreflight checks for Trino\nBefore\nrunning the Trino crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight check will be completed:\nCatalogs and schemas check\nâ\nâ\nCheck successful\nâ\nCheck failed for $missingObjectName\nTags:\nconnectors\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Trino?\nNext\nTroubleshooting Trino connectivity\nCatalogs and schemas check"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/provide-credentials-to-query-data",
    "content": "Use data\nInsights\nCredentials\nProvide credentials to query data\nOn this page\nProvide credentials to query data\nWho can do this?\nAny\nAtlan user\nwith\ndata access to the asset\nand their own credentials for the data store.\nWhen your admins have\nconfigured\nbring your own credentials\n(BYOC)\n, you must provide your own credentials to query data.\nDid you know?\nConnections that require you to provide your own user credentials have a small icon next to them. If the connection you want to query has no icon, you can query it with its default shared credentials. You only need to provide your own credentials for connections with this icon.\nSet up your own credentials\nâ\nAtlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports\nSSO authentication\n.\nTo set up your own credentials for a connection:\nFrom the left menu of any screen, click\nInsights\n.\nUnder the\nExplorer\ntab on the left, use the drop-down to select the connection that requires user credentials.\nA\nSetup user credentials for ...\ndialog will appear.\nIn the bottom right of the dialog, click\nGet started\n.\nThe options will vary slightly depending on the connection type. Refer to the set up step for your connection for more details of what to fill in for each:\nAmazon Athena\nAmazon Redshift\nDatabricks\nMySQL\nPostgreSQL\nPresto\nSnowflake\nTrino\nIn the bottom-left of the dialog, click the\nTest Authentication\nbutton.\nOnce successful, in the bottom right of the\nUser credential setup\ndialog, click\nDone\n.\nYou can then click\nDone\nagain.\nYou can now run queries using your own credentials! ð\nManage your credentials\nâ\nWho can do this?\nAny non-guest user who has previously set up their own credentials for a data store.\nTo manage your credentials for your connections:\nFrom the upper-right of any screen, click your username and then\nProfile\n.\nIn the sidebar that appears, click the\nUser credentials\n(bottom) icon.\nUnder\nUser credentials\n:\nTo remove a credential, hover over the row for the credential you want to delete and click the delete icon. When prompted for confirmation, click the\nDelete\nbutton.\nTo change a credential, click the row for the credential you want to change and follow the instructions above. Note that your existing credentials are not shown, and can only be overwritten. When complete, click the\nUpdate\nbutton in the lower right.\nTags:\nintegration\nconnectors\nPrevious\nHow to query without shared credentials\nNext\nWhat are the query builder actions?\nSet up your own credentials\nManage your credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/references/recommended-provider-package-versions",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nReferences\nProvider package versions for OpenLineage\nOn this page\nProvider package versions for OpenLineage\nThis document lists the minimum required versions for the most commonly used provider packages in Atlan.\nMinimum required provider package versions\nâ\nAtlan recommends using the latest version of each provider package, as it includes bug fixes, new features, and improvements to OpenLineage (OL) support. The minimum versions listed here guarantee that OpenLineage integration works as expected in Atlan.\nDid you know?\nEarlier versions may work but might lack recent fixes and features. Use the minimum version or later for full compatibility.\nProvider package\nMinimum version\nRelease date\nPyPI link\napache-airflow-providers-google\n14.0.0\n26 Feb 2025\nPyPI\napache-airflow-providers-amazon\n9.4.0\n26 Feb 2025\nPyPI\napache-airflow-providers-apache-spark\n5.0.1\n13 Mar 2025\nPyPI\napache-airflow-providers-snowflake\n6.1.0\n26 Feb 2025\nPyPI\napache-airflow-providers-dbt-cloud\n4.2.0\n26 Feb 2025\nPyPI\napache-airflow-providers-common-io\n1.5.1\n13 Mar 2025\nPyPI\napache-airflow-providers-ftp\n3.12.2\n26 Feb 2025\nPyPI\napache-airflow-providers-sftp\n5.1.0\n26 Feb 2025\nPyPI\napache-airflow-providers-apache-livy\n4.2.0\n26 Feb 2025\nPyPI\nTo check which operators have OpenLineage support, see the\nSupported classes â apache-airflow-providers-openlineage documentation\n. OpenLineage support can also be implemented for custom operators.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\napi\nrest-api\ngraphql\nPrevious\nPreflight checks for Apache Airflow\nNext\nTroubleshooting Apache Airflow/OpenLineage connectivity\nMinimum required provider package versions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/references/how-to-report-on-assets",
    "content": "Use data\nReporting\nReferences\nReport on assets\nOn this page\nReport on assets\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe assets dashboard in the reporting center helps you monitor your assets in Atlan. You can view a total count of all your assets as well as a summary of your assets by different categories, connectors,\ncustom metadata\n, and more.\nFilter assets\nâ\nYou can use a\nvariety of filters\nto customize your view of asset metrics. For example, you can even drill down to the schema level for a more granular view.\nTo filter assets at the schema level:\nFrom the left menu in Atlan, click\nReporting\nand then click\nAssets\n.\nUnder\nAssets\n, for the\nAll Asset Types\nfilter, select an asset type   -  for this example, we'll select\nTable\n.\nFor the\nAll Connectors/Connections\nfilter:\nSelect a connector   -  for this example, we'll select\nSnowflake\n.\nNext, select a connection   -  for this example, we'll select the\ndevelopment\nconnection.\nFor the\nAll Databases\nfilter, select a database   -  for this example, we'll select the\nDBT_FOOD_BEVERAGE\ndatabase.\nFor the\nAll Schemas\nfilter, select a schema   -  for this example, we'll select the\nPUBLIC\nschema.\n(Optional) To further\nrefine your search\n, click\nMore filters\n. For example, you can use the\nDomains\nfilter\nto filter assets by a single domain, multiple domains, or no domain   -  currently only supported for the assets dashboard.\n(Optional) Once the filtered assets are displayed, click any data point to view the assets in a sidebar.\n(Optional) In the top right of the sidebar, click the\nExport\nbutton to\nexport filtered assets\nto a spreadsheet.\nYou will now be able to view metrics for your filtered assets! ð\nReview enrichment completion\nâ\nThe\nAsset Enrichment\nsection displays a total count of assets grouped by type of metadata enrichment. You can also view the total count of assets that need to be updated   -  for example, a total count of assets without\nREADMEs\n.\nTo view assets without metadata enrichment:\nFrom the left menu in Atlan, click\nReporting\nand then click\nAssets\n.\nUnder\nAsset Enrichment\n, navigate to the card you want to view   -  for this example, we'll select the\nWith readme\ncard.\nIn the\nWith readme\ncard, click the assets remaining button to view all the assets without READMEs in a sidebar.\n(Optional) In the top right of the sidebar, click the\nExport\nbutton to\nexport assets without enrichment\nto a spreadsheet.\nWith this information in hand, you could\nplan a gamification drive\nto crowdsource documentation across your teams.\nDid you know?\nYou can also view a total count of assets with\nresources\nin the\nAsset Enrichment\nsection. To filter for assets without resources, use the\nHas resources\nfilter listed under\nProperties\nin the more filters menu.\nTrack lineage completion\nâ\nYou can track\nlineage\ncompletion for all your assets right from the assets dashboard.Â\nTo track lineage completion for your assets:\nFrom the left menu in Atlan, click\nReporting\nand then click\nAssets\n.\nIn the\nAssets\ndashboard, scroll down to the\nAssets with Lineage\nsection.\nTotal counts for lineage completion are reported in\n#\nnumbers by default. Under\nAssets with Lineage,\nclick the\n%\nicon to view total counts in percentage.\n(Optional) Click any asset type to view the assets in a sidebar.\n(Optional) In the top right of the sidebar, click the\nExport\nbutton to\nexport assets with lineage\nto a spreadsheet.\nDid you know?\nYou can also view metrics for assets that have data lineage using the\nHas lineage\nfilter listed under\nProperties\nin the filters menu.\nCustomize enrichment metrics\nâ\nThe enrichment tracker allows you to customize your view of enrichment metrics. For example, you can filter metrics for your assets within a custom date range.\nTo view asset enrichment metrics for a custom date range:\nFrom the left menu in Atlan, click\nReporting\nand then click\nAssets\n.\n(Optional) Under\nAssets\n, click the\nAll Connectors/Connections\nfilter to view enrichment metrics for a specific\nconnector\nor\nconnection\n.\nIn the\nAssets\ndashboard, scroll down to the\nEnrichment Tracker\nand click the date selector.\nFrom the date selector dropdown, click\nCustom range\n.\nTo the right of the date fields, click the\ncalendar\nicon\nand then select the dates for which you'd like to view enrichment metrics.\nAsset enrichment metrics will be displayed for your custom date range! ð\nYou can also view metadata and certificate updates over time for a custom date range and filter by connector or connection.\nDid you know?\nThe default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable.\nTags:\nintegration\nconnectors\nglossary\nbusiness-terms\ndefinitions\nPrevious\nSummarize metadata\nNext\nHow do I see views instead of materialized views in the reporting center?\nFilter assets\nReview enrichment completion\nTrack lineage completion\nCustomize enrichment metrics"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-automations",
    "content": "Use data\nReporting\nReport Types\nReport on automations\nOn this page\nReport on automations\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe automations dashboard in the reporting center\nprovides you with an overview of assets enriched using Atlan's automation features. You can view a summary of updates as well as top users for each feature.\nDid you know?\nThe default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable.\nSuggestions\nâ\nYou can track asset enrichment through\nsuggestions from similar assets\n. You can also view top users who have accepted automated suggestions.\nPlaybooks\nâ\nYou can monitor metrics related to\nplaybooks\n, such as a summary of updates, top users, a playbooks-runs-over-time graph, and run activity.\nTotal updates are reported in\n#\ncounts by default. Click the\n%\nicon to view total updates in percentage form.\nGoogle Sheets\nâ\nYou can view a summary of asset enriched through the\nAtlan add-on for Google Sheets\n.\nFor\nUpdates over time\n, click any metadata option in the graph to view updates only for that option   -  for example,\nAnnouncements\n.\nTags:\nintegration\nconnectors\nglossary\nbusiness-terms\ndefinitions\nPrevious\nReport on glossaries\nNext\nReport on queries\nSuggestions\nPlaybooks\nGoogle Sheets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/references/inventory-report-structure",
    "content": "Connect data\nStorage\nAmazon S3\nReferences\nS3 Inventory Report Structure\nOn this page\nS3 Inventory Report Structure\nThis reference outlines the expected folder layout and file format for\nAmazon S3 inventory reports\nused by Atlanâs S3 crawler when running\ninventory-based ingestion\n.\nimportant\nThe crawler supports a single destination bucket, with an optional prefix, to store all inventory reports from multiple source buckets.\nFolder structure\nâ\nTo enable successful\ninventory-based crawling\n, your destination bucket must follow this structure once inventory reports are generated:\nð¦ <destination-bucket>/\nâââ [ð <optional-prefix>/] (applies to all buckets)\nâ   âââ ð <source-bucket-1>/\nâ   â   âââ ð <inventory-config-1>/\nâ   â   â   âââ ð <YYYY-MM-DDTHH-MM-Z>/ (timestamp folders)\nâ   â   â   â   âââ ð manifest.json\nâ   â   â   âââ ð data/ (CSV or gzipped files)\nâ   â   â       âââ ð <inventory-file>.csv.gz\nâ   â   âââ ð <inventory-config-2>/\nâ   âââ ð <source-bucket-2>/\nâââ ...\nRequired\nComponent\nDescription\nExample\nâ\nDestination bucket\nSingle S3 bucket to store all inventory reports\natlan-inventory-reports\nâ\nPrefix\nFolder prefix to organize reports (same for all source buckets)\ninventory-reports\nâ\nSource bucket folder\nFolder named after each source bucket\nsource-bucket-1\nâ\nInventory config folder\nFolder named after the inventory configuration\ndaily-inventory\nâ\nTimestamp folder\nFolder with report generation timestamp\n2024-01-16T00-00Z\nâ\nManifest file\nmanifest.json\ncontaining report metadata\nmanifest.json\nâ\nData folder\nFolder containing compressed inventory files\ndata\nâ\nInventory files\n.csv.gz\nor\n.parquet\nfiles with actual data\ninventory-2024-01-16-00-00Z.csv.gz\nExamples\nâ\nð Basic structure (single source bucket)\nð¦ atlan-inventory-reports/\nâââ ð source-bucket-1/\nâââ ð inventory-config-1/\nâââ ð 2024-01-16T00-00Z/\nâ   âââ ð `manifest.json` {/* Required metadata file */}\nâââ ð data/\nâââ ð inventory-2024-01-16-00-00Z.csv.gz\nð Multiple source buckets\nð¦ atlan-inventory-reports/\nâââ ð source-bucket-1/\nâ   âââ ð inventory-config-1/\nâ       âââ ð 2024-01-16T00-00Z/\nâ       â   âââ ð `manifest.json` {/* Required metadata file */}\nâ       âââ ð data/\nâ           âââ ð inventory-2024-01-16-00-00Z.csv.gz\nâââ ð source-bucket-2/\nâââ ð inventory-config-1/\nâââ ð 2024-01-16T00-00Z/\nâ   âââ ð `manifest.json` {/* Required metadata file */}\nâââ ð data/\nâââ ð inventory-2024-01-16-00-00Z.csv.gz\nð With optional prefix\nð¦ atlan-inventory-reports/\nâââ ð inventory-reports/\nâââ ð source-bucket-1/\nâ   âââ ð inventory-config-1/\nâ       âââ ð 2024-01-16T00-00Z/\nâ       â   âââ ð `manifest.json` {/* Required metadata file */}\nâ       âââ ð data/\nâ           âââ ð inventory-2024-01-16-00-00Z.csv.gz\nâââ ð source-bucket-2/\nâââ ð inventory-config-1/\nâââ ð 2024-01-16T00-00Z/\nâ   âââ ð `manifest.json` {/* Required metadata file */}\nâââ ð data/\nâââ ð inventory-2024-01-16-00-00Z.csv.gz\nSee also\nâ\nSet up inventory reports for S3\n: Set up inventory reports for S3\nTags:\nconnectors\ns3\ninventory-reports\nPrevious\nCrawl S3 assets\nNext\nWhat does Atlan crawl from Amazon S3\nFolder structure\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/set-default-user-roles-for-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nGuides\nSet default user roles for SSO\nSet default user roles for SSO\nWho can do this?\nYou will need to be an admin user and\nconfigure SSO\nwith a provider first.\nAdmins can set default roles for new users joining\nthe Atlan workspace via SSO. Setting the default role to admin, member, or guest will provide the appropriate permissions to users as soon as they log into Atlan.\nTo set a default user role for SSO:\nIn the left menu from any screen in Atlan, click\nAdmin\n.\nUnder\nAdmin center\n, click\nSSO\n.Â\nOn the SSO page for your provider, under\nDefault Role\n, click the dropdown menu.\nFrom the dropdown menu, select\nGuest\n,\nMember\n, or\nAdmin\nas the default role for your users.\nTags:\nintegration\nconnectors\nPrevious\nLimit SSO automatically creating users when they log in\nNext\nSSO integration with PingFederate using SAML"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/set-up-a-private-network-link-to-amazon-athena",
    "content": "Connect data\nDatabases\nQuery Engines\nAmazon Athena\nGet Started\nSet up a private network link to Amazon Athena\nOn this page\nSet up a private network link to Amazon Athena\nWho can do this?\nYou will need your Amazon Athena or AWS administrator involved   -  you may not have access yourself to complete these steps.\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Amazon Athena and Atlan.\nRequest Atlan's details\nâ\nBefore configuring the connection, you will need the following:\nVPC endpoint ID of the Atlan VPC endpoint in the following format   -\nvpce-0d90d77d1be568544\n. This will be required to create the IAM policy.\nTo enter a hostname for\ncrawling Amazon Athena\n:\nIf\nprivate DNS hostnames are enabled\n, enter the default Athena endpoint in the following format   -\nhttps://athena.<region>.amazonaws.com\n-  and it will resolve to your VPC endpoint.\nIf privateÂ DNS hostnames are not enabled, enter the primary DNS name of the Atlan VPC endpoint in the following format   -\nvpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com\n-  as retrieved from Atlan support.\nRequest it from Atlan support\n.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions, follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"AllowAthenaListDataCatalog\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"athena:ListDataCatalogs\"\n]\n,\n\"Resource\"\n:\n\"*\"\n,\n\"Condition\"\n:\n{\n\"StringEquals\"\n:\n{\n\"aws:SourceVpce\"\n:\n[\n\"<vpce-endpoint-id>\"\n]\n}\n}\n}\n,\n{\n\"Sid\"\n:\n\"AllowAthenaActions\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"athena:StartQueryExecution\"\n,\n\"athena:GetQueryResults\"\n,\n\"athena:DeleteNamedQuery\"\n,\n\"athena:GetNamedQuery\"\n,\n\"athena:ListQueryExecutions\"\n,\n\"athena:StopQueryExecution\"\n,\n\"athena:GetQueryResultsStream\"\n,\n\"athena:ListNamedQueries\"\n,\n\"athena:CreateNamedQuery\"\n,\n\"athena:GetQueryExecution\"\n,\n\"athena:BatchGetNamedQuery\"\n,\n\"athena:BatchGetQueryExecution\"\n,\n\"athena:GetWorkGroup\"\n,\n\"athena:GetTableMetadata\"\n,\n\"athena:GetDatabase\"\n,\n\"athena:GetDataCatalog\"\n,\n\"athena:ListDatabases\"\n,\n\"athena:ListTableMetadata\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:athena:us-east-2:666568140392:datacatalog/*\"\n,\n\"arn:aws:athena:us-east-2:666568140392:workgroup/*\"\n]\n,\n\"Condition\"\n:\n{\n\"StringEquals\"\n:\n{\n\"aws:SourceVpce\"\n:\n[\n\"<vpce-endpoint-id>\"\n]\n}\n}\n}\n,\n{\n\"Sid\"\n:\n\"AllowGlueActionsViaAthena\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"glue:GetDatabase\"\n,\n\"glue:GetDatabases\"\n,\n\"glue:CreateDatabase\"\n,\n\"glue:GetTables\"\n,\n\"glue:GetTable\"\n,\n\"glue:SearchTables\"\n,\n\"glue:GetTableVersions\"\n,\n\"glue:GetTableVersion\"\n,\n\"glue:GetPartition\"\n,\n\"glue:GetPartitions\"\n,\n\"glue:GetUserDefinedFunctions\"\n,\n\"glue:GetUserDefinedFunction\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:glue:us-east-2:666568140392:tableVersion/*/*/*\"\n,\n\"arn:aws:glue:us-east-2:666568140392:catalog\"\n,\n\"arn:aws:glue:us-east-2:666568140392:table/*/*\"\n,\n\"arn:aws:glue:us-east-2:666568140392:database/*\"\n]\n,\n\"Condition\"\n:\n{\n\"ForAnyValue:StringEquals\"\n:\n{\n\"aws:CalledVia\"\n:\n[\n\"athena.amazonaws.com\"\n]\n}\n}\n}\n,\n{\n\"Sid\"\n:\n\"AllowS3ActionsOnDataViaAthena\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:GetBucketLocation\"\n,\n\"s3:ListBucket\"\n,\n\"s3:GetObject\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::demo-wide-world-importers\"\n,\n\"arn:aws:s3:::demo-wide-world-importers/*\"\n]\n,\n\"Condition\"\n:\n{\n\"ForAnyValue:StringEquals\"\n:\n{\n\"aws:CalledVia\"\n:\n[\n\"athena.amazonaws.com\"\n]\n}\n}\n}\n,\n{\n\"Sid\"\n:\n\"AllowS3ActionsOnMetadataViaAthena\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:GetBucketLocation\"\n,\n\"s3:GetObject\"\n,\n\"s3:ListBucket\"\n,\n\"s3:ListBucketMultipartUploads\"\n,\n\"s3:ListMultipartUploadParts\"\n,\n\"s3:AbortMultipartUpload\"\n,\n\"s3:CreateBucket\"\n,\n\"s3:PutObject\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::source-curation-athena-metadata\"\n,\n\"arn:aws:s3:::source-curation-athena-metadata/*\"\n]\n,\n\"Condition\"\n:\n{\n\"ForAnyValue:StringEquals\"\n:\n{\n\"aws:CalledVia\"\n:\n[\n\"athena.amazonaws.com\"\n]\n}\n}\n}\n]\n}\nReplace\n<vpce-endpoint-id>\nwith the\nVPC endpoint ID received from Atlan support\n.\nAttach this policy to the IAM user or role used for authentication. For more information, see\nChoose authentication mechanism\nor create a new IAM user by following the steps in the\nCreate an IAM user\nsection.\nCreate an IAM user\nâ\nCreate an AWS IAM user and attach the policy created above to this user.\nTo create an AWS IAM user:\nFollow\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn the\nSet permissions\npage, attach the policy created in the previous step to this user.\nRefer to\nmanaging access keys for IAM users\nto create an access key for the new user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nThe connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to\ncrawl Amazon Athena\nin Atlan! ð\nTags:\nconnectors\ndata\ncrawl\nPrevious\nSet up Amazon Athena\nNext\nCrawl Amazon Athena\nRequest Atlan's details\nCreate IAM policy\nCreate an IAM user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/set-up-a-private-network-link-to-hive",
    "content": "Connect data\nDatabases\nQuery Engines\nHive\nGet Started\nSet up a private network link to Hive\nOn this page\nSet up a private network link to Hive\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Hive and Atlan.\nWho can do this?\nYou will need your AWS administrator involved   -  you may not have access to run these tasks yourself.\nPrerequisites\nâ\nYou should already have the following:\nHive instance running in AWS (private EMR instance).\nAtlan hosted in the same region as the Hive instance.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from support\n.\nSet up network to EMR instance\nâ\nTo set up the private network of your Hive EMR instance, from within\nAWS\n:\nCopy network settings\nâ\nTo copy the network settings of your Hive EMR instance:\nFrom the left menu, under\nEMR on EC2\n, click\nClusters\n.\nIn the\nClusters\ntable, click on your Hive EMR cluster.\nFrom the cluster's\nNetwork and security\ntab, under\nNetwork\n, for\nVirtual Private Cloud (VPC)\n, click on your VPC to view more details.\nUnder your VPC's\nDetails\ntab, copy and save the value under the\nIPv4 CIDR\ncolumn.\nCreate inbound rule\nâ\nTo create an inbound rule allowing your VPC access to your Hive EMR instance:\nFrom the left menu, under\nEMR on EC2\n, click\nClusters\n.\nIn the\nClusters\ntable, click on your Hive EMR cluster.\nFrom the cluster's\nNetwork and security\ntab, click the downward arrow for\nEC2 security groups (firewall)\nto expand this section.\nUnder\nEC2 security groups (firewall)\n, click on a security group for the cluster.\nUnder the\nInbound rules\ntab, click the\nEdit inbound rules\nbutton.\nAt the bottom left of the\nInbound rules\ntable, click the\nAdd rule\nbutton.\nFor\nType\n,Â select\nAll traffic\n.\nFor\nPort\n,Â enter the port on which Hive is accessible.\nFor\nSource\n, choose\nCustom\nand enter the CIDR range for your Hive instance (see\nCopy network settings\n).\nBelow the bottom right of the\nInbound rules\ntable, click the\nSave rules\nbutton. Repeat steps 4 to 7 for each security group in the cluster.\nCreate internal Network Load Balancer\nâ\nStart creating NLB\nâ\nTo create an NLB, from within AWS:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nLoad Balancing\n, click on\nLoad Balancers\n.\nAt the top of the screen, click the\nCreate Load Balancer\nbutton.\nUnder the\nNetwork Load Balancer\noption, click the\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name,\nÂ enter a unique name.\nFor\nScheme\n, select\nInternal\n.\nFor\nIP address type\n, select\nIPv4\n.\nEnter the following\nNetwork mapping\nsettings for the load balancer:\nFor\nVPC\n, select the VPC where the Hive instance is located (see\nCopy network settings\n).\nFor\nMappings\n, select the availability zones with private subnets.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nPort\n, enter theÂ port value used in\nCreated inbound rule\n.\nFor\nDefault action\n, click the\nCreate target group\nlink. This will open the target group creation in a new browser tab.\nCreate target group\nâ\nTo create a target group for the NLB:\nEnter the following\nBasic configuration\nsettings for the target group:\nFor\nChoose target type\n, select\nInstances\n.\nFor\nTarget group name\n, enter a name.\nFor\nPort\n, enter the port value used in\nCreate inbound rule\n.\nFor\nVPC\n, select the VPC where the Hive instance is located (see\nCopy network settings\n).\nAt the bottom of the form, click the\nNext\nbutton.\nFrom the\nAvailable instances\ntable:\nClick the checkbox next to your Hive instance.\nEnter the port for the port value used in steps above.\nClick the\nInclude as pending below\nbutton.\nAt the bottom right of the form, click the\nCreate target group\nbutton.\nFinish creating NLB\nâ\nReturn to the browser tab where you started the NLB creation, and continue:\nUnder\nListeners and routing\n, click the refresh arrow to the far right of the\nDefault action\ndrop-down box.\nSelect the target group you created above in the\nDefault action\ndrop-down.\nAt the bottom right of the form click the\nCreate load balancer\nbutton.\nIn the resulting screen, click the\nView load balancer\nbutton.\nVerify target group is healthy\nâ\nTo verify the target group is healthy:\nFrom the EC2 menu on the left, under\nLoad Balancing\nclick\nTarget Groups\n.\nFrom the\nTarget groups\ntable, click the row for the target group you created above.\nAt the bottom of the screen, under the\nDetails\ntab, check that there is a 1 under both\nTotal targets\nand\nHealthy\n.\nCreate endpoint service\nâ\nTo create an endpoint service, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nAt the top of the page, click the\nCreate endpoint service\nbutton.\nEnter the following\nEndpoint service settings\n:\nFor\nName\n, enter a meaningful name.\nFor\nLoad balancer type\n, choose\nNetwork\n.\nFor\nAvailable load balancers\n, select the load balancer you created above in\nCreate internal Network Load Balancer\n.\nEnter the following\nAdditional settings\n:\nFor\nRequire acceptance for endpoint\n, enable\nAcceptance required\n.\nFor\nSupported IP address types\n, enable\nIPv4\n.\nAt the bottom right of the form, click the\nCreate\nbutton.\nUnder the\nDetails\nof the endpoint service, copy the hostname under\nService name\n.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of the\nAllow principals\ntable, click the\nAllow principals\nbutton.\nUnder\nPrincipals to add\nand\nARN\n, enter the Atlan account ID.\nAt the bottom right of the form, click the\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all the above steps are complete,\nprovide Atlan support\nwith the following information:\nThe hostname for the endpoint service created above.\nThe port number for your Hive instance.\nThere are additional steps Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within AWS:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nFrom the\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to the\nEndpoint connections\ntab.\nYou should see a row in the\nEndpoint connections\ntable with a\nState\nof\nPending\n.\nSelect this row, and click the\nActions\nbutton and then\nAccept endpoint connection request\n.\nIf prompted to confirm, type\naccept\ninto the field and click the\nAccept\nbutton.\nWait for this to complete, it could take about 30 seconds.\nð The connection is now established. You can now use the service endpoint provided by Atlan support as the hostname to\ncrawl Hive\nin Atlan! ð\nTags:\nintegration\nconnectors\nPrevious\nSet up Hive\nNext\nCrawl Hive\nPrerequisites\nSet up network to EMR instance\nCreate internal Network Load Balancer\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/set-up-a-private-network-link-to-trino",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nPrivate Network\nSet up a private network link to Trino\nOn this page\nSet up a private network link to Trino\nWho can do this?\nYou will need your AWS administrator involved   -  you may not have access to run these tasks yourself.\nAWS PrivateLink\ncreates a secure, private connection between services running in AWS. This document describes the steps to set this up between Trino and Atlan.\nPrerequisites\nâ\nYou should already have the following:\nTrino instance running in AWS (private EC2 instance).\nAtlan hosted in the same region as the Trino instance.\nDid you know?\nYou will also need Atlan's AWS account ID later in this process. If you do not already have this,\nrequest it now from support\n.\nSet up network to EC2 instance\nâ\nTo set up the private network of your Trino EC2 instance, from within\nAWS\n:\nCopy network settings\nâ\nTo copy the network settings of yourÂ EC2 instance:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nInstances\n, click\nInstances\n.\nIn the\nInstances\ntable, click your Trino EC2 instance.\nUnder the instance's\nDetails\ntab:\nUnder\nVPC ID\n, copy the VPC identifier.\nUnder\nSubnet ID\n, click the subnet for the instance.\nIn the\nSubnets\ntable, copy the value under the\nIPv4 CIDR\ncolumn.\nCreate inbound rule\nâ\nTo create an inbound rule allowing your private subnet access to your EC2 instance:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nInstances\n, click\nInstances\n.\nIn the\nInstances\ntable, click your Trino EC2 instance.\nUnder the instance's details, change to the\nSecurity\ntab.\nUnder\nSecurity groups\n, click the security group for the instance.\nUnder the\nInbound rules\ntab, click the\nEdit inbound rules\nbutton.\nAt the bottom left of the\nInbound rules\ntable, click the\nAdd rule\nbutton.\nFor\nType\n, select\nCustom TCP\n.\nFor\nPort range\n, enter the port on which Trino is accessible (for example,\n80\n).\nFor\nSource\n, choose\nCustom\nand enter the CIDR range for your Trino instance (see\nCopy network settings\n).\nBelow the bottom right of the\nInbound rules\ntable, click the\nSave rules\nbutton.\nCreate internal Network Load Balancer\nâ\nStart creating NLB\nâ\nTo create an NLB, from within AWS:\nNavigate to\nServices\n, then\nCompute\n, then\nEC2\n.\nOn the left, under\nLoad Balancing\n, click on\nLoad Balancers\n.\nAt the top of the screen, click the\nCreate Load Balancer\nbutton.\nUnder the\nNetwork Load Balancer\noption, click the\nCreate\nbutton.\nEnter the following\nBasic configuration\nsettings for the load balancer:\nFor\nLoad balancer name\n, enter a unique name.\nFor\nScheme\n, select\nInternal\n.\nFor\nIP address type\n, select\nIPv4\n.\nEnter the following\nNetwork mapping\nsettings for the load balancer:\nFor\nVPC\n, select the VPC where the Trino instance is located (see\nCopy network settings\n).\nFor\nMappings\n, select the availability zones with private subnets.\nEnter the following\nListeners and routing\nsettings for the load balancer:\nFor\nPort\n, enter\n80\n(or the non-default port value used in\nCreated inbound rule\n).\nFor\nDefault action\n, click the\nCreate target group\nlink. This will open the target group creation in a new browser tab.\nCreate target group\nâ\nTo create a target group for the NLB:\nEnter the following\nBasic configuration\nsettings for the target group:\nFor\nChoose target type\n, select\nInstances\n.\nFor\nTarget group name\n, enter a name.\nFor\nPort\n, enter\n80\n(or the non-default port value used in\nCreate inbound rule\n).\nFor\nVPC\n, select the VPC where the Trino instance is located (see\nCopy network settings\n).\nAt the bottom of the form, click the\nNext\nbutton.\nFrom the\nAvailable instances\ntable:\nClick the checkbox next to your Trino instance.\nEnter the port for the instance (80 or non-default value used in steps above).\nClick the\nInclude as pending below\nbutton.\nAt the bottom right of the form, click the\nCreate target group\nbutton.\nFinish creating NLB\nâ\nReturn to the browser tab where you started the NLB creation, and continue:\nUnder\nListeners and routing\n, click the refresh arrow to the far right of the\nDefault action\ndrop-down box.\nSelect the target group you created above in the\nDefault action\ndrop-down.\nAt the bottom right of the form click the\nCreate load balancer\nbutton.\nIn the resulting screen, click the\nView load balancer\nbutton.\nVerify target group is healthy\nâ\nTo verify the target group is healthy:\nFrom the EC2 menu on the left, under\nLoad Balancing\nclick\nTarget Groups\n.\nFrom the\nTarget groups\ntable, click the row for the target group you created above.\nAt the bottom of the screen, under the\nDetails\ntab, check that there is a 1 under both\nTotal targets\nand\nHealthy\n.\nCreate endpoint service\nâ\nTo create an endpoint service, from within\nAWS\n:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\nclick\nEndpoint services\n.\nAt the top of the page, click the\nCreate endpoint service\nbutton.\nEnter the following\nEndpoint service settings\n:\nFor\nName\nenter a meaningful name.\nFor\nLoad balancer type\nchoose\nNetwork\n.\nFor\nAvailable load balancers\nselect the load balancer you created above in\nCreate internal Network Load Balancer\n.\nEnter the following\nAdditional settings\n:\nFor\nRequire acceptance for endpoint\nenable\nAcceptance required\n.\nFor\nSupported IP address types\nenable\nIPv4\n.\nAt the bottom right of the form, click the\nCreate\nbutton.\nUnder the\nDetails\nof the endpoint service, copy the hostname under\nService name\n.\nAllow Atlan account access\nâ\nTo allow Atlan's account access to the service, from within the endpoint service screen:\nAt the bottom of the screen, change to the\nAllow principals\ntab.\nAt the top of the\nAllow principals\ntable, click the\nAllow principals\nbutton.\nUnder\nPrincipals to add\nand\nARN\nenter the Atlan account ID.\nAt the bottom right of the form, click the\nAllow principals\nbutton.\nNotify Atlan support\nâ\nOnce all the above steps are complete,\nprovide Atlan support\nwith the following information:\nThe hostname for the endpoint service created above.\nThe port number for the Trino instance.\nThere are additional steps Atlan then needs to complete:\nCreating a security group.\nCreating an endpoint.\nOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.\nAccept the consumer connection request\nâ\nTo accept the consumer connection request, from within AWS:\nNavigate to\nServices\n, then\nNetworking & Content Delivery\n, then\nVPC\n.\nFrom the menu on the left, under\nVirtual private cloud\n, click\nEndpoint services\n.\nFrom the\nEndpoint services\ntable, select the endpoint service you created in\nCreate endpoint service\n.\nAt the bottom of the screen, change to the\nEndpoint connections\ntab.\nYou should see a row in the\nEndpoint connections\ntable with a\nState\nof\nPending\n.\nSelect this row, and click the\nActions\nbutton and then\nAccept endpoint connection request\n.\nIf prompted to confirm, type\naccept\ninto the field and click the\nAccept\nbutton.\nWait for this to complete, it could take about 30 seconds.\nð The connection is now established. You can now use the service endpoint provided by Atlan support as the hostname to\ncrawl Trino\nin Atlan! ð\nTags:\nintegration\nconnectors\nPrevious\nCrawl Trino\nNext\nWhat does Atlan crawl from Trino?\nPrerequisites\nSet up network to EC2 instance\nCreate internal Network Load Balancer\nCreate endpoint service\nAllow Atlan account access\nNotify Atlan support\nAccept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka",
    "content": "Connect data\nEvent/Messaging\nAiven Kafka\nGet Started\nSet up Aiven Kafka\nOn this page\nSet up Aiven Kafka\nWho can do this?\nYou will probably need your\nAiven Kafka administrator\nto complete these steps   -  you may not have access yourself.\nAtlan supports the\nS3 extraction method\nfor fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nCreate user in Aiven Kafka\nâ\nTo\ncreate a new user\nfor\nextracting metadata from Aiven Kafka\n:\nLog in to your\nAiven console\nand select your active cluster.\nFrom the upper right of the cluster\nOverview\npage, click the\nUsers\ntab to create a new user:\nFor\nCreate a service user\n, under\nUsername\n, enter a name for the new user and then click\nAdd service user\n.\nThe new user will be listed under\nService users\n,Â on the\nUsers\npage. Copy the username and password for the new user and store them in a secure location.\n(Optional) If using\nclient certificate authentication\n, copy the access key and access certificate and store them in a secure location.\nFrom the upper right of the cluster\nOverview\npage, click the\nAccess Control List (ACL)\ntab to add a new ACL grant:\nUnder\nAccess Control List (ACL)\n, for\nACL Type\n, click\nACL For Topic\n.\nFor\nAdd access control entry\n, enter the following details:\nFor\nUsername\n, enter the username you created for the new user.\nFor\nTopic\n, enter an asterisk\n*\nto include all topics.\nFrom the\nPermission\ndropdown, select\nAdmin\n-  learn more about\nACL permission mapping\n.\nClick\nAdd entry\nto save your selections.\nNavigate to the\nOverview\ntab, copy or download the\nCA Certificate\nand store the details in a secure location.\nDid you know?\nOnce you have\nextracted metadata on-premises\nand\nuploaded the results to S3\n, you can\ncrawl the metadata from Aiven Kafka\ninto Atlan.\nTags:\nconnectors\ndata\nPrevious\nAiven Kafka\nNext\nCrawl Aiven Kafka\nCreate user in Aiven Kafka"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/how-tos/integrate-alteryx",
    "content": "Connect data\nETL Tools\nAlteryx\nGet Started\nSet up Alteryx\nOn this page\nSet up Alteryx\nPrivate preview\nSet up real-time integration between Alteryx and Atlan using OpenLineage. This integration automatically catalogs assets and creates lineage in Atlan whenever workflows run in Alteryx, providing immediate visibility into your ETL processes.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nYou have\nAdmin\nor\nWorkflow Admin\npermissions to create workflows and generate API tokens.\nYou have\nAdministrator\naccess to configure OpenLineage integration.\nAn\nAPI token\nfor authentication.\nCreate workflow in Atlan\nâ\nFollow these steps to create an Alteryx listener workflow that receives OpenLineage events:\nNavigate to the\nWorkflow\nsection.\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the filters along the top, click\nOrchestrator\n.\nFrom the list of packages, select\nAlteryx\nand then click\nCreate Listener\n.\nConfigure the connection\nâ\nImportant\nA single connection (namespace) must be used for only one Alteryx instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior.\nYou only need to configure the connection once to enable Atlan to receive incoming OpenLineage events.\nAfter setup, Atlan automatically processes these events as Alteryx workflows run, enabling seamless cataloging of Alteryx assets.\nTo configure the Alteryx connection in Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment.\nFor example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\nImportant\nIf no user or group is specified, the connection can't be managed, not even by admins.\nClick\nCreate connection\nat the bottom of the screen to save and activate it.\nConfigure the integration in Alteryx\nâ\nDid you know?\nYou need the Atlan API token and connection name to configure the integration in Alteryx. Contact your account manager to enable Alteryx data lineage on the Alteryx side. This enables Alteryx to connect with the OpenLineage API and send events to Atlan.\nAn Alteryx administrator configures the OpenLineage integration using:\nNavigate to the OpenLineage configuration settings in your Alteryx environment.\nPaste the\nAPI token you generated\ninto the\nAPI Token\nfield.\nEnter the\nconnection name\nyou used when creating the connection in Atlan (for example,\nproduction\n,\nanalytics\n).\nAdd your\nAtlan Hostname\nin the format\nhttps://your-workspace.atlan.com\nand click\nSave\n.\nVerify the connection\nâ\nTo verify that the integration is working correctly:\nRun a workflow in your Alteryx instance to trigger OpenLineage events.\nOpen Atlan and check for newly created Alteryx assets under the configured connection.\nConfirm that lineage information from the workflow appears in the asset view.\nOpen the\nEvent Logs\nin Atlan to review the OpenLineage events and verify successful ingestion.\nAtlan validates the existence of the corresponding Alteryx connection by checking that the connection name matches and the API token is valid.\nOnce the workflows finish running in Alteryx, Alteryx workflows and lineage generated from OpenLineage events appear automatically in Atlan.\nSee also\nâ\nAPI authentication\nTags:\nconnectors\netl-tools\nalteryx\nworkflow\nPrevious\nAlteryx\nNext\nWhat does Atlan crawl from Alteryx?\nPrerequisites\nCreate workflow in Atlan\nConfigure the connection\nConfigure the integration in Alteryx\nVerify the connection\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nAmazon DynamoDB\nGet Started\nSet up Amazon DynamoDB\nOn this page\nSet up Amazon DynamoDB\nwarning\nð¤ Who can do this?\nYou will probably need your Amazon DynamoDB administrator to run these commands   -  you may not have access yourself.\nAtlan supports the following authentication methods for fetching metadata from Amazon DynamoDB:\nIAM user authentication\n-  this method uses an AWS access key, secret key, and region to fetch metadata.\nIAM role authentication\n-  this method uses an AWS role ARN and region to fetch metadata.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions, follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"dynamodb:ListTables\"\n]\n,\n\"Resource\"\n:\n\"*\"\n}\n,\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"dynamodb:DescribeTable\"\n]\n,\n\"Resource\"\n:\n\"arn:aws:dynamodb:<region>:<account_id>:table/*\"\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Amazon DynamoDB instance.\nReplace\n<account_id>\nwith your AWS account ID.\nIAM permissions\nâ\nAtlan requires the following permissions:\ndynamodb:ListTables\n:\nFetches a list of your Amazon DynamoDB tables. This permission is used during the metadata extraction process to dynamically determine a list of tables.\nNote that this action does not support resource-level permissions and requires you to choose all resources, hence\n*\nfor\nResource\n.\ndynamodb:DescribeTable\n:\nFetches metadata for extracted tables. This action supports resource-level permissions, so for\nResource\n, you can either:Â\nGrant permission to all tables in the region for which you want to extract metadata:\narn:aws:dynamodb:<region>:<account_id>:table/*\nSpecify the table names for which you want to extract metadata:\narn:aws:dynamodb:<region>:<account_id>:table/table_name_1\n,\narn:aws:dynamodb:<region>:<account_id>:table/table_name_2\nChoose authentication mechanism\nâ\nUsing the\npolicy created above\n, configure one of the following options for authentication.\nUser-based authentication\nâ\nTo configure IAM user-based authentication:\nCreate an AWS IAM user by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn theÂ\nSet permissions\npage,\nattach the policy\ncreated in the previous step to this user.\nRefer to\nmanaging access keys for IAM users\nto create an access key for the new user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nRole delegation-based authentication\nâ\nTo configure role delegation-based authentication:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new role in your AWS account by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies,\nattach the policy\ncreated in the previous step to this role.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n}\n}\n]\n}\n(Optional) To use an\nexternal ID\nfor additional security, paste the external ID into the policy:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n\"StringEquals\"\n:\n{\n\"sts:ExternalId\"\n:\n\"<atlan_external_id>\"\n}\n}\n}\n]\n}\nReplace\n<atlan_external_id>\nwith the external ID you want to use.\nNow,\nreach out to Atlan support\nwith:\nThe name of the role you created above.\nThe ID of the AWS account where the role was created.\ndanger\nWait until the support team confirms the account is allowlisted to assume the role before running the crawler.\nTags:\nconnectors\ndata\nauthentication\nPrevious\nAmazon DynamoDB\nNext\nCrawl Amazon DynamoDB\nCreate IAM policy\nIAM permissions\nChoose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/set-up-s3",
    "content": "Connect data\nStorage\nAmazon S3\nGet Started\nSet up Amazon S3\nOn this page\nSet up Amazon S3\nThis guide walks you through creating IAM permissions and authentication credentials to allow Atlan to catalog your S3 buckets and objects.\nwarning\nThis integration catalogs only S3 buckets and objects. It doesn't support data lineage.\nPrerequisites\nâ\nBefore you begin:\nSet up S3 inventory reports\n, required only if you plan to use\ninventory-based ingestion\n.\nPermissions required\nâ\nTo complete this setup, you'll need:\nAWS Administrator access to create IAM policies and users/roles in AWS Management Console\nAtlan workflow access to configure connectors and workflows in Atlan\nAccess to configure S3 inventory reports only if you plan to use\ninventory ingestion\nCreate IAM policy\nâ\nChoose the appropriate policy depending on your ingestion method.\nDirect ingestion\nInventory ingestion\nIn AWS, go to\nIAM â Policies\nClick\nCreate policy\nSelect the\nJSON\ntab and paste:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"AllowAccessToBuckets\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:GetBucketLocation\"\n,\n\"s3:ListAllMyBuckets\"\n,\n\"s3:ListBucket\"\n,\n\"s3:GetObject\"\n,\n\"s3:GetEncryptionConfiguration\"\n,\n\"s3:GetBucketVersioning\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::<s3_bucket_1>\"\n,\n\"arn:aws:s3:::<s3_bucket_1>/*\"\n,\n\"arn:aws:s3:::<s3_bucket_2>\"\n,\n\"arn:aws:s3:::<s3_bucket_2>/*\"\n]\n}\n]\n}\nReplace\n<s3_bucket>\nwith your actual bucket name or pattern.\nClick\nNext\n, name your policy (e.g.\nAtlanS3CrawlerDirectPolicy\n), and create it.\nIn AWS, go to\nIAM â Policies\nClick\nCreate policy\nSelect the\nJSON\ntab and paste:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"AllowInventoryAccess\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"s3:ListBucket\"\n,\n\"s3:GetObject\"\n,\n\"s3:SelectObjectContent\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:s3:::<s3_bucket>\"\n,\n\"arn:aws:s3:::<s3_bucket>/*\"\n]\n}\n]\n}\nReplace\n<s3_bucket>\nwith your actual bucket name or pattern.\nClick\nNext\n, name your policy (e.g.\nAtlanS3CrawlerInventoryPolicy\n), and create it.\nSet up authentication\nâ\nChoose between IAM user (simpler) and IAM role (more secure and recommended for production).\nIAM user\nIAM role\nIn AWS, go to\nIAM â Users\nClick\nAdd users\n, give a name (e.g.\natlan-s3-crawler\n)\nSelect\nAttach policies directly\nand choose the policy you just created\nComplete the steps and create an\naccess key\nSave the\nAccess Key ID\nand\nSecret Access Key\nâ you'll need them in Atlan\nContact Atlan support for the\nNode Instance Role ARN\nof your Atlan EKS cluster\nIn AWS, go to\nIAM â Roles\nâ\nCreate role\nSelect\nTrusted entity type: AWS account\nEnter Atlanâs AWS account ID (available via support)\nAttach the policy you created earlier\nName the role (e.g.\nAtlanS3CrawlerRole\n) and create it\nEdit the trust relationship with this policy:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n}\n]\n}\nShare the\nrole name\nand\nAWS account ID\nwith Atlan support\nOnce Atlan confirms access, copy the Role ARN (e.g.\narn:aws:iam::<account-id>:role/<role-name>\n) for use in the workflow\nwarning\nWait for confirmation from Atlan before proceeding to workflow configuration.\nNeed help?\nâ\nCheck\nAWS IAM documentation\nfor detailed reference\nContact Atlan support\nfor help with setup or integration\nNext steps\nâ\nCrawl S3 assets\n: Configure your workflow and crawl S3 assets.\nTags:\nconnectors\ndata\ncrawl\nstorage\namazon-s3\naws\nPrevious\nAmazon S3\nNext\nSet up Inventory reports\nPrerequisites\nPermissions required\nCreate IAM policy\nSet up authentication\nNeed help?\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/how-tos/set-up-anomalo",
    "content": "Connect data\nData Quality & Observability\nAnomalo\nGet Started\nSet up Anomalo\nOn this page\nSet up Anomalo\nAtlan supports the API authentication method for fetching metadata from\nAnomalo\n. This method uses an API key to fetch metadata.\nYour Anomalo\nDeployment Admin Superuser\nmust also\nconfigure an Atlan integration in your Anomalo deployment\nto send events to Atlan when your checks run in Anomalo. This will update the check metadata in Atlan in real time. This configuration is required only after you have completed\nintegrating Anomalo in Atlan\n. You will need your Atlan hostname and an\nAPI token generated in Atlan\n.\nGenerate an API key\nâ\nWho can do this?\nYou must at least have an Anomalo\nViewer\nrole to\ngenerate an API key\n. Atlan will require read-only access to your connected data sources in Anomalo.\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the objects in your Anomalo environment.\nYou will need to create an API key in Anomalo for integrating with Atlan.\nTo create an API key for\ncrawling Anomalo\n:\nLog in to your Anomalo instance.\nFrom the left menu of your Anomalo instance, click\nSettings\n.\nOn the\nSettings\npage, in the\nAccount\ntab, change to the\nAPI keys\ntab.\nOn the API keys page, to generate a new API key:\nIf you have existing API keys, click the\nAdd an API key\nbutton.\nIf you do not have any API keys, click the\nCreate an API key\nbutton.\nIn the\nNew API Key\nÂ dialog, enter the following details:\nFor\nDescription\n, add a meaningful description for your API key   -  for example,\nAtlan connection\n.\nFor\nExpiration\n, keep the default selection or select a preferred option.\nClick\nSave\nto finish creating the API key.\nFrom the corresponding screen, copy the\nAPI\nKey\nvalue and store it in a secure location.\ndanger\nThe API key cannot be retrieved later. You must copy the key value before closing the dialog box.\nTags:\nconnectors\ndata\nintegration\napi\nauthentication\nconfiguration\nPrevious\nAnomalo\nNext\nHow to integrate Anomalo\nGenerate an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue",
    "content": "Connect data\nETL Tools\nAWS Glue\nGet Started\nSet up AWS Glue\nOn this page\nSet up AWS Glue\nwarning\nð¤ Who can do this?\nYou will need your AWS Glue Data Catalog administrator to run these commands   -  you may not have access yourself.\nDid you know?\nPrefixing all resources created for Atlan with\natlan-\nwill help you better identify them. You should also add AWS tags and descriptions to these resources for later reference.\nAtlan supports fetching metadata from\nAWS Glue Data Catalog\n. If you also want to be able to preview and query the data, you can\nset up an Amazon Athena connection\ninstead.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Sid\"\n:\n\"VisualEditor0\"\n,\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"glue:GetTables\"\n,\n\"glue:GetDatabases\"\n,\n\"glue:GetTable\"\n,\n\"glue:GetDatabase\"\n,\n\"glue:SearchTables\"\n,\n\"glue:GetTableVersions\"\n,\n\"glue:GetTableVersion\"\n,\n\"glue:GetPartition\"\n,\n\"glue:GetPartitions\"\n,\n\"glue:GetUserDefinedFunctions\"\n,\n\"glue:GetUserDefinedFunction\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:glue:<region>:<account_id>:tableVersion/*/*/*\"\n,\n\"arn:aws:glue:<region>:<account_id>:table/*/*\"\n,\n\"arn:aws:glue:<region>:<account_id>:catalog\"\n,\n\"arn:aws:glue:<region>:<account_id>:database/*\"\n]\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Glue instance.\nReplace\n<account_id>\nÂ with your account ID.\ndanger\nIf you're using AWS Lake Formation to manage access to your AWS resources, you will need to\ngrant permissions in AWS Lake Formation\nas well as to the objects you want to crawl.\nChoose authentication mechanism\nâ\nUsing the policy created above, configure one of the following options for authentication.\nUser-based authentication\nâ\nTo configure user-based authentication:\nCreate an AWS IAM user by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn theÂ\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nRole delegation-based authentication\nâ\nTo configure role delegation-based authentication:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new role in your AWS account by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies, attach the policy created in the previous step to this role.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n}\n}\n]\n}\n(Optional) To use an\nexternal ID\nfor additional security:\nGenerate the external ID within Atlan\n.\nPaste the external ID into the policy (replace\n<atlan_generated_external_id>\nwith it):\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n\"StringEquals\"\n:\n{\n\"sts:ExternalId\"\n:\n\"<atlan_generated_external_id>\"\n}\n}\n}\n]\n}\nNow,\nreach out to Atlan support\nwith:\nThe name of the role you created above.\nThe ID of the AWS account where the role was created.\ndanger\nWait until the support team confirms the account is allowlisted to assume the role before running the crawler.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nAWS Glue\nNext\nCrawl AWS Glue\nCreate IAM policy\nChoose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/how-tos/set-up-bigid",
    "content": "Connect data\nPrivacy & Security\nBigID\nGet Started\nSet up BigID\nOn this page\nSet up BigID\nCreate a BigID system user account and generate an API token for Atlan to access BigID metadata. This guide walks through creating a custom role, system user, and API token.\nPermissions required\nâ\nTo successfully set up BigID for Atlan integration, confirm that your user role has the necessary permissions:\nBigID\n: Administrator access to create roles and system users\nCreate custom role\nâ\nCreate a custom role for Atlan to access BigID metadata. Follow these steps that provide privileges only to read metadata of assets and not the actual data contained in Catalog objects.\nLog in to your BigID instance\nNavigate to\nSettings\nâ\nAccess Management\nâ\nRoles\nClick\nAdd New Role\nGive a meaningful and unique name. For example,\nAtlan Integration\nas the role name\nSelect\nroot\nas scope.\nAdd the following permissions:\nCatalog\n: Read, Export, Get Attributes Value, View Sensitive Values, Manual Fields (Read), Business Attributes (Read)\nData Sources\n: Read\nPolicies\n: Read\nSecurity Posture\n: Read\nClick\nSave\nCreate system user\nâ\nAtlan uses a system user to authenticate and retrieve metadata from BigID. Follow these steps to create a system user:\nNavigate to\nSettings\nâ\nAccess Management\nâ\nSystem Users\nClick\nAdd New Role\nFill in the required user details\nClick\nConnect Roles\nand select the\nAtlan Integration\nrole\nClick\nSave\nGenerate API token\nâ\nAtlan uses the API token in Workflow configure to autheticate with BigID. Follow these steps to generate an API token for the system user:\nSelect the system user you just created\nIn the details panel, click\nGenerate\nunder\nTokens\nSet the token expiry period and click\nGenerate\nCopy and save the token securely for use in Atlan workflow configuration\nClick\nSave\nNeed help\nâ\nIf you encounter issues during the BigID setup process:\nBigID documentation\n: Refer to the\nBigID documentation\nfor detailed information about roles, system users, and API tokens\nContact Atlan support\n: For issues related to Atlan integration,\ncontact Atlan support\nNext steps\nâ\nCrawl BigID\nTags:\nconnectors\ndata\ncrawl\nprivacy\nbigid\nPrevious\nBigID\nNext\nCrawl BigID\nPermissions required\nCreate custom role\nCreate system user\nGenerate API token\nNeed help\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-client-credentials-setup",
    "content": "Connect data\nCRM\nSalesforce\nGet Started\nSet up Salesforce\nSet up client credentials flow\nOn this page\nSet up client credentials flow\nImportant\nAtlan currently supports\nSalesforce Sales Cloud\nand\nFinancial Services Cloud\n(FSC).\nAtlan supports the Salesforce\nOAuth 2.0 client credentials\nflow for server-to-server integration. This flow enables Atlan to authenticate using a dedicated integration user and an external client app in Salesforce, providing secure, non-interactive access to Salesforce metadata and data for crawling.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nSalesforce administrator access\nNetwork connectivity between Atlan and your Salesforce instance\nCreate custom profile\nâ\nA custom profile defines the specific permissions and access levels for your integration user. You'll create this profile with the minimum necessary permissions for Atlan to crawl your Salesforce data securely.\nCreate a custom profile to manage permissions for the integration user:\nFrom\nSetup\n, enter\nprofiles\nin the\nQuick Find\nbox and select\nProfiles\n.\nClick\nNew Profile\n.\nSelect\nStandard User\nfrom the\nExisting Profile\ndropdown to clone.\nEnter a name, for example\nAtlanIntegrationProfile\n.\nClick\nSave\n.\nOn the new profile page, click\nEdit\n.\nUnder\nConnected App Access\n, check the External Client App you create.\nUnder\nAdministrative Permissions\n, uncheck all except:\nAPI Enabled\nView All Data\nRun Reports\nUnder\nStandard Object Permissions\nand\nCustom Object Permissions\n, select\nRead\nand\nView All\nfor all items.\nClick\nSave\n.\nCreate integration user\nâ\nThe integration user acts as the identity that Atlan uses to connect to Salesforce. This dedicated user ensures secure, auditable access separate from individual user accounts.\nCreate a dedicated Salesforce user for the external client app:\nFrom\nSetup\n, expand\nAdministration\nâ\nUsers\nand click\nUsers\n.\nClick\nNew User\n.\nEnter required details:\nFirst Name\n,\nLast Name\n,\nUsername\n,\nEmail\n,\nNickname\n.\nSelect\nSalesforce\nfor\nUser License\n.\nAssign the custom profile created in the previous step.\nClick\nSave\n.\nCreate external client app\nâ\nThe external client app provides the OAuth infrastructure for secure server-to-server authentication. This app generates the credentials that Atlan uses to authenticate without requiring user interaction.\nSet up the external client app for client credentials flow:\nFrom\nSetup\n, enter\nexternal client app manager\nin\nQuick Find\nand select\nExternal Client App Manager\n.\nClick\nNew External Client App\n.\nEnter:\nExternal Client App Name\n: for example,\nAtlanIntegration\nContact Email\n: your email\nDistribution State\n:\nLocal\nExpand\nAPI (Enable OAuth Settings)\n:\nCheck\nEnable OAuth\nSet\nCallback URL\n:\nhttps://localhost\n(placeholder, unused)\nMove the following scopes to\nSelected OAuth Scopes\n:\nManage user data via APIs (api)\nPerform requests at any time (refresh_token, offline_access)\nAccess Lightning applications (lightning)\nUnder\nFlow Enablement\n, check\nEnable Client Credentials Flow\n.\nEnable:\nRequire Secret for Web Server Flow\nRequire Secret for Refresh Token Flow\nOptional hardening:\nRequire Proof Key for Code Exchange (PKCE)\nEnable Refresh Token Rotation\nIssue JSON Web Token (JWT)-based access tokens\nClick\nCreate\n.\nOn the app details page, copy the\nConsumer Key (Client ID)\nand\nConsumer Secret\nfrom\nOAuth Settings\n.\nStore these credentials securelyâthey're required to configure the Atlan connection.\nConfigure policies\nâ\nAfter creating the external client app, you need to configure its security policies. These policies control which users and profiles can access the app and define the authentication flow settings.\nFrom\nExternal Client App Manager\n, locate your app and click\nEdit\n.\nOpen the\nPolicies\ntab.\nSet\nStart Page\nto\nNone\n.\nMove the integration custom profile to\nSelected Profiles\n.\nIf using permission sets, move relevant sets to\nSelected Permission Sets\n.\nIn\nOAuth Policies\n, set:\nPermitted Users\n:\nAdmin approved users are pre-authorized\nOAuth Start URL\n: leave blank unless required\nIn\nOAuth Flows and External Client App Enhancements\n:\nCheck\nEnable Client Credentials Flow\nRun As (Username)\n: enter the integration user username\nSet additional policies as required (IP Relaxation, Session Timeout, Refresh Token Policy)\nClick\nSave\n.\nTroubleshooting\nâ\nIf you encounter issues with Client Credentials authentication, see\nTroubleshooting Salesforce Connectivity\n.\nNext steps\nâ\nCrawl Salesforce\nto configure the connection in Atlan.\nTags:\nconnectors\nsalesforce\nauthentication\nPrevious\nSet up JWT bearer flow\nNext\nSet up username-password flow\nPrerequisites\nCreate custom profile\nCreate integration user\nCreate external client app\nTroubleshooting\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry",
    "content": "Connect data\nEvent/Messaging\nConfluent Schema Registry\nGet Started\nSet up Confluent Schema Registry\nOn this page\nSet up Confluent Schema Registry\nWho can do this?\nYou will probably need your Schema Registry administrator to complete these steps   -  you may not have access yourself.\nAtlan supports the API authentication method for fetching metadata from Confluent Schema Registry. This method uses an API key and secret to fetch metadata.\nCreate an API key\nâ\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the resources in your Confluent Cloud Schema Registry environment.\nTo\ncreate an API key\nfor\ncrawling Confluent Schema Registry\n:\nLog in to your\nConfluent Cloud\ninstance.\nFrom the left menu of the\nCloud Console\n, click\nEnvironments\nand then select your environment.\nOn your environment page, under\nStream Governance API\nin the right menu, for\nEndpoint\n, click the clipboard icon to copy the endpoint and store it in a secure location.\nUnder\nCredentials\nin the right menu, click\nView & manage\nif there are any existing API keys or click\n0 keys\nif there are none to open the API credentials dialog.\nIn the\nAPI credentials\ndialog, click\n+ Add key\nif there are any existing API keys or click\nCreate key\nif there are none to create a new schema registry API key.\nFrom the\nCreate a new Schema Registry API key\ndialog:\nFor\nKey\n, click the clipboard icon to copy the API key and store it in a secure location.\nFor\nSecret\n, click the clipboard icon to copy the API secret and store it in a secure location.\n(Optional) For\nDescription\n, enter a description for the new API key   -  for example,\nAtlan integration API key\n.\nDid you know?\nYou will need the schema registry endpoint, API key, and API secret for\ncrawling Confluent Schema Registry\n.\nTags:\nconnectors\ndata\ncrawl\napi\nauthentication\nPrevious\nConfluent Schema Registry\nNext\nCrawl Confluent Schema Registry\nCreate an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/how-tos/set-up-cratedb",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nGet Started\nSet up CrateDB\nOn this page\nSet up CrateDB\nThis guide provides steps to set up a dedicated database user and configure permissions to enable Atlan to securely connect to your CrateDB cluster and extract metadata for data discovery and governance.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdministrative access to your CrateDB cluster\nNetwork connectivity between Atlan and your CrateDB instance\nYour CrateDB cluster's HTTP endpoint and port information\nPermission required\nâ\nBefore setting up the CrateDB connector, you need:\nAdministrative access to your CrateDB cluster to create users and grant permissions\nTo run the setup statements for each schema you want to crawl\nCreate database user\nâ\nCreate a dedicated database user with the necessary permissions for Atlan to access your CrateDB cluster securely.\nCreate a dedicated user for Atlan with basic authentication:\nCREATE\nUSER\natlan_user\nWITH\n(\npassword\n=\n'<password>'\n)\n;\nReplace\n<password>\nwith a secure password for the\natlan_user\n.\nGrant DQL permissions on the target schema:\nGRANT\nDQL\nON\nSCHEMA\n<\nschema\n>\nTO\natlan_user\n;\nReplace\n<schema>\nwith the schema to which the user needs access.\nGrant DQL permissions on system schemas for metadata access:\nGRANT\nDQL\nON\nSCHEMA\ninformation_schema\nTO\natlan_user\n;\nGRANT\nDQL\nON\nSCHEMA\nsys\nTO\natlan_user\n;\nConfigure authentication\nâ\nChoose the authentication method that best fits your security requirements and infrastructure setup.\nBasic authentication\nCertificate-based authentication\nTo create a username and password for basic authentication:\nCREATE\nUSER\natlan_user\nWITH\n(\npassword\n=\n'<password>'\n)\n;\nGRANT\nDQL\nON\nSCHEMA\n<\nschema\n>\nTO\natlan_user\n;\nReplace\n<password>\nwith the password for the\natlan_user\nuser you are creating.\nCertificate-based authentication is available for Enterprise customers.\nGrant permissions\nâ\nGrant the necessary permissions to enable Atlan to access metadata and data from your CrateDB cluster.\nImportant\nDQL permissions grant both metadata access and data read access. CrateDB doesn't support metadata-only access.\nSystem schema permissions\n: Grant DQL permissions on system schemas for metadata access:\nGRANT\nDQL\nON\nSCHEMA\ninformation_schema\nTO\natlan_user\n;\nGRANT\nDQL\nON\nSCHEMA\nsys\nTO\natlan_user\n;\nThese system schemas contain CrateDB metadata:\ninformation_schema\n: Standard SQL metadata tables\nsys\n: CrateDB-specific system tables for cluster and table information\nData access permissions\n: Grant DQL permissions on user schemas for data preview and querying:\nGRANT\nDQL\nON\nTABLE\n<\nschema\n>\n.\n*\nTO\natlan_user\n;\nReplace\n<schema>\nwith the name of the schema you want Atlan to access.\nConfigure connection\nâ\nTo connect Atlan to your CrateDB cluster, youâll need to provide the following details during setup:\nHost\n: The HTTP endpoint of your CrateDB cluster. For example,\nhttps://your-cluster.crate.io\nUsername\n: The database username created in the\nCreate database user\nsection.\nPassword\n: The secure password set for the database user created in the\nCreate database user\nsection.\nDatabase\n: The name of the CrateDB database to crawl. Enter the specific database for metadata extraction.\nTroubleshooting\nâ\nIf you encounter connection or authentication issues, see\nConnection issues\n.\nNext steps\nâ\nCrawl CrateDB\n- Extract metadata from your CrateDB instance\nTags:\nconnectors\ncratedb\ndatabase\nsetup\nPrevious\nCrateDB\nNext\nCrawl CrateDB\nPrerequisites\nPermission required\nCreate database user\nConfigure authentication\nGrant permissions\nConfigure connection\nTroubleshooting\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/how-tos/set-up-dagster",
    "content": "Connect data\nOrchestration & Workflow\nDagster\nGet Started\nSet up Dagster\nOn this page\nSet up Dagster\nPrivate Preview\nConfigure Dagster to send lineage and asset metadata to Atlan. This setup enables automatic capture of Dagster assets and their lineage relationships.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdmin access to your Dagster instance\nAdmin access to your Atlan workspace\nAn Atlan API key.\nLearn how to create an API key\nConfigure Dagster\nâ\nFollow these steps to enable the Atlan integration in Dagster so lineage events from your Dagster assets flow into Atlan.\nContact Dagster support to enable the Atlan integration.\nShare the following details with Dagster support:\nYour Atlan API key.\nYour Atlan tenant domain. For example,\nhttps://[your-org].atlan.com/\n.\nThe connection name you plan to create in Atlan.\nAfter the integration is enabled, Dagster assets automatically send lineage events to Atlan.\nNext steps\nâ\nCrawl Dagster assets\n: Create a crawler workflow in Atlan to capture lineage from Dagster\nTags:\nconnectors\nlineage\ndagster\nPrevious\nDagster\nNext\nCrawl Dagster assets\nPrerequisites\nConfigure Dagster\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud",
    "content": "Connect data\nETL Tools\ndbt\nGet Started\nSet up dbt Cloud\nOn this page\nSet up dbt Cloud\nWho can do this?\nYou will probably need your dbt Cloud administrator to complete these steps   -  you may not have access yourself.\nIf you have a dbt Cloud account, Atlan can help enrich your assets with dbt metadata.\ndanger\nTo enable Atlan to fetch metadata for dbt models defined in your project, you must add the\ndbt docs generate\ncommand to the list of commands in the job run steps. This will produce a\ncatalog.json\nfile containing all the relevant metadata. Alternatively, you can select the\nGenerate docs on run\ncheckbox\nto automatically generate updated project docs each time a job runs. Refer to\ndbt documentation\nto learn more.\nCreate a token\nâ\nBe sure to copy the generated token for\ncrawling dbt\n.\nService account token\nâ\nOnly dbt Cloud administrators can generate service account tokens. This is required for authenticating as a service account user and to set up granular access permissions.\nTo generate a service account token, follow the\nsteps in dbt documentation\nand configure the following permissions:\nTeam plans: add\nRead-only\naccess to all projects you want to integrate into Atlan. This permission is required to authorize requests to both the\ndbt Cloud Administrative API\nand\ndbt Cloud Discovery API\n.\nEnterprise plans: add\nJob Viewer\naccess to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata. Learn more about\ndbt Cloud Enterprise permissions\n.\nPersonal access token\nâ\ndanger\nUser API tokens will be deprecated\nand replaced with account-scoped personal access tokens by October 22, 2024. If you have configured any dbt crawler workflows in Atlan with user API tokens, you may encounter errors. You must\nmodify the configuration\nfor any existing workflows with updated credentials   -  either a service account or personal access token.\nYou can also create an account-scoped personal access token for\ncrawling dbt\n.\nTo generate a personal access token, follow the\nsteps in dbt documentation\nand note the following:\nThe user creating the token must have\nJob Viewer\naccess to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata.\nTags:\nconnectors\ndata\ncrawl\nmodel\nPrevious\ndbt\nNext\nSet up dbt Core\nCreate a token"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/how-tos/set-up-domo",
    "content": "Connect data\nBI Tools\nCloud-based BI\nDomo\nGet Started\nSet up Domo\nOn this page\nSet up Domo\nWho can do this?\nYou will need your Domo administrator to complete these steps   -  you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from Domo. This method uses the following to fetch metadata:\nClient ID\nClient secret\nAccess token\nDomoStats dataset IDs\nCreate client ID and secret\nâ\ndanger\nYou will need your Domo\nAdmin\nto create a client ID and secret for crawling\nDomo datasets and dataset columns\n. If the user creating the client credentials does not have admin privileges, only datasets will be crawled.\nYou will need your Domo instance name to create a client ID and secret. This will be the name preceding your domo.com URL. For example, in the case of\ncompany.domo.com\n, the instance name will be\ncompany\n.\nTo create a client ID and secret:\nLog in to the\nDomo Developer Portal\nwith your Domo instance name   -  for example,\ncompany\n.\nOnce you have entered your instance name, enter your Domo user credentials when prompted.\nOn the\nCreate new client\npage, enter the following details:\nFor\nName\n, enter a meaningful name for the client application   -  for example,\nAtlan_connection\n.\n(Optional) For\nDescription\n, enter a brief description for the client application.\nFor\nApplication Scope\n, check the following boxes to assign the minimum permissions required to\ncrawl Domo\n:\nData\n-  this scope allows Atlan to access the\nDataSet API\n.\nDashboard\n-  this scope allows Atlan to access the\nPage API\n.\nClick\nCreate\nto complete the client application.\nAfter you have registered the client application, you will be redirected to the\nManage Clients\npage to view your newly provisioned client ID and secret. Copy the values for\nClient ID\nand\nSecret\nand store them in a secure location.\n(Optional) To access your client ID and secret at a later time, navigate to the Domo Developer Portal homepage, and then from the left menu, click\nManage clients\nto view and manage your existing client credentials.\nGenerate access token\nâ\nYou will need to create an\naccess token\nthat will allow Atlan to generate\nupstream lineage for Domo datasets\n.\nOnly a Domo\nAdmin\ndefault security role or a custom role with either the\nManage All Company Settings\nor\nManage All Access Tokens\ngrant enabled can generate access tokens. If you do not have either of these privileges, request an access token from your Domo administrator.\nTo generate an access token:\nLog in to your Domo instance as a Domo administrator.\nFrom the tabs along the top of your Domo homepage, click\nMore\nand then click\nAdmin\n.\nOn the\nAdmin\nsettings page, under\nAuthentication\n, click\nAccess tokens\n.\nIn the upper right of the\nManage access tokens\npage, click the\nGenerate access token\nbutton.\nTo specify the token information:\nFor\nAccess token description\n, enter a meaningful name for your token   -  for example,\nAtlan_connection_token\n.\nFor\nSearch users\n, search for and select the user to assign the token.\ndanger\nAccess tokens are associated with specific user accounts and grant the same access as the user who generated the token. If the user's permissions change, the access token will reflect the same.\nFor\nExpire after\n, select an expiration date for the token.\nClick\nGenerate\nto generate the access token.\nFrom the corresponding screen, copy the access token and store it in a secure location. The token will not be displayed again after you leave the\nManage access tokens\npage.\n(Optional) To revoke the access token, follow the steps in\nDomo documentation\n.\nSet up DomoStats connector\nâ\nThe\nDomoStats connector\nallows you to import usage metadata from your Domo instance. This connector is only available to Domo administrators.\nDue to limitations of the Domo APIs, you will need to set up the DomoStats connector to crawl metadata for Domo cards and catalog asset relationships between cards and dashboards as well as datasets and cards in Atlan. The DomoStats connector allows you to build datasets with the required metadata. Once you have created the datasets, Atlan will require the dataset IDs to fetch the metadata from DomoStats. All three DomoStats dataset IDs are required to\ncrawl Domo\n.\nTo set up DomoStats:\nLog in to your Domo instance as a Domo administrator.\nFrom the tabs along the top of your Domo homepage, click\nAppstore\n.\nIn the search bar, search for and select\nDomoStats\n.\nFrom the right panel of the\nDomoStats\npage, click the\nGet the Data\nbutton.\nIn the\nCreate DomoStats DataSet\npage, you will need to create three DomoStats datasets for card, card-dashboard relationship, and dataset-card relationship metadata. Except step 1 below, the remaining steps will be the same for all three datasets.\nÂ Under\nDetails\n, for\nReport\n, click the dropdown and then:\nClick\nCards\nto create a dataset for card metadata.\nClick\nCard Pages\nto create a dataset for card-dashboard relationship metadata.\nClick\nCard Datasource\nto create a dataset for dataset-card relationship metadata.\nClick\nNext\nto proceed.\nUnder\nScheduling\n, you can either keep the predefined update schedule or define a custom schedule, and then click\nNext\n.\nUnder\nName & Describe Your DataSet\n, enter the following details:\nFor\nDataset Name (Required)\n, enter a meaningful name for your dataset.\nFor\nDataset Description (Optional)\n, enter a brief description for your dataset.\nFor\nAdd Dataset to Cloud\n, keep the default Domo selection.\nClick\nSave\nto finish setup.\nOnce you have completed setting up a dataset, from the corresponding page, copy the dataset ID from the URL for all three datasets and store them in a secure location. For example, if the dataset URL is\ncompany.domo.com/datasource/ae6440eb-bef6-414a-a0e5-a5b2d58d1234/details/overview\n, then the dataset ID will be:\nae6440eb-bef6-414a-a0e5-a5b2d58d1234\n.\nTags:\nconnectors\ndata\ncrawl\nauthentication\nPrevious\nDomo\nNext\nCrawl Domo\nCreate client ID and secret\nGenerate access token\nSet up DomoStats connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nGet Started\nSet up Google BigQuery\nOn this page\nSet up Google BigQuery\nWho can do this?\nYou must be a Google BigQuery administrator to run these commands. For more information, see\nGoogle Cloud's Granting, changing, and revoking access to resources\n.\nAtlan\nextracts metadata from Google BigQuery\nthrough read-only access. Once you have crawled metadata for your Google BigQuery assets, you can\nmine query history\nto construct lineage. If you have enabled sample data preview or querying, asset previews and queries will be cost-optimized for\ntables\nonly. For\nviews\nand\nmaterialized views\n, Atlan will send you a cost nudge before you run the preview or query the data   -  learn more\nhere\n.\nYou must create a service account to enable Atlan to\nextract metadata from Google BigQuery\n. To create a service account, you can either use:\nGoogle Cloud console\nGoogle Cloud CLI\nPermissions\nâ\nAtlan requires the following permissions to extract metadata from Google BigQuery.\n(Required) For metadata crawling\nâ\nTo configure permissions for crawling metadata, add the following permissions to the custom role:\nbigquery.datasets.get\nenables Atlan to retrieve metadata about a dataset.\nbigquery.datasets.getIamPolicy\nenables Atlan to read a dataset's IAM permissions.\nbigquery.jobs.create\nenables Atlan to run jobs (including queries) within the project.\ndanger\nWithout this, Atlan can't query the source.\nbigquery.routines.get\nenables Atlan to retrieve routine definitions and metadata.\nbigquery.routines.list\nenables Atlan to list routines and metadata on routines.\nbigquery.tables.get\nenables Atlan to retrieve table metadata.\nbigquery.tables.getIamPolicy\nenables Atlan to read a table's IAM policy.\nbigquery.tables.list\nenables Atlan to list tables and metadata on tables.\nbigquery.readsessions.create\nenables Atlan to create a session to stream large results.\nbigquery.readsessions.getData\nenables Atlan to retrieve data from the session.\nbigquery.readsessions.update\nenables Atlan to cancel the session.\nresourcemanager.projects.get\nenables Atlan to retrieve project names and metadata.\n(Optional) To add data preview and querying\nâ\nTo configure permissions for previewing and querying data, add the following permissions to the custom role:\nbigquery.tables.getData\nenables Atlan to retrieve table data.\ndanger\nThis permission is also required for retrieving metadata such as the row count and update time of a table.\nbigquery.jobs.get\nenables Atlan to retrieve data and metadata on any job, including queries.\nbigquery.jobs.listAll\nenables Atlan to list all jobs and retrieve metadata on any job submitted by any user.\nbigquery.jobs.update\nenables Atlan to cancel any job, including a running query.\n(Optional) To add query history mining\nâ\nTo configure permissions for mining query history, add the following permissions to the custom role:\nbigquery.jobs.listAll\nenables Atlan to fetch all queries for a project.\nbigquery.jobs.get\nenables Atlan to access query text for queries.\n(Optional) To crawl tags\nâ\nTo configure permissions for crawling\nGoogle BigQuery tags and policy tags\n, add the following permissions to the custom role:\nresourcemanager.tagKeys.list\nenables Atlan to fetch all tag keys.\nresourcemanager.tagValues.list\nenables Atlan to fetch all tag values for tag keys.\ndatacatalog.taxonomies.list\nenables Atlan to fetch all policy tag taxonomies.\ndatacatalog.taxonomies.get\nenables Atlan to fetch all policy tag taxonomies.\nGoogle Cloud console\nâ\nCreate a custom role\nâ\nYou will need to\ncreate a custom role\nin the Google Cloud console for integration with Atlan.\nTo create a custom role:\nOpen the\nGoogle Cloud console\n.\nFrom the left menu under\nIAM and admin\n, click\nRoles\n.\nUsing the dropdown list at the top of the page, select the project in which you want to create a role.\nFrom the upper left of the\nRoles\npage, click\nCreate Role\n.\nIn the\nCreate role\npage, enter the following details:\nFor\nTitle\n, enter a meaningful name for the custom role   -  for example,\nAtlan User Role\n.\n(Optional) For\nDescription\n, enter a description for the custom role.\nFor\nID\n, the Google Cloud console generates a custom role ID based on the custom role name. Edit the ID if necessary   -  the ID cannot be changed later.\n(Optional) For\nRole launch stage\n, assign a stage for the custom role   -  for example,\nAlpha\n,\nBeta\n, or\nGeneral Availability\n.\nClick\nAdd permissions\nto select the permissions you want to include in the custom role. In the\nAdd permissions\ndialog, click the\nEnter property name or value\nfilter and add the required and any optional permissions.\nClick\nCreate\nto finish custom role setup.\nOnce you have created a custom role, you will need to\ncreate a service account\nand add your custom role to it.\nTo create a service account:\nOpen the\nGoogle Cloud console\n.\nFrom the left menu under\nIAM and admin\n, click\nService accounts\n.\nSelect a Google Cloud project.\nFrom the upper left of the\nService accounts\npage, click\nCreate Service Account\n.\nFor\nService account details\n, enter the following details:\nFor\nService account name\n, enter a service account name to display in the Google Cloud console.\nFor\nService account ID\n, the Google Cloud console generates a service account ID based on this name. Edit the ID if necessary   -  the ID cannot be changed later.\n(Optional) For\nService account description\n, enter a description for the service account.\nClick\nCreate and continue\nto proceed to the next step.\nFor\nGrant this service account access to the project\n, enter the following details:\nClick the\nSelect a role\ndropdown and then select the custom role you created in the previous step   -  for example,\nAtlan User Role\n.\nClick\nContinue\nto proceed to the next step.\nClick\nDone\nto finish the service account setup.\nCreate service account key\nâ\nOnce you have created a service account, you will need to\ncreate a service account key\nfor\ncrawling Google BigQuery\n.\nTo create a service account key:\nOpen the\nGoogle Cloud console\n.\nFrom the left menu under\nIAM and admin\n, click\nService accounts\n.\nSelect the Google Cloud project for which you created the service account.\nOn the\nService accounts\npage, click the email address of the service account that you want to create a key for.\nFrom the upper left of your service account page, click the\nKeys\ntab.\nOn the\nKeys\npage, click the\nAdd Key\ndropdown and then click\nCreate new key\n.\nIn the\nCreate private key\ndialog, for\nKey type\n, click\nJSON\nand then click\nCreate\n. This will create a service account key file. Download the key file and store it in a secure location   -  you will not be able to download it again.\nGoogle Cloud CLI\nâ\nPrerequisites\nâ\nYou will need to set up the Google Cloud CLI in any one of the following development environments:\nCloud Shell\n-  to use an online terminal with the gcloud CLI already set up, activate Cloud Shell:\nTo launch a Cloud Shell session from the Google Cloud console, open the\nGoogle Cloud console\n, and from the top right, click the\nActivate Cloud Shell\nicon.\nA Cloud Shell session will start and display a command-line prompt. It can take a few seconds for the session to initialize.\nLocal shell\n-  to use a local development environment,\ninstall\nand\ninitialize\nthe gcloud CLI.\nCreate a custom role\nâ\nTo create a custom role with the requisite and any optional permissions, run the following command:\ngcloud iam roles create atlanUserRole --project=<project_id> \\\n--title=\"Atlan User Role\" --description=\"Atlan User Role to extract metadata\" \\\n--permissions=\"bigquery.datasets.get,bigquery.datasets.getIamPolicy,bigquery.jobs.create,bigquery.readsessions.create,bigquery.readsessions.getData,bigquery.readsessions.update,bigquery.routines.get,bigquery.routines.list,bigquery.tables.get,bigquery.tables.getIamPolicy,bigquery.tables.list,resourcemanager.projects.get\" \\\n--stage=ALPHA\nReplace\n<project_id>\nwith the project ID of your Google Cloud project.\nCreate a service account\nâ\nTo create a service account, run the following command:\ngcloud iam service-accounts create atlanUser \\\n--description=\"Atlan Service Account to extract metadata\" \\\n--display-name=\"Atlan User\"\nTo add your custom role to your service account, run the following command:\ngcloud projects add-iam-policy-binding <project_id> \\\n--member=\"serviceAccount:atlanUser@<project_id>.iam.gserviceaccount.com\" \\\n--role=\"atlanUserRole\"\nReplace\n<project_id>\nwith the project ID of your Google Cloud project.\nCreate a service account key\nâ\nTo create a service account key, run the following command:\ngcloud iam service-accounts keys create  <key_file_path> \\\n--iam-account=atlanUser@<project_id>.iam.gserviceaccount.com\"\nReplace\n<key_file_path>\nwith path to a new output file for the private key   -  for example,\n~/atlanUser-private-key.json\n.\nReplace\n<project_id>\nwith the project ID of your Google Cloud project.\ndanger\nDue to limitations at source, Atlan currently does not support generating lineage using the\nbq cp\ncommands   -  for example,\nbq cp <source-table> <destination-table>\n.\nTags:\nconnectors\ndata\ncrawl\nPrevious\nGoogle BigQuery\nNext\nHow to enable SSO for Google BigQuery\nPermissions\nGoogle Cloud console\nGoogle Cloud CLI"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/how-tos/set-up-gcs",
    "content": "Connect data\nStorage\nGoogle GCS\nGet Started\nSet up Google Cloud Storage\nOn this page\nSet up Google Cloud Storage\nThis guide walks you through setting up Google Cloud Storage (GCS) to enable secure data ingestion from your GCS buckets.\nThis guide walks you through setting up Google Cloud Storage (GCS) to enable secure data ingestion from your GCS buckets. The connector catalogs GCS buckets and objects only.\nPrerequisites\nâ\nGCS bucket containing the data you want to ingest\nAppropriate permissions to create service accounts and manage IAM roles\nPermissions required\nâ\nMake sure you (or an administrator) can assign the following IAM roles to the service account that the connector uses:\nStorage Bucket Viewer\n(\nroles/storage.bucketViewer\n)\nStorage Object Viewer\n(\nroles/storage.objectViewer\n)\nYou also need permission to create a service account and generate its key.\nCreate a service account\nâ\nSelect your project from the project dropdown.\nCreating a dedicated service account avoids using personal credentials and lets you manage access centrally.\nIn the left navigation menu, go to\nIAM & Admin\n>\nService accounts\n.\nSelect\nCreate service account\n.\nEnter a name for your service account (for example,\natlan-gcs-connector\n).\nAdd an optional description.\nSelect\nCreate and continue\n.\nAssign roles and permissions\nâ\nAdd the following roles to your service account:\nThese roles grant read-only access so the connector can discover buckets and objects without modifying data.\nStorage Bucket Viewer\n: Lets you read bucket details (\nstorage.buckets.list\n).\nStorage Object Viewer\n: Lets you list objects and read object metadata (\nstorage.objects.list\n).\nSelect\nDone\n.\nGenerate a service account key\nâ\nIn the left navigation menu, go to\nIAM & Admin\n>\nService accounts\n.\nThe JSON key file is used by the connector to authenticate to GCP programmatically.\nSelect\nCreate key\n.\nDownload and store the key file securely.\nSelect\nJSON\nas the key type.\nSelect\nCreate\n.\nDownload the JSON file and store it securely.\nConfigure bucket permissions\nâ\nNavigate to\nCloud Storage\n.\nGrant the service account access to every bucket you want Atlan to crawl.\nSelect your bucket.\nGo to the\nPermissions\ntab.\nSelect\nAdd principal\n.\nEnter your service account email (for example,\n[email protected]\n).\nAssign the\nStorage Object Viewer\nrole.\nSelect\nSave\n.\nNeed help\nâ\nIf you run into issues during the GCS setup process:\nGCP documentation\n: Refer to the\nGoogle Cloud IAM documentation\nfor detailed information about roles and permissions.\nContact Atlan support\n: For issues related to Atlan integration,\ncontact Atlan support\n.\nNext steps\nâ\nCrawl GCS assets\n: Follow this guide to configure the crawler workflow and ingest metadata from your GCS buckets.\nTags:\nconnectors\ndata\ncrawl\nstorage\ngoogle-gcs\ngcp\nPrevious\nGoogle Cloud Storage\nNext\nCrawl GCS assets\nPrerequisites\nPermissions required\nCreate a service account\nAssign roles and permissions\nGenerate a service account key\nConfigure bucket permissions\nNeed help\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nGet Started\nSet up IBM Cognos Analytics\nOn this page\nSet up IBM Cognos Analytics\nWho can do this?\nYou must be an IBM Cognos Analytics administrator to complete these steps   -  you may not have access yourself.\nAtlan supports the following authentication methods for fetching metadata from IBM Cognos Analytics:\nBasic authentication -  this method uses a\nusername and password\nto fetch metadata.\nAPI authentication -  this method uses a\nusername\nand an\nAPI key\nto fetch metadata.\nOKTA authentication -  this method uses a\nusername and password\nof OKTA to fetch metadata.\nCreate user\nâ\nTo\ncreate a new user\nfor\ncrawling IBM Cognos Analytics\n:\nLog in to your IBM Cognos Analytics instance.\nExpand the left menu of your homepage and then click\nManage\n.\nFrom the corresponding menu, click\nPeople\nand then click\nAccounts\n.\nIn\nAccounts\n, under\nNamespaces\n, select your Cognos namespace to open it.\nFrom the upper right of your namespace page, click the new user icon to add a new user to the selected namespace.\nIn the\nNew user\nform, enter the following details:\nFor\nGiven name\n, enter a meaningful name for the new user.\nFor\nUser ID\n, create a username for the new user.\nFor\nPassword\n, create a password for the username.\nFor\nEmail\n, you can leave this blank.\nClick\nOK\nto save your configuration. The new user you created is added to the list of entries in your namespace.\n(Optional) Create an API key\nâ\nYou can also use API authentication for integrating with Atlan. In addition to the username for the new user created in the\nCreate user\nsection, you need an API key for authenticatingÂ the connection.\nTo\ncreate an API key\nfor\ncrawling IBM Cognos Analytics\n:\nLog in to your IBM Cognos Analytics instance as the new user created in the\nCreate user\nsection.\nIn the top right of your homepage, click the personal menu icon and then click\nProfile and settings\n.\nIn the\nProfile and settings\ntab, under\nAdvanced options\n, next to\nMy API keys\n, click\nManage\n.\nFrom the upper right of the\nMy API keys\npage, click the\nGenerate API key\nbutton.\nIn the\nGenerate API key\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for the API key.\n(Optional) For\nDescription\n, enter a brief description.\nClick\nNext\nto proceed.\nOnce the encrypted key has appeared on the screen, copy and store the value in a secure location.\ndanger\nIBM Cognos Analytics doesn't store the API key, you must copy and save it.\nClick\nDone\n. Your new API key appears in the list of keys on the\nMy API keys\npage.\nIf you experience any functionality issues with the newly created API key, you can renew your credentials. Navigate to the\nProfile and settings\nmenu, and then next to the\nCredentials\noption, click the\nRenew\nbutton to refresh your credentials.\n(Optional) Create user in OKTA\nâ\nIf the IBM Cognos Namespace type is \"OKTA\" and OKTA is used for login, a\ncorresponding user must be created in OKTA to enable login to IBM Cognos via\nOKTA.\nIf IBM Cognos is configured to use OKTA as the authentication provider (via the OKTA namespace type), each user must have a valid account in OKTA to successfully log in.\nFollow these instructions to\ncreate a new user\nin OKTA and assign a user type for accessing IBM Cognos Analytics:\nLog in to your OKTA instance with Admin credentials.\nFrom the left menu on the homepage, expand\nDirectory\nand select\nPeople\n.\nClick\nAdd Person\n.\nIn the New User form, fill in the following details:\nSelect the appropriate\nUser Type\n.\nEnter user's personal details.\nAssign the user to the relevant group.\nClick\nSave\nto complete the process.\nAdd user to a Cognos role\nâ\nTo add the new user to the Cognos\nReader\nrole:\nLog in to your IBM Cognos Analytics instance.\nExpand the left menu of your homepage and then click\nManage\n.\nFrom the corresponding menu, click\nAdministration console\n.\nFrom the tabs along the top of the\nIBM Cognos Administration\npage, click\nSecurity\n.\nIn the\nSecurity\ntab, select the\nCognos\nnamespace.\nFrom the list of standard roles, navigate to\nReaders\n. In the\nActions\ncolumn for\nReaders\n, click\nMore\n. This role allows read-only access to IBM Cognos Analytics, refer to the\nstandard roles documentation\nto learn more.\nIn the\nPerform an action - Readers\nscreen, under\nAvailable actions\n, click\nSet members\n.\nIn the\nMembers\ntab, click\nAdd\nto add a new entry to the list.\nIn the\nSelect entries (Navigate)\n- Readers\nscreen, from the\nAvailable entries\n, select the namespace where you created the new user.\nIn the corresponding screen, under\nDirectory\n, click the\nShow users in the list\ncheckbox and then select the\nnew user you created\n.\nClick the right-arrow button, and when the entry you want appears in the\nSelected entries\nbox, click\nOK\n.\nSet permissions\nâ\nAll entries such as folders, reports, modules, and more already have the\nReaders\nrole assigned to them by default. You will only need to set permissions for the new user to data server connections.\nTo set access permissions for the new user to Cognos entries:\nLog in to your IBM Cognos Analytics instance.\nExpand the left menu of your homepage and then click\nData server connections\n.\nOn the\nData server connections\npage, to set permissions for each data server connection, click the vertical 3-dot icon and then click\nProperties\n.\nFrom the tabs along the top of the\nProperties\npage, click the\nPermissions\ntab.\nIn the upper right of the\nPermissions\npage, click the\n+\nicon to add a new member.\nIn the\nAdd member\nform, select the\nCognos\nnamespace and then search for and select the\nReaders\nrole.\nClick\nAdd\n.\nOnce you have added the role, click\nSave\nto save your configuration.\nFind namespace\nâ\nYou must have the name of your namespace where you created the new user for authenticating the connection in Atlan. There are several ways to find the name of your namespace, here is one such method.\nTo find the namespace details where you created the new user:\nLog in to your IBM Cognos Analytics instance.\nExpand the left menu of your homepage and then click\nManage\n.\nFrom the corresponding menu, click\nAdministration console\n.\nFrom the tabs along the top of the\nIBM Cognos Administration\npage, click\nSecurity\n.\nIn the\nSecurity\ntab, select the namespace where you\ncreated the new user\n. Make sure that the new user is listed in the selected namespace.\nFrom the top right of your namespace page, click the\nSet properties\nchart icon.\nIn the\nSet properties (namespace)\npage, next to\nLocation\n, click the\nView the search path, ID and URL\nlink.\nIn the\nView the search path, ID and URL\nform, under\nSearch path\n, next to\nCAMID\n, the name of your namespace is shown enclosed within brackets   -  for example,\nCAMID(<YOUR NAMESPACE NAME>)\n. Copy the value for\n<YOUR NAMESPACE NAME>\nand store it in a secure location.\nTags:\nconnectors\ndata\ncrawl\napi\nauthentication\nPrevious\nIBM Cognos Analytics\nNext\nSet up on-premises IBM Cognos Analytics access\nCreate user\n(Optional) Create an API key\n(Optional) Create user in OKTA\nAdd user to a Cognos role\nSet permissions\nFind namespace"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/how-tos/set-up-informatica-cdi",
    "content": "Connect data\nETL Tools\nInformatica CDI\nGet Started\nSet up Informatica CDI\nOn this page\nSet up Informatica CDI\nConfigure user authentication and gather required parameter files to enable the Informatica Cloud Data Integration connector in Atlan. This guide walks you through creating a user with the Designer role and preparing the necessary parameter files for accurate lineage generation.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAccess to Informatica Cloud as an Org Administrator\nNetwork connectivity to your Informatica Cloud instance\nAdmin permissions in Atlan to create connections\nCreate user\nâ\nInformatica CDI connector supports\nNative authentication type\n. Perform the following steps as an Org Administrator in Informatica Cloud to set up user authentication for the connector.\nLog in to Informatica IICS as an Org Admin.\nGo to\nAdministrator\nâ\nUsers\n.\nClick the\nAdd User\nbutton (â).\nFollow the steps in the official document for\nCreating the user\n.\nAssign the\nDesigner\nrole to the user. For more information, see\nDesigner role official documentation\nGather parameter files\nâ\nParameter files\nare used to define source or target schema and table names referenced in job definitions. You need these files to generate accurate lineage.\nIdentify the\nIICS CDI Projects\nor\nFolders\nyou plan to include in the Atlan workflow.\nLocate the parameter files on your Informatica Cloud Secure Agent machines where the ETL jobs run.\nDownload the parameter files for each project or folder. These are needed when configuring the crawler in Atlan.\nNext steps\nâ\nCrawl Informatica CDI assets\n: Configure and run the crawler to discover and catalog your Informatica CDI assets\nTags:\nconnectors\netl-tools\ninformatica\ncdi\nsetup\nauthentication\nPrevious\nInformatica CDI\nNext\nCrawl Informatica CDI assets\nPrerequisites\nCreate user\nGather parameter files\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/set-up-inventory-reports-for-s3",
    "content": "Connect data\nStorage\nAmazon S3\nInventory reports\nSet up Inventory reports\nOn this page\nSet up Inventory reports\nSet up inventory reports for Amazon S3 to enable\ninventory-based ingestion\nthrough the crawler. This guide shows you how to configure inventory reports in the format required by Atlan's S3 crawler.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAWS permissions\n: Access to configure inventory reports on source buckets. Follow the official AWS documentation on\ninventory report configuration\nfor permissions.\nDestination bucket\n: A dedicated S3 bucket to store inventory reports.\nCreate destination bucket\nâ\nFirst, create a dedicated S3 bucket to store your inventory reports.\nSign in to the AWS Management Console.\nNavigate to\nS3\nâ\nBuckets\n.\nClick\nCreate bucket\n.\nEnter a unique bucket name (for example,\natlan-inventory-reports\n). Make a note of the bucket name as itâs required when configuring the Atlan workflow for inventory-based ingestion.\nSelect the appropriate region (keep this consistent for all inventory reports). Make a note of the region as itâs required when configuring the Atlan workflow for inventory-based ingestion.\nConfigure other settings as needed.\nClick\nCreate bucket\n.\nConfigure inventory reports\nâ\nNow configure inventory reports for each S3 bucket you want to catalog in Atlan.\nNavigate to\nS3\nâ\nBuckets\n.\nSelect the source bucket you want to catalog.\nGo to the\nManagement\ntab.\nScroll down to\nInventory configurations\nClick\nCreate inventory configuration\nand configure the following settings:\nInventory configuration name\n: Enter a meaningful name, such as\natlan-inventory-config\nInventory scope\n: Optionally choose a prefix to limit the report to specific objects. You can also use\nfilters\nin your workflow.\nObject versions\n: Select\nCurrent version only\n(Atlan doesn't support\nInclude all versions\n).\nConfigure the\nReport details\n:\nDestination bucket\n: Select the destination bucket you created earlier. Optionally specify a prefix to organize reports in a folder.\nNote\n: If you use a prefix, remember it for your Atlan workflow configuration and keep it consistent across all bucket reports.\nReport frequency\n: Choose daily or weekly.\nReport format\n: Select\nCSV\nor\nApache Parquet\n(only these formats are supported).\nStatus\n: Enable the inventory report by selecting\nEnabled\n.\nEncryption\n: Leave encryption disabled. Atlan's S3 crawler requires unencrypted inventory reports.\nMetadata fields\n: Select all available metadata fields. This ensures Atlan receives complete metadata information about your S3 objects.\nReview all settings and click\nCreate\n.\nFor multiple inventory reports, your destination bucket must follow a specific structure. See\nInventory Report Structure\nfor details.\nNeed help?\nâ\nIf you run into issues while setting up inventory reports:\nAWS documentation\n: See the\nAWS S3 Inventory documentation\nfor more information.\nAtlan support\n: If you have issues related to Atlan integration,\ncontact Atlan support\n.\nNext steps\nâ\nOnce you've configured your inventory reports:\nCrawl your S3 assets\n: Follow steps to crawl your S3 assets.\nTags:\nconnectors\ndata\ncrawl\nstorage\namazon-s3\naws\nPrevious\nSet up Amazon S3\nNext\nCrawl S3 assets\nPrerequisites\nCreate destination bucket\nConfigure inventory reports\nNeed help?\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-jwt-bearer-setup",
    "content": "Connect data\nCRM\nSalesforce\nGet Started\nSet up Salesforce\nSet up JWT bearer flow\nOn this page\nSet up JWT bearer flow\nImportant\nAtlan currently supports\nSalesforce Sales Cloud\nand\nFinancial Services Cloud\n(FSC).\nAtlan recommends using\nOAuth 2.0 JWT bearer flow\nfor secure server-to-server integration with Salesforce. This guide walks you through creating the connected app, uploading certificates, configuring policies, and preparing the integration user.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nSalesforce administrator access\nNetwork connectivity between Atlan and your Salesforce instance\nCreated a server key and certificate\n. Save the generated\nserver.crt\nand\nserver.key\nfiles securely. You need the\nserver.crt\nfile to upload to Salesforce and the\nserver.key\nfile to configure the connection in Atlan.\nCreate custom profile\nâ\nCreate a custom profile with the\nModify All Data\npermission to crawl all Salesforce objects, including custom objects. The View All Data permission isnât sufficient because it grants read-only access and can result in missing objects.\nFrom\nSetup\n, enter\nProfiles\nin\nQuick Find\nand select\nProfiles\n.\nClick\nNew Profile\nand clone\nStandard User\nEnter\nProfile Name\n, for example,\nAtlanIntegrationProfile\nClick\nSave\n, then click\nEdit\nUnder\nConnected App Access\n, check your connected app\nUnder\nAdministrative Permissions\n, select:\nAPI Enabled\nView All Data\nRun Reports\nUnder\nStandard Object Permissions\nand\nCustom Object Permissions\n, select\nRead\nand\nView All\nClick\nSave\nCreate integration user\nâ\nFollow these steps to create a dedicated user account for Atlan integration and assign the custom profile.\nFrom\nSetup\n, expand\nUsers\nunder\nAdministration\nClick\nUsers\nClick\nNew User\nEnter required fields:\nFirst Name\n,\nLast Name\n,\nUsername\n,\nEmail\n,\nNickname\nSet\nUser License\n:\nSalesforce\nSet\nProfile\n: custom profile created in the\nCreate custom profile\nsection\nClick\nSave\nIntegration user requires Salesforce license to crawl metadata in Atlan. If license is unavailable, check allowed license limit:\nSalesforce user licenses\nCreate connected app\nâ\nA connected app enables Atlan to authenticate with Salesforce using OAuth 2.0. This section guides you through creating the app and configuring OAuth settings.\nLog in to Salesforce.\nClick\nsettings\nicon\n, then click\nSetup\n.\nIn\nSetup\n, enter\nApp Manager\nin\nQuick Find\nand select\nApp Manager\n.\nClick\nNew Connected App\n.\nUnder\nBasic Information\n, enter:\nConnected App Name\n:\nAtlanConnector\nAPI Name\n: automatically populated\nContact Email\n: your email\nUnder\nAPI (Enable OAuth Settings)\n:\nCheck\nEnable OAuth Settings\nEnter\nCallback URL\n: your domain. For example,\nhttps://localhost\nAdd\nSelected OAuth Scopes\n:\nAccess Lightning applications (lightning)\nManage user data via APIs (api)\nPerform requests at any time (refresh_token, offline_access)\nCheck\nUse digital signatures\nClick\nChoose File\nand upload\nserver.crt\nClick\nSave\n, then\nContinue\nOn connected app page, click\nManage Consumer Details\nand copy\nConsumer Key\n(\nclient_id\n) and\nConsumer Secret\nBefore proceeding, wait approximately 10 minutes for connected app activation\nEdit policies\nâ\nConfigure OAuth policies to control who can access the connected app and from where. These settings provide secure access for Atlan's integration.\nFrom\nSetup\n, enter\nManage Connected Apps\nin\nQuick Find\nand select\nManage Connected Apps\n.\nLocate your connected app and click\nEdit Policies\n.\nUnder\nOAuth Policies\n:\nSet\nPermitted Users\nto\nAdmin approved users are pre-authorized\nSet\nIP Relaxation\nto\nRelax IP restrictions\nIf needed, set\nRefresh Token Policy\nto\nRefresh token is valid until revoked\nClick\nSave\nAdd server certificate\nâ\nTo add the server certificate (\nserver.crt\n) file to the connected app:\nFrom\nSetup\n, enter\napp manager\nin the\nQuick Find\nbox and select\nApp Manager\n.\nLocate your connected app, and then click the dropdown arrow and select\nEdit\n.\nFor\nAPI Enable OAuth Settings\n, check\nUse digital signatures\n.\nClick\nChoose File\nand upload the\nserver.crt\nfile.\nClick\nSave\n.\nAssign profile\nâ\nAssign the custom profile to the connected app so the integration user has the required permissions when accessing Salesforce.\nOpen connected app page\nScroll to\nManage Profile\nSelect the custom profile created in the\nCreate custom profile\nsection and click\nSave\nTroubleshooting\nâ\nIf you encounter issues with JWT Bearer authentication, see\nTroubleshooting Salesforce Connectivity\n.\nNext steps\nâ\nCrawl Salesforce\n: Configure and run your first crawl to discover Salesforce data and metadata\nTags:\nconnectors\nsalesforce\nauthentication\nPrevious\nSet up Salesforce\nNext\nSet up client credentials flow\nPrerequisites\nCreate custom profile\nCreate integration user\nCreate connected app\nTroubleshooting\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion",
    "content": "Connect data\nETL Tools\nMatillion\nGet Started\nSet up Matillion\nOn this page\nSet up Matillion\nConfigure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance. This setup creates a dedicated API user with read-only access to guarantee secure metadata extraction.\nImportant\nThis setup guide applies only to\nMatillion ETL\n.\nMatillion Data Productivity Cloud (DPC)\nisn't supported.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nMatillion Server Admin access\n: You need\nadministrator privileges\nto create users and manage permissions\nMatillion instance access\n: Ability to log in to your Matillion instance\nCreate user\nâ\nTo create a new user for\ncrawling Matillion\n:\nLog in to your Matillion instance.\nFrom the top header of your Matillion instance, click\nAdmin\n, and then from the dropdown, click\nUser Configuration\n.\nOn the\nUser Configuration\npage, click the\n+\nbutton to add a new user.\nIn the\nAdd user\ndialog, enter the following details:\nFor\nUsername\n, enter a username for the new user.\nFor\nPassword\nand\nRepeat Password\n, enter a password for the new user and confirm it in the next step.\nFor\nRole\n, click\nAPI\nto enable the new user to use the Matillion APIs.\nFor\nPermission Groups\n, click\nReader\nto enable the new user permission to only view the project and almost all parts of the instance including API profiles, credentials, OAuths, jobs, and variables - without edit access.\nClick\nOK\nto add the new user.\nOn the\nUser Configuration\npage, click\nApply changes\nto confirm new user creation.\nDid you know?\nAtlan only reads metadata from Matillion and never updates or changes any objects in your instance.\nSet permissions\nâ\nTo set permissions for the new user in Matillion:\nLog in to your Matillion instance.\nFrom the top header of your Matillion instance, click\nAdmin\n, and then from the dropdown, click\nManage Permissions\n.\nIn the\nPermissions\ndialog, for the\nReader\ngroup, click the pencil icon to grant permissions for the group.\nIn the group-specific\nPermissions - Reader\ndialog, for\nUser Permissions\n, under\nState\n, click the dropdown and then click\nGranted\n. Read permission is now available to members of the group at the project level and can override a\nForbidden\nExpected State\n.\nClick\nOK\nto confirm your selections.\nNext steps\nâ\nCrawl Matillion\n- Use the credentials you just created to establish a connection and crawl metadata from Matillion\nTags:\nconnectors\netl_tools\nmatillion\nsetup\nPrevious\nMatillion\nNext\nCrawl Matillion\nPrerequisites\nCreate user\nSet permissions\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs",
    "content": "Connect data\nEvent/Messaging\nMicrosoft Azure Event Hubs\nGet Started\nSet up Microsoft Azure Event Hubs\nOn this page\nSet up Microsoft Azure Event Hubs\nAtlan supports the following authentication methods for Microsoft Azure Event Hubs:\nSAS key\n-  this method uses a connection string-primary key to fetch metadata.\nService principal\n-  in addition to a connection string-primary key, this method requires a client ID, client secret, and tenant ID to fetch metadata.\nSAS key authentication\nâ\nWho can do this?\nYou will need your Microsoft Azure Event Hubs administrator to complete these steps   -  you may not have access yourself.\nCreate a shared access signature policy\nâ\nYou will need to create a shared access signature (SAS) policy in Microsoft Azure Event Hubs for authentication in Atlan.\nThe\nManage\npermission\nis required for the following:\nAtlan requires read permissions of the configurations set to event hubs and the event hub namespace. Since Atlan currently only supports SAS policy-based authentication,\nManage\npermission is required to provide this type of access. SAS policies do not support granular access control while\nSend\nor\nListen\npermission is insufficient to crawl configuration metadata. Granular permissions will only be available once Atlan supports other authentication methods that allow for the granular access control capabilities of Microsoft Azure.\nTo fetch the\nAzure Event Hub status attribute\nand\nAzure Event Hub consumer group assets\nthrough the Azure APIs.\nTo\ncreate a SAS policy\nfor\ncrawling Microsoft Azure Event Hubs\n:\nLog in to the\nAzure portal\n.\nOpen the menu and search for or click\nEvent Hubs\n.\nOn the\nEvent Hubs\npage, click the namespace of your event hub. Copy your\nEvent Hubs Namespace\nto use for\nauthentication in Atlan\n.\nIn the left menu of your event hub namespace, under\nSettings\n, click\nShared access policies\n.\nOn the _Shared access policie_s page, click\n+ Add\nto add a new SAS policy.\nIn the\nAdd SAS policy\nsidebar, enter the following details:\nFor\nPolicy name\n, enter a meaningful name   -  for example,\nAtlan integration policy\n.\nTo add the\nManage\npermission\nto your SAS policy, click\nManage\n.\nClick\nCreate\nto finish setup.\nOn the _Shared access policie_s page, select the newly created SAS policy.\nFrom the corresponding\nSAS Policy\ndialog, under\nConnection string-primary key\n, click the clipboard icon to copy the connection string-primary key and store it in a secure location.\nYou will need your event hub namespace and the connection string-primary key for\ncrawling Microsoft Azure Event Hubs\n.\nService principal authentication\nâ\nWho can do this?\nYou will need your Microsoft Azure Event Hubs administrator to\ncreate a shared access signature policy\nand\nCloud Application Administrator\nor\nApplication Administrator\nto\nregister an app with Microsoft Entra ID\nand\nadd it to the Event Hubs Data Sender role\n-  you may not have access yourself.\nYou need the following to authenticate the connection in Atlan:\nConnection string-primary key\n-  required to crawl Kafka assets\nClient ID\n(application ID),\nclient secret\n, and\ntenant ID\n(directory ID)   -  required to crawlÂ Microsoft Azure Event Hubs assetsÂ\nCreate a shared access signature policy\nâ\nFollow the steps in\nCreate a shared access signature policy\nto generate a connection string-primary key for\ncrawling Microsoft Azure Event Hubs\n.\nRegister app with Microsoft Entra ID\nâ\nYou will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret.\nTo register your app with Microsoft Entra ID:\nLog in to the\nAzure portal\n.\nIn the search bar, search for\nMicrosoft Entra ID\nand select it from the dropdown list.\nFrom the left menu of the\nMicrosoft Entra ID\npage, click\nApp registrations\n.\nFrom the toolbar on the\nApp registrations\npage, click\n+ New registration\n.\nOn the\nRegister an application\npage, for\nName\n, enter a name for your service principal application and then click\nRegister\n.\nOn the homepage of your newly created application, from the\nOverview\nscreen, copy the values for the following fields and store them in a secure location:\nApplication (client) ID\nDirectory (tenant) ID\nFrom the left menu of your newly created application page, click\nCertificates & secrets\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, click\n+ New client secret\n.\nIn the\nAdd a client secret\nscreen, enter the following details:\nFor\nDescription\n, enter a description for your client secret.\nFor\nExpiry\n, select when the client secret will expire.\nClick\nAdd\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, for the newly created client secret, click the clipboard icon to copy the\nValue\nand store it in a secure location.\nAdd app to Event Hubs Data Sender role\nâ\nYou will need to add the\nservice principal application\ncreated in the previous step to the\nAzure Event Hubs Data Sender role\n.\nTo add a service principal to the Azure Event Hubs Data Sender role:\nLog in to the\nAzure portal\n.\nOpen the menu and search for or click\nEvent Hubs\n.\nOn the\nEvent Hubs\npage, click the namespace of your event hub.\nFrom the left menu of your event hubs namespace page, click\nAccess Control (IAM)\n.\nIn the upper right of the\nAccess Control (IAM)\npage, navigate to the\nAdd a role assignment\ntile and then click\nAdd\n.\nOn the\nAdd a role assignment\npage, enter the following details:\nFor\nRole\n, click the dropdown to select\nAzure Event Hubs Data Sender\n-  this allows\nsend access to Azure Event Hubs resources\n.\nFor\nAssign access to\n, click the dropdown to select\nAzure AD user, group, or service principal\n.\nFor\nSelect\n, choose the\nservice principal application you created\nin the previous step.\nClick\nSave\nto save your role assignment.\nTags:\nconnectors\ndata\nauthentication\nPrevious\nMicrosoft Azure Event Hubs\nNext\nCrawl Microsoft Azure Event Hubs\nSAS key authentication\nService principal authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nGet Started\nSet up Microsoft Azure Synapse Analytics\nOn this page\nSet up Microsoft Azure Synapse Analytics\nAtlan supports crawling the followingÂ with the Microsoft Azure Synapse Analytics package:\nDedicated SQL pools\n(formerly SQL DW)\nServerless SQL pools\nAtlan supports the following authentication methods for fetching metadata from Microsoft Azure Synapse Analytics:\nBasic authentication\n-  this method uses a username and password to fetch metadata.\nService principal authentication\n-  this method requires a client ID, client secret, and tenant ID to fetch metadata.\nBasic authentication\nâ\nWho can do this?\nYou will need your\nMicrosoft Azure Synapse Analytics administrator\nto run these commands   -  you may not have access yourself.\nCreate a login\nâ\nYou must create a login within the\nmaster\ndatabase for the new user.\nTo create a login for the new user:\nCREATE\nLOGIN\n<\nlogin_name\n>\nWITH\nPASSWORD\n=\n'<password>'\n;\nReplace\n<login_name>\nwith the name of the login.\nReplace\n<password>\nwith the password for the login.\nCreate a user\nâ\nYou will need to create a new user for\nintegrating with Atlan\n.\nTo create a user for the\nnewly created login\n:\nCREATE\nUSER\n<\nusername\n>\nFOR\nLOGIN\n<\nlogin_name\n>\n;\nReplace\n<username>\nwith the username to use when integrating Atlan.\nReplace\n<login_name>\nwith the name of the login used in the previous step.\nCrawl assets and mine view lineage\nâ\nYou will need to connect to the target database that you want to\ncrawl in Atlan\n.\nThe following grant crawls all your Microsoft Azure Synapse Analytics assets and mines lineage for views.\nTo grant the minimum permissions required to crawl assets and mine view lineage from a SQL pool:\nGRANT\nVIEW\nDEFINITION\nON\nDATABASE\n::\n<\ndatabase_name\n>\nTO\n<\nusername\n>\n;\nReplace\n<database_name>\nwith the name of the database. You must grant these permissions to all the databases you want to crawl in Atlan.\nReplace\n<username>\nwith the\nusername created above\n.\nService principal authentication\nâ\nRegister app with Microsoft Entra ID\nâ\nWho can do this?\nYou will need your\nCloud Application Administrator\nor\nApplication Administrator\nto complete these stepsÂ   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization.\nYou will need to\nregister your service principal application\nwith Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret.\nTo register your app with Microsoft Entra ID:\nLog in to the\nAzure portal\n.\nIn the search bar, search for\nMicrosoft Entra ID\n, and select it from the dropdown list.\nFrom the left menu of the\nMicrosoft Entra ID\npage, click\nApp registrations\n.\nFrom the toolbar on the\nApp registrations\npage, click\n+ New registration\n.\nOn the\nRegister an application\npage, for\nName\n, enter a name for your service principal application and then click\nRegister\n.\nOn the homepage of your newly created application, from the\nOverview\nscreen, copy the values for the following fields and store them in a secure location:\nApplication (client) ID\nDirectory (tenant) ID\nFrom the left menu of your newly created application page, click\nCertificates & secrets\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, click\n+ New client secret\n.\nIn the\nAdd a client secret\nscreen, enter the following details:\nFor\nDescription\n, enter a description for your client secret.\nFor\nExpiry\n, select when the client secret will expire.\nClick\nAdd\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, for the newly created client secret, click the clipboard icon to copy the\nValue\nand store it in a secure location.\nCreate a service principal user\nâ\nTo create a service principal user:\nCREATE\nUSER\n<\nservice_principal_display_name\n>\nFROM\nEXTERNAL PROVIDER\n;\nReplace\n<service_principal_display_name>\nwith the name of the\nservice principal you created\nin the previous step.\nGrant SQL permissions\nâ\nTo grant SQL permissions to the\nservice principal\n:\nGRANT\nVIEW\nDEFINITION\nON\nDATABASE\n::\n<\ndatabase_name\n>\nTO\n<\nservice_principal_display_name\n>\n;\nReplace\n<database_name>\nwith the name of the database.\nReplace\n<service_principal_display_name>\nwith the name of the\nservice principal you created\n.\nAssign Synapse RBAC role\nâ\nWho can do this?\nYou will need your\nSynapse Administrator\nto complete these steps   -  you may not have access yourself.\nTo\nassign a Synapse role-based access control (RBAC) role\nto the service principal:\nOpen\nSynapse Studio\nand log in to your Synapse workspace.\nFrom the left menu of your Synapse workspace, click the\nManage\ntab. Then from under\nSecurity\n,Â click\nAccess control\n.\nFrom the options along the top of the\nAccess control\npage, click\n+ Add\n.\nIn the\nAdd role assignment\ntab, enter the following details:\nFor\nScope\n, select\nWorkspace\nÂ as the scope.\nFor\nRole\n, select\nSynapse Artifact User\nas the Synapse RBAC role to assign. The\nSynapse Artifact User\nrole provides read access to published code artifacts and their outputs. Although it can create new artifacts, it can neither publish changes nor run code without additional permissions.\nFor\nSelect user\n, search for and select the\nservice principal you created\n.\nClick\nApply\nto assign the Synapse RBAC role to the service principal.\nMine query history\nâ\ndanger\nAtlan currently only supports mining query history for dedicated SQL pools with the\nMicrosoft Azure Synapse Analytics miner\n. Mining query history for serverless SQL pools is currently not supported.\nTo mine query history from Microsoft Azure Synapse Analytics, complete these steps.\nEnable query store\nâ\nThe\nQuery Store\nis disabled by default for new Microsoft Azure Synapse Analytics databases. It stores 7 days of query history by default, which can be extended to 30 days.\nTo enable the Query Store for\nmining query history in Atlan\n, run the following T-SQL command:\nALTER\nDATABASE\n<\ndatabase_name\n>\nSET\nQUERY_STORE\n=\nON\n;\nReplace\n<database_name>\nwith the name of the database.\nGrant permissions\nâ\nTo mine query history, grant the following permissions:\nBasic authentication\n:\nGRANT\nVIEW\nDATABASE\nSTATE\nTO\n<\nusername\n>\nReplace\n<username>\nwith the\nusername\nyou created for basic authentication.\nService principal authentication\n:\nGRANT\nVIEW\nDATABASE\nSTATE\nTO\n<\nservice_principal_display_name\n>\nReplace\n<service_principal_display_name>\nwith the name of the\nservice principal you created\nfor service principal authentication.\nFind your SQL pool server\nâ\nTo find the server name of your SQL pool for\ncrawling Microsoft Azure Synapse Analytics\n:\nOpen\nSynapse Studio\n.\nOn the login page, select\nSynapse Workspace\n.\nFrom the left menu of your Synapse workspace, click the\nManage\ntab. Then from under\nAnalytics pools\n,Â click\nSQL pools\n.\nOn the\nSQL pools\npage, under\nName\n, select your SQL pool.\nIn the\nProperties\nform,Â navigate to\nWorkspace\nÂ\nSQL endpoint\nand copy the server name of your SQL pool and save it in a temporary location.\nTags:\nconnectors\ndata\ncrawl\nauthentication\nPrevious\nMicrosoft Azure Synapse Analytics\nNext\nMine Microsoft Azure Synapse Analytics\nBasic authentication\nService principal authentication\nMine query history\nFind your SQL pool server"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicroStrategy\nGet Started\nSet up MicroStrategy\nOn this page\nSet up MicroStrategy\nWho can do this?\nYou will probably need your\nMicroStrategy administrator\nto complete these steps   -  you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.\nCreate user in MicroStrategy\nâ\nYou will need to\ncreate a new user\nin your MicroStrategy Workstation and assign minimum permissions for integrating with Atlan.\nTo create a new user for\ncrawling MicroStrategy\n:\nOpen the\nWorkstation window\nwith the navigation pane in smart mode.\nFrom the left navigation menu, click\nUsers and Groups\n.\nIn the upper left of the\nUsers and Groups\npage, click the\nSelect an Environment\ndropdown and select your environment.\nIn the left menu of your selected environment, next to\nAll Users\n, click the\n+\nbutton to create a new user.\nIn the\nCreate\nNew User\ndialog:\nFor\nAccount and Credentials\n, enter the following details:\nFor\nFull Name\n, enter a meaningful name for the new user.\nFor\nEmail Address\n, enter an email address for the new user.\n(Optional) For\nDescription\n, enter a description.\nFor\nUsername (Login)\n, enter a username for the new user.\nFor\nPassword\n, create a password for the new user and confirm it in the next step.\nTo disallow the new user from changing the password, check the\nUser cannot change password\nbox.\nAt the bottom left of the form, check the\nActive User\nbox.\nFor\nUser Groups\n, all users are automatically members of the\nEveryone\ngroup, which typically has read permission for most objects. To assign any permissions not inherited from the default group to the new user:\nIn the top right of\nUser Groups\n, click\nManage User Group\nto add a new user group.\nClick\nUpdate\nto confirm your selections.\nTo assign\nuser privileges\n, in the left menu, clickÂ\nPrivileges\nand check the following boxes:\nUse Architect Editors\nÂ   -  for fetching attribute, fact, and table definitions\nUse Library Web\n-  for fetching project metadata\nWeb Report SQL\n-  for fetching SQL statements\nWeb use Metric Editor\n-  for fetching metric definitions\nWeb run Document\n-  for fetching document definitions\nWeb run Dossier\n-  for fetching dossier definitions\nClick\nSave\nto complete setup.\nTags:\nconnectors\ndata\ncrawl\nauthentication\nPrevious\nMicroStrategy\nNext\nCrawl MicroStrategy\nCreate user in MicroStrategy"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/how-tos/set-up-mode",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMode\nGet Started\nSet up Mode\nOn this page\nSet up Mode\nWho can do this?\nYou will probably need your Mode administrator to follow the below steps   -  you may not have access yourself. The Mode administrator will also need to be a connection admin for every connection you want Atlan to be able to crawl.\nInvite a user\nâ\nTo\ninvite a user\nfor Atlan to use when\nintegrating with Mode\n:\nFrom the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click\nInvite to Mode...\n.\nFor\nEmail Address\n, enter a valid email address, for example for the service account.\nClick the\nInvite\nbutton.\nIn your service account's email, open the email from Mode and click\nAccept invite\n.\nFor\nSet up your account\n, enter details about the service account:\nFor\nFull name\n, enter a name for the service account, such as\nAtlan Crawler\n.\nFor\nUsername\n, enter a username for the service account, such as\natlan_crawler\n.\nFor\nPassword\nand\nConfirm password\n, enter the same password to use for the service account.\nAt the bottom of the form, click the\nContinue\nbutton.\ndanger\nIf you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email.\nSet permissions\nâ\nTo set the minimum permissions required to\ncrawl Mode\n:\nLog into Mode as an administrator again. (If you just completed the steps above, you'll need to log out from the service account first.)\nFrom the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then\nWorkspace settings\n.\nUnder the\nPeople\nheading on the left, click\nMembers\n.\nNext to the\nSearch\nbox, click the dropdown and select\nCurrent members\n.\nConfirm the user you invited is listed with\nMember\nunder the\nStatus\ncolumn.\nIf not, change the\nSearch\nbox dropdown to\nPending members\nand confirm the invitation has been accepted.\nIf yes, change the\nSearch\nbox dropdown to\nFormer members & requests\n, click the three-dots icon to the far right of the service account's row, and then\nReinvite to org\n.\nUnder the\nData\nheading on the left, click\nManage Connections\n.\nFrom the\nManage Connections\ntable, for each connection you want to access in Atlan:\nClick the row for that connection.\nChange to the\nPermissions\ntab.\nAt the top of the\nConnection access\ntable, click the\nAdd members\nbutton.\nSearch for and select the service account user, and change the dropdown for access type to\nView\n. Learn more about the\nView\npermission in\nMode documentation\n.\nAt the bottom of the form, click the\nAdd members\nbutton.\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the workspaces, collections, reports, charts, or queries in your Mode instance.\nGenerate API token\nâ\nAtlan supports the following\nAPI tokens\ngenerated in Mode for authentication in Atlan:\nWorkspace token\nMember token\nPersonal token\nWorkspace token\nâ\nWorkspace tokens\nallow admin access to the workspace. You will need to be an admin user in Mode to create and manage a workspace token.\nTo generate a workspace API token for\ncrawling Mode\n:\nLog in to Mode as an administrator.\nFrom the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click\nWorkspace settings\n.\nUnder the\nFeatures\nheading on the left, click\nAPI Keys\n.\nOn the\nAPI Keys\npage, under\nWorkspace API Keys\n, click the\nCreate API key\nbutton.\nIn the\nCreate new API key\ndialog, enter the following details:\nFor\nDisplay name\n, enter a meaningful name   -  for example,\natlan-crawler\n.\nFor\nKey expiration\n, keep the default selection or set a longer expiration period.\nClick the\nCreate\nbutton.\nFrom the corresponding\nKey secret\ndialog, copy the values for\nKey ID\nand\nSecret\nand store them in a secure location. You will not be able to see them again in Mode after leaving this screen.\nMember token\nâ\ndanger\nBefore you can create a member token, you will need your Mode administrator to\nenable Member API key creation\n.\nMember tokens\nmatch an individual user's permissions to access workspace resources in Mode.\nTo generate a member API token for\ncrawling Mode\n:\nLog in to Mode as a member.\nFrom the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click\nWorkspace settings\n.\nUnder the\nWorkspace\nheading on the left, click\nPersonal\n.\nUnder the\nPersonal\nheading on the left, click\nMy API Keys\n.\nIn the upper right of the\nAPI Keys\nÂ page, click the\nCreate API key\nbutton.\nIn the\nCreate new API key\ndialog, enter the following details:\nFor\nDisplay name\n, enter a meaningful name   -  for example,\natlan-crawler\n.\nFor\nKey expiration\n, keep the default selection or set a longer expiration period.\nClick the\nCreate\nbutton.\nFrom the corresponding\nKey secret\ndialog, copy the values for\nKey ID\nand\nSecret\nand store them in a secure location. You will not be able to see them again in Mode after leaving this screen.\nPersonal token\nâ\ndanger\nMode will deprecate personal token use on February 28, 2025. You can currently continue to use existing\npersonal API tokens\n, but you will not be able to generate new personal tokens.\nTags:\nconnectors\ncrawl\nPrevious\nMode\nNext\nCrawl Mode\nInvite a user\nSet permissions\nGenerate API token"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/how-tos/set-up-mongodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMongoDB\nGet Started\nSet up MongoDB\nOn this page\nSet up MongoDB\nWho can do this?\nAtlan currently only supports integration with\nMongoDB Atlas\n. You will need your MongoDB\nOrganization Owner\nor\nProject Owner\nto complete these steps   -  you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a\nusername and password\nto fetch metadata.\nYou will also need the following\nconnection details\nfrom your MongoDB database deployment for\nintegrating with Atlan\n:\nHost name of your MongoDB database\nHost name of the SQL (or JDBC) endpoint of your MongoDB database obtained via\nData Federation\nName of the default database\nName of the authentication database\nCreate database user in MongoDB\nâ\nYou will need to create a database user in MongoDB to allow Atlan to\ncrawl MongoDB\n. A database user's access is determined by the role assigned to that user.\nYou can either:\nCreate a database user with a\nbuilt-in role\n-  provides read-only access to all databases.\nCreate a database user with a\ncustom role\n-  provides restricted access to selected databases and requires allowed actions.\nCreate database user with built-in role\nâ\nTo\nadd a database user\nwith a built-in role for\ncrawling MongoDB\n:\nSign in to your MongoDB database.\nFrom the left menu of the\nData Services\npage, under the\nSecurity\nheading, click\nDatabase Access\n.\nIn the upper right of the\nDatabase Access\npage, click\nAdd New Database User\n.\nIn the\nAdd New Database User\ndialog, enter the following details:\nFor\nAuthentication Method\n, keep the default\nPassword\n.\nFor\nPassword Authentication\n, there are two text fields:\nEnter a username for the new database user in the top text field   -  for example,\natlan_user\n.\nEnter a password in the lower text field or click the\nAutogenerate Secure Password\nbutton to copy and use an auto-generated password.\nTo assign database privileges to the new user, for\nDatabase Privileges\n, under\nBuilt-in Role\n, click the\nAdd Built-in Role\ndropdown to select a\nbuilt-in role\n:\nFrom the\nSelect role\ndropdown, click\nOnly read any database\nto assign read-only access to your MongoDB database(s).\n(Optional) By default, users can access all the clusters and federated database instances in the project. To restrict access to specific clusters and federated database instances:\nToggle on\nRestrict Access to Specific Clusters/Federated Database Instances\n.\nFor\nGrant Access To\n, check the boxes next to the clusters and federated database instances to which you want to grant access to the new database user.\nAt the bottom of the dialog, click\nAdd User\nto finish setup.\nCreate database user with custom role\nâ\nIf you have a large number of databases, you can programmatically create a custom role in MongoDB using Atlas API instead   -  refer to\nMongoDB documentation\nto learn more.\nTo\nadd a database user\nwith a custom role for\ncrawling MongoDB\n:\nSign in to your MongoDB database.\nFrom the left menu of the\nData Services\npage, under the\nSecurity\nheading, click\nDatabase Access\n.\nIn the\nDatabase Access\npage, change to the\nCustom Roles\ntab.\nIn the upper right of the\nCustom Roles\nÂ page, click\nAdd New Custom Role\n.\nIn the\nAdd Custom Role\ndialog, for\nCustom Role Name\n, enter a meaningful name   -  for example,\natlan_integration\n.\nFor\nAction or Role\n, click\nSelect Actions or Roles\nand grant the following privileges to the custom role:\nlistDatabases\n, listed under\nGlobal Actions and Roles\n-  to list all existing databases in the cluster.\nsqlGetSchema\n, listed under\nGlobal Actions and Roles\n-  to retrieve collection schema generated by MongoDB Atlas Data Federation without read or find permission on the database or collection.\nlistCollections\n, listed under\nDatabase Actions and Roles\n-  to list collections in a database.\nFor\nDatabase\n, specify all the databases you want to crawl in Atlan.\nFor\nCollection\n, you can either specify collections within selected databases or leave blank to include all.\ncollStats\n, listed under\nCollection Actions\n-  to retrieve collection metadata such as average document size, document count, and more.\nFor\nDatabase\n, specify all the databases you want to crawl in Atlan.\nFor\nCollection\n, you can either specify collections within selected databases or leave blank to include all.\nfind\n, listed under\nCollection Actions\n-  this action provides read permission on the data. Atlan requires this action for the MongoDB JDBC driver to validate Atlan's connection to the database.\nFor\nDatabase\n, specify all the databases you want to crawl in Atlan.\nFor\nCollection\n, you can either specify collections within selected databases, leave blank to include all, or restrict read access by specifying a nonexistent collection such as\nna\n,\nnone\n, or\n-\nagainst a selected database.\nClick\nAdd Custom Role\nto complete setup.\nIn the\nDatabase Access\npage, change to the\nDatabase Users\nÂ tab.\nIn the upper right of the\nDatabase Access\npage, click\nAdd New Database User\n.\nIn the\nAdd New Database User\ndialog, enter the following details:\nFor\nAuthentication Method\n, keep the default\nPassword\n.\nFor\nPassword Authentication\n, there are two text fields:\nEnter a username for the new database user in the top text field   -  for example,\natlan_user\n.\nEnter a password in the lower text field or click the\nAutogenerate Secure Password\nbutton to copy and use an auto-generated password.\nTo assign database privileges to the new user, for\nDatabase Privileges\n, under\nCustom Roles\n, click the\nAdd Custom Role\ndropdown. From the\nSelect role\ndropdown, select the custom role you created previously.\n(Optional) By default, users can access all the clusters and federated database instances in the project. To restrict access to specific clusters and federated database instances:\nToggle on\nRestrict Access to Specific Clusters/Federated Database Instances\n.\nFor\nGrant Access To\n, check the boxes next to the clusters and federated database instances to which you want to grant access to the new database user.\nAt the bottom of the dialog, click\nAdd User\nto finish setup.\nData Federation\nenables a SQL-like interface for Atlan to interact with MongoDB. It also provides schema access to collections that are either generated automatically through sampling or manual updates. This allows Atlan to fetch metadata without read access to databases or collections through the\nsqlGetSchema\npermission.\nRetrieve connection details\nâ\nTo\nretrieve connection details\nforÂ\ncrawling MongoDB\n:\nSign in to your MongoDB database.\nFrom the left menu of the\nData Services\npage, under the\nOverview\nheading, click\nDatabase\n.\nOn the\nDatabase Deployment\npage, navigate to the database deployment you want to crawl in Atlan and click\nConnect\n. From the corresponding page, under\nConnect to your application\n:\nClick\nDrivers\n, and then navigate to the\nAdd your connection string into your application code\nsection:\nCopy the host name of your MongoDB database from the code snippet and store it in a secure location. For example, in\nmongodb://myDBReader:D1fficultP%\n[email protected]\n:27017/?authSource=admin\n,\nmongodb0.example.com\nwill be the\nMongoDB native host\n.\nClose the dialog box and return to the\nConnect to your application\nÂ page.\nClick\nAtlas SQL\n, and then navigate to the\nSelect your driver\nheading:\nFrom the driver dropdown, click\nJDBC Driver\n.\nNavigate to the\nGet Connection String\nheading, and then for\nURL\n, copy the following connection details and store them in a secure location. As an example,\njdbc:mongodb://atlas-sql-64c0b504b658f37cd67dc406-xtapf.a.query.mongodb.net/atlan_db?ssl=trueauth&Source=admin\n:\nCopy the host name of the SQL (or JDBC) endpoint of your MongoDB databaseÂ\natlas-sql-64c0b504b658f37cd67dc406-xtapf.a.query.mongodb.net\nto enter as the\nSQL interface host name\n.\nCopy the name of the default database\natlan_db\nto enter as the\nDefault database\n.\nCopy the name of the authentication databaseÂ\nadmin\nto enter as the\nAuthentication database\n.\nTags:\nconnectors\ndata\nintegration\ncrawl\nauthentication\nPrevious\nMongoDB\nNext\nCrawl MongoDB\nCreate database user in MongoDB\nRetrieve connection details"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction",
    "content": "Connect data\nData Warehouses\nDatabricks\nOn-premises Setup\nSet up on-premises Databricks lineage extraction\nOn this page\nSet up on-premises Databricks lineage extraction\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Databricks instance details, including credentials.\nIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of lineage from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract lineage from your on-premises Databricks instance, you will need to use Atlan's databricks-extractor tool.\nDid you know?\nAtlan uses exactly the same databricks-extractor behind the scenes when it connects to Databricks in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the databricks-extractor tool\nâ\nTo get the databricks-extractor tool:\nRaise a support ticket\nto get the link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to extract lineage from Databricks:\nsudo docker load -i /path/to/databricks-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a\nDocker compose file\nfor the databricks-extractor tool.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises Databricks instance.\nThe file is\ndocker-compose.yaml\n.\nDefine Databricks connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your Databricks connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises Databricks instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nconnection-name:\n<<: *extract-lineage\nenvironment:\n<<: *databricks-defaults\nEXTRACT_QUERY_HISTORY: true\nQUERY_HISTORY_START_TIME_MS: 0\nvolumes:\n- ./output/connection-name:/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract-lineage\ntells the databricks-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nEXTRACT_QUERY_HISTORY\n-  specifies whether to extract query history for the Databricks connection, in addition to lineage. The query history output can then be used to calculate\nusage\nand\npopularity metrics\n.\nQUERY_HISTORY_START_TIME_MS\n-  specifies the time in epoch milliseconds from when to extract query history. If unspecified, the extractor will extract queries for the past 30 days by default. In Databricks, the\nquery history\nretains query data for the past 30 days.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Databricks connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your Databricks connections, you will need to provide a Databricks configuration file.\nThe Databricks configuration is a\n.ini\nfile with the following format:\n[DatabricksConfig]\nhost = <host>\nport = <port>\n# seconds to wait for a response from the server\ntimeout = 300\n# Databricks authentication type. Options: personal_access_token, aws_service_principal\nauth_type = personal_access_token\n# Required only if auth_type is personal_access_token.\n[PersonalAccessTokenAuth]\npersonal_access_token = <personal_access_token>\n# Required only if auth_type is aws_service_principal.\n[AWSServicePrincipalAuth]\nclient_id = <client_id>\nclient_secret = <client_secret>\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Databricks credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets:\ndatabricks_config:\nfile: ./databricks.ini\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a subsection of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the Databricks configuration file:\nsudo docker secret create databricks_config path/to/databricks.ini\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\ndatabricks_config:\nexternal: true\nname: databricks_config\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local Databricks configuration file.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\ndatabricks_config:\nexternal: true\nname: databricks_config\nx-templates:\n# ...\nservices:\ndatabricks-lineage-example:\n<<: *extract-lineage\nenvironment:\n<<: *databricks-defaults\nEXTRACT_QUERY_HISTORY: true\nQUERY_HISTORY_START_TIME_MS: 0\nvolumes:\n- ./output/databricks-lineage-example:/output\nsecrets:\n- databricks_config\nIn this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The\ndatabricks_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\ndatabricks-lineage-example\n. You can use any meaningful name you want.\nThe\n<<: *databricks-defaults\nsets the connection type to Databricks.\nThe\n./output/databricks-lineage-example:/output\nÂ line tells the extractor where to store results. In this example, the extractor will store results in theÂ\n./output/databricks-lineage-example\ndirectory on the local file system. We recommend you output the extracted lineage for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\nsecurity\naccess-control\npermissions\nPrevious\nCrawl on-premises Databricks\nNext\nSet up an AWS private network link to Databricks\nPrerequisites\nGet the compose file\nDefine Databricks connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-on-premises-microsoft-azure-synapse-analytics-miner-access",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nGet Started\nSet up on-premises Microsoft Azure Synapse Analytics miner access\nOn this page\nSet up on-premises Microsoft Azure Synapse Analytics miner access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Microsoft Azure Synapse Analytics instance details, including credentials.\nIn some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to\nmine query history from the Query Store\n. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the mining of query history from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nOnce you have mined query history on-premises and\nuploaded the results to S3\n, you can mine query history in Atlan:\nHow to mine Microsoft Azure Synapse Analytics\nPrerequisites\nâ\nTo mine query history from your on-premises Microsoft Azure Synapse Analytics instance, you will need to use Atlan's synapse-miner tool.\nDid you know?\nAtlan uses exactly the same synapse-miner behind the scenes when it connects to Microsoft Azure Synapse Analytics in the cloud.\ndanger\nIf you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the synapse-miner tool\nâ\nTo get the synapse-miner tool:\nRaise a support ticket\nto get a link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to mine Microsoft Azure Synapse Analytics:\nsudo docker load -i /path/to/synapse-miner-master.tar\nGet the compose file\nâ\nAtlan provides you with a configuration file for the synapse-miner tool. This is a\nDocker compose file\n.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises Microsoft Azure Synapse Analytics instance.\nThe file is\ndocker-compose.yml\n.\nDefine database connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your Microsoft Azure Synapse Analytics connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises Microsoft Azure Synapse Analytics instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nconnection-name:\n<<: *mine\nenvironment:\n<<: *synapsedb\nUSERNAME: <USERNAME>\nPASSWORD: <PASSWORD>\nHOST: <HOST>\nPORT: <PORT>\nDATABASE: <DATABASE>\nvolumes:\n- ./output/connection-name:/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *mine\ntells the synapse-miner tool to run.\nenvironment\ncontains all parameters for the tool:\nUSERNAME\n-  specify the database username.\nPASSWORD\n-  specify the database password.\nHOST\n-  specify the database host.\nPORT\n-  specify the database port.\nDATABASE\n-  specify the database name.\nvolumes\nspecifies where to store results. In this example, the miner will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Microsoft Azure Synapse Analytics connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Microsoft Azure Synapse Analytics credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nCreate a new Docker secret:\nprintf \"This is a secret password\" | docker secret create my_database_password -\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\nmy-database-password:\nexternal: true\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify\nPASSWORD_SECRET_PATH\nto use it as a password.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\nmy-database-password:\nexternal: true\nx-templates:\n# ...\nservices:\nmy-database:\n<<: *mine\nenvironment:\n<<: *synapsedb\nUSERNAME: <USERNAME>\nPASSWORD_SECRET_PATH: \"/run/secrets/my_database_password\"\n# ...\nvolumes:\n# ...\nsecrets:\n- my-database-password\nvolumes:\njars:\nTags:\nconnectors\ndata\nPrevious\nMine Microsoft Azure Synapse Analytics\nNext\nCrawl Microsoft Azure Synapse Analytics\nPrerequisites\nGet the compose file\nDefine database connections\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/set-up-on-premises-teradata-miner-access",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nMine\nSet up on-premises Teradata miner access\nOn this page\nSet up on-premises Teradata miner access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Teradata instance details, including credentials.\nIn some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the mining of query history from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nOnce you have mined query history on-premises and\nuploaded the results to S3\n, you can mine query history in Atlan:\nHow to mine Teradata\nPrerequisites\nâ\nTo mine query history from your on-premises Teradata instance, you will need to use Atlan's teradata-miner tool.\nDid you know?\nAtlan uses exactly the same teradata-miner behind the scenes when it connects to Teradata in the cloud.\ndanger\nIf you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the teradata-miner tool\nâ\nTo get the teradata-miner tool:\nRaise a support ticket\nto get a link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to mine Teradata:\nsudo docker load -i /path/to/teradata-miner-master.tar\nGet the compose file\nâ\nAtlan provides you with a configuration file for the teradata-miner tool. This is a\nDocker compose file\n.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises Teradata instance.\nThe file is\ndocker-compose.yml\n.\nDefine database connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your Teradata connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises Teradata instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nconnection-name:\n<<: *mine\nenvironment:\n<<: *teradatadb\nUSERNAME: <USERNAME>\nPASSWORD: <PASSWORD>\nHOST: <HOST>\nMARKER: \"0\"\nvolumes:\n- ./output/connection-name:/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *mine\ntells the teradata-miner tool to run.\nenvironment\ncontains all parameters for the tool:\nUSERNAME\n-  specify the database username.\nPASSWORD\n-  specify the database password.\nHOST\n-  specify the database host.\nMARKER\n-  specify the timestamp from when queries should be mined.\nvolumes\nspecifies where to store results. In this example, the miner will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Teradata connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Teradata credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nCreate a new Docker secret:\nprintf \"This is a secret password\" | docker secret create my_database_password -\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\nmy-database-password:\nexternal: true\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify\nPASSWORD_SECRET_PATH\nto use it as a password.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\nmy-database-password:\nexternal: true\nx-templates:\n# ...\nservices:\nmy-database:\n<<: *mine\nenvironment:\n<<: *teradatadb\nUSERNAME: <USERNAME>\nPASSWORD_SECRET_PATH: \"/run/secrets/my_database_password\"\n# ...\nvolumes:\n# ...\nsecrets:\n- my-database-password\nvolumes:\njars:\nTags:\nconnectors\ndata\nPrevious\nMine Teradata\nNext\nWhat does Atlan crawl from Teradata?\nPrerequisites\nGet the compose file\nDefine database connections\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/how-tos/set-up-prestosql",
    "content": "Connect data\nDatabases\nQuery Engines\nPrestoSQL\nGet Started\nSet up PrestoSQL\nOn this page\nSet up PrestoSQL\ndanger\nFor Starburst Presto, we recommend using the\nTrino connector\nbecause\nthe official Starburst documentation recommends using the Trino JDBC driver\n.\nWho can do this?\nYou will probably need your Presto administrator to run these commands   -  you may not have access yourself.\nAtlan only supports PrestoSQL until version 349   -  PrestoDB is not supported at present.\nCurrently, we only support basic (username and password) authentication for PrestoSQL. We recommend creating a separate user for Atlan with read-only access. Please ensure you are using frontend password authentication over HTTPS for clients.\nCreate user in PrestoSQL\nâ\nTo create a new user with password file authentication follow\nthe steps in the official Presto documentation\n.\nGrant read-only access\nâ\nTo grant read-only access to the user created above follow\nthe steps in the official Presto documentation\n. This includes adding a list of catalogs you wish to crawl to your\nrules.json\nfile, for example:\n{\n\"catalogs\"\n:\n[\n{\n\"user\"\n:\n\"atlan\"\n,\n\"catalog\"\n:\n\"postgresql\"\n,\n\"allow\"\n:\n\"read-only\"\n}\n,\n{\n\"user\"\n:\n\"atlan\"\n,\n\"catalog\"\n:\n\"mysql\"\n,\n\"allow\"\n:\n\"read-only\"\n}\n,\n...\n]\n}\nTags:\nconnectors\ndata\nauthentication\nPrevious\nPrestoSQL\nNext\nCrawl PrestoSQL\nCreate user in PrestoSQL\nGrant read-only access"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nGet Started\nSet up Redpanda Kafka\nOn this page\nSet up Redpanda Kafka\nWho can do this?\nYou will probably need your\nRedpanda Kafka administrator\nto complete these steps   -  you may not have access yourself.\nAtlan supports the\nS3 extraction method\nfor fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.\nCreate user in Redpanda Kafka\nâ\nTo\ncreate a new user\nfor\nextracting metadata from Redpanda Kafka\n:\nLog in to your\nRedpanda Console\nand select your active cluster.\nFrom the left menu of your cluster's\nOverview\npage, click the\nSecurity\ntab.\nFrom the upper right of the\nSecurity\npage, click\nCreate User\nto create a new user.\nIn the\nCreate User\ndialog, enter the following details:Â\nFor\nUsername\n, enter a name for the new user   -  for example,\nAtlan integration\n.\nFor\nPassword\n, set a password for the new user.\nFrom the\nMechanism\ndropdown, select a\nSCRAM mechanism\n.\nClick\nCreate\nto finish creating the new user.\nFrom the list of users on the\nSecurity\npage, select the newly created user to edit the\nassociated Access Control Lists (ACLs)\n.\nIn the\nEdit ACL\ndialog, enter the following details:\nFor\nTopics\n, click\nAdd Topic ACL\nto allow the following operations   -\nDescribe\nand\nDescribeConfigs\n.\nFor\nConsumer Groups\n, click\nAdd Consumer Group ACL\nto allow the following operation   -\nDescribe\n.\nFor\nTransactional ID\n, click\nAdd Transactional ID ACL\nto allow the following operation   -\nDescribe\n.\nFor\nClusters\n, click\nAdd Cluster ACL\nto allow the following operations   -\nDescribe\nand\nDescribeConfigs\n.\nOnce you have added all the required operations, click\nOK\nÂ to finish setup.\nDid you know?\nOnce you have\nextracted metadata on-premises\nand\nuploaded the results to S3\n, you can\ncrawl the metadata from Redpanda Kafka\ninto Atlan.\nTags:\nconnectors\ndata\nPrevious\nRedpanda Kafka\nNext\nCrawl Redpanda Kafka\nCreate user in Redpanda Kafka"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/set-up-salesforce",
    "content": "Connect data\nCRM\nSalesforce\nGet Started\nSet up Salesforce\nOn this page\nSet up Salesforce\nImportant\nAtlan currently supports\nSalesforce Sales Cloud\nand\nFinancial Services Cloud\n(FSC).\nTo integrate Salesforce with Atlan, you need to set up a connection between your Salesforce account and Atlan. The steps apply to both Salesforce Cloud and Financial Services Cloud (FSC).\nPrerequisites\nâ\nMake sure the following prerequisites are met, you have:\nSalesforce administrator access\nNetwork connectivity between Atlan and your Salesforce instance\nChoose authentication flow\nâ\nAtlan supports the following authentication flows for Salesforce, select the authentication method that best fits your use case:\nRecommended\nð\nJWT Bearer Flow\nSecure server-to-server integration\nNo password storage required\nProduction-ready security\nAutomated token refresh\nView Setup Guide\nð\nClient Credentials Flow\nAlternative server-to-server method\nClient ID/Secret based\nNo user interaction\nExisting credential workflows\nView Setup Guide\nð¤\nUsername-Password Flow\nTraditional authentication method\nSimple setup process\nWidely supported\nNo special hardware needed\nView Setup Guide\nNext steps\nâ\nAfter completing your chosen authentication setup, proceed to\ncrawl Salesforce\nto configure the connection in Atlan.\nTags:\nconnectors\nintegration\nsalesforce\nauthentication\nPrevious\nSalesforce\nNext\nSet up JWT bearer flow\nPrerequisites\nChoose authentication flow\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSisense\nGet Started\nSet up Sisense\nOn this page\nSet up Sisense\nWho can do this?\nYou will need your\nSisenseÂ administrator\nÂ to complete these steps   -  you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nNote that the Sisense connector does not support Sisense for Cloud Data Teams, formerly Periscope Data.\nCreate new user in Sisense\nâ\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the objects in your Sisense environment.\nYou will need to create a new user in your Sisense instance and assign the\nData Admin\nrole to the new user for integrating with Atlan. While Atlan can crawl all other\nSisense asset types\nwith the\nViewer\nrole, the\nData Admin\nrole is required to crawl and generate lineage for\ndata model tables\n. Atlan uses the\nDatamodels API\nto crawl data models and data model tables from Sisense. The\nViewer\nrole does not provide access to data models.\nTo create a new user for\ncrawling Sisense\n:\nLog in to your Sisense instance with the\nAdmin\nrole.\nFrom the tabs along the top, click\nAdmin\n.\nOn the\nAdmin\npage, in the\nSystem Management\nbox, click\nUsers\n.\nFrom the top right of the\nUsers\npage, click\n+ Users\nto add a new user.\nIn the\nAdd Users\ndialog, enter the following details:\nFor\nEmail\n, enter an email address for the new user   -  this will be the username for\nauthenticating the connection in Atlan\n.\n(Optional) For\nFirst Name\nand\nLast Name\n, enter a first and last name for the new user   -  for example,\nAtlan_integration\n.\nFor\nRole\n, click the role dropdown and then click\nData Admin\nto assign that role to the new user.\nToggle on\nDefine Password\n, and for\nSet Password\n, set a password for the new user. Confirm the password in the next step.\nClick\nSave\nto complete new user creation.\nTags:\nconnectors\ndata\ncrawl\napi\nauthentication\nPrevious\nSisense\nNext\nCrawl Sisense\nCreate new user in Sisense"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/set-up-teradata",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nGet Started\nSet up Teradata\nOn this page\nSet up Teradata\nWho can do this?\nYou will probably need your Teradata administrator to run these commands   -  you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from Teradata. This method uses a username and password to fetch metadata.\nTo create a username and password for basic authentication for Teradata, run the following commands:\nCreate role in Teradata\nâ\nCreate a role in Teradata\nusing the following commands:\nCREATE\nrole atlan_user_role\nCreate user in Teradata\nâ\nCreate a new user for\nintegrating with Atlan\nusing the following commands:\nCREATE\nUSER\natlan_user\nFROM\n[\ndatabase\n]\nAS\nPASSWORD\n=\n[\npassword\n]\nPERM\n=\n20000000\n;\nGrant access to the role or directly to the user with the following commands:\nGRANT\nSELECT\nON\ndbc\n.\ndatabases\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\ndbc\n.\ntables\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\ndbc\n.\nTablesV\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\nDBC\n.\nTableStatsV\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\ndbc\n.\ncolumns\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\ndbc\n.\nTableTextV\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\ndbc\n.\nTableSizeV\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\nDBC\n.\nColumnsV\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\nDBC\n.\nIndicesV\nTO\natlan_user_role\n;\nGRANT\nSELECT\nON\nDBC\n.\nAll_RI_ChildrenV\nTO\natlan_user_role\n;\nGrant additional permissions to\nmine query history\nwith the following commands:\nGRANT\nSELECT\nON\ndbc\n.\ndbqlogtbl\nTO\natlan_user_role\n;\nGrant role to user\nâ\nTo grant the\natlan_user_role\nto the new user:\nGRANT\natlan_user_role\nTO\natlan_user\n;\nTags:\nconnectors\ndata\ncrawl\nauthentication\nPrevious\nTeradata\nNext\nCrawl Teradata\nCreate role in Teradata\nCreate user in Teradata\nGrant role to user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nGet Started\nSet up ThoughtSpot\nOn this page\nSet up ThoughtSpot\nWho can do this?\nYou will probably need your ThoughtSpot instance administrator to complete these steps   -  you may not have access yourself.\nAtlan supports the following authentication methods for ThoughtSpot:\nBasic authentication\nTrusted authentication\nBasic authentication\nâ\nYou will need to create a new user in ThoughtSpot and authenticate in Atlan with username and password.\nCreate user in ThoughtSpot\nâ\nTo\ncreate a new user\nfor\ncrawling ThoughtSpot\n:\nLog in to your ThoughtSpot instance.\nTo navigate to the admin console, in the top header, click\nAdmin\n.\nIn the top left of the\nAdmin\npage, click\nAdd User\n.Â\nIn the\nAdd a new user\ndialog, enter the following details:\nFor\nUsername\n, enter a username for the new user.\nFor\nDisplay name\n, add a meaningful name for the new user   -  for example,\nAtlan\n.\nFor\nSharing visibility\n, keep the default selection   -\nSHAREABLE\n.\ndanger\nAtlan will only crawl assets that are either created by or shared with this user. If you add the user to a group in ThoughtSpot, ensure that you\nshare all the assets\nyou want to crawl in Atlan with that group.\nFor\nNew password\n, enter a password for the new user and confirm it in the next step.\nFor\nEmail\n, enter an email address for the new user.\nClick\nAdd\nto add the new user.\nThe new user will be assigned\nCan upload user data\nand\nCan download data\npermissions\nby default.\nTrusted authentication\nâ\ndanger\nYou will need ThoughtSpot Everywhere to use the trusted authentication option. ThoughtSpot Analytics users, however, can get ThoughtSpot Everywhere as an add-on to use trusted authentication. Learn more\nhere\n.\nYou will need to create a secret key in ThoughtSpot and authenticate in Atlan with username and secret key.\nCreate a secret key\nâ\nTo\ncreate a secret key\nfor\ncrawling ThoughtSpot\n:\nLog in to your ThoughtSpot instance.\nTo navigate to the developer console, in the top header, click\nDevelop\n.\nIn the left menu under\nCustomizations\n, clickÂ\nSecurity settings\n.Â\nIn the top right of the\nSecurity settings\npage, click\nEdit\n.\nScroll down to\nTrusted authentication\nand turn it on.\nTurning on trusted authentication will generate a secret key. Click the clipboard icon to copy the secret key and store it in a secure location.\nClick\nSave Changes\nto save your selections.\nTags:\nconnectors\ncrawl\nauthentication\nPrevious\nThoughtSpot\nNext\nSet up on-premises ThoughtSpot access\nBasic authentication\nTrusted authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-username-password-setup",
    "content": "Connect data\nCRM\nSalesforce\nGet Started\nSet up Salesforce\nSet up username-password flow\nOn this page\nSet up username-password flow\nImportant\nAtlan currently supports\nSalesforce Sales Cloud\nand\nFinancial Services Cloud\n(FSC).\nAtlan supports the Salesforce\nOAuth 2.0 username-password\nflow for scenarios where server-to-server integration isn't possible. This flow enables Atlan to authenticate using the integration user's credentials along with a connected app in Salesforce.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nSalesforce administrator access\nNetwork connectivity between Atlan and your Salesforce instance\nCreate connected app\nâ\nTo enable Atlan to connect to your Salesforce instance, you need to create a connected app. This app provides the OAuth credentials that Atlan uses for authentication.\nLog in to Salesforce.\nIn the upper right, click the\nsettings\nicon\nand select\nSetup\n.\nIn\nSetup\n, enter\napps\nin the\nQuick Find\nbox and select\nApp Manager\n.\nClick\nNew Connected App\nin the upper-right corner.\nUnder\nBasic Information\n, provide:\nConnected App Name\n: for example,\nAtlanConnector\nContact Email\n: your email address\nUnder\nAPI (Enable OAuth Settings)\n:\nCheck\nEnable OAuth Settings\nCheck\nEnable for Device Flow\nSet\nCallback URL\nto any domain, for example,\nhttps://localhost\n(unused)\nAdd the following\nSelected OAuth Scopes\n:\nAccess Lightning applications (lightning)\nManage user data via APIs (api)\nPerform requests at any time (refresh_token, offline_access)\nCheck\nRequire Secret for Web Server Flow\nCheck\nRequire Secret for Refresh Token Flow\nClick\nSave\n.\nOn the resulting page, click\nContinue\n.\nUnder\nAPI (Enable OAuth Settings)\n, click\nManage Consumer Details\nand copy:\nConsumer Key (Client ID)\nConsumer Secret (Client Secret)\nWait approximately 10 minutes for the connected app to activate.\nRetrieve security token\nâ\nSalesforce requires a security token in addition to your password for API access. You'll need to retrieve this token from your user settings and append it to your password when configuring the connection in Atlan.\nIn Salesforce, click the integration user's\nuser icon\nin the upper-right and select\nSettings\n.\nExpand\nMy Personal Information\nand click\nReset My Security Token\n.\nClick\nReset Security Token\n.\nCopy the resulting security token.\nImportant\nTo\ncrawl Salesforce\n, enter your\npassword + security token\nin the\nPassword\nfield (for example, password\nxyz\nand token\n123\nâ\nxyz123\n).\nTroubleshooting\nâ\nIf you encounter issues with Username-password authentication, see\nTroubleshooting Salesforce Connectivity\n.\nNext steps\nâ\nCrawl Salesforce\n: Configure and run your first crawl to discover Salesforce data and metadata\nTags:\nconnectors\nsalesforce\nauthentication\nPrevious\nSet up client credentials flow\nNext\nCrawl Salesforce\nPrerequisites\nCreate connected app\nRetrieve security token\nTroubleshooting\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/troubleshooting/task-and-crawl-issues",
    "content": "Connect data\nETL Tools\nInformatica CDI\nTroubleshooting\nTask and crawl issues\nOn this page\nTask and crawl issues\nResolve common connectivity and crawling issues when working with the Informatica CDI connector.\nMissing objects despite designer role assignment\nâ\nError\nSome objects are still missing after assigning the Designer role to the user.\nCause\nâ\nProjects/Folers/Tasks can have explicit restrictions that override role-based permissions at a parent level.\nSolution\nâ\nLog in to Informatica CDI with the same service account configured in Atlan.\nNavigate to the Mapping task and\nexport the task\nRepeat the same operation for the underlying Mapping\nIf either activity fails, contact your Informatica Cloud Admin for additional permissions.\nObjects or folders not in workflow filter appear in assets\nâ\nError\nObjects or folders that aren't part of the workflow filter configuration appear in Atlan.\nCause\nâ\nIn Informatica CDI, the Mapping Task and underlying Mappings can be present in different projects or folders.\nSolution\nâ\nCheck if unexpected object is linked to a Mapping Task that is configured as part of the filter.\nIf so, this is designed behaviour\nMissing source or target lineage for mapping task\nâ\nError\nSource or target lineage for a particular mapping task is missing.\nCause\nâ\nThis issue can occur when:\nSource or target assets referenced by the Mapping Task haven't been crawled into Atlan\nParameter files used by the Mapping Task aren't uploaded or are from a different environment (DEV vs PROD)\nYou might see debug messages like:\nDEBUG process - _create_process_asset_from_raw_data: Process <process_id> (Mapping Task: <mapping_task_name>): Source pattern: EDW/DEV/FINANCE_ARR_RAW, Target pattern: EDW/DEV/FINANCE_ARR_BRNZ\nDEBUG cache_resolver - resolve_asset_from_connection_cache: No assets found for pattern: EDW/DEV/FINANCE_ARR_BRNZ\nDEBUG process - _create_process_asset_from_raw_data: Skipping process <process_id> (Mapping Task: <mapping_task_name>, ID: <mapping_task_id>): Source resolved: True, Target resolved: False\nSolution\nâ\nVerify that source or target system are supported and the corresponding crawlers are set up with the correct filters.\nMake sure parameter files are uploaded and match the environment/project you're crawling.\nNeed help\nâ\nIf you need assistance after trying the steps, contact Atlan support:\nSubmit a request\n.\nTags:\nconnectors\netl-tools\ninformatica\ncdi\nPrevious\nWhat does Atlan crawl from Informatica CDI\nNext\nTasks, transformations, and lineage\nMissing objects despite designer role assignment\nObjects or folders not in workflow filter appear in assets\nMissing source or target lineage for mapping task\nNeed help"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/faq/tasks-transformations-and-lineage",
    "content": "Connect data\nETL Tools\nInformatica CDI\nFAQ\nTasks, transformations, and lineage\nOn this page\nTasks, transformations, and lineage\nFind answers to common questions about the Informatica Cloud Data Integration connector.\nWhat types of tasks does the connector support?\nâ\nThe connector currently supports\nMapping Tasks\n.\nWhat transformations are explicitly processed?\nâ\nThe following transformations receive special handling with detailed information extraction:\nSource\nTarget\nExpression\nLookup\nJoiner\nFilter\nRouter\nAggregator\nWhat about transformations not in the above list?\nâ\nTransformations that are not listed above are also processed and are taken into account for lineage calculations.\nHow are Lookup transformations handled?\nâ\nLookup transformations work similarly to source transformations and generate lineage.\nFor unconnected lookups, lineage is created based on the positioning of columns in the transformation.\nWhat IICS CDI API calls are executed by the connector?\nâ\nThe following API calls are executed:\nAPI\nPayload Details\nDescription\n/user/login\n{\"@type\":\"login\",\"username\":\"***\",\"password\":\"****\"}\nLogin to IICS Cloud\n/public/core/v3/objects\nq=type=='MTT'\nq=type=='PROJECT'\nq=type=='FOLDER'\n{{mappingTaskId}}/references?refType=usedBy\nGet listing for different objects\n/public/core/v3/export\n{\"name\":\"\",\"objects\":[{\"id\":\"{{mappingTaskId}}\",\"includeDependencies\":true}]}\n{\"name\":\"\",\"objects\":[{\"id\":\"{{taskfFlowId}}\",\"includeDependencies\":false}]}\nRun export job\n/public/core/v3/export/{{jobId}}\nâ\nMonitor export job\n/public/core/v3/export/{{jobId}}/package\nâ\nDownload export job results\n/api/v2/connection/name/{{connectionName}}\n/api/v2/connection/\nâ\nGet connection object details\nAre parameter files processed for lineage?\nâ\nYes. When you upload parameter files to the Atlan Informatica workflow, the connector processes them to create accurate lineage connections.\nAre taskflow-level parameters processed?\nâ\nYes. The connector processes parameters defined at the Taskflow level when you upload the corresponding parameter files to the workflow.\nWhat content formats are supported for parameter files?\nâ\nThe connector supports parameter files that follow\nIICS CDI guidelines\nfor parameterization.\nIn what form must parameter files be uploaded?\nâ\nUpload parameter files as a\n.zip\narchive with these supported MIME types:\napplication/zip\napplication/x-zip\napplication/x-zip-compressed\nWhat source and target systems are supported for lineage?\nâ\nLineage is generated for sources and targets that are\nrelational\nin nature, such as Snowflake and Oracle.\nNot supported:\nFiles, web services, and similar non-relational systems.\nIs lineage generated at the field level?\nâ\nYes. The connector creates lineage at the\nfield and column level\nbetween sources and targets.\nTo view field-level details, expand the source or target table in the lineage diagram.\nIf a custom query (SQL override) is used for a source transformation, is lineage generated?\nâ\nYes. The connector parses SQL queries and generates lineage based on the parsing results.\nWhat are the limitations?\nâ\nParameter file handling limitations\nParameter file names defined in Taskflows aren't always applied to Mapping Tasks correctly.\nView supported and unsupported scenarios\nSupported scenarios:\nParameter and directory mentioned in Assignment transformation level and type as content\nParameter and directory mentioned in Data task level and type as content\nUnsupported scenarios:\nParameter and directory mentioned in Start as input field\nWorkaround:\nCreate a\nparam.meta\nfile and upload it with your parameter files. Format the content as key-value pairs:\n<\nMappingName\n>\n=\n<\nUploaded_Param_filename\n>\nOther limitations\nMapplets:\nTreated as single transformations within mappings. Inner transformations aren't exposed individually.\nSQL parsing:\nHas known limitations.\nIncremental extraction:\nNot supported. The connector scans all objects in each run based on your filter configuration.\nSecure Agent:\nThe connector is currently not available to run as a Secure Agent application\nAuthentication Types:\n- The connector currently supports only basic authentication. Support for SAML based authentication is planned to be added.\nTags:\nconnectors\netl-tools\ninformatica\ncdi\nfaq\ntasks\ntransformations\nlineage\nPrevious\nTask and crawl issues"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/concepts/transformation-logic",
    "content": "Connect data\nETL Tools\nInformatica CDI\nConcepts\nTransformations\nOn this page\nTransformations\nWhen you build mappings in Informatica Cloud Data Integration, you embed business logic in Transformation objects. These transformations determine how source data is processed to form target data.\nHow Atlan captures transformation logic\nâ\nWhen Atlan crawls your Informatica CDI environment, it reads key transformation logic and surfaces it for easier consumption.\nThe following table shows the transformation types and the business logic that Atlan captures. The table also indicates where you can find this information in Atlan's interface:\nTransformation Type\nWhere in Atlan\nExpression\nTransformation Field > Sidebar > Expression\nAggregator - Group by columns\nTransformation > Sidebar > Expression\nAggregator - Aggregate functions\nTransformation > Sidebar > Expression\nRouter\nTransformation > Sidebar > Expression\nJoiner - Join Condition\nTransformation > Sidebar > Expression\nJoiner - Join Type\nTransformation > Sidebar > Expression\nLookup - Lookup Condition\nTransformation > Sidebar > Expression\nLookup - Lookup Query\nTransformation > Sidebar > Query\nSource - Source Filter\nTransformation > Sidebar > Query\nSource - Source Query\nTransformation > Sidebar > Query\nSee also\nâ\nWhat does Atlan crawl from Informatica CDI\n: Complete reference of all assets and metadata discovered during crawling\nTags:\nconnectors\netl-tools\ninformatica\ntransformations\nlogic\nconcepts\nPrevious\nCrawl Informatica CDI assets\nNext\nWhat does Atlan crawl from Informatica CDI\nHow Atlan captures transformation logic\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/troubleshooting/troubleshooting-aws-glue-connectivity",
    "content": "Connect data\nETL Tools\nAWS Glue\nTroubleshooting\nTroubleshooting AWS Glue connectivity\nOn this page\nTroubleshooting AWS Glue connectivity\nWhat are the known limitations of the AWS Glue connector?\nâ\nAtlan currently does not support the following:\nParsing\nMAP\ntype objects for columns and nested columns.\nWhat does the sizeBytes property mean in AWS Glue crawls?\nâ\nIn Atlan, the sizeBytes property is derived from the SizeKey parameter returned by AWS Glue. This value represents the total size of all files associated with a table, in bytes. For example, if a Glue table points to an S3 location with three files, each 1 MB in size, sizeBytes shows 3,145,728 bytes (3 Ã 1,048,576). For more information, see\nParameters set on Data Catalog tables by crawler - AWS Glue\n.\nTags:\nintegration\nconnectors\nPrevious\nWhat does Atlan crawl from AWS Glue?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/troubleshooting/troubleshooting-metabase-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMetabase\nTroubleshooting\nTroubleshooting Metabase connectivity\nOn this page\nTroubleshooting Metabase connectivity\nWhy are assets missing for a collection?\nâ\nCheck\npermissions attached to the Atlan group\n. The group should have\nView\npermission for the collection.\nWhy are Metabase questions missing for a dashboard?\nâ\nCheck the Metabase question's collection. Was the collection excluded by the\nmetadata filters configured when crawling Metabase\n? If so, you can\nmodify the workflow and re-run it\n.\nTags:\nintegration\nconnectors\nPrevious\nPreflight checks for Metabase"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/troubleshooting/troubleshooting-microsoft-teams",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nMicrosoft Teams\nTroubleshooting\nTroubleshooting Microsoft Teams\nOn this page\nTroubleshooting Microsoft Teams\nWhy do I get an error while adding Atlan to Microsoft Teams?\nâ\nIf you get an error message   -  for example,\nApp not found\n-  while\nadding the Atlan app\nfrom the\nApps\npage to your team in Microsoft Teams, it may be due to caching issues. In that case, youÂ can try installing the app directly from the Teams admin center. Search for the Atlan app in the Teams admin center, click the button to the left of the Atlan logo and select the\nAdd to a team\noption to integrate the Atlan app with your team.\nTags:\nintegration\nconnectors\nPrevious\nLink your Microsoft Teams account\nNext\nWhat is included in the Microsoft Teams integration?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/troubleshooting/troubleshooting-mode-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMode\nTroubleshooting\nTroubleshooting Mode connectivity\nOn this page\nTroubleshooting Mode connectivity\nWhat are the known limitations of the Mode connector?\nâ\nAtlan currently does not support the following:\nLineage between\nMode queries\nand Glue assets when connected through Trino.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\nPrevious\nPreflight checks for Mode"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/troubleshooting/troubleshooting-redash-connectivity",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nRedash\nTroubleshooting\nTroubleshooting Redash connectivity\nOn this page\nTroubleshooting Redash connectivity\nDoes Atlan crawl unpublished dashboards?\nâ\nAtlan does not support crawling unpublished dashboards. Unlike unpublished queries, unpublished dashboards are only visible to the users who created them in Redash.\nWhy don't I see Redash alerts?\nâ\nAtlan currently does not support Redash alerts as assets.\nTags:\ndashboards\nvisualization\nanalytics\nintegration\nconnectors\nalerts\nmonitoring\nnotifications\nPrevious\nPreflight checks for Redash"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/troubleshooting/troubleshooting-scim-provisioning",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSCIM\nTroubleshooting SCIM provisioning\nOn this page\nTroubleshooting SCIM provisioning\nAtlan currently supports SCIM provisioning for the following SSO providers:\nAzure AD\nOkta\nWhat information does Atlan sync from SSO providers?\nâ\nAtlan syncs the user's first name, last name, username, email ID, group information, and user status through group mapping. The username and email ID are only synced once when the user is provisioned in Atlan for the first time.\nCan I change the username of a provisioned user in Atlan?\nâ\nNo, once you have\nintegrated SCIM in Atlan\n, the usernames of provisioned users will be dependent on your SCIM provider. For example, if a username has changed due to an automation at source or in the case of a migration from one provider to another, you will not be able to update usernames in Atlan.\nUsernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. Ensure that your username in the SCIM provider matches that in Atlan.\nWhat will happen if an SSO or Atlan group is renamed?\nâ\nIf SCIM provisioning is enabled and an SSO group that is mapped to Atlan is renamed, changes will sync automatically. Renaming an Atlan group does not affect SCIM functionality.\nWhat happens if an SSO group is deleted?\nâ\nIf an SSO group is deleted in the SSO provider, then the group mapping will also be deleted in Atlan. The corresponding group in Atlan will remain active, but all the users will be removed from that group.\nHowever, if you would like to retain the group membership for your users in Atlan, you can first delete the group mapping in Atlan and then delete your SSO group in the SSO provider.\nWhat happens if a user is deleted from the SSO provider?\nâ\nIf users are removed from your SSO provider, then the same users will also be\ndeactivated in Atlan\n. Their status will be displayed as\nDisabled\n. To permanently delete them from Atlan, you can\nremove the users and transfer ownership of assets\n.\nWhat happens if a username already exists in Atlan?\nâ\nIf a user with the username\nuser.name\nand email address\n[email protected]\nalready exists in Atlan and another user with the same username\nuser.name\nbut different email address\n[email protected]\nis to be added via SSO, it will create a conflict in Atlan. The existing user will remain in Atlan while the new SSO user will not be synced.\nWhen does the SCIM token expire?\nâ\nThe SCIM token does not expire by default and can only be revoked if deleted.\nCan user removal affect the SCIM tokens that user created?\nâ\nYes,\nuser removal\nwill also result in the deletion of any SCIM tokens created by that user. For more guidance, see the\nUser management FAQ\n.\nDoes SCIM provisioning work only after a provisioned user has logged into Atlan?\nâ\nNo, SCIM provisioning works as soon as the user has been provisioned from the SSO provider. For example, even if the user is yet to log into Atlan, the user profile can be updated or the user disabled in Atlan directly from the SSO provider.\nIf SCIM is enabled and a user has never logged into Atlan, the status of the user will be\nEnabled\nby default. Once the user has logged in, their last login activity will be displayed in the\nLast Active\ncolumn.\nCan I assign SCIM provisioned users as asset owners before their first login?\nâ\nYes, you can assign\nasset ownership\nto SCIM provisioned users even if they are yet to log into Atlan for the first time.\nHow can I manage users in Atlan?\nâ\nFollowing are the detailed permissions for managing your users in Atlan:\nPermission\nSCIM on (SSO enforced)\nSCIM on (SSO not enforced)\nSCIM off (SSO enforced)\nSCIM off (SSO not enforced)\nInvite user from Atlan\nâ\nâ\nâ\nâ\nEdit user profile in Atlan\nâ\nâ\nâ\nâ\nAdd users to Atlan groups\nâ Only for unmapped groups\nâ Only for unmapped groups\nâ\nâ\nEnable or disable users in Atlan\nâ\nâ\nâ\nâ\nTags:\nintegration\nconnectors\nPrevious\nHow to enable Okta for SCIM provisioning\nNext\nSSO Integration"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/troubleshooting/troubleshooting-servicenow",
    "content": "Configure Atlan\nIntegrations\nProject Management\nServiceNow\nTroubleshooting\nTroubleshooting ServiceNow\nOn this page\nTroubleshooting ServiceNow\nWhy is the security_admin role required to complete the ServiceNow integration?\nâ\nAtlan strongly recommends that a System Administrator with a\nsecurity_admin role\ncompletes the integration from start to finish. However, it is not necessary and may depend on the rules or policies configured on your ServiceNow instance.\nFor example, while a default System AdministratorÂ can\ncreate an OAuth application\n, they may not be permitted to modify access control lists (ACLs)Â onÂ ServiceNow. The security_admin role has elevated privileges precisely for creating or modifying ACLs.Â This is required to\nconfigure the Atlan integration in ServiceNow\n.\nWhat is the Atlan auto-approve workflow attached to the Atlan Data Access catalog in ServiceNow? Can it be removed?\nâ\nThe Atlan auto-approve workflow is a basic workflow that auto-approves the Requested Item (RITM) for requests created in the Atlan Data Access catalog. This does not mean that requests have been auto-completed. Atlan will wait for the final request state to be updated to the state selected in the governance workflow before completing the request. For example, if you have selected\nClosed Complete\n, the request state in ServiceNow must reflect that state before Atlan can mark the request as completed.\nYes, you can remove the Atlan auto-approve workflow from your ServiceNow instance. You can configure the Process Engine setup for any requests in the Atlan Data Access catalog to adhere to existing processes in your organization.\nWhy are our custom request states from ServiceNow not showing up in Atlan?\nâ\nIf your custom request states from ServiceNow are not showing up in Atlan, please\ncontact Atlan support\n.\nThe commit update set action is failing. How can I resolve it?\nâ\nIf the commit update set action is failing, please\ncontact Atlan support\nas this may be due to conflicts. The Atlan team may need to generate a different update set or work with your ServiceNow System Administrator to set up the process manually.\nCan we modify the Atlan Data Access catalog?\nâ\nAtlan strongly recommends that you refrain from modifying the Atlan Data Access catalog in ServiceNow. Removing any existing variables used by Atlan can cause errors in the request creation process. However, you can add any non-mandatory variables to the catalog. Bear in mind that Atlan will not populate such variables.\nTags:\nintegration\nconnectors\nsecurity\naccess-control\npermissions\nPrevious\nLink your ServiceNow account"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/troubleshooting/troubleshooting-sisense-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSisense\nTroubleshooting\nTroubleshooting Sisense connectivity\nOn this page\nTroubleshooting Sisense connectivity\nWhat are the known limitations of the Sisense connector?\nâ\nAtlan currently does not support column-level lineage for Sisense assets.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\nPrevious\nPreflight checks for Sisense"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nTroubleshooting\nTroubleshooting Slack\nOn this page\nTroubleshooting Slack\nWhat do the colors in Slack notifications for modified assets mean?\nâ\nThese colors are based on\nasset certifications\n, which are as follows:\nGreen =\nVerified\nYellow =\nDraft\nRed =\nDeprecated\nBlue =\nNo certificate\nHow do I send messages or search assets from Slack?\nâ\nSlack is\nintegrated with Atlan\n. However, sending messages and searching assets from Slack are disabled. You cannot push messages from Slack to Atlan directly. You can, however, add a Slack thread to an asset profile in Atlan using the Slack icon.\nIn the channel that you've integrated with Atlan, you can use the\n/search-term <term_name>\ncommand to search for glossary terms or the\n/search-query <saved query name>\ncommand to search for queries.\nWhy am I getting a\nThere was error processing the request\nmessage?\nâ\nIf you encounter a\nThere was error processing the request\nerror message while attempting to\ntake action on a request in Slack\n, this means that the request is no longer active in Atlan. That may be due to one of the following reasons:\nThe request raised was being approved by two or more approvers simultaneously.\nThe request was deleted by the requester.\nTags:\nintegration\nconnectors\nfaq\ntroubleshooting\nPrevious\nLink your Slack account\nNext\nHow do I send messages or search assets from Slack?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/troubleshooting/troubleshooting-spreadsheets",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nTroubleshooting\nTroubleshooting spreadsheets\nOn this page\nTroubleshooting spreadsheets\nWhy do I need admin consent for exporting assets to Microsoft Excel?\nâ\nYou are likely to\nrequire admin consent\nfor exporting assets to Microsoft Excel in the following scenarios:\nIf your Microsoft Entra admin has turned on the\nAssignment required\nsetting for the app, each user in the organization will require admin approval to provide consent to Atlan for exporting assets. Learn more about the setting in\nMicrosoft documentation\n.\nIf the app has been uninstalled by your Microsoft Entra admin, this request will automatically reinstall the app once your admin has approved it.\nWhy do I get a 400 status code on exporting assets?\nâ\nAtlan recommends that you try again after disconnecting and then reconnecting the spreadsheet tool integration from your user profile. If the issue persists,\ncontact Atlan support\n.\nWhy does Microsoft Excel sync show that zero changes were detected?\nâ\nMicrosoft Excel may take longer to autosave changes on the workbook. If you notice that zero changes were detected, retry syncing after a few seconds.\nDoes the asset export option distinguish between description and user description?\nâ\nNo, the\nDescription\ncolumn on the spreadsheet only shows one value, the latest\ndescription\n, whether you\nexport assets from Atlan\nor use the Atlan extension in spreadsheets.\nTags:\nintegration\nconnectors\nPrevious\nHow to update column metadata in Microsoft Excel\nNext\nSend alerts for workflow events"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/troubleshooting/troubleshooting-thoughtspot-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nTroubleshooting\nTroubleshooting ThoughtSpot connectivity\nOn this page\nTroubleshooting ThoughtSpot connectivity\nWhat are the known limitations of the ThoughtSpot connector?\nâ\nAtlan currently does not support the following:\nColumn-level lineage between ThoughSpot tables and views.\nUpstream lineage to source assets for\nThoughtSpot views\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\nupstream-dependencies\ndata-sources\nPrevious\nWhat does Atlan crawl from ThoughtSpot?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/update-column-metadata-in-microsoft-excel",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nHow to update column metadata in Microsoft Excel\nOn this page\nUpdate column metadata in Microsoft Excel\nOnce you've\nconnected Atlan with Microsoft Excel\n, you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.\nAtlan currently supports importing and updating column metadata for the following asset types:\nTables\nViews\nMaterialized views\nLooker explores\nMicrosoft Power BI tables\nSalesforce objects\nTableau data sources\ndanger\nYou will need to be logged into your Atlan instance before you can start importing column metadata for your Atlan assets in Microsoft Excel. If you do not have the\npermission\nto update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Microsoft Excel. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access.\nImport column metadata\nâ\nYou can import column metadata for your data assets directly into Microsoft Excel.\nTo import column metadata for your data assets to Microsoft Excel:\nIn the menu bar of your Microsoft Excel workbook, click\nAtlan\n.\nFrom the\nAtlan\ntab, click\nEnrich metadata\nto view a list of your data assets in a sidebar.\n(Optional) To filter your assets by a specific asset type, in the Atlan sidebar, click the asset type   -  for example,\nTableau Datasources\n.\nIn the Atlan sidebar on your worksheet, you can either:\nIndividually select the data asset(s) you want to import.\nTo the left of the\nImport\nbutton, click the\nSelect All\ncheckbox to select all the assets that have loaded in the sidebar. (Optional) Scroll down and click\nLoad more\nto load more assets in the sidebar.\nClick\nImport\nto import column metadata for your selected assets.\nThe column metadata for your selected assets are now available in Microsoft Excel! ð\nDid you know?\nIf any changes are made to your imported columns in Atlan, you'll need to import those columns once again to access the updated version in Microsoft Excel.\nUpdate column metadata\nâ\nOnce you've imported your data assets from Atlan, you can edit the metadata for your selected data asset in Microsoft Excel. You can make changes to the column metadata once all the columns have been successfully imported.\nYou can only make changes to the metadata in the following columns:\nDescription\nCertification Status\nCertification Message\nAnnouncement Type\nAnnouncement Title\nAnnouncement Message\nTags\nYou\ncannot\nmake the following changes:\nEdit headers for any of the columns\nEdit the metadata in the\nColumn Name\n,\nData Type\n,\nPropagated Tags\n, and\nQualified Name\ncolumns\nDelete any columns or rows\nAny of these changes will not be pushed to Atlan and you'll receive an error message.\nBulk update tags for columns\nâ\ndanger\nYou cannot make any changes to the metadata in the\nPropagated Tags\ncolumn.\nNavigate to the\nTags\ncolumn to add tags to your column assets in Microsoft Excel:\nWhen adding tags to columns:\nThe tag must already exist in Atlan. If the tag does not exist in Atlan, updates will not sync and you will receive an error message.\nTag match is case-sensitive, ensure that the tag is formatted exactly as it exists in Atlan. For example, if the tag in Atlan is formatted as\nMarketing Analysis\n, then columns tagged with\nmarketing analysis\nwill not sync.\nYou can add multiple tags in the\nTags\ncolumn   -  separate multiple tags with a comma\n,\n.\nIf you have added tags that exist in Atlan as well as ones that do not,\nonly\nthe existing tags will be synced. The unsupported tags will not sync and you will receive an error message.\nTag propagation\nis disabled by default in Atlan, hence tags will not be propagated.\nPush your changes to Atlan\nâ\nOnce you've made changes to the column metadata, to push your changes:\nIn the menu bar of your Microsoft Excel workbook, click\nAtlan\n.\nFrom the\nAtlan\ntab, click\nSync to Atlan\n.\nThe Atlan sidebar will appear once the changes have synced.\n(Optional) Click\nOpen in Atlan\nto verify the changes.\nIn Atlan, an\nUpdated using Microsoft Excel\nstamp will appear in the\nactivity log\nfor updated assets. (Optional) Click theÂ\nMicrosoft Excel\nÂ link to view the source spreadsheet from Atlan.\nDid you know?\nYou can\ndownload impacted assets\nfor impact analysis in Microsoft Excel.\nTags:\nconnectors\ndata\nintegration\ncrawl\nPrevious\nHow to update column metadata in Google Sheets\nNext\nTroubleshooting spreadsheets\nImport column metadata\nUpdate column metadata\nPush your changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/administration/logs/how-tos/view-event-logs",
    "content": "Configure Atlan\nAdministration\nMonitoring\nHow to view event logs\nOn this page\nView event logs\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to view event logs.\nEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.\nYou will first need to configure any of the following supported sources to receive events:\nAirflow/OpenLineage\nAmazon MWAA/OpenLineage\nAstronomer/OpenLineage\nGoogle Cloud Composer/OpenLineage\nApache Spark/OpenLineage\nAnomalo\nOnce you have configured a supported source, you can view event logs for your events from the admin center:\nView a list of the 20 most recently received events. For every event, you can also view the timestamp for when it was received in Atlan based on your local timezone and 24-hour time notation, name of the connector configured, and event details.\nFilter events by connectors â Airflow (also includes all other supported distributions, Amazon MWAA, Astronomer, and Google Cloud Composer), Apache Spark, and Anomalo.\nExpand any event to view the full JSON code.\nView event logs\nâ\nYou can view event logs in Atlan through two methods, depending on your role and workflow preferences:\nVia admin panel\nâ\nTo view event logs:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder the\nLogs\nheading of your admin\nWorkspace\n, click\nEvent logs\n.\nOn the\nEvent logs\npage, you can view a list of up to 20 most recently received events in Atlan.\n(Optional) Click the\nConnector\ndropdown to filter events by the connector configured:\nClick\nAirflow\nto view events received through\nAirflow\n,\nAmazon MWAA\n,\nAstronomer\n, or\nGoogle Cloud Composer\nconnections.\nClick\nSpark\nto view events received through\nApache Spark\nconnections.\nClick\nAnomalo\nto view events received through\nAnomalo\nconnections.\n(Optional) Click the refresh button to refresh event logs and view the latest events.\nFor any event listed in the event logs, you can view the timestamp for when it was received in Atlan, name of the connector configured, and event details. (Optional) Click any event to view more details in the\nEvent details\nsidebar:\nView the JSON code, connector name, and timestamp for the event received. When viewing the code, you can also click the brackets to collapse or expand them.\nClick the copy icon to copy the event details.\nClick the expand icon to view the JSON code in fullscreen mode.\nVia connection profile\nâ\ncaution\nð¤\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view event logs.\nNavigate to\nWorkflow > Manage\n.\nSelect the\nListeners\ntab.\nClick or search for the connection you want.\nClick the connection to open its profile.\nClick the\nEvents\ntab.\nThis view shows events specific to that connection.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nintegration\nconnectors\nPrevious\nRestrict glossary visibility\nNext\nHow to view query logs\nView event logs"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/references/what-does-atlan-crawl-from-aiven-kafka",
    "content": "Connect data\nEvent/Messaging\nAiven Kafka\nReferences\nWhat does Atlan crawl from Aiven Kafka?\nOn this page\nWhat does Atlan crawl from Aiven Kafka?\nAtlan crawls and maps the following assets and properties from Aiven Kafka.\nOnce you've\ncrawled Aiven Kafka\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nTopics\n-  Message count, size (MB), partition count, and cleanup policy filters\nConsumer groups\n-  Member count and topic name filters\nTopics\nâ\nAtlan maps topics from Aiven Kafka to its\nKafkaTopic\nasset type.\nSource property\nAtlan property\nTopic\nname\nPartitionCount\nkafkaTopicPartitionsCount\nReplicationFactor\nkafkaTopicReplicationFactor\nsegment.byte\nkafkaTopicSegmentBytes\ncompression.type\nkafkaTopicCompressionType\ncleanup.policy\nkafkaLogTopicCleanupPolicy\nisInternal\nkafkaTopicIsInternal\nsizeInBytes\nkafkaTopicSizeInBytes\nrecordCount\nkafkaTopicRecordCount\nretention.ms\nkafkaTopicRetentionTimeInMs\nConsumer groups\nâ\nAtlan maps consumer groups from Aiven Kafka to its\nKafkaConsumerGroup\nasset type.\nDid you know?\nConsumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Aiven Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset.\nSource property\nAtlan property\nGROUP\nname\nmemberCount\nkafkaConsumerGroupMemberCount\nReplicationFactor\nkafkaTopicReplicationFactor\ntopic_names\nkafkaTopicNames\nTOPIC\nkafkaConsumerGroupTopicConsumptionProperties.topicName\nPARTITION\nkafkaConsumerGroupTopicConsumptionProperties.topicPartition\nLAG\nkafkaConsumerGroupTopicConsumptionProperties.topicLag\nCURRENT-OFFSET\nkafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Aiven Kafka\nNext\nPreflight checks for Aiven Kafka\nTopics\nConsumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/references/what-does-atlan-crawl-from-amazon-msk",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nReferences\nWhat does Atlan crawl from Amazon MSK?\nOn this page\nWhat does Atlan crawl from Amazon MSK?\nOnce you've\ncrawled Amazon MSK\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nTopics\n-  Message count, size (MB), partition count, and cleanup policy filters\nConsumer groups\n-  Member count and topic name filters\nAtlan crawls and maps the following assets and properties from Amazon MSK.\nAtlan currently only supports asset-level lineage between topics and consumer groups. Upstream, downstream, and column-level lineage are currently not supported.\nTopics\nâ\nAtlan maps topics from Amazon MSK to its\nKafkaTopic\nasset type.\nSource property\nAtlan property\nTopic\nname\nPartitionCount\nkafkaTopicPartitionsCount\nReplicationFactor\nkafkaTopicReplicationFactor\nsegment.byte\nkafkaTopicSegmentBytes\ncompression.type\nkafkaTopicCompressionType\ncleanup.policy\nkafkaLogTopicCleanupPolicy\nisInternal\nkafkaTopicIsInternal\nsizeInBytes\nkafkaTopicSizeInBytes\nrecordCount\nkafkaTopicRecordCount\nretention.ms\nkafkaTopicRetentionTimeInMs\nConsumer groups\nâ\nAtlan maps consumer groups from Amazon MSK to its\nKafkaConsumerGroup\nasset type.\nDid you know?\nConsumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Amazon MSK will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset.\nSource property\nAtlan property\nGROUP\nname\nmemberCount\nkafkaConsumerGroupMemberCount\nReplicationFactor\nkafkaTopicReplicationFactor\ntopic_names\nkafkaTopicNames\nTOPIC\nkafkaConsumerGroupTopicConsumptionProperties.topicName\nPARTITION\nkafkaConsumerGroupTopicConsumptionProperties.topicPartition\nLAG\nkafkaConsumerGroupTopicConsumptionProperties.topicLag\nCURRENT-OFFSET\nkafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Amazon MSK\nNext\nPreflight checks for Amazon MSK\nTopics\nConsumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage/references/what-does-atlan-crawl-from-amazon-mwaa-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nAmazon MWAA OpenLineage\nReferences\nWhat does Atlan crawl from Amazon MWAA/OpenLineage?\nOn this page\nWhat does Atlan crawl from Amazon MWAA/OpenLineage?\nOnce you have\nintegrated Amazon MWAA/OpenLineage\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported:\nStatus filter   -  last run status for an asset\nDuration filter   -  last run duration for an asset\nAtlan maps the following assets and properties from Amazon MWAA/OpenLineage. Asset lineage support depends on the\nlist of operators supported by OpenLineage\n.\nDAGs\nâ\nAtlan maps DAGs (directed acyclic graphs) from Amazon MWAA/OpenLineage to its\nAirflowDAG\nasset type.\nSource property\nAtlan property\nschedule_interval\nairflowDagSchedule\ntimetable\nairflowDagScheduleDelta\ntags\nairflowTags\nversion\nairflowRunVersion\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nrunid\nairflowRunName\nrun_type\nairflowRunType\neventTime\nairflowRunStartTime\neventTime\nairflowRunEndTime\neventType\nairflowRunOpenLineageState\nquery\nairflowTaskSql\ngroup_id\nairflowTaskGroupName\nTasks\nâ\nAtlan maps tasks from Amazon MWAA/OpenLineage to its\nAirflowTask\nasset type.\nSource property\nAtlan property\nretries\nairflowTaskRetryNumber\npool\nairflowTaskPool\npool_slots\nairflowTaskPoolSlots\nqueue\nairflowTaskQueue\npriority_weight\nairflowTaskPriorityWeight\ntrigger_rule\nairflowTaskTriggerRule\noperator_class\nairflowTaskOperatorClass\ndag_id\nairflowDagName\nconn_id\n`airflowTaskConnectionId`\nsql\nairflowTaskSql\ntags\nairflowTags\nversion\nairflowRunVersion\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nrunid\nairflowRunName\nrun_type\nairflowRunType\neventTime\nairflowRunStartTime\neventTime\nairflowRunEndTime\neventType\nairflowRunOpenLineageState\nTags:\nconnectors\ncrawl\nPrevious\nHow to integrate Amazon MWAA/OpenLineage\nDAGs\nTasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/references/what-does-atlan-crawl-from-anomalo",
    "content": "Connect data\nData Quality & Observability\nAnomalo\nReferences\nWhat does Atlan crawl from Anomalo?\nOn this page\nWhat does Atlan crawl from Anomalo?\nOnce you have\nintegrated Anomalo\n, Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.\nYou can\nuse connector-specific filters\nfor quick asset discovery. The following Anomalo filters are currently available for supported SQL assets:\nHas checks   -  filter SQL assets associated with Anomalo checks\nCheck status   -  filter SQL assets by overall data quality status such as\nPass\nor\nFail\nCheck methods   -  filter SQL assets by specific\ntypes of checks configured\nNumber of checks   -  filter SQL assets by total count of checks configured in Anomalo\nLast checked   -  filter SQL assets by timestamp for when any associated checks were last scanned in Anomalo\nAtlan crawls and maps the following assets and properties from Anomalo.\nChecks\nâ\nAtlan maps checks from Anomalo to its\nAnomaloCheck\nasset type.\nSource property\nAtlan property\nname\nname\ndescription\ndescription\nrun_config._metadata.check_type\nanomaloCheckCategoryType\nrun_config.check\nanomaloCheckType\nrun_config._metadata.priority_level\nanomaloCheckPriorityLevel\nrun_config._metadata.is_system_check\nanomaloCheckIsSystemAdded\nstatus\nanomaloCheckStatus\ncheck_status_image_url\nanomaloCheckStatusImageUrl\ncompleted_at\nanomaloCheckLastRunCompletedAt\nresults.evaluated_message\nanomaloCheckLastRunEvaluatedMessage\ncheck_run_url\nanomaloCheckLastRunUrl\nresults.history_message\nanomaloCheckHistoricRunStatus\nSupported sources\nâ\nIf you have crawled supported data sources, you can view Anomalo checks on your existing assets in Atlan:\nAmazon Athena\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nMySQL\nOracle\nPostgreSQL\nPrestoSQL\nSAP HANA\nSnowflake\nTeradata\nTags:\nconnectors\ndata\ncrawl\nPrevious\nHow to integrate Anomalo\nNext\nPreflight checks for Anomalo\nChecks\nSupported sources"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/references/what-does-atlan-crawl-from-apache-airflow-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nReferences\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nOn this page\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nOnce you have\nintegrated Apache Airflow/OpenLineage\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported:\nStatus filter   -  last run status for an asset\nDuration filter   -  last run duration for an asset\nAtlan maps the following assets and properties from Apache Airflow/OpenLineage. Asset lineage support depends on the\nlist of operators supported by OpenLineage\n.\nDAGs\nâ\nAtlan maps DAGs (directed acyclic graphs) from Apache Airflow/OpenLineage to its\nAirflowDAG\nasset type.\nSource property\nAtlan property\nDescription\njob.name\nname\nName of the Airflow DAG\n-\nqualifiedName\nUnique identifier for the DAG in Atlan\ndescription\ndescription\nDescription of the DAG from Airflow\nowners\nsourceOwners\nOriginal owner information from Airflow\n-\nownerUsers\nValidated Atlan usernames (mapped from source owners)\nschedule_interval\nairflowDagSchedule\nDAG's schedule interval (cron expression or preset)\ndelta\nairflowDagScheduleDelta\nSchedule interval in seconds\ntags\nairflowTags\nTags assigned to the DAG\nrun_id\nairflowRunName\nUnique identifier for the DAG run\nrun_type\nairflowRunType\nType of run (scheduled, manual, backfill)\neventTime (start)\nairflowRunStartTime\nTimestamp when the DAG run started\neventTime (end)\nairflowRunEndTime\nTimestamp when the DAG run completed\neventType\nairflowRunOpenLineageState\nFinal status of the DAG run\nversion\nairflowRunVersion\nAirflow version\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nOpenLineage adapter version\n-\nsourceURL\nDirect link to the DAG in Airflow UI\n-\nconnectionName\nName of the connector instance\n-\nconnectionQualifiedName\nUnique identifier for the connector instance\n-\nconnectorName\nName of the connector type\nDid you know?\nIf a DAG has more than 10 valid owner email addresses (comma-separated), only the first 10 will be captured and published.\nTasks\nâ\nAtlan maps tasks from Apache Airflow/OpenLineage to its\nAirflowTask\nasset type.\nSource property\nAtlan property\nDescription\njob.name (partial)\nname\nName of the task (extracted from full job name)\n-\nqualifiedName\nUnique identifier for the task in Atlan\n-\nairflowDagName\nName of the parent DAG\n-\nairflowDagQualifiedName\nUnique identifier for the parent DAG in Atlan\noperator_class\nairflowTaskOperatorClass\nType of operator used for the task\nconn_id\nairflowTaskConnectionId\nConnection ID used by the task\nsql\nairflowTaskSql\nSQL query (for SQL-based operators)\nowner\nsourceOwners\nOwner information from the task definition\neventTime (start)\nairflowRunStartTime\nTimestamp when the task started\neventTime (end)\nairflowRunEndTime\nTimestamp when the task completed\neventType\nairflowRunOpenLineageState\nFinal status of the task run\nrun_id\nairflowRunName\nUnique identifier for the task run\nrun_type\nairflowRunType\nType of run (from parent DAG)\npool\nairflowTaskPool\nWorker pool assigned to the task\npool_slots\nairflowTaskPoolSlots\nNumber of pool slots used by the task\npriority_weight\nairflowTaskPriorityWeight\nPriority weight for execution order\nqueue\nTags:\nconnectors\ncrawl\nPrevious\nHow to implement OpenLineage in Airflow operators\nNext\nPreflight checks for Apache Airflow\nDAGs\nTasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/references/what-does-atlan-crawl-from-apache-kafka",
    "content": "Connect data\nEvent/Messaging\nApache Kafka\nReferences\nWhat does Atlan crawl from Apache Kafka?\nOn this page\nWhat does Atlan crawl from Apache Kafka?\nAtlan crawls and maps the following assets and properties from Apache Kafka.\nOnce you've\ncrawled Apache Kafka\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nTopics\n-  Message count, size (MB), partition count, and cleanup policy filters\nConsumer groups\n-  Member count and topic name filters\nTopics\nâ\nAtlan maps topics from Apache Kafka to its\nKafkaTopic\nasset type.\nSource property\nAtlan property\nTopic\nname\nPartitionCount\nkafkaTopicPartitionsCount\nReplicationFactor\nkafkaTopicReplicationFactor\nsegment.byte\nkafkaTopicSegmentBytes\ncompression.type\nkafkaTopicCompressionType\ncleanup.policy\nkafkaLogTopicCleanupPolicy\nisInternal\nkafkaTopicIsInternal\nsizeInBytes\nkafkaTopicSizeInBytes\nrecordCount\nkafkaTopicRecordCount\nretention.ms\nkafkaTopicRetentionTimeInMs\nConsumer groups\nâ\nAtlan maps consumer groups from Apache Kafka to its\nKafkaConsumerGroup\nasset type.\nDid you know?\nConsumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Apache Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset.\nSource property\nAtlan property\nGROUP\nname\nmemberCount\nkafkaConsumerGroupMemberCount\nReplicationFactor\nkafkaTopicReplicationFactor\ntopic_names\nkafkaTopicNames\nTOPIC\nkafkaConsumerGroupTopicConsumptionProperties.topicName\nPARTITION\nkafkaConsumerGroupTopicConsumptionProperties.topicPartition\nLAG\nkafkaConsumerGroupTopicConsumptionProperties.topicLag\nCURRENT-OFFSET\nkafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset\nTags:\nconnectors\ncrawl\nPrevious\nCrawl on-premises Kafka\nNext\nPreflight checks for Apache Kafka\nTopics\nConsumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/references/what-does-atlan-crawl-from-apache-spark-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nApache Spark OpenLineage\nReferences\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nOn this page\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nOnce you have\nintegrated Apache Spark/OpenLineage\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported:\nStatus filter   -  last run status for an asset\nDuration filter   -  last run duration for an asset\nAtlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.\nJobs\nâ\nAtlan maps jobs from Apache Spark to its\nSparkJob\nasset type. Atlan also supports column-level lineage for Spark jobs.\nSource property\nAtlan property\nDescription\njob.name\nname\nName of the Spark job\n-\nqualifiedName\nUnique identifier for the job in Atlan\nDerived from job.name\nsparkAppName\nName of the Spark application (substring before first '.')\nspark.master\nsparkMaster\nSpark master URL (for example, yarn, local, and more.)\n-\nconnectionQualifiedName\nUnique identifier for the connector instance\n-\nconnectorName\nName of the connector instance\nOpenLineage metadata\nâ\nAtlan reports OpenLineage operational metadata for Spark jobs.\nSource\nAtlan property\nDescription\nrun.runId\nsparkRunId\nUnique run identifier\nrun.facets.spark_version.spark-version\nsparkRunVersion\nSpark runtime version\nrun.facets.spark_version.openlineage-spark-version\nsparkRunOpenLineageVersion\nOpenLineage library version\nSTART\nevent timestamp\nsparkRunStartTime\nJob start time\nCOMPLETE\n/\nABORT\n/\nFAIL\nevent timestamp\nsparkRunEndTime\nJob end time\nFinal event type\nsparkRunOpenLineageState\nStatus of the job (COMPLETE, FAIL, ABORT)\nTags:\nconnectors\ndata\ncrawl\nPrevious\nHow to integrate Apache Spark/OpenLineage\nNext\nTroubleshooting Apache Spark/OpenLineage connectivity\nJobs\nOpenLineage metadata"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage/references/what-does-atlan-crawl-from-astronomer-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nAstronomer OpenLineage\nReferences\nWhat does Atlan crawl from Astronomer/OpenLineage?\nOn this page\nWhat does Atlan crawl from Astronomer/OpenLineage?\nOnce you have\nintegrated Astronomer/OpenLineage\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported:\nStatus filter   -  last run status for an asset\nDuration filter   -  last run duration for an asset\nAtlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the\nlist of operators supported by OpenLineage\n.\nDAGs\nâ\nAtlan maps DAGs (directed acyclic graphs) from Astronomer/OpenLineage to its\nAirflowDAG\nasset type.\nSource property\nAtlan property\nschedule_interval\nairflowDagSchedule\ntimetable\nairflowDagScheduleDelta\ntags\nairflowTags\nversion\nairflowRunVersion\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nrunid\nairflowRunName\nrun_type\nairflowRunType\neventTime\nairflowRunStartTime\neventTime\nairflowRunEndTime\neventType\nairflowRunOpenLineageState\nquery\nairflowTaskSql\ngroup_id\nairflowTaskGroupName\nTasks\nâ\nAtlan maps tasks from Astronomer/OpenLineage to its\nAirflowTask\nasset type.\nSource property\nAtlan property\nretries\nairflowTaskRetryNumber\npool\nairflowTaskPool\npool_slots\nairflowTaskPoolSlots\nqueue\nairflowTaskQueue\npriority_weight\nairflowTaskPriorityWeight\ntrigger_rule\nairflowTaskTriggerRule\noperator_class\nairflowTaskOperatorClass\ndag_id\nairflowDagName\nconn_id\n`airflowTaskConnectionId`\nsql\nairflowTaskSql\ntags\nairflowTags\nversion\nairflowRunVersion\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nrunid\nairflowRunName\nrun_type\nairflowRunType\neventTime\nairflowRunStartTime\neventTime\nairflowRunEndTime\neventType\nairflowRunOpenLineageState\nTags:\nconnectors\ncrawl\nPrevious\nHow to integrate Astronomer/OpenLineage\nDAGs\nTasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/references/what-does-atlan-crawl-from-bigid",
    "content": "Connect data\nPrivacy & Security\nBigID\nReferences\nWhat does Atlan crawl from BigID?\nOn this page\nWhat does Atlan crawl from BigID?\nOnce you have\ncrawled BigID\n, you can\nuse connector-specific filters\nfor quick asset discovery. This document provides details on the metadata and assets that Atlan crawls from BigID.\nAttributes\nâ\nAtlan maps BigID Attributes, associated with Catalog Objects as a result of scans, to the\nCustom Metadata Property\nspecified in the configuration. Multiple attributes are concatenated with comma as the delimiter.\nTags\nâ\nAtlan sources both system-assigned and user-assigned tags on BigID Catalog Objects to source-linked Atlan Tags on the associated Atlan assets. The sourced tags can then be propagated and assigned like regular Atlan Tags.\nwarning\nAssigning values for BigID-sourced Tags on Atlan and reverse-sync of those values back to BigID is currently not supported.\nPolicy violations\nâ\nAtlan maps Policy violations detected for Catalog Objects during BigID scans as\nAnnouncements\non Atlan. The Announcement messages indicate the details of the Policies found violated. Once these violations are marked resolved on BigID, the corresponding Atlan Announcements are updated or removed appropriately.\nSee also\nâ\nSet up BigID\nCrawl BigID\nTags:\nconnectors\ndata\ncrawl\nprivacy\nbigid\nPrevious\nCrawl BigID\nAttributes\nTags\nPolicy violations\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/references/what-does-atlan-crawl-from-cloudera-impala",
    "content": "Connect data\nDatabases\nQuery Engines\nCloudera Impala\nReferences\nWhat does Atlan crawl from Cloudera Impala?\nOn this page\nWhat does Atlan crawl from Cloudera Impala?\nAtlan crawls and maps the following assets and properties from Cloudera Impala. This integration helps you understand and govern data stored in Impala by organizing metadata into Atlan asset types and enabling downstream visibility, including lineage.\nLineage\nâ\nAtlan supports the following lineage in Cloudera Impala.\nAsset-level and field-level lineage for\nTables &rarr; Views\nAsset-level and field-level lineage for\nViews &rarr; Views\nSchema\nâ\nAtlan maps databases from Cloudera Impala schema to its\nSchema\nasset type.\nSource property\nAtlan property\nowner\nsourceOwners\ncomment\ndescription\nexternal_location\nschemaExternalLocation\nTables\nâ\nAtlan maps Cloudera Impala tables to its\nTables\nasset type.\nSource property\nAtlan property\nrows\nrowCount\nsizeBytes\nsizeBytes\nowner\nsourceOwners\nstats.partitionCount\n(Only applicable for Kudu tables)\npartitionCount\nOutputFormat\n(Not available for Impala-managed Kudu tables for Impala 3.3+)\nexternalLocationFormat\nLocation\nexternalLocation\nTable Type\nsubType\ncomment\ndescription\ntable_query\ntableDefinition\nViews\nâ\nAtlan maps tables from Cloudera Impala views to its\nViews\nasset type.\nSource property\nAtlan property\nrows\nrowCount\nsizeBytes\nsizeBytes\nowner\nsourceOwners\nTable Type\nsubType\nKUDU\n(only set for Kudu tables)\ntableType\ncomment\ndescription\nVIEW_DEFINITION\ndefinition\nColumns\nâ\nAtlan maps columns from Cloudera Impala to its\nColumn\nasset type.\nSource property\nAtlan property\ncolumn_name\nname\nENCODING\n(only applicable for Kudu table types)\ncolumnEncoding\nCOMPRESSION\n(only applicable for Kudu table types)\ncolumnCompression\nPRIMARY_KEY\n(only applicable for Kudu tables)\nisPrimary\nISNULLABLE\nisNullable\nTYPE\ndataType\nCOMMENT\ndescription\nTags:\nlineage\ndata-lineage\nimpact-analysis\nschema\nschema-drift\nschema-monitoring\nintegration\nconnectors\ndownstream-impact\ndependencies\nPrevious\nCrawl Cloudera Impala\nNext\nPreflight Checks for Cloudera Impala\nLineage\nSchema\nTables\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/references/what-does-atlan-crawl-from-confluent-kafka",
    "content": "Connect data\nEvent/Messaging\nConfluent Kafka\nReferences\nWhat does Atlan crawl from Confluent Kafka?\nOn this page\nWhat does Atlan crawl from Confluent Kafka?\nAtlan crawls and maps the following assets and properties from Confluent Kafka.\nOnce you've\ncrawled Confluent Kafka\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nTopics\n-  Message count, size (MB), partition count, and cleanup policy filters\nConsumer groups\n-  Member count and topic name filters\nTopics\nâ\nAtlan maps topics from Confluent Kafka to its\nKafkaTopic\nasset type.\nSource property\nAtlan property\nTopic\nname\nPartitionCount\nkafkaTopicPartitionsCount\nReplicationFactor\nkafkaTopicReplicationFactor\nsegment.byte\nkafkaTopicSegmentBytes\ncompression.type\nkafkaTopicCompressionType\ncleanup.policy\nkafkaLogTopicCleanupPolicy\nisInternal\nkafkaTopicIsInternal\nsizeInBytes\nkafkaTopicSizeInBytes\nrecordCount\nkafkaTopicRecordCount\nretention.ms\nkafkaTopicRetentionTimeInMs\ndanger\nRetrieving\nsizeInBytes\nConfluent Kafka to be set up with a Cloud API key and secret\n. If the Cloud API key is not configured or\nthe API key and Secret are not set\n,\nsizeInBytes\nwill be set to\n0\n.\nConsumer groups\nâ\nAtlan maps consumer groups from Confluent Kafka to its\nKafkaConsumerGroup\nasset type.\nDid you know?\nConsumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Confluent Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset.\nSource property\nAtlan property\nGROUP\nname\nmemberCount\nkafkaConsumerGroupMemberCount\nReplicationFactor\nkafkaTopicReplicationFactor\ntopic_names\nkafkaTopicNames\nTOPIC\nkafkaConsumerGroupTopicConsumptionProperties.topicName\nPARTITION\nkafkaConsumerGroupTopicConsumptionProperties.topicPartition\nLAG\nkafkaConsumerGroupTopicConsumptionProperties.topicLag\nCURRENT-OFFSET\nkafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset\nTags:\nconnectors\ncrawl\nPrevious\nCrawl on-premises Kafka\nTopics\nConsumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/references/what-does-atlan-crawl-from-confluent-schema-registry",
    "content": "Connect data\nEvent/Messaging\nConfluent Schema Registry\nReferences\nWhat does Atlan crawl from Confluent Schema Registry?\nOn this page\nWhat does Atlan crawl from Confluent Schema Registry?\nAtlan crawls and maps the following assets and properties from Confluent Schema Registry.\nSubjects\nâ\nAtlan maps subjects from Confluent Schema Registry to its\nSubject\nasset type.\nSource property\nAtlan property\nsubject\nname\nschemaType\nschemaRegistrySchemaType\nid\nschemaRegistrySchemaId\nsubject\nschemaRegistrySubjectBaseName\nsubject\nschemaRegistrySubjectIsKeySchema\ncompatibilityLevel\nschemaRegistrySubjectSchemaCompatibility\nversion\nschemaRegistrySubjectLatestSchemaVersion\nschema\nschemaRegistrySubjectLatestSchemaDefinition\nTags:\nschema\nschema-drift\nschema-monitoring\nintegration\nconnectors\nPrevious\nCrawl Confluent Schema Registry\nNext\nPreflight checks for Confluent Schema Registry\nSubjects"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/references/what-does-atlan-crawl-from-cratedb",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nReferences\nWhat does Atlan crawl from CrateDB?\nOn this page\nWhat does Atlan crawl from CrateDB?\nCrateDB uses a single-cluster architecture where all data is stored within one logical database. Unlike traditional databases that support multiple databases, CrateDB organizes data using schemas within a single cluster. This affects how Atlan maps CrateDB assets:\nDatabase\n: Represents the single CrateDB cluster\nSchemas\n: Organize tables, views, and other objects within the cluster\nTables/Views\n: Store the actual data and metadata\nOnce you have\ncrawled CrateDB\n, you can\nuse connector-specific filters\nfor quick asset discovery. Atlan extracts and maps the following assets and properties from CrateDB during crawling.\nDatabases\nâ\nAtlan maps databases from CrateDB to its\nDatabase\nasset type.\nSource property\nAtlan property\nDescription\nTABLE_CATALOG\nname\nDatabase name\nSCHEMA_COUNT\nschemaCount\nNumber of schemas in the database\nSchemas\nâ\nAtlan maps schemas from CrateDB to its\nSchema\nasset type.\nSource property\nAtlan property\nDescription\nTABLE_SCHEMA\nname\nSchema name\nTABLE_COUNT\ntableCount\nNumber of tables in the schema\nVIEW_COUNT\nviewsCount\nNumber of views in the schema\n'crate'\n(literal)\ndatabaseName\nDatabase name\nTables\nâ\nAtlan maps tables from CrateDB to its\nTable\nasset type.\nSource property\nAtlan property\nDescription\nTABLE_NAME\nname\nTable name\nREMARKS\ndescription\nTable description\nCOLUMN_COUNT\ncolumnCount\nNumber of columns in the table\nROW_COUNT\nrowCount\nNumber of rows in the table\nSIZE_BYTES\nsizeBytes\nTable size in bytes\nIS_PARTITIONED\nisPartitioned\nWhether the table is partitioned\nPARTITION_STRATEGY\npartitionStrategy\nPartitioning strategy used\nPARTITION_COUNT\npartitionCount\nNumber of partitions\nTable partitions\nâ\nAtlan maps table partitions from CrateDB to its\nTablePartition\nasset type.\nSource property\nAtlan property\nDescription\nTABLE_NAME\nname\nPartition name\nPARTITION_STRATEGY\npartitionStrategy\nPartitioning strategy\nROW_COUNT\nrowCount\nNumber of rows in the partition\nSIZE_BYTES\nsizeBytes\nPartition size in bytes\nPARTITION_COUNT\npartitionCount\nNumber of sub-partitions\nCOLUMN_COUNT\ncolumnCount\nNumber of columns in the partition\nViews\nâ\nAtlan maps views from CrateDB to its\nView\nasset type.\nSource property\nAtlan property\nDescription\nTABLE_NAME\nname\nView name\nREMARKS\ndescription\nView description\nCOLUMN_COUNT\ncolumnCount\nNumber of columns in the view\nVIEW_DEFINITION\ndefinition\nSQL definition of the view\nColumns\nâ\nAtlan maps columns from CrateDB to its\nColumn\nasset type.\nSource property\nAtlan property\nDescription\nCOLUMN_NAME\nname\nColumn name\nREMARKS\ndescription\nColumn description\nORDINAL_POSITION\norder\nColumn position in the table\nDATA_TYPE\ndataType\nData type of the column\nIS_NULLABLE\nisNullable\nWhether the column accepts NULL values\nIS_PARTITION_COLUMN\nisPartition\nWhether the column is used for partitioning\nPARTITION_ORDER\npartitionOrder\nOrder of the column in partitioning\nIS_PRIMARY_KEY\nisPrimary\nWhether the column is part of the primary key\nCOLUMN_DEFAULT\ndefaultValue\nDefault value for the column\nIS_GENERATED\nis_generated\nWhether the column is atuomatically generated\nTags:\nconnectors\ncratedb\ndatabase\nmetadata\ncrawl\nPrevious\nCrawl CrateDB\nNext\nPreflight checks for CrateDB\nDatabases\nSchemas\nTables\nTable partitions\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/references/what-does-atlan-crawl-from-dagster",
    "content": "Connect data\nOrchestration & Workflow\nDagster\nReferences\nWhat does Atlan crawl from Dagster\nOn this page\nWhat does Atlan crawl from Dagster\nPrivate Preview\nDagster assets are crawled in as the FlowControlOperation Atlan type. This reference provides details about the metadata attributes that Atlan captures from Dagster and where they appear in the Atlan interface.\nMetadata attributes\nâ\nSource Attribute\nAtlan Attribute\nWhere in Atlan\nAsset Key\nname\nAsset profile and overview sidebar\nDescription\ndescription\nAsset profile and overview sidebar\nOrganization\nflowProjectName\nOverview sidebar\nAsset Group\nflowFolderName\nAsset profile and overview sidebar\nLatest Materialization Run Id\nflowRunId\nOverview sidebar\nLatest Materialization Run Timestamp\nflowStartedAt\nOverview sidebar\nPartition Key [\ndagster/partition\n]\nflowInputParameters.dagsterPartitionKey\nAPI only\nLatest Materialization Job Name\nflowInputParameters.dagsterJobName\nAPI only\nKinds [\ndagster/kind\n]\nflowInputParameters.dagsterKinds\nAPI only\nLatest Materialization Error Message\nflowErrorMessage\nOverview sidebar\nLatest Materialization Status\nflowStatus\nAsset profile and overview sidebar\nOwners [ref]\nownerUsers\nOverview sidebar\nTables [\ndagster/table_name\n]\nflowDataResults\nRelations tab in sidebar\nTags:\nconnectors\ndagster\nlineage\nmetadata\nreference\nPrevious\nCrawl Dagster assets\nNext\nDagster integration\nMetadata attributes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/references/what-does-atlan-crawl-from-fivetran",
    "content": "Connect data\nETL Tools\nFivetran\nReferences\nWhat does Atlan crawl from Fivetran?\nOn this page\nWhat does Atlan crawl from Fivetran?\nLineage\nâ\nAtlan uses Fivetran's log events from the Fivetran Platform Connector to generate lineage associated with\nFivetran connectors\n. This is particularly useful for creating lineage between different tools, such as Salesforce and Snowflake.\ndanger\nThe assets involved in lineage (tables, columns, objects, fields, and so on) must already be crawled by Atlan before running the\nFivetran Enrichment\npackage to enrich them.\nSpecifically, Atlan will:\nCreate lineage between each data asset in Atlan that is associated with a Fivetran connector. (For example, between Salesforce objects and Snowflake tables.)\nAtlan creates Fivetran\nProcess\nobjects for each data asset that is replicated.\nAtlan creates column-level lineage to connect the sources (inputs) and destinations (outputs) of each\nProcess\n. (For example, between Salesforce fields and Snowflake columns.)\nLink each\nProcess\nto its corresponding connector in the Fivetran UI.\nFor any Fivetran sources or destinations that are not natively supported or are supported but have not been crawled yet, Atlan will create\npartial assets\nto provide you with a complete view of lineage.\nSupported sources and destinations for Fivetran Platform Connector\nâ\nSources\nâ\nAtlan's Fivetran Platform Connector integration supports all sources listed in\nFivetran documentation\nfor lineage.\nFor any sources that are not natively supported or are\nsupported\nbut have not been crawled yet, Atlan will create\npartial assets\nto provide you with a complete view of lineage. Note that Atlan will only catalog partial column assets and generate column-level lineage for\nSQL sources natively supported in Atlan\n.\nDestinations\nâ\nAtlan's Fivetran Platform Connector integration supports all destinations listed in\nFivetran documentation\nfor lineage.\nFor any destinations that are not natively supported or are\nsupported\nbut have not been crawled yet, Atlan will create\npartial assets\nto provide you with a complete view of lineage. Note that Atlan will only catalog partial column assets and generate column-level lineage for\nSQL sources natively supported in Atlan\n.\nNote that for\ncrawler configuration\n, only the\ndestinations listed here\nare supported.\nConnectors\nâ\nAtlan maps the following metadata from Fivetran to its\nFivetranConnector\nasset type. This is only applicable to metadata crawled using the Fivetran Platform Connector.\nSource property\nAtlan property\nWhere in Atlan\nconnector.NAME\nfivetranConnectorConnectorName\noverview sidebar\nconnector_type.ID\nfivetranConnectorConnectorType\noverview sidebar\ndynamically created by script\nfivetranConnectorConnectorURL\noverview sidebar\ndestination.NAME\nfivetranConnectorDestinationName\noverview sidebar\ndestination.TYPE\nfivetranConnectorDestinationType\noverview sidebar\ndynamically created by script\nfivetranConnectorDestinationURL\noverview sidebar\nconnector.SIGNED_UP\nfivetranConnectorSyncSetupOn\noverview sidebar\nconnector.SYNC_FREQUENCY\nfivetranConnectorSyncFrequency\noverview sidebar\nconnector.PAUSED\nfivetranConnectorSyncPaused\nAPI only\nuser.GIVEN_NAME\n+\nuser.FAMILY_NAME\nfivetranConnectorSyncSetupUserFullName\nAPI only\nconnector.USER_EMAIL\nfivetranConnectorSyncSetupUserEmail\nAPI only\nincremental_mar. INCREMENTAL_ROWS\nfivetranConnectorMonthlyActiveRowsFree\noverview sidebar\nincremental_mar. INCREMENTAL_ROWS\nfivetranConnectorMonthlyActiveRowsPaid\noverview sidebar\nfivetranConnectorMonthlyActiveRowsFree\n+\nfivetranConnectorMonthlyActiveRowsPaid\nfivetranConnectorMonthlyActiveRowsTotal\nAPI only\ncalculated by script\nfivetranConnectorMonthlyActiveRowsChangePercentageFree\nAPI only\ncalculated by script\nfivetranConnectorMonthlyActiveRowsChangePercentagePaid\nAPI only\ncalculated by script\nfivetranConnectorMonthlyActiveRowsChangePercentageTotal\noverview sidebar\ncalculated by script\nfivetranConnectorMonthlyActiveRowsFreePercentageOfAccount\nAPI only\ncalculated by script\nfivetranConnectorMonthlyActiveRowsPaidPercentageOfAccount\noverview sidebar\ncalculated by script\nfivetranConnectorMonthlyActiveRowsTotalPercentageOfAccount\nAPI only\ndynamically generated by script\nsourceURL\noverview sidebar\nconnector_type.CREATED_AT\nsourceCreatedAt\nasset preview, profile, and filter, overview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncId\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncStartedAt\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncFinishedAt\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranLastSyncStatusfivetranConnectorLastSyncReason\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncTaskType\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncRescheduledAt\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncTablesSynced\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranLastSyncRecordsUpdated\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncExtractTimeSeconds\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncExtractVolumeMegabytes\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncLoadTimeSeconds\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncLoadVolumeMegabytes\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncProcessTimeSeconds\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncProcessVolumeMegabytes\noverview sidebar\ncalculated from\nLOG\ntable\nfivetranConnectorLastSyncTotalTimeSeconds\noverview sidebar\nTags:\nconnectors\ndata\ncrawl\napi\nconfiguration\nPrevious\nCrawl Fivetran\nNext\nTroubleshooting Fivetran connectivity\nLineage\nSupported sources and destinations for Fivetran Platform Connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage/references/what-does-atlan-crawl-from-google-cloud-composer-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nGoogle Cloud OpenLineage\nReferences\nWhat does Atlan crawl from Google Cloud Composer/OpenLineage?\nOn this page\nWhat does Atlan crawl from Google Cloud Composer/OpenLineage?\nOnce you have\nintegrated Google Cloud Composer/OpenLineage\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported:\nStatus filter   -  last run status for an asset\nDuration filter   -  last run duration for an asset\nAtlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the\nlist of operators supported by OpenLineage\n.\nDAGs\nâ\nAtlan maps DAGs (directed acyclic graphs) from Google Cloud Composer/OpenLineage to its\nAirflowDAG\nasset type.\nSource property\nAtlan property\nschedule_interval\nairflowDagSchedule\ntimetable\nairflowDagScheduleDelta\ntags\nairflowTags\nversion\nairflowRunVersion\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nrunid\nairflowRunName\nrun_type\nairflowRunType\neventTime\nairflowRunStartTime\neventTime\nairflowRunEndTime\neventType\nairflowRunOpenLineageState\nquery\nairflowTaskSql\ngroup_id\nairflowTaskGroupName\nTasks\nâ\nAtlan maps tasks from Google Cloud Composer/OpenLineage to its\nAirflowTask\nasset type.\nSource property\nAtlan property\nretries\nairflowTaskRetryNumber\npool\nairflowTaskPool\npool_slots\nairflowTaskPoolSlots\nqueue\nairflowTaskQueue\npriority_weight\nairflowTaskPriorityWeight\ntrigger_rule\nairflowTaskTriggerRule\noperator_class\nairflowTaskOperatorClass\ndag_id\nairflowDagName\nconn_id\n`airflowTaskConnectionId`\nsql\nairflowTaskSql\ntags\nairflowTags\nversion\nairflowRunVersion\nopenlineageAdapterVersion\nairflowRunOpenLineageVersion\nrunid\nairflowRunName\nrun_type\nairflowRunType\neventTime\nairflowRunStartTime\neventTime\nairflowRunEndTime\neventType\nairflowRunOpenLineageState\nTags:\nconnectors\ncrawl\nPrevious\nHow to integrate Google Cloud Composer/OpenLineage\nDAGs\nTasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/references/what-does-atlan-crawl-from-informatica-cdi",
    "content": "Connect data\nETL Tools\nInformatica CDI\nReferences\nWhat does Atlan crawl from Informatica CDI\nOn this page\nWhat does Atlan crawl from Informatica CDI\nAtlan discovers and catalogs various Informatica Cloud Data Integration (CDI) assets to provide comprehensive data lineage and metadata management.\nImportant!\nThe Informatica GUID (source system global unique identifier) for an object is available as part of the Atlan object Qualified Name (QN) and additionally in the\nflowId\nattribute (for SDK based consumption).\nProject\nâ\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nname\ndescription\ndescription\ndescription\nupdatedTime\nsourceUpdatedAt\nLast updated (on Informatica CDI)\nupdatedBy\nsourceUpdatedBy\nLast updated (on Informatica CDI) by\nFolder\nâ\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nname\ndescription\ndescription\ndescription\nupdatedTime\nsourceUpdatedAt\nLast updated (on Informatica CDI)\nupdatedBy\nsourceUpdatedBy\nLast updated (on Informatica CDI) by\nMapping task\nâ\nSource property\nWhere in Atlan\nAtlan property\nTask Name\nname\nname\nDescription\ndescription\ndescription\nLocation\nflowProjectName, flowFolderName\nProject, Folder\nMapping\nâ\nSource property\nWhere in Atlan\nAtlan property\nname\nname\nname\nLocation\nflowProjectName, flowFolderName\nProject, Folder\nNumber of transformations in the mapping\nflowDatasetCount\nTransformation\nâ\nSource property\nWhere in Atlan\nAtlan property\nName\nname\nname\nDescription\ndescription\ndescription\nType\nflowType\ntransformationType\nNumber of fields in the transformation\nflowFieldCount\nTransformation field\nâ\nSource property\nWhere in Atlan\nAtlan property\nName\nname\nname\nType\nflowDataType\nField Type\nTags:\nconnectors\netl-tools\ninformatica\nPrevious\nTransformations\nNext\nTask and crawl issues\nProject\nFolder\nMapping task\nMapping\nTransformation\nTransformation field"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/references/what-does-atlan-crawl-from-matillion",
    "content": "Connect data\nETL Tools\nMatillion\nReferences\nWhat does Atlan crawl from Matillion?\nOn this page\nWhat does Atlan crawl from Matillion?\nAtlan crawls and maps the following assets and properties from Matillion.\nOnce you've\ncrawled Matillion\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these Matillion assets:\nJobs\n-  Job type, job path, version, and job schedule filters\nGroups\nâ\nAtlan maps groups from Matillion to its\nMatillionGroup\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile, and overview and properties sidebar\ndynamically generated from total projects crawled\nmatillionProjectCount\nasset profile and overview sidebar\nProjects\nâ\nAtlan maps projects from Matillion to its\nMatillionProject\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nprojects.objects.name\nname\nasset profile, and overview and properties sidebar\nversionExports.objects.name\nmatillionVersions\nasset profile and overview sidebar\nenvironmentExports.objects.name\nmatillionEnvironments\nAPI only\ndynamically generated from number of jobs per project version\nmatillionProjectJobCount\nasset profile, and overview and properties sidebar\ndynamically generated from group and project name\nsourceURL\noverview sidebar\nJobs\nâ\nAtlan maps jobs from Matillion to its\nMatillionJob\nasset type.\nSource property\nAtlan property\nWhere in Atlan\njobs.objects.info.name\nname\nasset profile, and overview and properties sidebar\njobs.objects.info.description\ndescription\nasset profile, and overview and properties sidebar\njobs.objects.info.type\nmatillionJobType\nasset profile and filter, and overview sidebar\njobs.objects.path\nmatillionJobPath\nasset profile and filter, and overview sidebar\nversionExports.objects.name\nmatillionVersion\nasset profile and filter, and overview sidebar\nscheduleExports.objects.daysOfWeek\nor\nscheduleExports.objects.daysOfMonth\nmatillionJobSchedule\nasset profile and filter, and overview sidebar\ndynamically generated from number of components per job\nmatillionJobComponentCount\nasset profile and overview sidebar\ndynamically generated from group, project, and job name\nsourceURL\noverview sidebar\nComponents\nâ\nAtlan maps components from Matillion to its\nMatillionComponent\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile, and overview and properties sidebar\njobObject.components.id\nmatillionComponentId\nasset profile and overview sidebar\njobObject.components.implementationID\nmatillionComponentImplementationId\nAPI only\nversionExports.objects.name\nmatillionVersion\nasset profile and overview sidebar\ndynamically generated during metadata processing\nmatillionComponentLinkedJob\nasset profile and overview sidebar\ntasks.state\nmatillionComponentLastRunStatus\nasset profile and overview sidebar\ntasks.state\nmatillionComponentLastFiveRunStatus\nproperties sidebar\noutput.sql\nmatillionComponentSqls\nproperties sidebar\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Matillion\nNext\nWhat lineage does Atlan extract from Matillion?\nGroups\nProjects\nJobs\nComponents"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/references/what-does-atlan-crawl-from-metabase",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMetabase\nReferences\nWhat does Atlan crawl from Metabase?\nOn this page\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase.\ndanger\nCurrently Atlan only represents the assets marked with ð in lineage.\nCollections\nâ\nAtlan maps collections from Metabase to its\nMetabaseCollection\nasset type.\nSource property\nAtlan property\nname\nname\nslug\nmetabaseSlug\ncolor\nmetabaseColor\nnamespace\nmetabaseNamespace\npersonal_owner_id\nmetabaseIsPersonalCollection\nDashboards ð\nâ\nAtlan maps dashboards from Metabase to its\nMetabaseDashboard\nasset type.\nSource property\nAtlan property\nname\nname\ncollection\nmetabaseCollectionName\ncreated_at\nsourceCreatedAt\nupdated_at\nsourceUpdatedAt\nlast-edit-info\nsourceUpdatedBy\nordered_cards\nmetabaseQuestionCount\ncollection (\nOfficial\n)\ncertificateStatus (VERIFIED)\nQuestions ð\nâ\nAtlan maps questions from Metabase to its\nMetabaseQuestion\nasset type.\nSource property\nAtlan property\nname\nname\ncollection\nmetabaseCollectionName\ncreated_at\nsourceCreatedAt\nupdated_at\nsourceUpdatedAt\nlast-edit-info\nsourceUpdatedBy\nordered_cards\nmetabaseDashboardCount\nquery_type\nmetabaseQueryType\nquery\nmetabaseQuery\ncollection (\nOfficial\n)\ncertificateStatus (VERIFIED)\ninvalid non-native queries\ncertificateStatus (DEPRECATED)\nTags:\nlineage\ndata-lineage\nimpact-analysis\ndashboards\nvisualization\nanalytics\nintegration\nconnectors\nPrevious\nCrawl Metabase\nNext\nPreflight checks for Metabase\nCollections\nDashboards ð\nQuestions ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/references/what-does-atlan-crawl-from-microsoft-azure-cosmos-db",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMicrosoft Azure Cosmos DB\nReferences\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nOn this page\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nOnce you have\ncrawled Microsoft Azure Cosmos DB\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for Microsoft Azure Cosmos DB assets:\nAccounts   -  you can use the following filters:\nAccount Type\n-  filter accounts by account type\nSubscription ID\n-  filter accounts by subscription ID\nAccount Resource Group\n-  filter accounts by resource group\nCollections   -\nType\nfilter, filtering options include\nTime Series\n,\nCapped\n, and\nClustered\ncollection types\nAtlan crawls and maps the following assets and properties from Microsoft Azure Cosmos DB. Atlan currently does not support lineage for Microsoft Azure Cosmos DB assets.\nAccounts\nâ\nAtlan maps accounts from Microsoft Azure Cosmos DB to its\nAccount\nasset type.\nvCore-based deployment\nâ\nSource property\nAtlan property\nname\nname\ntype\ncosmosMongoDBAccountType\ndynamically generated\ncosmosMongoDBDatabaseCount\nRU-based deployment\nâ\nSource property\nAtlan property\nname\nname\ncreatedAt\nsourceCreatedAt\ninstanceId\ncosmosMongoDBAccountInstanceId\ndynamically generated\ncosmosMongoDBDatabaseCount\ntype\ncosmosMongoDBAccountType\nsubscription\ncosmosMongoDBAccountSubscriptionId\nresource group\ncosmosMongoDBAccountResourceGroup\ndocument endpoint\ncosmosMongoDBAccountDocumentEndpoint\nmongo endpoint\ncosmosMongoDBAccountMongoEndpoint\npublicNetworkAccess\ncosmosMongoDBAccountPublicNetworkAccess\nenableAutomaticFailover\ncosmosMongoDBAccountEnableAutomaticFailover\nenableMultipleWriteLocations\ncosmosMongoDBAccountEnableMultipleWriteLocations\nenablePartitionKeyMonitor\ncosmosMongoDBAccountEnablePartitionKeyMonitor\nisVirtualNetworkFilterEnabled\ncosmosMongoDBAccountIsVirtualNetworkFilterEnabled\nlocations\ncosmosMongoDBAccountLocations\nreadLocations\ncosmosMongoDBAccountReadLocations\nwriteLocations\ncosmosMongoDBAccountWriteLocations\ndefaultConsistencyLevel\ncosmosMongoDBAccountConsistencyPolicy\nDatabases\nâ\nAtlan maps databases from Microsoft Azure Cosmos DB to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CAT\nname\nTABLE_COUNT\nmongoDBDatabaseCollectionCount\nCollections\nâ\nAtlan maps collections from Microsoft Azure Cosmos DB to its\nCollection\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nTABLE_CAT\ndatabaseName\nCOLUMN_COUNT\ncolumnCount\ncollStats.storageSize\nsizeBytes\ncollStats.count\nrowCount\ntype\nmongoDBCollectionSubtype\nschema\nmongoDBCollectionSchemaDefinition\ncollStats.capped\nmongoDBCollectionIsCapped\ncollStats.maxSize\nmongoDBCollectionMaxSize\ncollStats.max\nmongoDBCollectionMaximumDocumentCount\noptions.timeseries.timeField\nmongoDBCollectionTimeField\noptions.timeseries.granularity\nmongoDBCollectionTimeGranularity\noptions.expireAfterSeconds\nmongoDBCollectionExpireAfterSeconds\ncollStats.numOrphanDocs\nmongoDBCollectionNumOrphanDocs\ncollStats.nindexes\nmongoDBCollectionNumIndexes\ncollStats.totalIndexSize\nmongoDBCollectionTotalIndexSize\ncollStats.avgObjSize\nmongoDBCollectionAverageObjectSize\nColumns\nâ\nTo extract columns from Microsoft Azure Cosmos DB collections, Atlan\nanalyzes document schemas\n, which must be enabled and configured to crawl columns.\nAtlan consolidates multiple document schemas into a unified view, capturing all possible fields with their nesting levels and data types. The system then performs a depth-first traversal, converting each unique field path into column entries while tracking parent-child relationships and hierarchy through qualified names. The total column count is calculated by summing up all unique paths, including nested structures, with arrays processed to identify additional column patterns.\nAtlan maps the following metadata from Microsoft Azure Cosmos DB to its\nColumn\nasset type.\nAtlan supports nested columns up to level 35 for parent columns.\nColumn-level lineage is currently not supported.\nTag propagation is supported from:\ncollections to columns\nparent to nested columns\nAtlan property\nWhere in Atlan\nname\nasset preview and profile, overview sidebar\norder\nasset preview and profile, overview sidebar\ndataType\nasset preview and profile, overview sidebar\nrawdataTypeDefinition\n(raw schema of nested columns in a given parent column)\nasset preview and profile, overview sidebar\npath\n(complete hierarchy from parent column to child column)\nasset preview and profile, overview sidebar\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl Microsoft Azure Cosmos DB\nNext\nTroubleshooting Microsoft Azure Cosmos DB connectivity\nAccounts\nDatabases\nCollections\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/references/what-does-atlan-crawl-from-microsoft-azure-event-hubs",
    "content": "Connect data\nEvent/Messaging\nMicrosoft Azure Event Hubs\nReferences\nWhat does Atlan crawl from Microsoft Azure Event Hubs?\nOn this page\nWhat does Atlan crawl from Microsoft Azure Event Hubs?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs.\nOnce you've\ncrawled Microsoft Azure Event Hubs\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nEvent hubs\n-  Message count, size (MB), partition count, and cleanup policy filters\nConsumer groups\n-  Member count and topic name filters\nEvent hubs\nâ\nAtlan maps event hubs from Microsoft Azure Event Hubs to its\nAzureEventHub\nasset type.\nSource property\nAtlan property\nTopic\nname\nPartitionCount\nkafkaTopicPartitionsCount\nReplicationFactor\nkafkaTopicReplicationFactor\nsegment.byte\nkafkaTopicSegmentBytes\ncompression.type\nkafkaTopicCompressionType\ncleanup.policy\nkafkaLogTopicCleanupPolicy\nisInternal\nkafkaTopicIsInternal\nsizeInBytes\nkafkaTopicSizeInBytes\nrecordCount\nkafkaTopicRecordCount\nstatus\nazureEventHubStatus\nretention.ms\nkafkaTopicRetentionTimeInMs\nConsumer groups\nâ\nAtlan maps consumer groups from Microsoft Azure Event Hubs to either of the\nfollowing asset types\n:\nKafkaConsumerGroup\n-  managed via Kafka clients.\nAzureEventHubConsumerGroup\n-  managed via Azure portal, SDK, or Azure Resource Manager templates.\nDid you know?\nConsumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Microsoft Azure Event Hubs will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset.\nSource property\nAtlan property\nGROUP\nname\nmemberCount\nkafkaConsumerGroupMemberCount\nReplicationFactor\nkafkaTopicReplicationFactor\ntopic_names\nkafkaTopicNames\nTOPIC\nkafkaConsumerGroupTopicConsumptionProperties.topicName\nPARTITION\nkafkaConsumerGroupTopicConsumptionProperties.topicPartition\nLAG\nkafkaConsumerGroupTopicConsumptionProperties.topicLag\nCURRENT-OFFSET\nkafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Microsoft Azure Event Hubs\nEvent hubs\nConsumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/references/what-does-atlan-crawl-from-mongodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMongoDB\nReferences\nWhat does Atlan crawl from MongoDB?\nOn this page\nWhat does Atlan crawl from MongoDB?\nAtlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.\nOnce you have\ncrawled MongoDB\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filter is currently supported for\nMongoDB collections\n:\nCollection type\nfilter   -  filtering options include\nTime Series\n,\nCapped\n, and\nClustered\ncollection types\nDatabases\nâ\nAtlan maps databases from MongoDB to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CAT\nname\nTABLE_COUNT\nmongoDBDatabaseCollectionCount\nCollections\nâ\nAtlan maps collections from MongoDB to its\nCollection\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nTABLE_CAT\ndatabaseName\nCOLUMN_COUNT\ncolumnCount\ncollStats.storageSize\nsizeBytes\ncollStats.count\nrowCount\ntype\nmongoDBCollectionSubtype\nschema\nmongoDBCollectionSchemaDefinition\ncollStats.capped\nmongoDBCollectionIsCapped\ncollStats.maxSize\nmongoDBCollectionMaxSize\ncollStats.max\nmongoDBCollectionMaximumDocumentCount\noptions.timeseries.timeField\nmongoDBCollectionTimeField\noptions.timeseries.granularity\nmongoDBCollectionTimeGranularity\noptions.expireAfterSeconds\nmongoDBCollectionExpireAfterSeconds\ncollStats.numOrphanDocs\nmongoDBCollectionNumOrphanDocs\ncollStats.nindexes\nmongoDBCollectionNumIndexes\ncollStats.totalIndexSize\nmongoDBCollectionTotalIndexSize\ncollStats.avgObjSize\nmongoDBCollectionAverageObjectSize\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl MongoDB\nNext\nTroubleshooting MongoDB connectivity\nDatabases\nCollections"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/references/what-does-atlan-crawl-from-mysql",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nReferences\nWhat does Atlan crawl from MySQL?\nOn this page\nWhat does Atlan crawl from MySQL?\nAtlan crawls and maps the following assets and properties from MySQL.\nOnce you have\ncrawled MySQL\n, you can\nuse connector-specific filters\nfor quick asset discovery.\nDatabases\nâ\nAtlan maps databases from MySQL to its\nDatabase\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_CATALOG\nname\nasset preview and profile, overview sidebar\nSCHEMA_COUNT\nschemaCount\nasset filters\nSchemas\nâ\nAtlan maps schemas from MySQL to its\nSchema\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_SCHEMA\nname\nasset preview and profile, overview sidebar\nTABLE_COUNT\ntableCount\nasset preview and profile\nVIEW_COUNT\nviewsCount\nasset preview and profile\nTABLE_CATALOG\ndatabaseName\nasset preview and profile, overview sidebar\nTables\nâ\nAtlan maps tables from MySQL to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview and profile, overview sidebar\nREMARKS\ndescription\nasset preview and profile, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview and profile, overview sidebar\nROW_COUNT\nrowCount\nasset preview and profile, overview sidebar\nBYTES\nsizeBytes\nasset preview and profile, overview sidebar\nTABLE_TYPE\nsubType\nAPI only\nIS_PARTITION\nisPartitioned\nAPI only\nPARTITION_STRATEGY\npartitionStrategy\nAPI only\nPARTITION_COUNT\npartitionCount\nAPI only\nPARTITIONS\npartitionList\nAPI only\nCREATE_TIME\nsourceCreatedAt\nasset preview and profile, properties sidebar\nViews\nâ\nAtlan maps views from MySQL to its\nView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview and profile, overview sidebar\nREMARKS\ndescription\nasset preview and profile, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview and profile, overview sidebar\nVIEW_DEFINITION\ndefinition\nasset profile and overview sidebar\nIS_PARTITION\nisPartitioned\nAPI only\nPARTITION_COUNT\npartitionCount\nAPI only\nCREATE_TIME\nsourceCreatedAt\nasset preview and profile, properties sidebar\nColumns\nâ\nAtlan maps columns from MySQL to its\nColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nCOLUMN_NAME\nname\nasset profile and filter, overview sidebar\nREMARKS\ndescription\nasset profile and filter, overview sidebar\nORDINAL_POSITION\norder\nparent asset profile\nTYPE_NAME\ndataType\nasset preview and filter, overview sidebar, parent asset profile\nCONSTRAINT_TYPE (PRIMARY KEY)\nisPrimary\nasset preview and filter, overview sidebar, parent asset profile\nCONSTRAINT_TYPE (FOREIGN KEY)\nisForeign\nasset preview and filter, overview sidebar, parent asset profile\nIS_NULLABLE\nisNullable\nAPI only\nNUMERIC_SCALE\nnumericScale\nAPI only\nCHARACTER_MAXIMUM_LENGTH\nmaxLength\nAPI only\nStored procedures\nâ\nAtlan maps stored procedures from MySQL to its\nProcedure\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nPROCEDURE_NAME\nname\nAPI only\nREMARKS\ndescription\nAPI only\nPROCEDURE_TYPE\nsubType\nAPI only\nROUTINE_DEFINITION\ndefinition\nAPI only\nCREATED\nsourceCreatedAt\nAPI only\nLAST_ALTERED\nsourceUpdatedAt\nAPI only\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl MySQL\nNext\nPreflight checks for MySQL\nDatabases\nSchemas\nTables\nViews\nColumns\nStored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/references/what-does-atlan-crawl-from-postgresql",
    "content": "Connect data\nDatabases\nSQL Databases\nPostgreSQL\nReferences\nWhat does Atlan crawl from PostgreSQL?\nOn this page\nWhat does Atlan crawl from PostgreSQL?\nAtlan crawls and maps the following assets and properties from PostgreSQL.\nOnce you have\ncrawled PostgreSQL\n, you can\nuse connector-specific filters\nfor quick asset discovery.\nDatabases\nâ\nAtlan maps databases from PostgreSQL to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from PostgreSQL to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nTables\nâ\nAtlan maps tables from PostgreSQL to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nTABLE_KIND (p)\n,\nTABLE_TYPE (PARTITIONED TABLE)\nisPartitioned\nPARTITION_STRATEGY\npartitionStrategy\nPARTITION_COUNT\npartitionCount\nIS_INSERTABLE_INTO\nis_insertable_into\nIS_TYPED\nis_typed\nSELF_REFERENCING_COL_NAME\nself_referencing_col_name\nREF_GENERATION\nref_generation\nIS_TRANSIENT\nis_transient\nTable partitions\nâ\nAtlan maps table partitions from PostgreSQL to its\nTablePartition\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nPARTITION_CONSTRAINT\nconstraint\nTABLE_KIND (p)\n,\nTABLE_TYPE (PARTITIONED TABLE)\nisPartitioned\nPARTITION_STRATEGY\npartitionStrategy\nPARTITION_COUNT\npartitionCount\nIS_INSERTABLE_INTO\nis_insertable_into\nIS_TYPED\nis_typed\nSELF_REFERENCING_COL_NAME\nself_referencing_col_name\nREF_GENERATION\nref_generation\nIS_TRANSIENT\nis_transient\nViews\nâ\nAtlan maps views from PostgreSQL to its\nView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nVIEW_DEFINITION\ndefinition\nIS_INSERTABLE_INTO\nis_insertable_into\nIS_TYPED\nis_typed\nSELF_REFERENCING_COL_NAME\nself_referencing_col_name\nREF_GENERATION\nref_generation\nIS_TRANSIENT\nis_transient\nMaterialized views\nâ\nAtlan maps materialized views from PostgreSQL to its\nMaterialisedView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nVIEW_DEFINITION\ndefinition\nColumns\nâ\nAtlan maps columns from PostgreSQL to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl PostgreSQL\nNext\nPreflight checks for PostgreSQL\nDatabases\nSchemas\nTables\nTable partitions\nViews\nMaterialized views\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/references/what-does-atlan-crawl-from-qlik-sense-enterprise-on-windows",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nQlik Sense Enterprise on Windows\nReferences\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\nOn this page\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\nAtlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows.\nOnce you've\ncrawled Qlik Sense Enterprise on Windows\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nApps\nand\nsheets\n-  Is Published filter\nStreams\nâ\nAtlan maps streams from Qlik Sense Enterprise on Windows to its\nQlikStream\nasset type.\nSource property\nAtlan property\ntype\nqlikStreamType\nownerId\nqlikOwnerId\nid\nqlikId\ncreatedAt\nsourceCreatedAt\nupdatedAt\nsourceUpdatedAt\nApps\nâ\nAtlan maps apps from Qlik Sense Enterprise on Windows to its\nQlikApp\nasset type.\nSource property\nAtlan property\nattributes.name\nname\nattributes.description\ndescription\nattributes.resourceId\nqlikId\nstatic_byte_size\nqlikAppStaticByteSize\nattributes.spaceId\nqlikSpaceId\nattributes.resourceCreatedAt\nsourceCreatedAt\nattributes.resourceUpdatedAt\nsourceUpdatedAt\nattributes.ownerId\nqlikOwnerId\nattributes.resourceAttributes.originAppId\nqlikOriginAppId\nattributes.resourceAttributes.hasSectionAccess\nqlikHasSectionAccess\nattributes.resourceAttributes.directQueryMode\nqlikIsDirectQueryMode\nattributes.resourceAttributes.published\nqlikIsPublished\nSheets\nâ\nAtlan maps sheets from Qlik Sense Enterprise on Windows to its\nQlikSheet\nasset type.\nSource property\nAtlan property\nqProperty.qMetaDef.title\nname\nqProperty.qMetaDef.description\ndescription\nqProperty.qInfo.qId\nqlikId\nspaceId\nqlikSpaceId\nappId\nqlikAppId\napproved\nqlikIsApproved\npublished\nqlikIsPublished\nCharts\nâ\nAtlan maps charts from Qlik Sense Enterprise on Windows to its\nQlikChart\nasset type.\nSource property\nAtlan property\nqProperty.qInfo.qId\nqlikId\nqProperty.subtitle\nqlikChartSubtitle\nqProperty.footnote\nqlikChartFootnote\nqProperty.qInfo.qType\nqlikChartType\nqProperty.options.dimensionsOrientation\nqlikChartOrientation\nTags:\nconnectors\ncrawl\nPrevious\nCrawl Qlik Sense Enterprise on Windows\nStreams\nApps\nSheets\nCharts"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/references/what-does-atlan-crawl-from-redpanda-kafka",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nReferences\nWhat does Atlan crawl from Redpanda Kafka?\nOn this page\nWhat does Atlan crawl from Redpanda Kafka?\nAtlan crawls and maps the following assets and properties from Redpanda Kafka.\nOnce you've\ncrawled Redpanda Kafka\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these assets:\nTopics\n-  Message count, size (MB), partition count, and cleanup policy filters\nConsumer groups\n-  Member count and topic name filters\nTopics\nâ\nAtlan maps topics from Redpanda Kafka to its\nKafkaTopic\nasset type.\nSource property\nAtlan property\nTopic\nname\nPartitionCount\nkafkaTopicPartitionsCount\nReplicationFactor\nkafkaTopicReplicationFactor\nsegment.byte\nkafkaTopicSegmentBytes\ncompression.type\nkafkaTopicCompressionType\ncleanup.policy\nkafkaLogTopicCleanupPolicy\nisInternal\nkafkaTopicIsInternal\nsizeInBytes\nkafkaTopicSizeInBytes\nrecordCount\nkafkaTopicRecordCount\nretention.ms\nkafkaTopicRetentionTimeInMs\nConsumer groups\nâ\nAtlan maps consumer groups from Redpanda Kafka to its\nKafkaConsumerGroup\nasset type.\nDid you know?\nConsumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Redpanda Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset.\nSource property\nAtlan property\nGROUP\nname\nmemberCount\nkafkaConsumerGroupMemberCount\nReplicationFactor\nkafkaTopicReplicationFactor\ntopic_names\nkafkaTopicNames\nTOPIC\nkafkaConsumerGroupTopicConsumptionProperties.topicName\nPARTITION\nkafkaConsumerGroupTopicConsumptionProperties.topicPartition\nLAG\nkafkaConsumerGroupTopicConsumptionProperties.topicLag\nCURRENT-OFFSET\nkafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset\nTags:\nconnectors\ncrawl\nPrevious\nCrawl on-premises Kafka\nNext\nPreflight checks for Redpanda Kafka\nTopics\nConsumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/references/what-does-atlan-crawl-from-sisense",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSisense\nReferences\nWhat does Atlan crawl from Sisense?\nOn this page\nWhat does Atlan crawl from Sisense?\nOnce you've\ncrawled Sisense\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for these asset types:\nDashboards:\nWidget count\nfilter   -  filter dashboards by widget count\nData models:\nType\nfilter   -  filter data models by\nlive\nor\nextract\n(formerly called Elasticube) type\nData model tables:\nTags\nfilter   -  filter data model tables by Sisense tags\nIs hidden\nfilter   -  use this filter to find hidden data model tables\nAtlan crawls and maps the following assets and properties from Sisense.\ndanger\nCurrently, Atlan only represents the assets marked with ð in lineage. Column-level lineage is not supported at present.\nDashboards ð\nâ\nAtlan maps dashboards from Sisense to its\nSisenseDashboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\ndesc\ndescription\nasset profile and overview sidebar\ncreated\ncreatedAt\nproperties sidebar\nlastUpdated\nupdatedAt\nproperties sidebar\ncalculated by Atlan\nwidgetCount\nasset filter and overview sidebar\ngenerated using the dashboard ID\nsourceURL\nView in Sisense\nbutton on asset profile\nWidgets ð\nâ\nAtlan maps widgets from Sisense to its\nSisenseWidget\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ntitle\nname\nasset profile and overview sidebar\ndesc\ndescription\nasset profile and overview sidebar\ncreated\ncreatedAt\nproperties sidebar\nlastUpdated\nupdatedAt\nproperties sidebar\ncalculated by Atlan\ncolumnCount\noverview sidebar\nsubType\nsubType\nAPI only\nsize\nsize\nAPI only\nData models\nâ\nAtlan maps data models from Sisense to its\nSisenseDataModel\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndesc\ndescription\nasset profile and overview sidebar\ncreated\ncreatedAt\nproperties sidebar\nlastUpdated\nupdatedAt\nproperties sidebar\ncalculated by Atlan\ntableCount\nAPI only\nserver\nserver\nAPI only\nrevision\nrevision\nAPI only\nlastBuildTime\nlastBuildTime\nAPI only\nlastSuccessfulBuildTime\nlastSuccessfulBuildTime\nAPI only\ntype\ntype\nasset filter and overview sidebar\nrelationshipType\nrelationshipType\nAPI only\nData model tables ð\nâ\nAtlan maps data model tables from Sisense to its\nSisenseDataModelTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ndescription\ndescription\nasset profile and overview sidebar\ncreated\ncreatedAt\nproperties sidebar\nlastUpdated\nupdatedAt\nproperties sidebar\ncalculated by Atlan\ncolumnCount\noverview sidebar\ntype\ntype\nAPI only\nexpression\nexpression\nAPI only\nisMaterialized\nisMaterialized\nAPI only\nisHidden\nisHidden\nasset filter and overview sidebar\nschedule\nschedule\nAPI only\nliveQuerySettings\nliveQuerySettings\nAPI only\ntags\nassetTags\nproperties sidebar\nFolders\nâ\nAtlan maps folders from Sisense to its\nSisenseFolder\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset profile and overview sidebar\ncreated\ncreatedAt\nproperties sidebar\nlastUpdated\nupdatedAt\nproperties sidebar\nTags:\nconnectors\ndata\ncrawl\nmodel\nPrevious\nCrawl Sisense\nNext\nPreflight checks for Sisense\nDashboards ð\nWidgets ð\nData models\nData model tables ð\nFolders"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/references/what-does-atlan-crawl-from-soda",
    "content": "Connect data\nData Quality & Observability\nSoda\nReferences\nWhat does Atlan crawl from Soda?\nOn this page\nWhat does Atlan crawl from Soda?\nAtlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.\nOnce you have\ncrawled Soda\n, you can\nuse connector-specific filters\nfor quick asset discovery. The following filters are currently supported for Soda assets:\nCheck status   -  filter Soda checks by status\nCheck owner   -  filter Soda checks by email address of check owner\nLast scanned at   -  filter Soda checks by timestamp for last scanned in Soda\nThe following Soda filters are currently available for supported SQL assets:\nData quality status   -  filter SQL assets by overall data quality status, including\nPass\n,\nWarn\n,\nFail\n, and\nNot evaluated\nCheck count   -  filter SQL assets by total count of associated Soda checks\nScanned date   -  filter SQL assets by timestamp for last scanned in Soda\nLast synced (in Atlan)   -  filter SQL assets by timestamp for when any associated checks were last updated in Atlan\nAtlan crawls and maps the following assets and properties from Soda.\nChecks\nâ\nAtlan maps checks from Soda to its\nSodaCheck\nasset type.\nSource property\nAtlan property\nname\nname\ndescription\ndescription\nid\nsodaCheckId\nevaluationStatus\nsodaCheckEvaluationStatus\ndefinition\nsodaCheckDefinition\nincidents\nsodaCheckIncidentCount\nlastCheckRunTime\nsodaCheckLastScanAt\ncloudUrl\nsourceURL\nlastUpdated\nsourceUpdatedAt\nowner.email\nsourceOwners\nSupported sources\nâ\nIf you have crawled supported data sources, you can view Soda checks on your existing assets in Atlan:\nAmazon Athena\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nHive\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nSnowflake\nTrino\nSoda checks can also be cataloged for some data sources that are not natively supported in Atlan. These will require additional configuration at source. To ensure that the datasets are mapped to your assets in Atlan, set the value of the\ndata_source_name\nfield to\n<database>.<schema>\nwhen connecting to:\nDask and Pandas\nApache Spark\nTags:\nconnectors\ndata\ncrawl\nPrevious\nCrawl Soda\nNext\nPreflight checks for Soda\nChecks\nSupported sources"
  },
  {
    "url": "https://docs.atlan.com/product/connections/concepts/what-is-the-crawler-logic-for-a-deprecated-asset",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nConcepts\nWhat is the crawler logic for a deprecated asset?\nWhat is the crawler logic for a deprecated asset?\nIf an asset is deprecated at source, it will be\narchived\n(or soft-deleted) in Atlan. The status of the asset will remain unchanged during the next crawler run.\nChanging the\ncertification status\nof an asset to\nDeprecated\nin Atlan has no impact on the crawler logic.\nTags:\nintegration\nconnectors\nPrevious\nWhat are preflight checks?\nNext\nAdditional connectivity to data sources"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/references/what-lineage-does-atlan-extract-from-matillion",
    "content": "Connect data\nETL Tools\nMatillion\nReferences\nWhat lineage does Atlan extract from Matillion?\nOn this page\nWhat lineage does Atlan extract from Matillion?\nAtlan uses Matillion's metadata API to generate lineage associated with\nMatillion connectors\n. This is particularly useful for creating lineage between different tools.\nDue to limitations in metadata provisioning at source, Atlan currently only supports lineage for data transformations between Snowflake tables as both source and destination. The related Matillion processes neither have asset profiles nor are they discoverable. Atlan recommends upgrading to the latest version of\nMatillion ETL\nto allow for enhanced metadata provisioning.\ndanger\nThe assets involved in lineage (tables, columns, and so on) must already be\ncrawled by Atlan\nbefore running the\nMatillionÂ package\nto enrich them.\nSpecifically, Atlan will:\nCreate lineage between each data asset in Atlan that is associated with a Matillion connector. (For example, between Snowflake tables and columns.)\nAtlan creates Matillion\nProcess\nobjects for each data asset that is replicated. Learn more about\nprocesses\nhere.\nSupported sources and destinations\nâ\nSources\nâ\nAtlan's Matillion integration supports the following sources:\nSnowflake\nDestinations\nâ\nAtlan's Matillion integration supports the following destinations:\nSnowflake\nDid you know?\nWe welcome feedback   -  have a burning need for another source or destination? Please\nlet us know\n!\nTags:\nconnectors\ndata\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Matillion?\nNext\nTroubleshooting Matillion connectivity\nSupported sources and destinations"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/references/what-lineage-does-atlan-extract-from-microsoft-azure-data-factory",
    "content": "Connect data\nETL Tools\nMicrosoft Azure Data Factory\nReferences\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nOn this page\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nAtlan uses the\nMicrosoft Azure Data Factory REST API\nto generate lineage associated with\nMicrosoft Azure Data Factory connectors\n. This is particularly useful for creating lineage between different tools.\nAtlan currently only supports lineage for the following:\nSupported sources and sinks listed below.\nData loaded through\nsupported activities\nin data factory pipelines.\ndanger\nThe assets involved in lineage must already be crawled by Atlan before running the\nMicrosoft Azure Data Factory package\nto enrich them.\nSpecifically, Atlan will:\nCreate lineage between each data asset in Atlan that is associated with a supported Microsoft Azure Data Factory connector. (For example, between Microsoft Azure Cosmos DB collections and ADLS objects.)\nAtlan creates Microsoft Azure Data Factory\nProcess\nobjects for each data asset that is replicated. Learn more about\nprocesses\nhere.\nSupported sources and sinks\nâ\nSources\nâ\nAtlan's Microsoft Azure Data Factory integration supports the following sources:\nAzure Data Lake Storage (ADLS)\nAzure Databricks\nMicrosoft Azure Cosmos DB for MongoDB\nSnowflake\nSinks\nâ\nAtlan's Microsoft Azure Data Factory integration supports the following sinks:\nAzure Data Lake Storage (ADLS)\nAzure Databricks\nMicrosoft Azure Cosmos DB for MongoDB\nSnowflake\nDid you know?\nWe welcome feedback   -  have a burning need for another source or sink? Please\nlet us know\n!\nTags:\nconnectors\ndata\ncrawl\napi\nPrevious\nWhat does Atlan crawl from Microsoft Azure Data Factory?\nNext\nTroubleshooting Microsoft Azure Data Factory connectivity\nSupported sources and sinks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/references/what-lineage-does-atlan-extract-from-microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nReferences\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\nOn this page\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics?\ndanger\nAtlan currently only supports mining query history for dedicated SQL pools with the\nMicrosoft Azure Synapse Analytics miner\n. Mining query history for serverless SQL pools is currently not supported.\nAtlan uses the\nAzure Synapse Analytics REST API\nto generate lineage associated with\nMicrosoft Azure Synapse AnalyticsÂ connectors\n. This is particularly useful for creating lineage between different tools.\nAtlan currently only supports lineage for the following:\nSupported sources and destinations listed below.\nData loaded through\nCopy activity\nin Synapse pipelines.\nSupported sources and destinations\nâ\nSources\nâ\nAtlan's Microsoft Azure Synapse Analytics integration supports the following sources:\nAzure Data Lake Storage (ADLS) Gen2\nMicrosoft Azure Synapse Analytics\nDestinations\nâ\nAtlan's Microsoft Azure Synapse Analytics integration supports the following destinations:\nMicrosoft Azure Synapse Analytics\nMicrosoft Power BI\nDid you know?\nWe welcome feedback   -  have a burning need for another source or destination? Please\nlet us know\n!\nTags:\nconnectors\ndata\napi\nPrevious\nPreflight checks for Microsoft Azure Synapse Analytics\nSupported sources and destinations"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-lineage-does-atlan-extract-from-microsoft-power-bi",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicrosoft Power BI\nReferences\nWhat lineage does Atlan extract from Microsoft Power BI?\nOn this page\nWhat lineage does Atlan extract from Microsoft Power BI?\nAtlan currently supports the following lineage for\nMicrosoft Power BI\n:\nLineage between Microsoft Power BI assets crawled in Atlan\nUpstream lineage to SQL warehouse assets, includes table- and column-level lineage for the following supported SQL sources:\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nMicrosoft Azure Synapse Analytics\nMicrosoft SQL Server\nMySQL\nOracle\n-  Atlan generates lineage for the following methods of Oracle connectivity:\nconnection string   -  for example,\n<host_name>:<port>/<service_name>\nconnect descriptor   -  for example,\n(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=<host_name>)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=<service_name>)))\nLineage generation for TNS name connectivity is currently not supported.\nSAP HANA\nSnowflake\nTeradata\nSalesforce\nThis document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.\nLineage generation\nâ\nAtlan generates lineage for your Microsoft Power BI assets as follows:\nYou connect to a SQL data source such as Snowflake and extract relevant SQL tables to create a table in Microsoft Power BI for analysis.\nOnce the data has been loaded, you can perform Microsoft Power BI native operations as required.\nEach table created in Microsoft Power BI and part of a dataset has a Power Query expression associated with it. For example:\nlet\nSource = Snowflake.Databases(\n\"example.snowflakecomputing.com\"\n,\n\"EXAMPLE_WAREHOUSE\"\n,\n[\nRole=\n\"EXAMPLE_ROLE\"\n]\n)\n,\nEXAMPLE_DB = Source\n{\n[\nName=\n\"EXAMPLE_DATABASE_NAME\"\n,\nKind=\n\"Database\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Sch = EXAMPLE_DB\n{\n[\nName=\n\"EXAMPLE_SCHEMA_NAME\"\n,\nKind=\n\"Schema\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Table_Var = EXAMPLE_Sch\n{\n[\nName=\n\"EXAMPLE_TABLE_NAME\"\n,\nKind=\n\"Table\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_Table_Var\nAtlan retrieves the Power Query expression as a plain string from the\nMicrosoft Power BI API\nresponse.\nAtlan's custom query parser then parses the Power Query expression to derive lineage between the upstream SQL tables and Microsoft Power BI table asset.\nHowever, note that the Power Query expression can be modified in the Power Query Editor of the Power BI Desktop application. These modifications may involve using parameter substitutes and variable naming patterns in the Power Query expression.\nThese modifications may lead to inconsistent behavior in Atlan's query parser. This is because the latter is built on the standard format of a Power Query expression, without any modifications.\nLimitations of query parser\nâ\nTo create seamless lineage generation, Atlan recommends the following when building tables in Microsoft Power BI.\nUsing parameters\nâ\nThe Power Query expression associated with a table can be manually modified to serve different use cases. For example, if you're creating multiple tables using data from the same database and schema at source, you may want to use\ndynamic M query parameters\nto substitute common values in Power Query expressions.\nAtlan recommends the following:\nAvoid using the following words to define your parameter names:\nDatabase\nSchema\nTable\nView\nWarehouse\nRole\nAvoid including any spaces in your parameter names   -  for example,\n( Example : Example DB )\nFor example, Atlan's query parser doesn't support the following:\nlet\nSource = Snowflake.Databases(\n\"example.snowflakecomputing.com\"\n,\nWarehouseName\n,\n[\nRole=\n\"EXAMPLE_ROLE\"\n]\n)\n,\nDatabaseName = Source\n{\n[\nName=DatabaseName\n,\nKind=\n\"Database\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Sch = DatabaseName\n{\n[\nName=SchemaName\n,\nKind=\n\"Schema\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Table_Var = EXAMPLE_Sch\n{\n[\nName=TableName\n,\nKind=\n\"Table\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_Table_Var\nThis example includes\nWarehouseName\n,\nDatabaseName\n,\nSchemaName\n, and\nTableName\nas parameters, which aren't supported in the query parser.\nParameter syntax\nâ\nThere are different formats for the syntax used in parameter names for Power Query expressions. For example,\nparam_name\n,\n#âparam_nameâ\n, or\n#\"param nameâ\n.\nAtlan recommends the following for parameter names:\nUse plain text format\nAvoid any special characters   -  for example,\n#\n,\n\"\n, and more\nFor example, Atlan's query parser doesn't support the following:\nlet\nSource = Snowflake.Databases(\n\"example.snowflakecomputing.com\"\n,\n\"EXAMPLE_WAREHOUSE\"\n,\n[\nRole=\n\"EXAMPLE_ROLE\"\n]\n)\n,\nDatabaseName = Source\n{\n[\nName=#\n\"DatabaseName\"\n,\nKind=\n\"Database\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Sch = DatabaseName\n{\n[\nName=\n\"EXAMPLE_SCHEMA_NAME\"\n,\nKind=\n\"Schema\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Table_Var = EXAMPLE_Sch\n{\n[\nName=\n\"EXAMPLE_TABLE_NAME\"\n,\nKind=\n\"Table\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_Table_Var\nThis example includes\n#\"DatabaseName\"\nas parameter name, which isn't supported in the query parser.\nVariable names\nâ\nWhile using parameters in Power Query expressions, make sure that the variable names don't match the parameter names. For example, Atlan's query parser doesn't support the following:\nlet\nSource = Snowflake.Databases(\n\"example.snowflakecomputing.com\"\n,\n\"EXAMPLE_WAREHOUSE\"\n,\n[\nRole=\n\"EXAMPLE_ROLE\"\n]\n)\n,\nDatabaseName = Source\n{\n[\nName=DatabaseName\n,\nKind=\n\"Database\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Sch = DatabaseName\n{\n[\nName=\n\"EXAMPLE_SCHEMA_NAME\"\n,\nKind=\n\"Schema\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Table_Var = EXAMPLE_Sch\n{\n[\nName=\n\"EXAMPLE_TABLE_NAME\"\n,\nKind=\n\"Table\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_Table_Var\nIn this example,\nDatabaseName\nis used as both a parameter name and variable name, which isn't supported in the query parser.\nUser-defined expressions\nâ\nParts of a Power Query expression can be parameterized and cross-referenced in other Power Query expressions. Atlan's query parser currently only parses standard forms of Power Query expressions, hence these user-defined expressions aren't supported.\nExample of a supported Power Query expression:\nlet\nSource = Snowflake.Databases(\n\"example.snowflakecomputing.com\"\n,\n\"EXAMPLE_WAREHOUSE\"\n,\n[\nRole=\n\"EXAMPLE_ROLE\"\n]\n)\n,\nEXAMPLE_DB = Source\n{\n[\nName=\n\"EXAMPLE_DATABASE_NAME\"\n,\nKind=\n\"Database\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Sch = EXAMPLE_DB\n{\n[\nName=\n\"EXAMPLE_SCHEMA_NAME\"\n,\nKind=\n\"Schema\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Table_Var = EXAMPLE_Sch\n{\n[\nName=\n\"TBL_AGG_SALES_HT_POS_BEER\"\n,\nKind=\n\"Table\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_Table_Var\nExample of an unsupported Power Query expression:\nlet\nSource = db_source\n,\nEXAMPLE_Sch = db_source\n{\n[\nName=\n\"EXAMPLE_SCHEMA_NAME\"\n,\nKind=\n\"Schema\"\n]\n}\n[\nData\n]\n,\nEXAMPLE_Table_Var = EXAMPLE_Sch\n{\n[\nName=\n\"EXAMPLE_TABLE_NAME\"\n,\nKind=\n\"Table\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_Table_Var\nExample of a reference expression, parameterized as\ndb_source\n:\nlet\nSource = Snowflake.Databases(\n\"example.snowflakecomputing.com\"\n,\n\"EXAMPLE_WAREHOUSE\"\n,\n[\nRole=\n\"EXAMPLE_ROLE\"\n]\n)\n,\nEXAMPLE_DB = Source\n{\n[\nName=\n\"EXAMPLE_DATABASE_NAME\"\n,\nKind=\n\"Database\"\n]\n}\n[\nData\n]\nin\nEXAMPLE_DB\nTable functions\nâ\nFor column-level lineage generation, Atlan's custom query parser currently supports parsing expressions with the following\nTable Functions\n:\nTable.RenameColumns\nTable.TransformColumnNames\nTable.TransformColumns\nPower query functions\nâ\nUpstream lineage isn't supported when the data source expression involves the use of certain built-in Power Query functions. The following functions aren't supported:\nCsv.Document\nDateTime.LocalNow\nExcel.Workbook\nFolder.Files\nJson.Document\nList.Dates\nSharePoint.Files\nSharePoint.Tables\nUsageMetricsDataConnector.GetMetricsData\nXml.Tables\nTags:\nconnectors\ndata\ncrawl\nPrevious\nPreflight checks for Microsoft Power BI\nNext\nTroubleshooting Microsoft Power BI connectivity\nLineage generation\nLimitations of query parser"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/sso-user-provisioning",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nFAQ\nWhat type of user provisioning does Atlan support for SSO integrations?\nWhat type of user provisioning does Atlan support for SSO integrations?\nAtlan currently supports\nSystem for Cross-domain Identity Management\n(SCIM) capabilities for user provisioning for:\nAzure AD\nOkta\nAtlan also supports\njust-in-time\n(JIT) user provisioning and automated assignment of user groups during every login for several\nSSO providers\n. Once a user is deprovisioned in your SSO provider, the user will not be able to log into Atlan. Since the user profile in Atlan is not deleted automatically, you will also need to\ndisable the user\n.\nTags:\nintegration\nconnectors\nfaq-integrations\nPrevious\nCan we use a Microsoft SSO login?\nNext\nWhen does Atlan become a personal data processor or subprocessor?"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/athena-vs-glue",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nFAQ\nWhat's the difference between connecting to Athena and Glue?\nWhat's the difference between connecting to Athena and Glue?\nAtlan recommends setting up AWS Glue or Amazon Athena based on your intended outcome:\nTo only crawl metadata, you can\nset up an AWS Glue connection\n.\nTo also preview and query the data, you can\nset up an Amazon Athena connection\n.\nNote that Amazon Athena also sources\nasset metadata\ndirectly from\nAWS Glue\n.\nTags:\nintegration\nconnectors\nfaq-connections\nPrevious\nWhat column keys does Atlan crawl?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/invite-email",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nFAQ\nWhy did my users not receive an invite email from Atlan?\nWhy did my users not receive an invite email from Atlan?\nIf you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:\nCheck if the email has been routed to the user's spam mailbox.\nAdd users to the Atlan app in your SSO provider. When\nSSO is integrated\n, you do not need to\ninvite users\nvia the Atlan UI (using their email). Instead, the first time a user logs into Atlan via SSO, their user profile will be automatically created.\nTags:\nintegration\nconnectors\nfaq-integrations\nPrevious\nWhen does Atlan become a personal data processor or subprocessor?\nNext\nProject Management Integrations"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/faq/browser-extension-error",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nFAQ\nWhy do I get an error message when I click on Atlan's browser extension?\nWhy do I get an error message when I click on Atlan's browser extension?\nRefer to\nTroubleshooting the Atlan browser extension\n.\nTags:\nintegration\nconnectors\nPrevious\nCan I add Atlan's browser extension for everyone in my organization?\nNext\nWhy is Atlan's browser extension not loading?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/faq/salesforce-description-not-showing",
    "content": "Connect data\nCRM\nSalesforce\nFAQ\nWhy does the description from Salesforce not show up in Atlan?\nWhy does the description from Salesforce not show up in Atlan?\nAtlan supports extracting and displaying description metadata for your\nSalesforce objects\n.\nTags:\nconnectors\ndata\ncrawl\nsalesforce\nPrevious\nDoes Atlan require an admin user in Salesforce?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/faq/browser-extension-not-loading",
    "content": "Configure Atlan\nIntegrations\nAutomation\nBrowser Extension\nFAQ\nWhy is Atlan's browser extension not loading?\nWhy is Atlan's browser extension not loading?\nRefer to\nTroubleshooting the Atlan browser extension\n.\nTags:\nintegration\nconnectors\nPrevious\nWhy do I get an error message when I click on Atlan's browser extension?\nNext\nConnections Integration"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-custom-metadata",
    "content": "Use data\nDiscovery\nConfiguration\nAdd custom metadata\nAdd custom metadata\nWho can do this?\nAny user with\npermission to edit custom metadata\ncan add or change custom metadata values on assets. (An admin user must first\ncreate the custom metadata structures\nfor those values, though.)\nTo add custom metadata to an asset:\nNavigate to the asset. You could do this by searching, through discovery, or even through our\nChrome extension\n.\nIn the right sidebar, click the icon for the metadata structure you want to enrich.\nIf no values yet exist, click the\nStart editing\nbutton. Otherwise, in the upper-right of the custom metadata panel in the sidebar, click the\nEdit\nbutton.\nEnter values for any properties you want to enrich. (You do not need to fill in every property.)\nIn the upper-right of the custom metadata panel in the sidebar, click the\nUpdate\nbutton.\nThat's it, the asset is now enriched with your organization's own custom metadata! ð\nTags:\ndata\nintegration\nasset-profile\nPrevious\nHow to use the filters menu\nNext\nHow do I use the filters menu?"
  },
  {
    "url": "https://docs.atlan.com/faq/ai-and-automation-features",
    "content": "Configure Atlan\nFrequently Asked Questions\nAI and Automation Features\nOn this page\nAI and Automation Features\nGuide to Atlan's AI capabilities and automation features for enhanced data governance and productivity.\nDo you provide any machine learning or AI assistance?\nâ\nAtlan makes intelligent recommendations to support your documentation initiatives.\nFor example:\nSuggestions to populate context and metadata\nAutomating asset tag propagation\nUse Atlan AI for documentation and other use cases\nWhere can third-party developers contribute?\nâ\nThe\nAtlan marketplace\nis a foundational part of our connector and package ecosystem. We provide out-of-the-box connectors to popular data sources and tools. We also have packages that can power\nactive metadata use cases\n.\nOur vision is to open this up to the developer community, who can help build more packages to achieve complex value streams. Our intention is to support a package manager (similar to npm or pip) for building, managing, monitoring, and orchestrating container-native workflows on Kubernetes, powered by\nArgo\n.\nWhy can't I see the Atlan logo in my data tool?\nâ\nRefer to the\nTroubleshooting the Atlan browser extension\ndocument.\nIs the PII tagging of data or metadata automated?\nâ\nAtlan propagates\ntags\nbased on hierarchy and lineage, which means tags can be propagated to connected assets. For example, if you tag a table as PII and that table has downstream columns, Atlan will tag those downstream columns as PII as well â only if propagation is enabled.\nAtlan does not auto-detect PII data. Atlan will only propagate the PII tag to downstream assets if you have enabled tag propagation manually or\nautomated the task using playbooks\n.\nCan I integrate Atlan with any web application?\nâ\nAtlan provides\nAPI-backed SDKs\nfor integration. You can use these to:\nExtract and manipulate any metadata curated in Atlan,\nIngest or enrich any metadata into Atlan,\nAnd ultimately power\nactive metadata use cases\n.\nWe're always happy to learn more about what folks are hoping to accomplish, so feel free to\nreach out\nif you want to discuss specific ideas or possibilities in more detail.\nTags:\ndata\nintegration\napi\nfaq-automation\nPrevious\nAdministration and Configuration\nNext\nData Connections and Integration"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/sql-dump-lineage-support",
    "content": "Use data\nLineage\nFAQ\nCan Atlan read a dump of SQL statements to create lineage?\nCan Atlan read a dump of SQL statements to create lineage?\nAtlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan\nmines SQL queries\nfrom query history at source. These are the SQL statements that Atlan parses:\nCREATE TABLE\nCREATE VIEW\nCREATE TABLE AS SELECT\nMERGE\nINSERT INTO\nUPDATE\nCLONE\nTags:\ndata\nfaq\nfaq-lineage\nPrevious\nCan Atlan integrate with Airflow to generate lineage?\nNext\nCan I be notified if there is a change in downstream dashboards or a schema drift?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/lineage-change-notifications",
    "content": "Use data\nLineage\nFAQ\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nYou can\ncreate webhooks\nin Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events.\nTags:\ndata\nintegration\nfaq\nfaq-lineage\nPrevious\nCan Atlan read a dump of SQL statements to create lineage?\nNext\nDoes Atlan support field-level lineage for BI tools?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/query-any-dw-dl",
    "content": "Use data\nInsights\nFAQ\nCan I query any DW/DL?\nCan I query any DW/DL?\nYou can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's\nsupported sources\n. Once integrated, you will be able to query the underlying data using the\nInsights\nfeature.\nTags:\ndata\nintegration\nfaq\nfaq-insights\nPrevious\nAre there any limits on concurrent queries?\nNext\nCan I turn off sample data preview for the entire organization?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/concepts/what-are-data-models",
    "content": "Configure Atlan\nData Models\nConcepts\nWhat are data models?\nOn this page\nData Models\nData models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network.\nAtlan enables you to ingest your entityârelationship (ER) models and associate them with existing data assets in Atlan. Cataloging your ER model metadata in Atlan can help you:\nFoster collaboration   -  business and technical users work best when they share a common understanding of the data landscape without tool boundaries.\nHandle change management through impact analysis   -  data models enable visualization of an asset's lifecycle within an organization, helping users assess business impact due to technical changes with accuracy and vice versa.\nImplement data governance   -  define access control mechanisms, data retention policies, and data governance rules spanning different systems by understanding relationships between data assets. When business-approved data models are coupled with technical objects, trust and accountability are established between key stakeholders.\nIngest ER models\nâ\nYou can ingest your ER models in Atlan using the following methods:\nData model ingestion\n-  Atlan recommends using this custom package to ingest your ER models via an Excel template.\nAtlan SDK\nAtlan REST API\nEntityârelationship models\nâ\nEntityârelationship (ER) models focus on entities (objects/concepts) and the attributes (characteristics) and relationships (associations) between those entities.\nIn the context of entityârelationship modeling, a model encompasses the entities, attributes, and relationships that define how data is organized and interactions between different elements within a specific domain.\nData models can be used to represent information at different levels of abstraction:\nConceptual   -  overall structure of content without specific details. This acts as a starting point for new data initiatives and is the most abstract form of the model.\nLogical   -  implementation-agnostic breakdown of data into specific objects and interactions between these objects.\nPhysical   -  a refined adaptation of data concepts conforming to a particular software application or data storage system. This level takes into account finer nuances like naming conventions, optimizations, partitioning, and more.\nEntity-relationship diagrams\nâ\nAn entity-relationship diagram (ERD) is a visual representation of data that illustrates the entities (objects or concepts) within a system, relationships between those entities, and their attributes.\nEntity\n-  in an ERD, an entity is a fundamental component that represents a real-world object or concept within a database. For example, entities are typically nouns, such as\nCustomer\n,\nOrder\n, or\nProduct\nand data can be stored about them.\nAttribute\n-  an entity has attributes, which are the properties or characteristics of the entity. For example, a\nCustomer\nentity may have attributes like\nCustomerID\n,\nName\n,\nEmail\n, and\nPhone Number\n.\nRelationship\n-  a relationship determines how two entities interact with each other. For example, a\nCustomer\nplaces an\nOrder\n. A relationship encompasses several elements, like:\nCardinality   -  defines the quantitative aspect of a relationship. For example, a\nQuote\nprovides pricing for many related\nOrders\n(one-to-many).\nOptionality   -  defines whether a relationship is mandatory in an entity. For example, an\nOrder\nmust have an associated\nCustomer\n.\nCardinality and optionality can be combined to define business rules. For example, in a\nLibrary\nsystem, a\nMember\ncan borrow 0-n book(s).\nTypes of relationships:\nAssociation   -  refers to a peer-to-peer relationship between two entities.\nGeneralization   -  refers to a parent-child relationship between two entities. For example, a\nLoan\nentity can be of type\nHome Loan\n,\nAuto Loan\n,\nBusiness Loan\n, and so on.\nModel\n-  in the context of ER modeling, a model encompasses the entities, attributes, and relationships that define how data is organized and how different elements interact within a specific domain.\nModels can be of different types   -  conceptual, logical, and physical.\nMapping   -  entities within a model can be mapped to entities within another model of a different type. For example, a logical entity\nOrder\ncan be mapped to your assets in Atlan, such as an\nOrder\ntable in Snowflake.\nTags:\ndata\nmodel\nPrevious\nHow to view data models\nNext\nTroubleshooting data models\nIngest ER models\nEntityârelationship models\nEntity-relationship diagrams"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/download-impacted-assets-in-microsoft-excel",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nDownload impacted assets in Microsoft Excel\nOn this page\nDownload impacted assets in Microsoft Excel\nOnce you've\nconnected Atlan with Microsoft Excel\n, you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for\nimpact analysis\n.\ndanger\nYou need to be logged into your Atlan instance before you can download impacted assets from Atlan in Microsoft Excel. If you do not have the\npermission\nto update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Microsoft Excel. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access.\nDownload impacted assets in Microsoft Excel\nâ\nTo import impacted assets in Microsoft Excel:\nIn the menu bar of your Microsoft Excel workbook, click\nAtlan\n.\nFrom the\nAtlan\ntab, clickÂ\nImport Lineage\nto open a list of your assets in a sidebar.\n(Optional) To filter your assets by a specific asset type, in the Atlan sidebar, select the asset type.\nIn the Atlan sidebar on your worksheet, you can either:\nIndividually select the data asset(s) you want to import.\nTo the left of the\nImport\nbutton, click the\nSelect All\ncheckbox to select all the assets that have loaded in the sidebar. (Optional) Scroll down and click\nLoad more\nto load more assets in the sidebar.\nTo select the type of impacted assets you'd like to download, in the Atlan sidebar, from the\nDirection\ndropdown:\nTo download downstream assets for\nimpact analysis\n, click\nDownstream\n.\nTo download upstream assets for\nroot cause analysis\n, click\nUpstream\n.\nTo download all impacted assets, click\nBoth\n.\nTo download the impacted assets in Microsoft Excel:\nClick\nImport Lineage\nto download all the impacted assets in one sheet.Â\nClick the\nvertical three dots\nand then click\nImport to new sheet\nto download the assets in separate sheet tabs.\n(Optional) Once download is successful, click the asset links in your spreadsheet to view the assets in Atlan.\nThe impacted assets are now available in Microsoft Excel! ð\nUpdate column metadata for impacted assets\nâ\nOnce you've imported your impacted assets from Atlan, you can edit the column metadata for impacted assets in Microsoft Excel. You can make changes to the column metadata once all the columns have been imported successfully.\nYou can only make changes to the metadata in the following columns:\nDescription\nOwner Users\nOwner Groups\nCertification Status\nCertification Message\nAnnouncement Type\nAnnouncement Title\nAnnouncement Message\nTags\nYou\ncannot\nmake the following changes:\nEdit headers for any of the columns\nEdit the metadata in the following columns:\nSource Asset\nSource Asset Connector\nSource Asset Type\nImpacted Asset\nImpacted Asset Connector\nImpacted Asset Type\nDirection\nTerms\nPropagated Tags\nSource URL\nQualified Name\nSource Asset GUID\nImpacted Asset GUID\nImmediate Upstream\nand\nImmediate Downstream\nDelete any columns or rows\nAny of these changes will not be pushed to Atlan and you'll receive an error message.\nDid you know?\nWhen adding tags or owners for impacted assets in Microsoft Excel, the tag or user name for the owner user or group must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags or owners as comma-separated values.\nPush changes to Atlan\nâ\nOnce you've made changes to the column metadata, to push your changes:\nIn the menu bar of your Microsoft Excel workbook, click the\nAtlan\ntab.\nFrom the top left of the\nAtlan\ntab, click\nAtlan\nand then click\nSync to Atlan\n.\nA dialog box will appear once the changes have synced\nTags:\ndata\nintegration\nPrevious\nDownload impacted assets in Google Sheets\nNext\nHow to export assets\nDownload impacted assets in Microsoft Excel\nUpdate column metadata for impacted assets\nPush changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/update-personas-glossary-terms",
    "content": "Build governance\nGlossary\nFAQ\nUse personas to update a term in a glossary\nHow can I use personas to update a term in a glossary?\nBy default, any user in Atlan can view all\nglossaries\nand nested categories and terms in the\nGlossary\nsection.\nUsers who do not have the permission to update assets like a glossary term   -  for example, a\nmember\nuser who has not been granted access to update terms via a\nglossary policy\n-  can\nraise a request\nto suggest changes to the term. Even though the user will be able to view the request they made on a specific glossary, they will neither be able to approve nor reject the request.\nTags:\ndata\nfaq\nfaq-governance\nPrevious\nWhat is the default permission for a glossary?\nNext\nCan I create backups of glossaries?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/how-tos/integrate-apache-spark-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nApache Spark OpenLineage\nGet Started\nHow to integrate Apache Spark/OpenLineage\nOn this page\nIntegrate Apache Spark/OpenLineage\nAtlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to\nOpenLineage configuration and facets\n.\nTo integrate Apache Spark/OpenLineage with Atlan, review the\norder of operations\nand then complete the following steps.\nCreate an API token in Atlan\nâ\nBefore running the workflow, you will need to\ncreate an API token\nin Atlan.\nSelect the source in Atlan\nâ\nTo select Apache Spark/OpenLineage as your source, from within Atlan:\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the list of packages, select\nSpark Assets\nÂ and then click\nSetup Workflow\n.\nConfigure the integration in Atlan\nâ\nYou will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your jobs run to catalog your assets.\nTo configure the Apache Spark/OpenLineage connection, from within Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\nTo run the workflow, at the bottom of the screen, click the\nRun\nbutton.\nConfigure the integration in Apache Spark\nâ\nDid you know?\nYou will need the Atlan API token and connection name to configure the integration in Apache Spark/OpenLineage. This will allow Apache Spark to connect with the OpenLineage API and send events to Atlan.\nSpark has a\ndefault SparkListener interface\nthat OpenLineage leverages to collect information about Spark jobs.\nTo configure Apache Spark to send OpenLineage events to Atlan, you can either:\nTo activate the listener, add the following properties to your Spark configuration:\n#\nInitialize\nSpark\nsession\nspark\n=\n(\nSparkSession\n.\nbuilder\n.\nmaster\n(\n'local'\n)\n.\nappName\n(\n\"SparkJobs\"\n)\n.\nconfig\n(\n'spark.jars.packages'\n,\n\"io.openlineage:openlineage-spark:<latest OpenLineage version>\"\n)\n.\nconfig\n(\n'spark.extraListeners'\n,\n'io.openlineage.spark.agent.OpenLineageSparkListener'\n)\n.\nconfig\n(\n'spark.openlineage.transport.type'\n,\n'http'\n)\n.\nconfig\n(\n'spark.openlineage.transport.url'\n,\n'https://<instance>.atlan.com'\n)\n.\nconfig\n(\n'spark.openlineage.transport.endpoint'\n,\n'/events/openlineage/spark/api/v1/lineage'\n)\n.\nconfig\n(\n'spark.openlineage.namespace'\n,\n'<connection-name>'\n)\n.\nconfig\n(\n'spark.openlineage.transport.auth.type'\n,\n'api_key'\n)\n.\nconfig\n(\n'spark.openlineage.transport.auth.apiKey'\n,\n'<Atlan_api_key>'\n)\n.\ngetOrCreate\n(\n)\n)\nAtlan recommends using the latest available version of the OpenLineage package for the Apache Spark integration. Replace\n<latest OpenLineage version>\nwith the\nlatest version of OpenLineage\n.\nurl\n: set the URL of your Atlan instance   -  for example,\nhttps://<instance>.atlan.com\n.\nendpoint\n: points to the service that will consume OpenLineage events   -  for example,\n/events/openlineage/spark/api/v1/lineage\n.\nnamespace\n: set the connection name as exactly configured in Atlan.\napiKey\n: set the API token generated in Atlan.\nAdd the above configuration to your cluster's\nspark-defaults.conf\nfile or specific jobs on submission via the\nspark-submit\ncommand.\nOnce the data processing tool has completed running, you will see Spark jobs along with lineage from OpenLineage events in Atlan! ð\nYou can also\nview event logs\nin Atlan to track and debug events received from OpenLineage.\nTags:\ndata\napi\nauthentication\nconfiguration\nPrevious\nApache Spark OpenLineage\nNext\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nCreate an API token in Atlan\nSelect the source in Atlan\nConfigure the integration in Atlan\nConfigure the integration in Apache Spark"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nHow to integrate Atlan with Microsoft Excel\nOn this page\nIntegrate Atlan with Microsoft Excel\nThe Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel.\nIntegrating Atlan with Microsoft Excel will allow you to:\nImport column metadata\nfor your data assets to Microsoft Excel\nUpdate column metadata\nfor your imported assets directly in Microsoft Excel\nDownload impacted assets\nin Microsoft Excel\nTo integrate Atlan with Microsoft Excel:\nIf your Atlan tenant is hosted on the standard domain\nhttps://<your-tenant-name>.atlan.com\n, you can either:\nInstall and connect the Atlan add-in as an individual user\nDeploy and publish the Atlan add-in for your organization as a Microsoft 365 admin\nÂ If your Atlan tenant is hosted on a custom domain\nhttps://<your-tenant-name>.mycompany.com\n, your Microsoft 365 admin will need to\nconfigure the Atlan add-in using PowerShell\nSet up the add-in as a user\nâ\nWho can do this?\nAny individual in your organization with access to Atlan can install the Atlan add-in for Microsoft Excel. Your Atlan tenant must be hosted on the standard domain\natlan.com\nto set up the add-in as a user.\nInstall Atlan in Microsoft Excel\nâ\nTo install the Atlan add-in directly in Microsoft Excel:\nOpen a new Microsoft Excel workbook.\nFrom the upper right of the\nHome\ntab, click the\nAdd-ins\nbutton, and then from the dropdown, click\nMore Add-ins\n.\nIn the\nOffice add-ins\ndialog, click\nStore\n.\nIn the search bar of your\nOffice Store\n, type\nAtlan\nand press enter.\nSelect the\nAtlan\nadd-in and click\nAdd\n.\nIf you see a dialog asking for permissions, click\nContinue\nto proceed.\nConnect Atlan to Microsoft Excel\nâ\nTo connect Atlan with your Microsoft Excel workbook:\nIn the menu bar of your Microsoft Excel workbook, click the\nAtlan\ntab.\nFrom the top left of the\nAtlan\ntab, click\nSetup\nto set up Atlan in your Microsoft Excel workbook.\nIn the Atlan sidebar on the right, enter your Atlan tenant name   -  for example,\nhttps://<your-tenant-name>.atlan.com\n. If you have a custom domain,Â\nadditional configuration\nwill be required.\nClick\nLogin\nto connect Atlan to Microsoft Excel. If you haven't logged into Atlan, you will be prompted to enter your credentials   -  including SSO, if enforced in your organization. Once connected, you can either\nenrich column metadata\nor\ndownload impacted assets for lineage analysis\n.\nCongrats on connecting Atlan with Microsoft Excel! ð\ndanger\nFor every new Microsoft Excel workbook that you create, you will need to follow the steps outlined above to connect Atlan to that workbook. The Atlan add-in will remain connected for all worksheets within an already connected workbook.\nDeploy and publish the add-in as an admin\nâ\nWho can do this?\nYou will need your Microsoft 365 administrator to complete these steps   -  you may not have access yourself. Before you begin, you may need to\nDetermine if Centralized Deployment of add-ins works for your organization\n.\nThe Atlan add-in can be installed at the workspace level for:\nStandard domains\n-  your Atlan tenant must be hosted as a subdomain of\natlan.com\nto deploy the add-in using the steps below.\nCustom domains\n-  if your Atlan tenant is hosted under a custom domain belonging to your organization, you will need to\nconfigure the Atlan add-in using PowerShell\n.\nTo install the Atlan add-in directly in Microsoft Excel:\nSign in atÂ\nadmin.microsoft.com\n.\nFrom the left menu of the admin center, click the\nSettings\ndropdown and then click\nIntegrated apps\n.\nOn the\nIntegrated apps\npage, under\nDeployed apps\n, click\nGet apps\n.\nIn the top right of the\nMicrosoft 365 Apps\npublished apps page, navigate to the search bar, type\nAtlan\nand press enter.\nSelect the\nAtlan\nadd-in for Microsoft Excel and click\nGet it now\n.\nIf you see a dialog asking for permissions, click\nGet it now\nto proceed.\nIn the\nDeploy New App\ndialog, enter the following details:\nIn the\nAdd users\npage, for\nAssign users\n, you can either:\nClick\nEntire organization\nto deploy the add-in to all users in your organization.\nClick\nSpecific users/groups\nto deploy the add-in to a subset of users in your organization. Use the search box to find specific users or groups.\nClick\nNext\nto continue.\nIn the\nAccept permissions requests\npage, the app capabilities and permissions of the apps are listed. If the app needs consent, select\nAccept permissions\n. Otherwise, click\nNext\nto continue.\nIn the\nReview and finish deployment\npage, review the deployment and click\nFinish deployment\n.\nOnce deployment is completed, click\nDone\nto finish setup. Note that it can take up to 24 hours for an add-in to show up for all your users.\nAll your users will need to do next is\nconnect Atlan to Microsoft Excel\n! ð\nTags:\ndata\nintegration\nPrevious\nHow to integrate Atlan with Google Sheets\nNext\nLink your account\nSet up the add-in as a user\nDeploy and publish the add-in as an admin"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/integrate-jira-data-center",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nGet Started\nHow to integrate Jira Data Center\nOn this page\nIntegrate Jira Data Center\nWho can do this?\nYou will need to be an\nadmin\nin Atlan to configure the Jira Data Center integration. You will also need inputs and approval from an administrator of your Jira workspace.\ndanger\nIf your Jira Data Center instance is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or\nsubmit a request\n. (If you are not using the IP allowlist, you can skip this step.)\nTo integrate Jira Data Center and Atlan, follow th\nese steps.\nConfigure an incoming app link in Jira Data Center\nâ\nYou will need to\nconfigure an incoming link\nwith an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider.\nAtlan requires the\nminimum scope\nof\nWRITE\nto create issues in Jira Data Center. However, actual permissions are capped at what the authorizing user can do. For example, if the authorizing user lacks the permission to delete issues or projects, then Atlan will not have the permission to do so even with the\nWRITE\nscope.\nTo configure an incoming link for Atlan, from within Jira Data Center:\nLog in to your Jira instance.\nCopy the Jira site URL\nfrom your browser's address bar and store it in a secure location. If you're viewing the dashboard, the site URL is everything that comes before\n/secure/Dashboard.jspa\n.\nFrom the top right of your Jira instance, click the settings icon, and from the dropdown, click\nApplications\n.\nIn the left menu of the\nApplications\npage, under\nIntegrations\n, click\nApplication links\n.\nFrom the top right of the\nApplication links\npage, click\nCreate link\nto create a new application link for Atlan.\nIn the\nCreate link\ndialog, enter the following details:\nFor\nApplication type\n, click\nExternal application\nto link to an external application using OAuth 2.0.\nFor\nDirection\n, click\nIncoming\nto allow Atlan to access data from Jira.\nIn the corresponding\nConfigure an incoming link\npage, enter the following details:\nFor\nName\n, enter a meaningful name for your application   -  for example,\nAtlan_integration\n.\nUnder\nApplication details\n, for\nRedirect URL\n, enter the redirect URL in the following format   -\nhttps://${client-domain}.atlan.com/oauth-callback\n.\nUnder\nApplication permissions\n, for\nPermissions\n, click the dropdown and then click\nWrite\n. The\nWRITE\nscope\nwill allow Atlan to:\nView projects and issues\nCreate, update, and delete projects and issues\nClick\nSave\nÂ to save your selections.\nFrom the corresponding\nCredentials\npage, click\nCopy\nto copy the\nClient ID\nand\nClient secret\nand store them in a secure location.\nConnect Atlan to Jira Data Center\nâ\nOnce you have retrieved the\nJira instance URL and client ID and client secret\nfrom Jira, you can proceed to connecting Atlan to Jira Data Center.\ndanger\nYou must have at least one issue already created in Jira before integrating it with Atlan.\nTo connect Atlan to Jira Data Center, from within Atlan:\nFrom the left menu, clickÂ\nAdmin\n.\nUnderÂ\nWorkspace\n, clickÂ\nIntegrations\n.\nIn theÂ\nJira\ntile, to the right of the\nConnect\nbutton, click the downward arrow and then click\nConnect with Jira Data Center\n.\nA new\nJira Data Center\nwindow will open and you'll be asked to install the Atlan app and create an application link in Jira. Click\nNext\nto proceed.\nIn the corresponding\nJira Data Center\ndialog, for\nAdd credentials\n, enter the following details:\nFor\nInstance URL\n, enter the\nURL of your Jira instance\n.\nFor\nClient ID\n, enter the\nclient ID you copied\nin Jira.\nFor\nClient secret\n, enter the\nclient secret you copied\nin Jira.\nIn the\nOAuth 2.0 Authorization Consent\npopup, click\nAllow\nto complete the connection.\nClick the\nAdd to Jira\nbutton to install the Atlan app in Jira Data Center.\nInstall Atlan app in Jira Data Center\nâ\nBefore you can install the Atlan app in Jira Data Center, navigate to the\nAtlan for Jira Data Center app\nURL and click\nGet it now\nto download the app.\nTo install the Atlan app, from within Jira Data Center:\nLog in to your Jira instance.\nUnder\nAdministration\n, from the tabs along the top, click\nManage apps\n.\nFrom the left menu under\nAtlassian Marketplace\n, click\nManage apps\n.\nFrom the upper right of the\nManage apps\npage, click\nUpload app\n.\nIn the\nUpload app\ndialog, for\nUpload the .jar or .obr file for a custom or third-party app here.\n, select the\napp file you downloaded\n.\nClick\nUpload\nto complete the installation.\nChanges to the apps in your instance will affect Jira search index. After you make changes to the app, you'll get the following message in the\nAdministration\nview:\nWe recommend that you perform a re-index, as configuration changes were made to\nSECTION\nby\nUSER\nat\nTIME\n. If you have other changes to make, complete them first so that you don't perform multiple re-indexes\n. You will need to perform a full re-index for the integration to succeed, follow the steps in the\nofficial Jira documentation\nto do so.\nConfigure integration from Atlan to Jira Data Center\nâ\nTo configure the Jira Data Center integration from Atlan, from the\nIntegrations\nsub-menu:\nExpand theÂ\nJira\ntile. (You may need to refresh the page before the following options appear.)\nUnder theÂ\nConfigurations\ntab, for\nProjects\n, select the Jira project to use as your default project from the dropdown and click\nUpdate\n.\n(Optional) At any future time, you can review theÂ\nOverview\ntab to see the number of linked issues between Jira Data Center and Atlan.\nAtlan is now connected to Jira Data Center! ð\nDid you know?\nThe default project is preselected when creating or linking issues to an asset in Atlan. You can change the project while creating or linking issues as needed.\nTags:\ndata\nintegration\nPrevious\nHow to integrate Jira Cloud\nNext\nLink your Jira account\nConfigure an incoming app link in Jira Data Center\nConnect Atlan to Jira Data Center\nInstall Atlan app in Jira Data Center\nConfigure integration from Atlan to Jira Data Center"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/how-tos/integrate-servicenow",
    "content": "Configure Atlan\nIntegrations\nProject Management\nServiceNow\nGet Started\nHow to integrate ServiceNow\nOn this page\nIntegrate ServiceNow\nWho can do this?\nYou will need to be an\nadmin\nin Atlan to configure the ServiceNow integration. You will also need inputs and approval from a System Administrator of your ServiceNow instance with a\nsecurity_admin role\n.\ndanger\nIf your ServiceNow instance is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan or\nsubmit a request\n. (If you are not using the IP allowlist, you can skip this step.)\nIf your Atlan admin has\nenabled the governance workflows and inbox module\nin your Atlan workspace, you can create a ServiceNow integration to allow your users to\ngrant or revoke data access\nfor governed assets in Atlan or any other data source.\nThis is only applicable if you:\nEnable governance workflows\nWant to use the data access approval workflow\nTo integrate ServiceNow and Atlan, follow these st\neps.\nCreate an OAuth application in ServiceNow\nâ\nYou will need to create an OAuth application endpoint for Atlan to access your ServiceNow instance.\nTo\ncreate an OAuth application\n, from within ServiceNow:\nLog in to your ServiceNow instance as a System Administrator with a security_admin role.\nFrom the address bar at the top of your browser window, copy the ServiceNow instance URL   -  for example,\nhttps://<instance_name>.service-now.com\n. This will be required to\nconnect Atlan to your ServiceNow instance\n.\nFrom the top header of your ServiceNow instance, click\nAll\n. From the dropdown, search for and select\nSystem OAuth\nand then click\nApplication Registry\n.\nIn the top-right corner of the\nApplication Registries\npage, click\nNew\nto create a new OAuth application.\nIn the corresponding screen, for\nWhat kind of OAuth application?\n, click\nCreate an OAuth API endpoint for external clients\n.\nIn the\nApplication Registries New record\nform, enter the following details:\nFor\nName\n, enter\nAtlan OAuth App\n.\nFor\nRedirect URL\n, enter the redirect URL in the following format   -\nhttps://<atlan_instance_name>/oauth-callback\n. Replace\n<atlan_instance_name>\nwith the name of your Atlan instance.\nFor\nLogo URL\n, copy and paste\nhttps://assets.atlan.com/assets/atlan-primary-logo-png.png\n.\nClick\nSubmit\nto create the OAuth application in ServiceNow.\nFrom the\nApplication Registries\npage, select the OAuth application you created above. Copy the values for\nClient ID\nand\nClient Secret\nand store them in a secure location.\nConnect Atlan to ServiceNow\nâ\nTo connect Atlan to ServiceNow, you will need the following:\nServiceNow instance URL   -  for example,\nhttps://<instance_name>.service-now.com\nClient ID and client secret of the OAuth application you created in ServiceNow\nTo connect Atlan to ServiceNow, from within Atlan:\nLog in to your Atlan instance as an admin user.\nFrom the left menu, click\nAdmin\n.\nUnder\nWorkspace\n, click\nIntegrations\n.\nIn the\nServiceNow\ntile, click the\nConnect\nbutton.\nIn the corresponding\nAdd to ServiceNow\ndialog, for\nServiceNow URL\n, enter the\nURL of your ServiceNow instance\n-  for example,\nhttps://<instance_name>.service-now.com\n.\nClick\nNext\nto proceed.\nThis step requires the creation of an OAuth application in ServiceNow,\nfollow the steps to do so\n. If you have already created it, in the\nCreate OAuth app\nsection, for\nCopy the Client ID and Secret from the new OAuth app and paste below\n, enter the following:\nFor\nClient ID\n, enter the\nclient ID you copied from ServiceNow\n.\nFor\nClient Secret\n, enter the\nclient secret you copied from ServiceNow\n.\nClick\nNext\nto proceed.\nIn the\nCommit update set\nsection, click\nAtlan Update Set.xml\nto download the update set XML file from Atlan to\nimport and commit in ServiceNow\n.\nConfigure the Atlan integration in ServiceNow\nâ\nTo configure the Atlan integration in ServiceNow, your ServiceNow System Administrator with a security_admin role will need to complete the following two steps:\nImport and commit the update set XML file\ndownloaded from Atlan to create an Atlan data access catalog and business rule in ServiceNow.\nCreate a new user\nin ServiceNow for the Atlan integration.\nImport and commit the update set XML file\nâ\nTo\nimport and commit the update set XML file\n, from within ServiceNow:\nLog in to your ServiceNow instance as a System Administrator with a security_admin role.\nFrom the top header of your ServiceNow instance, click\nAll\n. From the dropdown, search for and select\nSystem Update Sets\nand then click\nRetrieved Update Sets\n.\nOn the\nRetrieved Update Sets\npage, under\nRelated Links\n, click the\nImport Update Set from XML\nlink.\nOn the\nImport XML\npage, to upload the\nupdate set XML file downloaded from Atlan\n:\nFor\nStep 1: Choose file to upload\n, click\nChoose file\nto upload the\nAtlan Update Set.xml\nfile.\nFor\nStep 2: Upload the file\n, click the\nUpload\nbutton.\nIn the top-left corner of your screen, click the back button to return to the\nRetrieved Update Sets\npage. The Atlan update set will appear on the\nRetrieved Update Set\nlist in a\nLoaded\nstate. Once the XML file has successfully loaded, select the\nAtlan Update Set\n.\nClick\nPreview Update Set\nto preview the update set and address any issues. The update set includes the following:\nAtlan Data Access\ncatalog   -  Atlan will create data access requests in this catalog.\nAtlan Business Rule\n-  this is required for Atlan to receive events from your ServiceNow instance to detect any changes in the status of data access requests created in Atlan and automatically update governance workflow requests.\nAtlan service role and access control list (ACL) updates   -  the Atlan service account requires a role with write access on the\nsc_request\ntable to update specific fields such as\ndescription\n,\nshort_description\n, and more. This operation especially requires the security_admin role to commit the update set from Atlan in ServiceNow.\nScripted REST API\n-  this is initially required to retrieve the username and sys_id of the Atlan user completing the ServiceNow integration. Atlan creates a Scripted REST API\n/api/snc/oauth_userinfo\nthat returns the username and sys_id for an authenticated user. Once the integration has been completed, Atlan will have the access token required for the integration to continue working.\nClick\nCommit Update Set\n.\nIf the commit action fails,\ncontact Atlan support\n.\nCreate a new user\nâ\nTo\ncreate a new user\n, from within ServiceNow:\nLog in to your ServiceNow instance as a System Administrator with a security_admin role.\nFrom the top header of your ServiceNow instance, click\nAll\n. From the dropdown, search for and select\nUser Administration\nand then click\nUsers\n.\nIn the top-right corner of the\nUsers\npage, click\nNew\nto create a new user.\nIn the\nUser New record\nform, enter the following details:\nFor\nUser ID\n, enter\natlan.service\n.\nFor\nFirst name\nand\nLast name\n, enter\nAtlan\nand\nService\n, respectively.\nClick\nSubmit\nto create the new user.\nFrom the\nUsers\npage, search for and select the\natlan.service\nuser you created.\nOn the\nUser atlan.service\npage, scroll down to the table at the bottom of the screen. In the table, change to the\nRoles\ntab and then click the\nEdit\nbutton.\nOn the\nEdit Members\npage, configure the following:\nIn the\nCollection\nlist, search for and select\natlan_service_account_role\n.\nClick the greater than icon to add the role to the\natlan.service\nuser you created.\nClick\nSave\nto save your role assignment for the new user.\nIn the\nUser atlan.service\npage, click the\nSet Password\nbutton to create a password for the new user.\nIn the\nSet Password\ndialog, click the\nGenerate\nbutton to generate a password. Once a password has been generated, click the clipboard icon to copy the value and store it in a secure location. Click\nSave Password\n. This password will be required to\nconfigure the ServiceNow integration in Atlan\n.\nConfigure the ServiceNow integration in Atlan\nâ\nTo configure the ServiceNow integration in Atlan, from within Atlan:\nLog in to your Atlan instance as an admin user.\nFrom the left menu, click\nAdmin\n.\nUnder\nWorkspace\n, click\nIntegrations\n.\nExpand the\nServiceNow\ntile.\nIn the\nCommit update set\nsection, for\nPassword\n, enter the\npassword you copied from ServiceNow for the new user\n.\nClick the\nConnect and test\nbutton to test the ServiceNow integration. This ensures that the update set was committed successfully in ServiceNow and Atlan can receive webhook events. To test the latter, Atlan will create a sample request in ServiceNow, wait for a few seconds to receive a webhook event, and then display the status of the connection as\nCONNECTED\n.\n(Optional) Under the\nConfigurations\ntab, for\nTest Connection\n, click the\nRun test\nbutton to verify that the ServiceNow integration is working properly in Atlan at any time.\n(Optional) At any future time, you can review the\nOverview\ntab to see the number of linked issues between ServiceNow and Atlan.\nAtlan is now connected to ServiceNow! ð\nTags:\ndata\nintegration\nPrevious\nServiceNow\nNext\nLink your ServiceNow account\nCreate an OAuth application in ServiceNow\nConnect Atlan to ServiceNow\nConfigure the Atlan integration in ServiceNow\nConfigure the ServiceNow integration in Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/link-your-account",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nLink your account\nOn this page\nLink your account\nTo\nexport assets to and bulk enrich metadata from\na supported spreadsheet tool, you may first need to link your Google or Microsoft online account. This is done automatically for the admin user that\nintegrated the spreadsheet tool\nto enable asset export, but not for other users.\nAlthough you will be prompted to sign in with your Google or Microsoft account while exporting assets, Atlan also provides an additional option to connect your account from the user profile. Atlan uses the same set of permissions to connect to your organizational Google or Microsoft online account as specified\nhere\n.\nLink your account\nâ\nTo link your spreadsheet tool account:\nFrom any screen, in the upper right, navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nFrom the\nIntegrations\ntab, you can either:\nFor\nGoogle Sheets\n, click theÂ\nConnect\nlink.\nFor\nMicrosoft Excel\n,Â click theÂ\nConnect\nlink.\nIn the resulting popup, scroll to the bottom and clickÂ\nAllow\n.\nUnlink your account\nâ\nTo unlink your spreadsheet tool account:\nFrom any screen, in the upper right, navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nFrom the\nIntegrations\ntab, you can either:\nFor\nGoogle Sheets\n, click the\nDisconnect\nÂ link.\nFor\nMicrosoft Excel\n, click the\nDisconnect\nÂ link.\nIn the confirmation dialog, clickÂ\nConfirm\n.\nTags:\ndata\nintegration\nPrevious\nHow to integrate Atlan with Microsoft Excel\nNext\nHow to update column metadata in Google Sheets\nLink your account\nUnlink your account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/how-tos/link-your-servicenow-account",
    "content": "Configure Atlan\nIntegrations\nProject Management\nServiceNow\nGet Started\nLink your ServiceNow account\nOn this page\nLink your ServiceNow account\nTo request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that\nset up the ServiceNow integration\n, but not for other users.\nLink your ServiceNow account\nâ\nTo link your ServiceNow account:\nFrom any screen, in the upper right navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnder\nServiceNow\n, click the\nConnect\nlink.\nIn the resulting popup, scroll to the bottom and click\nAllow\n.\nUnlink your ServiceNow account\nâ\nTo unlink your ServiceNow account:\nFrom any screen, in the upper right navigate to your name, then click\nProfile\n.\nClick the four dots icon in the resulting dialog to get to integrations.\nUnder\nServiceNow\n, click the\nDisconnect\nlink.\nIn the confirmation dialog, click\nConfirm\n.\nTags:\ndata\nintegration\nPrevious\nHow to integrate ServiceNow\nNext\nTroubleshooting ServiceNow\nLink your ServiceNow account\nUnlink your ServiceNow account"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/references/preflight-checks-for-soda",
    "content": "Connect data\nData Quality & Observability\nSoda\nReferences\nPreflight checks for Soda\nPreflight checks for Soda\nBefore running the\nSoda crawler\n, you can run\npreflight checks\nto perform the necessary technical validations. The following preflight checks are completed:\nVerify datasources have datasets:\nâ Check successful\nâ No datasets found for datasources: datasource1, datasource2\nTags:\ndata\ncrawl\nPrevious\nWhat does Atlan crawl from Soda?"
  },
  {
    "url": "https://docs.atlan.com/faq/security-and-compliance",
    "content": "Configure Atlan\nFrequently Asked Questions\nSecurity and Compliance\nOn this page\nSecurity and Compliance\nComplete guide to Atlan's security features, compliance certifications, and data protection capabilities.\nDo you support HIPAA?\nâ\nYes, Atlan is HIPAA compliant. Visit\nAtlan's security portal\nto view the attestation letter from an external auditor.\nDo you support SOC 2?\nâ\nYes, Atlan maintains SOC 2 Type II compliance certification. Visit\nAtlan's security portal\nto access compliance documentation and reports.\nHow does Atlan protect the data at rest?\nâ\nAtlan implements comprehensive data protection measures. For detailed information, see\nEncryption and key management\n.\nHow's security enforced in Atlan?\nâ\nAtlan uses Kubernetes in an Atlan-managed VPC (virtual private cloud). The data in Atlan is secured in the following ways:\nInfrastructure security\n: Restrict network access to the control planes as well as nodes.\nAccess policies\n: Administrators can restrict user access to certain assets.\nBring your own credentials (BYOC)\n: Users can provide their own data store credentials to query data.\nWhat data is Atlan actually bringing in?\nâ\nAtlan enables you to search and discover metadata, not the data itself.\nAs a data catalog of all your data assets, Atlan enables you to:\nExtract metadata from source systems via pushdown queries or API requests.\nProcess data with the\nsample data\nand\nquery\nfeatures, both of which can be turned off.\nPush down queries when sample data or query functionality is used, so, the results are neither cached nor stored in Atlan.\nIntegrate with your\nsupported data sources\nvia a service account with read-only permissions to the data source and complete control over these permissions.\nWhat Atlan IP address can I add to my organization's firewall?\nâ\nIf your organization's firewall only allows access from whitelisted IP locations, you can\ncontact Atlan support\nto provide you with your Atlan IP address.\nTags:\ndata\nauthentication\nfaq-security\nPrevious\nData Connections and Integration\nNext\nTags and Metadata Management"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/set-up-amazon-msk",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nGet Started\nSet up Amazon MSK\nOn this page\nSet up Amazon MSK\nwarning\nð¤ Who can do this?\nYou will need your Amazon MSK administrator to run these commands   -  you may not have access yourself.\nAtlan supports IAM role authentication for fetching metadata from Amazon Managed Streaming for Apache Kafka (Amazon MSK). This method uses an AWS role ARN and region to fetch metadata.\nFor IAM role authentication, Atlan supports TLS encryption to ensure secure and encrypted communication between Atlan and your Amazon MSK cluster.\nAdditionally, Atlan currently only supports the following:\nApache Kafka 2.7.1 or higher for Amazon MSK\nProvisioned deployment\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions, follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"kafka-cluster:Connect\"\n\"kafka-cluster:DescribeCluster\"\n,\n\"kafka-cluster:DescribeGroup\"\n,\n\"kafka-cluster:DescribeTopic\"\n,\n\"kafka-cluster:DescribeTopicDynamicConfiguration\"\n,\n\"kafka-cluster:DescribeClusterDynamicConfiguration\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:kafka:<region>:<account_id>:cluster/<cluster_name>/<cluster_uuid>\"\n,\n\"arn:aws:kafka:<region>:<account_id>:group/<cluster_name>/<cluster_uuid>/*\"\n,\n\"arn:aws:kafka:<region>:<account_id>:topic/<cluster_name>/<cluster_uuid>/*\"\n]\n,\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Amazon MSK cluster.\nReplace\n<account_id>\nwith your AWS account ID.\nReplace\n<cluster_name>\nwith the name of your Amazon MSK cluster.\nReplace\n<cluster_uuid>\nwith the universally unique identifier (UUID) of your Amazon MSK cluster.\nIAM permissions\nâ\nAtlan requires the following permissions:\nkafka-cluster:Connect\n-  grants permission to connect to the Amazon MSK cluster as a Kafka client, allowing the user or service to interact with Kafka brokers for producing and consuming messages.\nkafka-cluster:DescribeCluster\n-  grants permission to extract metadata about the Amazon MSK cluster, such as its configuration, status, and associated brokers.\nkafka-cluster:DescribeGroup\n-  grants permission to describe consumer groups in the Kafka cluster. This includes metadata such as consumer group, members, and their assigned partitions.\nkafka-cluster:DescribeTopic\n-  grants permission to describe Kafka topics, including metadata such as partitions and replication factor for a topic.\nkafka-cluster:DescribeTopicDynamicConfiguration\n-  allows access to view the dynamic configurations of Kafka topics. This includes topic-level overrides for configurations like retention periods, which can be changed without requiring a cluster restart.\nkafka-cluster:DescribeClusterDynamicConfiguration\n-  allows access to view the dynamic configuration settings of a Kafka cluster. These configurations can change without restarting the cluster and include parameters like replication settings, broker properties, and more.\nRole delegation-based authentication\nâ\nUsing the policy created above, configure IAM role delegation-based authentication.\nTo configure role delegation-based authentication:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new role in your AWS account by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies, attach the policy created in the previous step to this role.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n}\n}\n]\n}\nNow,\nreach out to Atlan support\nwith:\nThe name of the role you created above.\nThe ID of the AWS account where the role was created.\ndanger\nWait until the support team confirms the account is allowlisted to assume the role before running the crawler.\nTags:\ndata\nauthentication\nPrevious\nAmazon MSK\nNext\nSet up a private network link to Amazon MSK\nCreate IAM policy\nIAM permissions\nRole delegation-based authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight",
    "content": "Connect data\nBI Tools\nCloud-based BI\nAmazon QuickSight\nGet Started\nSet up Amazon QuickSight\nOn this page\nSet up Amazon QuickSight\nwarning\nð¤ Who can do this?\nYou will probably need your Amazon QuickSight administrator to run these commands   -  you may not have access yourself.\nAtlan currently only supports IAM user authentication for Amazon QuickSight.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions, follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"quicksight:ListAnalyses\"\n,\n\"quicksight:ListDataSets\"\n,\n\"quicksight:ListDashboards\"\n,\n\"quicksight:ListFolders\"\n,\n\"quicksight:ListDataSources\"\n,\n\"quicksight:DescribeAnalysis\"\n,\n\"quicksight:DescribeDashboard\"\n,\n\"quicksight:DescribeDataSet\"\n,\n\"quicksight:DescribeFolder\"\n,\n\"quicksight:ListFolderMembers\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:quicksight:<region>:<account_id>:*\"\n]\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Amazon QuickSight instance.\nReplace\n<account_id>\nwith your AWS account ID.\nConfigure user-based authentication\nâ\nUsing the IAM policy created above, configure user-based authentication.\nTo configure user-based authentication:\nCreate an AWS IAM user by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn theÂ\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nTags:\ndata\nauthentication\nPrevious\nAmazon QuickSight\nNext\nCrawl Amazon QuickSight\nCreate IAM policy\nConfigure user-based authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nGet Started\nSet up Amazon Redshift\nOn this page\nSet up Amazon Redshift\nWho can do this?\nYou will need your Amazon Redshift administrator to run these commands   -  you may not have access yourself.\nAtlan supports fetching metadata from Amazon Redshift for the following types of deployment:\nProvisioned\nRA3\nDC2\nServerless\ndanger\nIf you're using the DC2 node type, Redshift restricts cross-database joins and metadata access to a single database. For more information, see\nConsiderations - Amazon Redshift\n. Because of this restriction, you must set up a separate workflow for each database you want to crawl.\nGrant permissions\nâ\nFor all supported authentication mechanisms\nexcept\nIAM role authentication on serverless deployment\n, you must first grant the following permissions on Amazon Redshift. For IAM role authentication on serverless deployment only, skip to\nthis step\n.\nCreate a group and user\nâ\nTo create a group and user, run the following commands:\nCREATE\nGROUP\natlan_users\n;\nCREATE\nUSER\natlan_user password\n'<pass>'\nIN\nGROUP\natlan_users\n;\nReplace\n<pass>\nwith the password for the\natlan_user\nbeing created.\nTo\ncrawl Amazon Redshift\n, for\nUsername\n, you must enter the username you configured for the database user. For example,\natlan_user\n.\nGrant required permissions to group\nâ\nTo grant the minimum required permissions, run the following commands:\nGRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users;\nGRANT SELECT ON pg_catalog.svv_table_info TO GROUP atlan_users;\nReplace\n<schema_name>\nwith the name of your schema.\nRepeat the above commands for all the databases in your schema(\n<schema_name>\n).\nThe permissions are used for the following:\nSVV_TABLE_INFO\nis used to obtain information on the table ID to table/schema/database relation.\nExternal schema\nâ\nIf your Redshift instance setup external schemas, you must grant permissions for each schema.\nGrant USAGE permissions\nâ\nFor external schemas, use the following command to grant\nUSAGE\npermission:\nGRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users;\nReplace\n<schema_name>\nwith the name of your schema.\nRepeat this command for all external schemas.\nDid you know?\nIf your external tables are sourced from Amazon S3 and AWS Glue Catalog, granting only the\nUSAGE\npermission is sufficient, provided that the IAM role associated with the Redshift cluster has read access to the data.\n(Optional) Grant SELECT permissions\nâ\nFor Redshift-based external schemas, you must explicitly grant\nSELECT\nalong with\nUSAGE\npermissions to allow metadata crawling. Use the following command to grant this permission:\nGRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO GROUP atlan_users;\nReplace\n<schema_name>\nwith the name of your schema.\nRepeat the command for all the Redshift-based external schemas.\nVerify external schema permissions\nâ\nFollow these steps to verify permissions granted to your external schema:\nLog in to the system using the IAM role created earlier.\nRun the following command using any database viewer tool:\nSELECT * FROM SVV_EXTERNAL_TABLES WHERE schema_name = '<external_schema_name>'\nReplace\n<external_schema_name>\nwith the name of your external schema.\nIf the tables appear in the results, the permissions are correctly configured.\nIf you can't provide\nSELECT\nor\nUSAGE\naccess, create a cloned schema and grant access to the\natlan_users\ngroup. For more information, see\nCloned schema for restricted access\nsection.\nCloned schema for restricted access\nâ\nIf you can't grant\nUSAGE\nor\nSELECT\npermissions to the\natlan_users\ngroup, you must create a cloned schema containing the necessary metadata tables. Then, grant permissions to the cloned schema. Follow these steps to create a cloned schema and provide required permissions:\nLog in as\ndbadmin\n.\nCreate a new schema and give it a meaningful name. For example,\natlan\n.\nClone the following views as tables from the\npg_catalog\nschema into the cloned schema:\npg_views\nSVV_TABLES\nSVV_EXTERNAL_TABLES\nSVV_COLUMNS\nClone the following views as tables from the\ninformation_schema\ninto the cloned schema:\nkey_column_usage\nas\ninformation_schema_key_column_usage\ntable_constraints\nas\ninformation_schema_table_constraints\nGrant\nUSAGE\nand\nSELECT\naccess to the\natlan_users\ngroup on the cloned schema:\nGRANT USAGE ON SCHEMA <cloned_schema_name> TO GROUP atlan_users;\nGRANT SELECT ON ALL TABLES IN SCHEMA <cloned_schema_name> TO GROUP atlan_users;\nReplace\n<cloned_schema_name>\nwith the name of your cloned schema.\nSince Atlan relies on these tables to crawl metadata, schedule a cron job to refresh the cloned tables periodically.\nDid you know?\nYou can reach out to Atlan support if you need assistance with setting up a Cloned Schema.\n(Optional) Grant permissions for role-based authentication on serverless\nâ\nFor IAM role-based authentication on Amazon Redshift serverless deployment only, you must first grant the following permissions on Amazon Redshift.\nCreate a role\nâ\nTo create a role, run the following commands:\nCREATE\nROLE atlan_role\n;\nGrant required permissions to role\nâ\nTo grant the minimum required permissions, run the following commands:\nGRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users;\nGRANT SELECT ON pg_catalog.svv_table_info TO GROUP atlan_users;\nReplace\n<schema_name>\nwith the name of your schema.\nRepeat the above commands for all the databases in your schema(\n<schema_name>\n).\nThe permissions are used for the following:\nSVV_TABLE_INFO\nis used to obtain information on the table ID to table/schema/database relation.\nExternal schema\nâ\nIf your Redshift setup uses external schemas, you must grant permissions for each schema. You can do this in one of the following ways:\nGrant USAGE permissions\nâ\nFor external schemas, use the following command to grant\nUSAGE\npermission:\nGRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users;\nReplace\n<schema_name>\nwith the name of your schema.\nRepeat this command for all external schemas.\nDid you know?\nIf your external tables are sourced from Amazon S3 and AWS Glue Catalog, granting only the\nUSAGE\npermission is sufficient, provided that the IAM role associated with the Redshift cluster has read access to the data.\n(Optional) Grant SELECT permissions\nâ\nFor Redshift-based external schemas, you must explicitly grant\nSELECT\nalong with\nUSAGE\npermissions to allow metadata crawling. Use the following command to grant this permission:\nGRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO GROUP atlan_users;\nReplace\n<schema_name>\nwith the name of your schema.\nRepeat the command for all the Redshift-based external schemas.\nVerify external schema permissions\nâ\nFollow these steps to verify permissions granted to your external schema:\nLog in to the system using the IAM role created earlier.\nRun the following command using the\nAmazon Redshift Data API\n:\nSELECT * FROM SVV_EXTERNAL_TABLES WHERE schema_name = '<external_schema_name>'\nReplace\n<external_schema_name>\nwith the name of your external schema.\nIf the tables appear in the results, the permissions are correctly configured.\nIf you can't provide\nSELECT\nor\nUSAGE\naccess, create a cloned schema and grant access to the\natlan_users\ngroup. For more information, see\nCloned schema for restricted access\nsection.\nCloned schema for restricted access\nâ\nIf you can't grant\nUSAGE\nor\nSELECT\npermissions to the\natlan_users\ngroup, you must create a cloned schema containing the necessary metadata tables. Then, grant permissions to the cloned schema. Follow these steps to create a cloned schema and provide required permissions:\nLog in as\ndbadmin\n.\nCreate a new schema and give it a meaningful name. For example,\natlan\n.\nClone the following views as tables from the\npg_catalog\nschema into the cloned schema:\npg_views\nSVV_TABLES\nSVV_EXTERNAL_TABLES\nSVV_COLUMNS\nClone the following views as tables from the\ninformation_schema\ninto the cloned schema:\nkey_column_usage\nas\ninformation_schema_key_column_usage\ntable_constraints\nas\ninformation_schema_table_constraints\nGrant\nUSAGE\nand\nSELECT\naccess to the\natlan_users\ngroup on the cloned schema:\nGRANT USAGE ON SCHEMA <cloned_schema_name> TO GROUP atlan_users;\nGRANT SELECT ON ALL TABLES IN SCHEMA <cloned_schema_name> TO GROUP atlan_users;\nReplace\n<cloned_schema_name>\nwith the name of your cloned schema.\nSince Atlan relies on these tables to crawl metadata, schedule a cron job to refresh the cloned tables periodically.\nDid you know?\nYou can reach out to Atlan support if you need assistance with setting up a Cloned Schema.\nGrant additional permissions for mining query history\nâ\nDid you know?\nFor mining query history from Redshift Serverless, permissions on STL and SVL views are not required as they do not exist in serverless deployment.\nTo grant the additional permissions needed to mine query history, run the following commands:\nGRANT\nSELECT\non\npg_catalog\n.\nstl_ddltext\nto\nGROUP\natlan_users\n;\nGRANT\nSELECT\non\npg_catalog\n.\nstl_query\nto\nGROUP\natlan_users\n;\nGRANT\nSELECT\non\npg_catalog\n.\nstl_connection_log\nto\nGROUP\natlan_users\n;\nGRANT\nSELECT\non\npg_catalog\n.\nstl_undone\nto\nGROUP\natlan_users\n;\nGRANT\nSELECT\non\npg_catalog\n.\nstl_insert\nto\nGROUP\natlan_users\n;\nGRANT\nSELECT\non\npg_catalog\n.\nsvl_statementtext\nto\nGROUP\natlan_users\n;\nALTER\nUSER\natlan_user SYSLOG ACCESS UNRESTRICTED\n;\nThe additional permissions are used for the following:\nSTL_DDLTEXT\nis used for DDL queries.\nSTL_QUERY\nis used for DML and regular queries.\nSTL_CONNECTION_LOG\nis used to obtain the session ID that a query is part of.\nSTL_UNDONE\nis used to obtain information about transactions that have been undone or rolled back.\nSTL_INSERT\nis used to obtain the table ID used in the insert queries.\nSVL_STATEMENTTEXT\nis used to obtain the query text for all queries.\nSYSLOG ACCESS UNRESTRICTED\nis used to access all queries performed by any user in the system tables above.\nTo use basic authentication, your setup is now complete. To configure IAM-based authentication, you need to continue with the following steps.\n(Optional) Create IAM policy\nâ\nAll IAM-based authentication mechanisms require an IAM policy to be created. For all supported authentication mechanisms\nexcept\nIAM role authentication on serverless deployment\n, create the following IAM policy. For IAM role authentication on serverless deployment only, skip to\nthis step\n.\nTo create an IAM policy with the necessary permissions follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"redshift:GetClusterCredentials\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:redshift:<region>:<account_id>:dbuser:<redshift_cluster_identifier>/atlan_user\"\n,\n\"arn:aws:redshift:<region>:<account_id>:dbname:<redshift_cluster_identifier>/<database>\"\n]\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Redshift instance.\nReplace\n<account_id>\nwith your account ID.\nReplace\n<redshift_cluster_identifier>\nwith your Redshift cluster identifier.\nReplace\n<database>\nwith the name of the Redshift database.\n(Optional) Create IAM policy for role-based authentication on serverless\nâ\nFor IAM role-based authentication on Amazon Redshift serverless deployment only, create an IAM policy with the necessary permissions. Follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"redshift-serverless:GetCredentials\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:redshift-serverless:<region>:<account_id>:workgroup/<workgroup_identifier>\"\n,\n]\n}\n]\n}\nReplace\n<region>\nwith the AWS region of your Amazon Redshift instance.\nReplace\n<account_id>\nwith your AWS account ID.\nReplace\n<workgroup_identifier>\nwith your Amazon Redshift serverless workgroup identifier.\nConfigure tag for IAM role:\n{\nRedshiftDbRoles : <role>\n}\nReplace\n<role>\nwith the\nrole you created\n.\n(Optional) Choose IAM-based authentication mechanism\nâ\nUsing the policy created above, configure one of the following options for authentication.\nUser-based authentication\nâ\nTo configure user-based authentication:\nCreate an AWS IAM user by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn the\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user's\naccess key ID\nand\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nRole delegation-based authentication\nâ\nTo configure role delegation-based authentication:\nRaise a support ticket\nto get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster.\nCreate a new role in your AWS account by following\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies, attach the policy created in the previous step to this role.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n}\n}\n]\n}\n(Optional) To use an\nexternal ID\nfor additional security, paste the external ID into the policy:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n\"Condition\"\n:\n{\n\"StringEquals\"\n:\n{\n\"sts:ExternalId\"\n:\n\"<atlan_external_id>\"\n}\n}\n}\n]\n}\nReplace\n<atlan_external_id>\nwith the external ID you want to use.\nNow,\nreach out to Atlan support\nwith:\nThe name of the role you created above.\nThe ID of the AWS account where the role was created.\ndanger\nWait until the support team confirms the account is allowlisted to assume the role before running the crawler.\nTags:\ndata\ncrawl\nauthentication\nPrevious\nAmazon Redshift\nNext\nHow to enable SSO for Amazon Redshift\nGrant permissions\n(Optional) Grant permissions for role-based authentication on serverless\nGrant additional permissions for mining query history\n(Optional) Create IAM policy\n(Optional) Create IAM policy for role-based authentication on serverless\n(Optional) Choose IAM-based authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-an-azure-private-network-link-to-databricks",
    "content": "Connect data\nData Warehouses\nDatabricks\nPrivate Network Setup\nSet up an Azure private network link to Databricks\nOn this page\nSet up an Azure private network link to Databricks\nAzure Private Link\ncreates a secure, private connection between services running in Azure. This document describes the steps to set this up between Databricks and Atlan.\nWho can do this?\nYou will need Databricks support, and probably your Databricks administrator involved   -  you may not have access or the tools to run these tasks.\nPrerequisites\nâ\nYour Databricks instance must be Azure-managed and created from the Azure marketplace.\nFor all details, see\nDatabricks documentation\n.\nNotify Atlan support\nâ\nProvide Atlan support\nwith the following information:\nResource ID of your Azure-managed Databricks instance   -  the resource ID will be in this format:\n/subscriptions/<subscriptionID>/resourceGroups/azure-databricks/providers/Microsoft.Databricks/workspaces/<databricks-workspace>\n.\nThere are additional steps that Atlan will need to complete. Once the Atlan team has confirmed that the configuration is ready, please continue with the remaining steps.\nApprove the endpoint connection request\nâ\nTo approve the endpoint connection request from Atlan:\nOpen your Azure-managed Databricks workspace.\nIn the left menu, click\nNetworking\nand then click the\nPrivate endpoint connections\ntab.\nFrom the list of endpoints, search for the endpoint connection request from Atlan. In the\nConnection state\ncolumn for the Atlan endpoint connection, click the\nApprove\nbutton to approve the request .\nOnce the endpoint connection from Atlan has been approved, the status of the private endpoint in Atlan will change to\nApproved\n.\nWhen you use this endpoint in the configuration for\ncrawling Databricks\n, Atlan will connect to Databricks over Azure Private Link.\nTags:\ndata\nconfiguration\nPrevious\nSet up an AWS private network link to Databricks\nNext\nHow to extract lineage and usage from Databricks\nPrerequisites\nNotify Atlan support\nApprove the endpoint connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/how-tos/set-up-confluent-kafka",
    "content": "Connect data\nEvent/Messaging\nConfluent Kafka\nGet Started\nSet up Confluent Kafka\nOn this page\nSet up Confluent Kafka\nWho can do this?\nYou will need your\nOrganizationAdmin\n,\nEnvironmentAdmin\n, or\nCloudClusterAdmin\nrole to complete these steps   -  you may not have access yourself.\nAtlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata.\nCreate an API Key\nâ\nThis section provides steps for creating an API key to access metadata from your Confluent Kafka environment.\ninfo\nðª\nNote:\nAtlan does\nnot\nperform any API requests or queries that modify the resources in your Confluent Kafka environment.\nResource-Specific API Key\nâ\nTo\ncreate a resource-specific API key\nfor\ncrawling Confluent Kafka\n, follow these steps:\nLog in to your\nConfluent Cloud\ninstance with a\nOrganizationAdmin\n,\nEnvironmentAdmin\n, or\nCloudClusterAdmin\nrole.\nFrom the\nCloud Console\n, select your active cluster.\nUnder\nCluster Overview\nin the left menu for your active cluster, click\nAPI Keys\n.\nIn the upper right of the\nAPI Keys\npage, click\n+ Add key\n.\nOn the\nCreate key\npage, enter the following details:\nFor\nAccess Control\n, under\nSelect Scope for API key\n, select\nGranular access\nto define a specific set of access rules, then click\nNext\nto proceed.\nFor\nService Account\n, click\nCreate a new one\nand enter the following details:\nFor\nNew service account name\n, enter a meaningful name for your API key. For example,\nAtlan\n.\n(Optional) For\nDescription\n, add a description for your API key. For example,\nAtlan crawler connection\n.\nClick\nNext\nto proceed.\nFor\nAdd ACLs to service account\n, click\n+ Add ACLs\nto add and allow the following minimum permissions required for your Confluent Kafka resources:\nCluster:\nDescribeConfigs\nGroup:\nDescribe\nTopic:\nDescribe\n,\nDescribeConfigs\nOnce you've added all the permissions, click\nNext\nto proceed.\nFor\nCreate key\n, under\nGet your API key\n, copy or download the API key and secret. Make sure to store them securely, as the secret can't be retrieved later.\n(Optional) Cloud API Key\nâ\nTo access Kafka metrics(\nsizeInBytes\n), you need a Cloud API key. Follow these steps to generate one:\nClick the hamburger menu (â°) icon in the top right corner to open the menu.\nIn the menu, click\nAPI Keys\n.\nOn the API Key listing screen, click the\nAdd API Key\nbutton to add a new API key.\nOn the\nSelect an account for API key\nscreen, select the account appropriate for your service or user account, then click\nNext\n.\nOn the\nSelect resource scope for API key\nscreen, choose\nCloud resource management\n, then click\nNext\n.\nOn the\nAPI details screen\n, enter the required details:\nName\n: Provide a unique and meaningful name for your API key. For example, Atlan Kafka Metrics Key.\nDescription\n: Add a description that illustrates the purpose of the key. For example, Atlan Kafka metrics API key with read-only permissions.\nClick\nCreate API Key\nto generate the key.\nOn the\nAPI key download screen\n, copy the API key and secret or click\nDownload API Key\nto save them as a file. Make sure to store them securely, as the secret can't be retrieved later.\nClick\nComplete\nto finish the API key creation process. You will be able to see the generated API key on the API key listing screen.\nTags:\ndata\napi\nauthentication\nPrevious\nConfluent Kafka\nNext\nCrawl Confluent Kafka\nCreate an API Key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/how-tos/set-up-datastax-enterprise",
    "content": "Connect data\nDatabases\nNoSQL Databases\nDatastax Enterprise\nGet Started\nSet up DataStax Enterprise\nOn this page\nSet up DataStax Enterprise\nð¤\nWho can do this?\nYou need your DataStax Enterprise administrator or a user who has create role permissions to run these commands; you may not have access yourself. For more details, refer to the\nApache Cassandra documentation on role-based access control\n.\nThis guide outlines the steps to configure your DataStax Enterprise instance so Atlan can crawl its metadata.\nPrerequisites\nâ\nBefore you begin, ensure you have the following:\nAccess to a DataStax Enterprise instance\n- You need the necessary credentials to connect to your Cassandra instance.\ncqlsh\ninstalled\n- The Cassandra Query Language Shell (cqlsh) is required to execute commands for user and permission management. If\ncqlsh\nis not installed, refer to the\nApache Cassandra documentation for\ncqlsh\n.\nCreate an Atlan user role\nâ\nThis section guides you through creating a dedicated role in DataStax Enterprise for Atlan.\nConnect to the DataStax Enterprise instance\n: Use\ncqlsh\nto connect to your DataStax Enterprise instance.\nCreate a dedicated role for Atlan\n: Run the following command in\ncqlsh\nto create a role.\nCREATE ROLE <username> WITH SUPERUSER = false AND LOGIN = true AND PASSWORD = '<password>';\nReplace the placeholders\n<username>\n: The username for the Atlan role.\n<password>\n: A strong and unique password.\nGrant permissions to the Atlan user role\nâ\nAtlan needs specific permissions to access metadata in DataStax Enterprise Follow these steps to grant the required permissions:\nGrant\nDESCRIBE\non all keyspaces with:\nGRANT DESCRIBE ON ALL KEYSPACES TO atlan-admin;\nYou may also choose to grant\nDESCRIBE\npermission on specific keyspaces by running the following command:\nGRANT DESCRIBE ON KEYSPACE <keyspace_name> TO <username>;\nReplace placeholders:\n<keyspace_name>\n: The name of the keyspace.\n<username>\n: The username for the Atlan role.\nRun this command for each keyspace you want Atlan to access.\nTags:\ndata\ncrawl\nPrevious\nDataStax Enterprise\nNext\nCrawl DataStax Enterprise\nPrerequisites\nCreate an Atlan user role\nGrant permissions to the Atlan user role"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/how-tos/set-up-fivetran",
    "content": "Connect data\nETL Tools\nFivetran\nGet Started\nSet up Fivetran\nOn this page\nSet up Fivetran\nFivetran Platform Connector\nâ\nWho can do this?\nYou need your Fivetran\nAccount Administrator\n(who can create, view, edit, and delete destinations and connectors) to complete the steps below   -  you may not have access yourself.\nNote\n: You also need\nFivetran Enterprise\n(or above) to use this integration. If you're not on such a plan yet, Atlan may be able to help you access a trial period from Fivetran. Just reach out to your Atlan contact!\nCreate a destination\nâ\nThe Fivetran Platform Connector delivers your logs and account or destination metadata to a schema in your destination. Fivetran automatically adds this connector to every new destination you create. You need to set up at least one destination to receive the log events.\nAtlan currently supports the following destinations, refer to Fivetran documentation linked below to configure them in Fivetran:\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nPostgreSQL\nSnowflake\nIf you have already configured a destination in Fivetran, skip to the next step.\nConfigure Fivetran Platform Connector\nâ\nOnce you have set up a supported destination, follow the steps in this\nsetup guide\nfrom Fivetran to set up your Fivetran Platform Connector account-wide.\nYou must configure the Fivetran Platform Connector on an account level.\nAtlan recommends not\nexcluding any columns\n, as this can impact the successful execution of the Fivetran enrichment package.\nThe warehouse credentials or role configured in Atlan must have the necessary permissions to query tables in\n<Fivetran_destination_database>.<FPC_schema_name>\n. Refer to the\nFivetran documentation\nfor available\nFPC_schema_name\nvalues.\nThe role must have the required permissions to access these tables. Refer to the relevant guide below for setting up permissions:\nSet up Snowflake\nSet up Google BigQuery\nSet up Databricks\nSet up Amazon Redshift\nSet up PostgreSQL\nThis enables you to sync all the metadata and logs for all the connectors in your account to a single destination.\nTags:\ndata\nintegration\ncrawl\napi\nconfiguration\nPrevious\nFivetran\nNext\nCrawl Fivetran\nFivetran Platform Connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/set-up-hive",
    "content": "Connect data\nDatabases\nQuery Engines\nHive\nGet Started\nSet up Hive\nOn this page\nSet up Hive\nWho can do this?\nYou will need your Hadoop administrator to run these commands   -  you may not have access yourself.\nCurrently, we only support basic username and password authentication for Hive. Complete the following steps to configure it.\nSet the right permissions\nâ\nTo configure basic authentication for Hive, enter the following details:\nFor\nHost Name\n, enter theÂ Atlan-accessible Hive instance URL.\nFor\nPort\n, enter the port number where your Hive instance is accessible.\nFor\nDefault Schema\n, enter the default schema name in your Hive instance for connection. Atlan will crawl other schemas too   -  not just the default one.\nGrant read permission on objects\nâ\nGrant read permission on objects with the following commands:\nGRANT\nSELECT\nON\nDATABASE\n<\ndatabase_name\n>\nTO\nUSER\n<\nusername\n>\n;\nAtlan requires read permission for all the objects you want to crawl in Hive.\nDid you know?\nAvailable users and access control may also be controlled or affected by HDFS ACL, LDAP, and any other policy engine that is in effect. Overall, Atlan requires the authenticating user to have read permission at a minimum.\nTags:\ndata\ncrawl\nauthentication\nPrevious\nHive\nNext\nSet up a private network link to Hive\nSet the right permissions\nGrant read permission on objects"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMicrosoft Azure Cosmos DB\nGet Started\nSet up Microsoft Azure Cosmos DB\nOn this page\nSet up Microsoft Azure Cosmos DB\nDid you know?\nAtlan currently only supports crawling\nMicrosoft Azure Cosmos DB for MongoDB\nwith the Microsoft Azure Cosmos DB connector.\nAtlan supports the following deployment types for fetching metadata from Microsoft Azure Cosmos DB:\nvCore-based deployment   -  you can use SCRAM-SHA authentication for vCore-based accounts. You will need to authenticate the connection in Atlan with a primary connection string to fetch metadata from vCore-based accounts. Atlan provides multi-account support for fetching metadata.\nRU-based deployment   -  you can use service principal authentication for request unit (RU)-based accounts. You will need to authenticate the connection in Atlan with a client ID, client secret, and tenant ID to fetch metadata from RU-based accounts. Atlan provides multi-account support for fetching metadata.\nIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the\nvCore and RU\ndeployment option to\ncrawl your Microsoft Azure Cosmos DB assets\n.\nvCore deployment\nâ\nWho can do this?\nYou will need your Microsoft Azure Cosmos DB administrator to complete these steps   -  you may not have access yourself.\nFor vCore-based accounts, you will need the primary connection string of your Microsoft Azure Cosmos DB deployment to use SCRAM-SHA authentication for\nintegrating with Atlan\n.\nTo retrieve the primary connection string for vCore-based accounts:\nLog in to the\nAzure portal\nas an admin.\nIn the portal, search for and select\nAzure Cosmos DB\n.\nOn the\nAzure Cosmos DB\npage, select your Azure Cosmos DB for MongoDB (vCore) account.\nFrom the\nOverview\npage, copy the value of the\nAdmin username\n. For password, you will need the password that was set up during your Microsoft Azure Cosmos DB deployment.\nIn the left menu of the account page, under\nSettings\n, click\nConnection strings\n.\nCopy the value of the\nPrimary Connection String\nand store it in a secure location. You will need to add the values of the admin username and password to the placeholder values in the primary connection string you copied. Repeat steps 1 to 6 for all the vCore-based accounts you want to crawl in Atlan.\nRU-based deployment\nâ\nFor request Unit (RU)-based accounts, you will need a client ID, client secret, and tenant ID for service principal authentication. Microsoft Azure Cosmos DB for MongoDB deployment currently does not support service principal authentication for vCore-based accounts.\nRegister app with Microsoft Entra ID\nâ\nWho can do this?\nYou will need your\nCloud Application Administrator\nor\nApplication Administrator\nto complete these stepsÂ   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization.\nYou will need to\nregister your service principal application\nwith Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret.\nTo register your app with Microsoft Entra ID:\nLog in to the\nAzure portal\n.\nIn the search bar, search for\nMicrosoft Entra ID\n, and select it from the dropdown list.\nFrom the left menu of the\nMicrosoft Entra ID\npage, click\nApp registrations\n.\nFrom the toolbar on the\nApp registrations\npage, click\n+ New registration\n.\nOn the\nRegister an application\npage, for\nName\n, enter a name for your service principal application and then click\nRegister\n.\nOn the homepage of your newly created application, from the\nOverview\nscreen, copy the values for the following fields and store them in a secure location:\nApplication (client) ID\nDirectory (tenant) ID\nFrom the left menu of your newly created application page, click\nCertificates & secrets\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, click\n+ New client secret\n.\nIn the\nAdd a client secret\nscreen, enter the following details:\nFor\nDescription\n, enter a description for your client secret.\nFor\nExpiry\n, select when the client secret will expire.\nClick\nAdd\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, for the newly created client secret, click the clipboard icon to copy the\nValue\nand store it in a secure location.\nSet permissions\nâ\nWho can do this?\nYou will need your Microsoft Azure Cosmos DB administrator to complete these steps   -  you may not have access yourself.\nYou will need to add the service principal to the\nCosmos DB Account Reader Role\n. This will allow the service principal read-only access to your Azure Cosmos DB account data.\nTo add the service principal to the\nCosmos DB Account Reader Role\n:\nLog in to the\nAzure portal\n.\nOpen the menu and search for or select\nAzure Cosmos DB\n.\nOn the\nAzure Cosmos DB\npage, select your Azure Cosmos DB for MongoDB (RU) account.\nFrom the left menu of your Azure Cosmos DB for MongoDB (RU) account page, click\nAccess control (IAM)\n.\nFrom the tabs along the top of the\nAccess control (IAM)\npage, click\nAdd\nand then click\nAdd role assignment\n.\nOn the\nAdd role assignment\npage, configure the following:\nIn the\nRoles\ntab, from the list of roles under\nJob function roles\n,Â select\nCosmos DB Account Reader Role\nÂ   -  this allows\nread-only access to Azure Cosmos DB account data\n-  and then click\nNext\n. You will need to assign this role to all the RU-based accounts you want to crawl in Atlan.\nIn the\nMembers\ntab, enter the following details:\nFor\nAssign access to\n, click\nUser, group, or service principal\n.\nFor\nMembers\n, click\n+ Select members\nand then select the\nservice principal\nyou created. Click\nNext\nto proceed to the next step.\nIn the\nReview + assign\ntab, click\nReview + assign\nto add role assignment.\n(Optional) Whitelist Atlan IP range\nâ\nYou may need to whitelist Atlan's IP range to allow Atlan to\ncrawl Microsoft Azure Cosmos DB\n.\nTo whitelist the Atlan IP range:\nLog in to the\nAzure portal\n.\nOpen the menu and search for or select\nAzure Cosmos DB\n.\nOn the\nAzure Cosmos DB\npage, select your Azure Cosmos DB for MongoDB account.\nFrom the left menu of your Azure Cosmos DB for MongoDB account page, click\nNetworking\n.\nOn the\nNetworking\npage, under\nPublic network access\n, check the following:\nIf\nAll networks\nis enabled, no further action required.\nIf\nSelect networks\nis enabled,\nraise an Atlan support request\nto obtain Atlan's IP range. Once received from Atlan support, for\nIP (Single IPv4 or CIDR range)\n, enter Atlan's IP range and click the\nSave\nbutton.\nTags:\ndata\ncrawl\nauthentication\nPrevious\nMicrosoft Azure Cosmos DB\nNext\nCrawl Microsoft Azure Cosmos DB\nvCore deployment\nRU-based deployment\n(Optional) Whitelist Atlan IP range"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-azure-data-factory",
    "content": "Connect data\nETL Tools\nMicrosoft Azure Data Factory\nGet Started\nSet up Microsoft Azure Data Factory\nOn this page\nSet up Microsoft Azure Data Factory\nAtlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata.\nRegister app with Microsoft Entra ID\nâ\nWho can do this?\nYou will need your\nCloud Application Administrator\nor\nApplication Administrator\nto complete these stepsÂ   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization.\nYou will need to\nregister your service principal application\nwith Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret.\nTo register your app with Microsoft Entra ID:\nLog in to the\nAzure portal\n.\nIn the search bar, search for\nMicrosoft Entra ID\n, and select it from the dropdown list.\nFrom the left menu of the\nMicrosoft Entra ID\npage, click\nApp registrations\n.\nFrom the toolbar on the\nApp registrations\npage, click\n+ New registration\n.\nOn the\nRegister an application\npage, for\nName\n, enter a name for your service principal application and then click\nRegister\n.\nOn the homepage of your newly created application, from the\nOverview\nscreen, copy the values for the following fields and store them in a secure location:\nApplication (client) ID\nDirectory (tenant) ID\nFrom the left menu of your newly created application page, click\nCertificates & secrets\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, click\n+ New client secret\n.\nIn the\nAdd a client secret\nscreen, enter the following details:\nFor\nDescription\n, enter a description for your client secret.\nFor\nExpiry\n, select when the client secret will expire.\nClick\nAdd\n.\nOn the\nCertificates & secrets\npage, under\nClient secrets\n, for the newly created client secret, click the clipboard icon to copy the\nValue\nand store it in a secure location.\nSet permissions\nâ\nWho can do this?\nYou will need your Microsoft Azure Data Factory administrator to complete these steps   -  you may not have access yourself.\nYou will need to add the service principal to the\nReader role\n. This will allow the service principal read-only access to your Microsoft Azure Data Factory account.\nTo add the service principal to the\nReader\nrole:\nLog in to the\nAzure portal\n.\nOpen the menu and search for or select\nData factories\n.\nOn the\nData factories\npage, select the data factory you want to crawl in Atlan.\nFrom the left menu of your data factory page, click\nAccess control (IAM)\n.\nFrom the tabs along the top of the\nAccess control (IAM)\npage, click\nAdd\nand then click\nAdd role assignment\n.\nOn the\nAdd role assignment\npage, configure the following:\nIn the\nRoles\ntab, from the list of roles under\nJob function roles\n, select\nReader\nÂ   -  this allows\nread-only access to your data factory\n-  and then click\nNext\n. You will need to assign this role to all the data factories you want to crawl in Atlan.\nIn the\nMembers\ntab, enter the following details:\nFor\nAssign access to\n, click\nUser, group, or service principal\n.\nFor\nMembers\n, click\n+ Select members\nand then select the service principal you created. Click\nNext\nto proceed to the next step.\nIn the\nReview + assign\ntab, click\nReview + assign\nto add role assignment.\nAtlan will extract metadata from all the data factories you specified in your Microsoft Azure Data Factory account with\nReader\naccess.\nTags:\ndata\napi\nauthentication\nPrevious\nMicrosoft Azure Data Factory\nNext\nCrawl Microsoft Azure Data Factory\nRegister app with Microsoft Entra ID\nSet permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nGet Started\nSet up Microsoft SQL Server\nOn this page\nSet up Microsoft SQL Server\nWho can do this?\nYou will probably need your Microsoft SQL Server administrator to run these commands   -  you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from Microsoft SQL Server. This method uses a username and password to fetch metadata.\nCreate a login\nâ\nTo create a login with a specific password to integrate into Atlan:\nCREATE\nLOGIN\n<\nlogin_name\n>\nWITH\nPASSWORD\n=\n'<password>'\n;\nReplace\n<login_name>\nwith the name of the login.\nReplace\n<password>\nwith the password for the login.\nCreate a user\nâ\nTo create a user for that login:\nCREATE\nUSER\n<\nusername\n>\nFOR\nLOGIN\n<\nlogin_name\n>\n;\nReplace\n<username>\nwith the username to use when integrating Atlan.\nReplace\n<login_name>\nwith the name of the login used in the previous step.\nGrant permissions\nâ\nCrawl assets and mine view lineage\nâ\nThe following grant crawls all your Microsoft SQL Server assets and mines lineage for views. As the option of least privilege access, it avoids accessing any raw data.\nTo grant the minimum permissions required to crawl assets and mine view lineage from Microsoft SQL Server:\nGRANT\nVIEW\nDEFINITION\nON\nDATABASE\n::\n<\ndatabase_name\n>\nTO\n<\nusername\n>\n;\nReplace\n<database_name>\nwith the name of the database.\nReplace\n<username>\nwith the username created above.\n(Optional) Preview and query assets\nâ\nTo grant the minimum permissions required to also preview sample data and query assets from Microsoft SQL Server:\nGRANT\nSELECT\nON\nDATABASE\n::\n<\ndatabase_name\n>\nTO\n<\nusername\n>\n;\nReplace\n<database_name>\nwith the name of the database.\nReplace\n<username>\nwith the username created above.\ndanger\nYou\nmust\ngrant permissions to the user for all the databases you want to crawl in Atlan except the system databases (\nmaster\n,\ntempdb\n,\nmsdb\n,\nmodel\n). The Microsoft SQL Server crawler will only fetch database and schema names without these permissions and no other metadata for other asset types.\nTags:\ndata\ncrawl\nauthentication\nPrevious\nMicrosoft SQL Server\nNext\nCrawl Microsoft SQL Server\nCreate a login\nCreate a user\nGrant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-mysql",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nGet Started\nSet up MySQL\nOn this page\nSet up MySQL\nWho can do this?\nYou will probably need your MySQL administrator to run these commands   -  you may not have access yourself.\nDid you know?\nAtlan supports both of the following AWS database engines   -  RDS MySQL and Aurora MySQL.\nCurrently we support the following authentication mechanisms. You will need to choose one and configure it according to the steps below.\nBasic authentication\nIdentity and Access Management (IAM) authentication\nBasic authentication\nâ\nTo configure basic authentication for MySQL, run the following commands:\nCREATE\nUSER\n'{{db-username}}'\n@'%'\nIDENTIFIED\nBY\n'{{password}}'\n;\nGRANT\nSELECT\n,\nSHOW\nVIEW\n,\nEXECUTE\nON\n*\n.\n*\nTO\n'{{db-username}}'\n@'%'\n;\nFLUSH\nPRIVILEGES\n;\nReplace\n{{db-username}}\nwith the username you want to create.\nReplace\n{{password}}\nwith the password to be used for that username.\nAtlan requires the following privileges to:\nSELECT\n:\nFetch the technical metadata persisted in the\nINFORMATION_SCHEMA\n.\n*.*\nis required because\nINFORMATION_SCHEMA\ntables cannot be granted access directly. Metadata is inferred from the access that the querying user has on the underlying tables.\nEnable users to preview or query the underlying tables and views   -  this functionality can also be turned off.\nSHOW VIEW\nenables the use of the\nSHOW CREATE VIEW\nstatement to fetch view definitions for generating lineage.\nEXECUTE\nis only required if using MySQL 5.7 and any earlier versions.\nIdentity and Access Management (IAM) authentication\nâ\nTo configure IAM authentication for MySQL follow each of these steps.\nEnable IAM authentication\nâ\nTo enable IAM authentication for your database instance:\nFor Amazon RDS, follow\nthe steps in the Amazon RDS documentation\n.\nFor Aurora, follow\nthe steps in the User Guide for Aurora documentation\n.\nWhen given the option, apply the changes immediately and wait until they are complete.\nCreate database user\nâ\nTo create a database user with the necessary permissions run the following commands:\nCREATE\nUSER\n'{{db-username}}'\n@'%'\nIDENTIFIED\nWITH\nAWSAuthenticationPlugin\nas\n'RDS'\n;\nGRANT\nSELECT\n,\nSHOW\nVIEW\n,\nEXECUTE\nON\n*\n.\n*\nTO\n'{{db-username}}'\n@'%'\n;\nFLUSH\nPRIVILEGES\n;\nReplace\n{{db-username}}\nwith the username you want to create.\nThese permissions will allow you to crawl metadata, preview and query data from within Atlan.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"rds-db:connect\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\"\n]\n}\n]\n}\nReplace\n{{aws-region}}\nwith the AWS region of your database instance.\nReplace\n{{account-id}}\nwith your account ID.\nReplace\n{{resource-id}}\nwith the resource ID.\nReplace\n{{db-username}}\nwith the username created in the previous step.\nAttach IAM policy\nâ\nTo attach the IAM policy for Atlan's use, you have two options:\nIAM role\n: Attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster. Please\nraise a support ticket\nto use this option.\nIAM user\n: Create an AWS IAM user and attach the policy to this user. To create an AWS IAM user:\nFollow\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn the\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user's\naccess key ID\nand\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nTags:\ndata\nauthentication\nPrevious\nMySQL\nNext\nSet up a private network link to MySQL\nBasic authentication\nIdentity and Access Management (IAM) authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nGet Started\nSet up on-premises IBM Cognos Analytics access\nOn this page\nSet up on-premises IBM Cognos Analytics access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details, including credentials.\nIn some cases you will not be able to expose your IBM Cognos Analytics instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your on-premises IBM Cognos Analytics instance, you will need to use Atlan's cognos-extractor tool.\nDid you know?\nAtlan uses exactly the same cognos-extractor behind the scenes when it connects to IBM Cognos Analytics in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the cognos-extractor tool\nâ\nTo get the cognos-extractor tool:\nRaise a support ticket\nto get the link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to crawl IBM Cognos Analytics:\nsudo docker load -i /path/to/cognos-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a\nDocker compose file\nfor the cognos-extractor tool.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises IBM Cognos Analytics instance.\nThe file is\ndocker-compose.yaml\n.\nDefine IBM Cognos Analytics connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your IBM Cognos Analytics connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises IBM Cognos Analytics instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\ncognos-example:\n<<: *extract\nenvironment:\n<<: *cognos-defaults\nEXCLUDE_FILTER: '{}'\nINCLUDE_FILTER: '{}'\nvolumes:\n- ./output/cognos-example:/output/process\nReplace\ncognos-example\nwith the name of your connection.\n<<: *extract\ntells the cognos-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nEXCLUDE_FILTER\nand\nINCLUDE_FILTER\n-  specify a regular expression to filter assets to exclude or include, respectively. For example, to exclude a folder with the ID\n76471ff1e0f02c7d3349\nin\nteam_content\n, configure the\nEXCLUDE_FILTER\nas follows   -\n'{\"team_content\": {\"76471ff1e0f02c7d3349\": {}}'\n.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/cognos-example\nÂ folder on the local file system.\nYou can add as many IBM Cognos Analytics connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your IBM Cognos Analytics connections, you will need to provide an IBM Cognos Analytics configuration file.\nThe IBM Cognos Analytics configuration is a\n.ini\nfile with the following format:\n[CognosConfig]\nhost=http://cognos-application-host-example.us-east-2.compute.amazonaws.com\nport=9300\nnamespace=CognosEx\n# possible values are \"basic_auth\", \"okta_auth\" and \"api_key\"\nauth_type=basic_auth\n# Only required when auth_type = basic_auth\n[BasicAuth]\n[email protected]\npassword=<password>\n# Only required when auth_type = okta_auth\n[OKTAAuth]\n[email protected]\npassword=<password>\n# Only required when auth_type = api_key\n[APIKeyAuth]\nkey=<yourAPIkey>\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep IBM Cognos Analytics credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets:\ncognos_config:\nfile: ./cognos.ini\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a sub-section of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the IBM Cognos Analytics configuration file:\nsudo docker secret create cognos_config path/to/cognos.ini\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\ncognos_config:\nexternal: true\nname: cognos_config\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local IBM Cognos Analytics configuration file.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\ncognos_config:\nexternal: true\nname: cognos_config\nx-templates:\n# ...\nservices:\ncognos-example:\n<<: *extract\nenvironment:\n<<: *cognos-defaults\nEXCLUDE_FILTER: '{}'\nINCLUDE_FILTER: '{}'\nvolumes:\n- ./output/cognos-example:/output/process\nsecrets:\n- cognos_config\nIn this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The\ncognos_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\ncognos-example\n. You can use any meaningful name you want.\nThe\n<<: *cognos-defaults\nsets the connection type to IBM Cognos Analytics.\nThe\nEXCLUDE_FILTER\nand\nINCLUDE_FILTER\ntells the extractor to filter folders.\nThe\n./output/cognos-example:/output/process\nline tells the extractor where to store results. In this example, the extractor will store results in the\n./output/cognos-example\ndirectory on the local file system. We recommend you output the extracted metadata for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\ndata\ncrawl\nPrevious\nSet up IBM Cognos Analytics\nNext\nCrawl IBM Cognos Analytics\nPrerequisites\nGet the compose file\nDefine IBM Cognos Analytics connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nGuides\nSet up on-premises Kafka access\nOn this page\nSet up on-premises Kafka access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Kafka instance details, including credentials.\nIn some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your Kafka instance, you will need to use Atlan's kafka-extractor tool.\nDid you know?\nAtlan uses exactly the same kafka-extractor behind the scenes when it connects to Kafka in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the kafka-extractor tool\nâ\nTo get the kafka-extractor tool:\nRaise a support ticket\nto get the link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to crawl Kafka:\nsudo docker load -i /path/to/kafka-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a\nDocker compose file\nfor the kafka-extractor tool.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises Kafka instance.\nThe file is\ndocker-compose.yaml\n.\nDefine Kafka connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. Keep the default settings; no changes are required.\nservices\nis where you will define your Kafka connections.\nvolumes\ncontains mount information. Keep the default settings; no changes are required.\nDefine services for Apache Kafka\nâ\nFor each Apache Kafka instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\n# Example Apache Kafka connection\nconnection-name\n:\n<<\n:\n*extract\nenvironment\n:\n<<\n:\n*kafka-defaults\n# Kafka bootstrap servers (semicolon-separated)\nKAFKA_BOOTSTRAP_SERVERS\n:\n\"localhost:9092\"\n# Skip topics that are internal to Kafka (e.g. __consumer_offsets)\nSKIP_INTERNAL_TOPICS\n:\n\"true\"\nvolumes\n:\n# You can change './output/connection-name' to any output location you want\n-\n./output/connection\n-\nname\n:\n/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\ntells the kafka-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Apache Kafka connections as you want.\nDefine services for Confluent Kafka\nâ\nFor each Confluent Kafka instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\n# Example Confluent Kafka connection\nconnection-name\n:\n<<\n:\n*extract\nenvironment\n:\n<<\n:\n*kafka-defaults\n# Kafka bootstrap servers (semicolon-separated)\nKAFKA_BOOTSTRAP_SERVERS\n:\n\"localhost:9092\"\n# Skip topics that are internal to Kafka (e.g. __consumer_offsets)\nSKIP_INTERNAL_TOPICS\n:\n\"true\"\nKAFKA_FLAVOUR\n:\n\"CONFLUENT_CLOUD\"\nCONFLUENT_AUTH\n:\n\"<cloud_api_key>:<cloud_api_secret>\"\nCONFLUENT_CLUSTER_ID\n:\n\"<lkc-xxxx>\"\nvolumes\n:\n# You can change './output/connection-name' to any output location you want\n-\n./output/connection\n-\nname\n:\n/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\n: Tells the kafka-extractor tool to run.\nenvironment\n: Contains all parameters for the tool.\nKAFKA_FLAVOUR\n: Defines the Kafka distribution being used. Use\nCONFLUENT_CLOUD\nwhen working with Confluent Cloud.\nCONFLUENT_AUTH\n: Configures authentication using a Cloud API key and secret.\nReplace\ncloud_api_key\nwith the Cloud API key retrieved during setup. For more information, see\nHow to set up Confluent Kafka Cloud API Key\n.\nReplace\ncloud_api_secret\nwith the corresponding secret for your Cloud API key. For more information, see\nHow to set up Confluent Kafka Cloud API Key\n.\nCONFLUENT_CLUSTER_ID\n: The unique ID of your Kafka cluster. You can find this in the cluster overview page of the Confluent Cloud console. The cluster ID follows the\nlkc-xxxx\nformat.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Confluent Kafka connections as you want.\nDefine services for Aiven Kafka\nâ\nFor each Aiven Kafka instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\n# Example Aiven Kafka connection\nconnection-name\n:\n<<\n:\n*extract\nsecrets\n:\n-\nkafka_client_config\n-\nkafka_ca_cert\n# Uncomment the following lines if you are using Aiven Kafka with Client Certificate Authentication\n#      - kafka_access_cert\n#      - kafka_access_key\nenvironment\n:\n<<\n:\n*kafka-defaults\n# Kafka bootstrap servers (semicolon-separated)\nKAFKA_BOOTSTRAP_SERVERS\n:\n\"localhost:9092\"\n# Skip topics that are internal to Kafka (e.g. __consumer_offsets)\nSKIP_INTERNAL_TOPICS\n:\n\"true\"\nvolumes\n:\n# You can change './output/connection-name' to any output location you want\n-\n./output/connection\n-\nname\n:\n/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\ntells the kafka-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Aiven Kafka connections as you want.\nDefine services for Redpanda Kafka\nâ\nFor each Redpanda Kafka instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\n# Example Redpanda Kafka connection\nconnection-name\n:\n<<\n:\n*extract\nenvironment\n:\n<<\n:\n*kafka-defaults\n# Kafka bootstrap servers (semicolon-separated)\nKAFKA_BOOTSTRAP_SERVERS\n:\n\"localhost:9092\"\n# Skip topics that are internal to Kafka (e.g. __consumer_offsets)\nSKIP_INTERNAL_TOPICS\n:\n\"true\"\nvolumes\n:\n# You can change './output/connection-name' to any output location you want\n-\n./output/connection\n-\nname\n:\n/output\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\ntells the kafka-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Redpanda Kafka connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your Kafka connections, you will need to provide a Kafka client configuration file. For managed Kafka instances such as\nConfluent Cloud\nand\nAiven\n, this configuration can be copied directly from the console.Â\nHere is an example that would be compatible with all Kafka variants: Apache Kafka, Confluent Cloud, and Aiven Kafka. This is just an example, your cluster configuration may vary:\n# Required connection configs for Kafka producer, consumer, and admin\n# If SSL enabled, use SASL_SSL, otherwise use SASL_PLAINTEXT (when using with basic auth)\nsecurity.protocol=SASL_SSL\n# If basic auth is enabled\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='{{ USERNAME or CLUSTER_API_KEY }}' password='{{ PASSWORD or CLUSTER_API_SECRET }}';\nsasl.mechanism=PLAIN\n# Best practice for higher availability in Apache Kafka clients prior to 3.0\nsession.timeout.ms=45000\nRedpanda Kafka only supports the\nSCRAM authentication method\n. Here is an example configuration:\nsasl.mechanism=<SCRAM\n-\nSHA\n-\n256 or SCRAM\n-\nSHA\n-\n512 depending on your config\n>\nsecurity.protocol=SASL_SSL\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"<username\n>\n\" password=\"<password\n>\n\";\n# Best practice for higher availability in Apache Kafka clients prior to 3.0\nsession.timeout.ms=45000\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Kafka credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets\n:\nkafka_client_config\n:\n# Change it to the actual location of your kafka config file (MANDATORY)\nfile\n:\n./kafka.client.config\nkafka_ca_cert\n:\n# Change it to the actual location of your kafka CA cert file (OPTIONAL - only use if using custom CA)\nfile\n:\n./ca.pem\nkafka_access_cert\n:\n# Change it to the actual location of your kafka access cert file (OPTIONAL - only use if using Client Certificate auth)\nfile\n:\n./service.cert\nkafka_access_key\n:\n# Change it to the actual location of your kafka access key file (OPTIONAL - only use if using Client Certificate auth)\nfile\n:\n./service.key\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a sub-section of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the Kafka configuration file:\nsudo docker secret create kafka_client_config path/to/kafka.client.config\n# Optional\nsudo docker secret create kafka_ca_cert path/to/ca.pem\nsudo docker secret create kafka_access_cert path/to/service.cert\nsudo docker secret create kafka_access_key path/to/service.key\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets\n:\nkafka_client_config\n:\nexternal\n:\ntrue\nname\n:\nkafka_client_config\nkafka_ca_cert\n:\nexternal\n:\ntrue\nname\n:\nkafka_ca_cert\nkafka_access_cert\n:\nexternal\n:\ntrue\nname\n:\nkafka_access_cert\nkafka_access_key\n:\nexternal\n:\ntrue\nname\n:\nkafka_access_key\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local Kafka configuration file.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets\n:\nkafka_client_config\n:\nexternal\n:\ntrue\nname\n:\nkafka_client_config\nx-templates\n:\n# ...\nservices\n:\n# Example Apache Kafka connection\napache-kafka-example\n:\n<<\n:\n*extract\nenvironment\n:\n<<\n:\n*kafka-defaults\n# Kafka bootstrap servers (semicolon-separated)\nKAFKA_BOOTSTRAP_SERVERS\n:\n\"localhost:9092\"\n# Skip topics that are internal to Kafka (e.g. __consumer_offsets)\nSKIP_INTERNAL_TOPICS\n:\n\"true\"\nvolumes\n:\n# You can change './output/apache-kafka-example' to any output location you want\n-\n./output/apache\n-\nkafka\n-\nexample\n:\n/output\nIn this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The\nkafka_client_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\napache-kafka-example\n. You can use any meaningful name you want.\nThe\n<<: *kafka-defaults\nsets the connection type to Kafka.\nKAFKA_BOOTSTRAP_SERVERS\ntells the extractor about the Kafka hosts or brokers.\nSKIP_INTERNAL_TOPICS\ntells the extractor whether to extract internal topics or skip them.\nTheÂ\n./output/apache-kafka-example:/output\nÂ line tells the extractor where to store results. In this example, the extractor will store results in theÂ\n./output/apache-kafka-example\nÂ directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\ndata\ncrawl\nPrevious\nCrawl Redpanda Kafka\nNext\nCrawl on-premises Kafka\nPrerequisites\nGet the compose file\nDefine Kafka connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nGet Started\nSet up on-premises Looker access\nOn this page\nSet up on-premises Looker access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Looker access details, including credentials.\nIn some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your on-premises Looker instance you will need to use Atlan's looker-extractor tool.\nDid you know?\nAtlan uses exactly the same looker-extractor behind the scenes when it connects to Looker in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the looker-extractor tool\nâ\nTo get the looker-extractor tool:\nRaise a support ticket\nto get a link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to crawl Looker:\nsudo docker load -i /path/to/looker-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a configuration file for the looker-extractor tool. This is a\nDocker compose file\n.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises databases.\nThe file is\ndocker-compose.yaml\n.\nDefine Looker connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your Looker connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises Looker instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nCONNECTION-NAME:\n<<: *extract\nenvironment:\n<<: *looker-defaults\nINCLUDE_PROJECTS: \"project1,project2\"\nUSE_FIELD_LEVEL_LINEAGE: \"true\"\nvolumes:\n- ./output/looker-example:/output/process\nReplace\nCONNECTION-NAME\nwith the name of your connection.\n<<: *extract\ntells the looker-extractor tool to run.\nenvironment\ncontains all parameters for the tool. Replaces the values given for\nINCLUDE_PROJECTS\nwith the names of your own Looker projects you want to extract. Separate each project name by a comma.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/looker-example\nfolder on the local file system.\nYou can add as many Looker connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your Looker connections you will need to provide:\nA\nLooker SDK configuration\nfile\nA private key to access your git repository via ssh (to extract field-level lineage)\nA passphrase to decipher the private key (to extract field-level lineage)\nThe Looker metadata includes the git repo locations.\nThe Looker SDK configuration is a\n.ini\nfile with the following format:\n[Looker]\n# Base URL for your looker instance API. Do not include /api/* in the URL.\nbase_url=https://<host>:<port>\n# API 3 client id\nclient_id=YourClientID\n# API 3 client secret\nclient_secret=YourClientSecret\nverify_ssl=True\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Looker credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets:\nlooker_config:\nfile: ./looker.ini\nlooker_git_private_key:\nfile: ./id_ed25519\nlooker_git_private_key_passphrase:\nfile: ./passphrase.txt\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a sub-section of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the Looker SDK configuration file:\nsudo docker secret create looker_config path/to/looker.ini\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\nlooker_config:\nexternal: true\nname: looker_config\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local Looker SDK configuration file.\ninfo\nðª\nDid you know?\nYou can use the same steps to create Docker secrets for your git details, as well. Replace the name (\nlooker_config\n) and path to the file, but otherwise run the same command.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\nlooker_config:\nexternal: true\nname: looker_config\nlooker_git_private_key:\nfile: ./id_ed25519\nlooker_git_private_key_passphrase:\nexternal: true\nname: looker_git_private_key_passphrase\nx-templates:\n# ...\nservices:\nmy-looker:\n<<: *extract\nenvironment:\n<<: *looker-defaults\nINCLUDE_PROJECTS: \"project1,project2\"\nUSE_FIELD_LEVEL_LINEAGE: \"true\"\nvolumes:\n- ./output/looker-example:/output/process\nsecrets:\n- looker_config\n- looker_git_private_key\n- looker_git_private_key_passphrase\nvolumes:\njars:\nIn this example we've defined the secrets at the top of the file (you could also define them at the bottom):\nlooker_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nlooker_git_private_key\nrefers to a local file.\nlooker_git_private_key_passphrase\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\nmy-looker\n. You can use any meaningful name you want.\nThe\n<<: *looker-defaults\nsets the connection type to Looker.\nINCLUDE_PROJECTS\ntells the extractor to only extract\nproject1\nand\nproject2\nfrom Looker.\nUSE_FIELD_LEVEL_LINEAGE\ntells the extractor to extract field-level lineage. This means the git private key information is also required.\nThe\n./output/looker-example:/output/process\nline tells the extractor where to store results. In this example, the extractor will store results in the\n./output/looker-example\ndirectory on the local file system. We recommend you output metadata for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\ndata\ncrawl\nPrevious\nSet up Looker\nNext\nCrawl Looker\nPrerequisites\nGet the compose file\nDefine Looker connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nGet Started\nSet up on-premises Tableau access\nOn this page\nSet up on-premises Tableau access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your Tableau instance details, including credentials.\nIn some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your on-premises Tableau instance, you will need to use Atlan's tableau-extractor tool.\nDid you know?\nAtlan uses exactly the same tableau-extractor behind the scenes when it connects to Tableau in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the tableau-extractor tool\nâ\nTo get the tableau-extractor tool:\nRaise a support ticket\nto get the link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to crawl Tableau:\nsudo docker load -i /path/to/tableau-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a\nDocker compose file\nfor the tableau-extractor tool.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises Tableau instance.\nThe file is\ndocker-compose.yaml\n.\nDefine Tableau connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your Tableau connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises Tableau instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nconnection-name:\n<<: *extract\nenvironment:\n<<: *tableau-defaults\nEXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\"\nCRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\"\nCERT_PATH: \"\"\nvolumes:\n- ./output/connection-name:/output/process\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\ntells the tableau-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nCERT_PATH\n-  if applicable, specify the\nSSL certificate\npath and store it as a new volume.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name\nfolder on the local file system.\nYou can add as many Tableau connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your Tableau connections, you will need to provide a Tableau configuration file.\nThe Tableau configuration is a\n.ini\nfile with the following format:\n[TableauConfig]\n# Tableau instance URL. Do not include /api/* in the URL.\nserver_url=https://:<hostname>:<port>\n# Tableau site name. Leaving this empty will select the default site.\nsite_name=YourTableauSite\n# Tableau authentication type. Options: basic, personal_access_token.\nauth_type=basic\n# Required only if auth_type is basic.\n[BasicAuth]\nusername=YourTableauUsername\npassword=YourTableauPassword\n# Required only if auth_type is personal_access_token.\n[PersonalAccessTokenAuth]\ntoken_name=YourTableauTokenName\ntoken_value=YourTableauTokenValue\ndanger\nFor basic authentication, ensure that your password does not contain the special character\n%\n. If the percent sign is included in your password, add another\n%\nto escape it.\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep Tableau credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets:\ntableau_config:\nfile: ./tableau.ini\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a sub-section of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the Tableau configuration file:\nsudo docker secret create tableau_config path/to/tableau.ini\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\ntableau_config:\nexternal: true\nname: tableau_config\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local Tableau configuration file.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\ntableau_config:\nexternal: true\nname: tableau_config\nx-templates:\n# ...\nservices:\nmy-tableau:\n<<: *extract\nenvironment:\n<<: *tableau-defaults\nEXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\"\nCRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\"\nCERT_PATH: \"/tmp/tab-cert.pem\"\nvolumes:\n- ./output/my-tableau:/output/process\n- ./tab-cert.pem:/tmp/tab-cert.pem\nsecrets:\n- tableau_config\nIn this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The\ntableau_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\nmy-tableau\n. You can use any meaningful name you want.\nThe\n<<: *tableau-defaults\nsets the connection type to Tableau.\nEXCLUDE_PROJECTS_REGEX\ntells the extractor to filter out all the projects whose names match the\nTest1.*\nand\nTest2.*\nregex patterns in the extracted metadata.\nCRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS\ntells the extractor to include all hidden or unpublished worksheets and dashboards that are part of a Tableau workbook in the extracted metadata.\nCRAWL_EMBEDDED_DASHBOARDS\ntells the extractor to create relationships between Tableau dashboards used within another dashboard as a\nWeb Page\nitem.\nThe\nCERT_PATH\ntells the extractor where to store the\nSSL certificate\n, if applicable. In this example, the extractor will store results in the\n./tab-cert.pem\ndirectory on the local file system. If the SSL certificate is not stored in the same folder as the compose file, you will need to specify the full path.\nThe\n./output/my-tableau:/output/process\nline tells the extractor where to store results. In this example, the extractor will store results in the\n./output/my-tableau\ndirectory on the local file system. We recommend you output the extracted metadata for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\ndata\ncrawl\nPrevious\nSet up Tableau\nNext\nSet up a private network link to Tableau server\nPrerequisites\nGet the compose file\nDefine Tableau connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nGet Started\nSet up on-premises ThoughtSpot access\nOn this page\nSet up on-premises ThoughtSpot access\nWho can do this?\nYou will need access to a machine that can run Docker on-premises. You will also need your ThoughtSpot instance details, including credentials.\nIn some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nPrerequisites\nâ\nTo extract metadata from your on-premises ThoughtSpot instance, you will need to use Atlan's thoughtspot-extractor tool.\nDid you know?\nAtlan uses exactly the same thoughtspot-extractor behind the scenes when it connects to ThoughtSpot in the cloud.\nInstall Docker Compose\nâ\nDocker Compose\nis a tool for defining and running applications composed of many\nDocker\ncontainers. (Any guesses where the name came from? ð)\nTo install Docker Compose:\nInstall Docker\nInstall Docker Compose\nDid you know?\nInstructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the\nGet started with Docker Compose\ntutorial if you want to learn Docker Compose basics first.\nGet the thoughtspot-extractor tool\nâ\nTo get the thoughtspot-extractor tool:\nRaise a support ticket\nto get the link to the latest version.\nDownload the image using the link provided by support.\nLoad the image to the server you'll use to crawl ThoughtSpot:\nsudo docker load -i /path/to/thoughtspot-extractor-master.tar\nGet the compose file\nâ\nAtlan provides you with a\nDocker compose file\nfor the thoughtspot-extractor tool.\nTo get the compose file:\nDownload the\nlatest compose file\n.\nSave the file to an empty directory on the server you'll use to access your on-premises ThoughtSpot instance.\nThe file is\ndocker-compose.yaml\n.\nDefine ThoughtSpot connections\nâ\nThe structure of the compose file includes three main sections:\nx-templates\ncontains configuration fragments. You should ignore this section   -  do not make any changes to it.\nservices\nis where you will define your ThoughtSpot connections.\nvolumes\ncontains mount information. You should ignore this section as well   -  do not make any changes to it.\nDefine services\nâ\nFor each on-premises ThoughtSpot instance, define an entry under\nservices\nin the compose file.\nEach entry will have the following structure:\nservices:\nconnection-name:\n<<: *extract\nenvironment:\n<<: *thoughtspot-defaults\nEXCLUDE_TAGS_REGEX: \"Test1.*|Test2.*\"\nWITHOUT_TAGS: \"true\"\nvolumes:\n- ./output/connection-name/filter:/output/filter\nReplace\nconnection-name\nwith the name of your connection.\n<<: *extract\ntells the thoughtspot-extractor tool to run.\nenvironment\ncontains all parameters for the tool.\nEXCLUDE_TAGS_REGEX\n-  specify a regular expression to exclude ThoughtSpot assets based on ThoughtSpot tags.\nWITHOUT_TAGS\n-  specify a Boolean configuration to determine whether to crawl ThoughtSpot assets without any ThoughtSpot tags.\nvolumes\nspecifies where to store results. In this example, the extractor will store results in the\n./output/connection-name/filter\nfolder on the local file system.\nYou can add as many ThoughtSpot connections as you want.\nDid you know?\nDocker's documentation\ndescribes the\nservices\nformat in more detail.\nProvide credentials\nâ\nTo define the credentials for your ThoughtSpot connections, you will need to provide a ThoughtSpot configuration file.\nThe ThoughtSpot configuration is a\n.ini\nfile with the following format:\n[ThoughtSpotConfig]\nhost=atlan.thoughtspot.cloud\nport=443\nauth_type=basic_auth; This will use BasicAuth;\nauth_type=trusted_auth; This will use TruestedAuth;\nauth_type=oauth_access_token; This will use OAuth;\n[BasicAuth]\nusername={{username}}\npassword={{password}}\n[TrustedAuth]\nusername={{username}}\nsecret_key={{secret_key}}\n[OAuth]\ntoken={{oauth_access_token}}\n[ExtractionConfig]\noffset=1\nlimit=10\nSecure credentials\nâ\nUsing local files\nâ\ndanger\nIf you decide to keep ThoughtSpot credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use\nDocker secrets\nto store the sensitive passwords.\nTo specify the local files in your compose file:\nsecrets:\nthoughtspot_config:\nfile: ./thoughtspot.ini\ndanger\nThis\nsecrets\nsection is at the same top-level as the\nservices\nsection described earlier. It is not a sub-section of the\nservices\nsection.\nUsing Docker secrets\nâ\nTo create and use Docker secrets:\nStore the ThoughtSpot configuration file:\nsudo docker secret create thoughtspot_config path/to/thoughtspot.ini\nAt the top of your compose file, add a\nsecrets\nelement to access your secret:\nsecrets:\nthoughtspot_config:\nexternal: true\nname: thoughtspot_config\nThe\nname\nshould be the same one you used in the\ndocker secret create\ncommand above.\nOnce stored as a Docker secret, you can remove the local ThoughtSpot configuration file.\nWithin the\nservice\nsection of the compose file, add a new secrets element and specify the name of the secret within your service to use it.\nExample\nâ\nLet's explain in detail with an example:\nsecrets:\nthoughtspot_config:\nexternal: true\nname: thoughtspot_config\nx-templates:\n# ...\nservices:\nthoughtspot-example:\n<<: *extract\nenvironment:\n<<: *thoughtspot-defaults\nEXCLUDE_TAGS_REGEX: \"Test1.*|Test2.*\"\nWITHOUT_TAGS: \"true\"\nvolumes:\n- ./output/connection-name/filter:/output/filter\nIn this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The\nthoughtspot_config\nrefers to an external Docker secret created using the\ndocker secret create\ncommand.\nThe name of this service is\nthoughtspot-example\n. You can use any meaningful name you want.\nThe\n<<: *thoughtspot-defaults\nsets the connection type to ThoughtSpot.\nThe\n./output/thoughtspot_example/filter:/output/filter\nline tells the extractor where to store results. In this example, the extractor will store results in the\n./output/thoughtspot_example/filter\ndirectory on the local file system. We recommend you output the extracted metadata for different connections in separate directories.\nThe\nsecrets\nsection within\nservices\ntells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file.\nTags:\ndata\ncrawl\nPrevious\nSet up ThoughtSpot\nNext\nCrawl ThoughtSpot\nPrerequisites\nGet the compose file\nDefine ThoughtSpot connections\nProvide credentials\nSecure credentials\nExample"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/how-tos/set-up-oracle",
    "content": "Connect data\nDatabases\nSQL Databases\nOracle\nGet Started\nSet up Oracle\nOn this page\nSet up Oracle\nWho can do this?\nYou need your Oracle database administrator or a similar role to run these commands  - you may not have access yourself.\nAtlan supports the basic authentication method for fetching metadata from Oracle. This method uses a username and password to fetch metadata.\nCreate user in Oracle\nâ\nTo create a username and password for basic authentication for Oracle, run the following commands:\nCREATE USER <username> IDENTIFIED BY <password>;\nGRANT CREATE SESSION TO <username>;\nReplace\n<username>\nwith the username you want to create.\nReplace\n<password>\nwith the password to use for that username.\nGrant permissions\nâ\nAtlan requires specific privileges to crawl assets and fetch technical metadata from Oracle.\nGrant permissions for metadata extraction\nâ\nRun the following commands to grant permissions for metadata extraction:\nGRANT SELECT_CATALOG_ROLE TO <username>;\nGRANT SELECT ON DBA_TABLES TO <username>;\nGRANT SELECT ON DBA_VIEWS TO <username>;\nGRANT SELECT ON DBA_TAB_COLUMNS TO <username>;\nGRANT SELECT ON DBA_SYNONYMS TO <username>;\nReplace\n<username>\nwith the username you created.\nIf these permissions arenât sufficient in your environment, use the optional approach below. Before proceeding, revoke the previously granted DBA permissions.\n(Optional) Grant permissions to query and preview data\nâ\nGrant permissions on specific tables\nâ\nTo grant permissions to query and preview data for specific tables, run the following command for each table you want to provide access to.\nGRANT SELECT ON <schema_name>.<table_name> TO <username>;\nReplace\n<schema_name>\nwith the name of the schema you want to crawl.\nReplace\n<table_name>\nwith the name of the table (or view) you want to crawl.\nReplace\n<username>\nwith the username you created.\nGrant permissions on any table\nâ\nTo grant permissions on specific tables, run the following command for each table you want to provide access to.\nGRANT SELECT ANY TABLE TO <username>;\nReplace\n<username>\nwith the username you created.\nThis\npermission\nallows the new user to query tables or views in any schema except\nSYS\nand\nAUDSYS\n.\ndanger\nOracle\nrecommends\ngranting\nANY\nprivileges only to trusted users.\nThese permissions allow you to crawl metadata, preview data, and run queries in Atlan, depending on the privileges granted.\nTags:\ndata\ncrawl\nauthentication\nPrevious\nOracle\nNext\nCrawl Oracle\nCreate user in Oracle\nGrant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-postgresql",
    "content": "Connect data\nDatabases\nSQL Databases\nPostgreSQL\nGet Started\nSet up PostgreSQL\nOn this page\nSet up PostgreSQL\nWho can do this?\nYou will probably need your PostgreSQL administrator to run these commands   -  you may not have access yourself.\nCreate a database role\nâ\nTo configure a database role for PostgreSQL, run the following commands:\nCREATE\nrole atlan_user_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\n<\nschema\n>\nTO\natlan_user_role\n;\nReplace\n<schema>\nwith the schema to which the user should have access.\ndanger\nYou (or your administrator) will need to run these statements for each database and schema you want to crawl.\nAtlan requires the following privileges:\nUSAGE\n:\nAccess a schema and fetch metadata. By default, users cannot access any objects in schemas that they do not own. The owner of a schema must grant the\nUSAGE\nprivilege on the schema to allow access.\nFetch the technical metadata persisted in the\nINFORMATION_SCHEMA\n.\nThese permissions enables Atlan to crawl metadat from PostgreSQL.\n(Optional) Grant permissions to query and preview data\nâ\nTo grant permissions to query data and preview sample data:\nGRANT\nSELECT\n,\nREFERENCES\nON\nALL\nTABLES\nIN\nSCHEMA\nschema_name\nTO\natlan_user_role\n;\nReplace\nschema_name\n: Name of the schema you want Atlan to access.\nReplace\natlan_user_role\n: Role assigned to Atlan in your database.\nThe\nSELECT\nprivilege is required to preview and query data from within Atlan.\nChoose authentication mechanism\nâ\nAtlan currently supports the following authentication mechanisms. You will need to choose one and configure it according to the steps below.\nBasic authentication\nIdentity and Access Management (IAM) authentication\nBasic authentication\nâ\nTo create a username and password for basic authentication for PostgreSQL run the following commands:\nCREATE\nUSER\natlan_user password\n'<pass>'\n;\nGRANT\natlan_user_role\nTO\natlan_user\n;\nReplace\n<pass>\nwith the password for the\natlan_user\nuser you are creating.\nIdentity and Access Management (IAM) authentication\nâ\nTo configure IAM authentication for PostgreSQL follow each of these steps.\nEnable IAM authentication\nâ\nTo enable IAM authentication for your database instance follow\nthe steps in the Amazon RDS documentation\n.\nWhen given the option, apply the changes immediately and wait until they are complete.\nCreate database user\nâ\nTo create a database user with the necessary permissions run the following commands:\nConnect to the database:\npsql -h {{endpoint}} -U {{username}} -d {{database}}\nReplace\n{{endpoint}}\nwith the database or cluster endpoint.\nReplace\n{{username}}\nwith the master username (admin account) for the database.\nReplace\n{{database}}\nwith the name of the database.\nCreate a database user:\nCREATE\nUSER\n{{db\n-\nusername}}\nWITH\nLOGIN\n;\nGRANT\natlan_user_role\n,\nrds_iam\nTO\n{{db\n-\nusername}}\n;\nReplace\n{{db-username}}\nwith the name for the database user to create.\nCreate IAM policy\nâ\nTo create an IAM policy with the necessary permissions follow\nthe steps in the AWS Identity and Access Management User Guide\n.\nCreate the policy using the following JSON:\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Action\"\n:\n[\n\"rds-db:connect\"\n]\n,\n\"Resource\"\n:\n[\n\"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\"\n]\n}\n]\n}\nReplace\n{{aws-region}}\nwith the AWS region of your database instance.\nReplace\n{{account-id}}\nwith your account ID.\nReplace\n{{resource-id}}\nwith the resource ID.\nReplace\n{{db-username}}\nwith the username created in the previous step.\nAttach IAM policy\nâ\nTo attach the IAM policy for Atlan's use, you have two options:\nIAM role\n: Create a new role in your AWS account and attach the policy to this role. To create an AWS IAM role:\nFollow\nthe steps in the AWS Identity and Access Management User Guide\n.\nWhen prompted for policies, attach the policy created in the previous step to this role.\nRaise a support ticket\nto provide the AWS IAM role ARN to Atlan and get the ARN of the\nNode Instance Role\nfor your Atlan EKS cluster from Atlan.\nWhen prompted, create a trust relationship for the role using the following trust policy. (Replace\n<atlan_nodeinstance_role_arn>\nwith the ARN received from Atlan support.)\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n:\n[\n{\n\"Effect\"\n:\n\"Allow\"\n,\n\"Principal\"\n:\n{\n\"AWS\"\n:\n\"<atlan_nodeinstance_role_arn>\"\n}\n,\n\"Action\"\n:\n\"sts:AssumeRole\"\n,\n}\n]\n}\nIAM user\n: Create an AWS IAM user and attach the policy to this user. To create an AWS IAM user:\nFollow\nthe steps in the AWS Identity and Access Management User Guide\n.\nOn the\nSet permissions\npage, attach the policy created in the previous step to this user.\nOnce the user is created, view or download the user'sÂ\naccess key ID\nandÂ\nsecret access key\n.\ndanger\nThis will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen.\nTags:\ndata\ncrawl\nPrevious\nPostgreSQL\nNext\nCrawl PostgreSQL\nCreate a database role\n(Optional) Grant permissions to query and preview data\nChoose authentication mechanism\nBasic authentication\nIdentity and Access Management (IAM) authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/how-tos/set-up-redash",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nRedash\nGet Started\nSet up Redash\nOn this page\nSet up Redash\nWho can do this?\nYou will probably need your Redash administrator to complete the following steps   -  you may not have access yourself.\nAtlan supports the API authentication method for fetching metadata from Redash. This method uses an API key to fetch metadata.\nCreate user in Redash\nâ\nTo\ncreate a new user\nfor Atlan to use when integrating with Redash:\nLog in to your Redash instance.\nIn the top right of your Redash instance, click your profile name, and from the dropdown, click\nUsers\n.\nOn the\nSettings\npage, under the\nUsers\ntab, click the\nNew User\nbutton.\nIn the\nCreate a New User\ndialog, enter the following details:\nFor\nName\n, add a meaningful name for the new user   -  for example,\nAtlan\n.\nFor\nEmail address\n, enter the email address for the new user.\nClick\nCreate\nto create the new user.\nConfigure new user\nâ\nOnce the new user has accepted the invitation, the new user will be added to the list of users in your Redash instance. You will need to configure the new user for integration with Atlan.\nTo configure the new user for\ncrawling Redash\n:\nLog in to your Redash instance.\nIn the top right of your Redash instance, click your profile name, and from the dropdown, click\nUsers\n.\nOn the\nSettings\npage, under the\nUsers\ntab, select the\nnew user you created\n.\nFrom the new user screen, complete the following steps:\nEach new user is added to the\nDefault\ngroup automatically in Redash. To configure\ngroup permissions\n, for\nGroups\n, click the dropdown and select\nAdmin\nto add the new user to the admin group for\nfull access\n.\nFor\nAPI Key\n, click the clipboard icon to copy the API key for the new user and save it in a secure location.\nTags:\ndata\napi\nauthentication\nPrevious\nRedash\nNext\nCrawl Redash\nCreate user in Redash\nConfigure new user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana",
    "content": "Connect data\nDatabases\nSQL Databases\nSAP HANA\nGet Started\nSet up SAP HANA\nOn this page\nSet up SAP HANA\nWho can do this?\nYou will probably need your SAP HANA administrator to run these commands   -  you may not have access yourself.\nDid you know?\nThis connector supports both SAP HANA on-premise as well as\nSAP HANA Cloud\nand\nSAP HANA Platform\ndatabase deployments.\nAtlan currently only supports basic username and password authentication for fetching metadata from SAP HANA. Complete the following steps to configure it:\nCreate a database user\nâ\nCreate a database user with the following commands:\nCREATE\nUSER\n<\nusername\n>\nPASSWORD\n<\npassword\n>\nNO\nFORCE_FIRST_PASSWORD_CHANGE\n;\nReplace\n<username>\nwith the username you want to create.\nReplace\n<password>\nwith the password for that username.\nGrant read permission on schema\nâ\nGrant read permission on schema with the following commands.\nTo crawl metadata as well as preview and query data in Atlan:\nGRANT\nSELECT\n,\nSELECT\nMETADATA\nON\nSCHEMA\n<\nschema\n>\nTO\n<\nusername\n>\n;\nTo only crawl metadata in Atlan:\nGRANT\nSELECT\nMETADATA\nON\nSCHEMA\n<\nschema\n>\nTO\n<\nusername\n>\n;\nReplace\n<schema>\nwith the name of the schema you want to crawl.\nTo crawl\ncalculation views\nin Atlan:\nGRANT\nSELECT\nON\n_SYS_REPO\n.\nACTIVE_OBJECT\nTO\n<\nusername\n>\n;\nGRANT\nSELECT\nON\n_SYS_BI\n.\nBIMC_PROPERTIES\nTO\n<\nusername\n>\n;\ndanger\nYour SAP HANA administrator will need to run these statements for each schema you want to crawl.\nTags:\ndata\ncrawl\nauthentication\nPrevious\nSAP HANA\nNext\nCrawl SAP HANA\nCreate a database user\nGrant read permission on schema"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/how-tos/set-up-soda",
    "content": "Connect data\nData Quality & Observability\nSoda\nGet Started\nSet up Soda\nOn this page\nSet up Soda\nWho can do this?\nYou will need your\nSoda Cloud administrator\nto complete these steps   -  you may not have access yourself. You will also need to scan your datasets using the latest version of\nSoda Library\nor\nmigrate from Soda Core to Soda Library\nto ensure the best possible experience in Atlan. Associated checks for datasets scanned using an older version of Soda Library may be unavailable or missing the relationship with datasets in Atlan.\nAtlan supports the API authentication method for fetching metadata from Soda. This method uses an API key ID and API secret to fetch metadata.\nCreate an API key\nâ\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the objects in your Soda instance.\nYou will need to\ncreate an API key in Soda\nfor integration with Atlan.\nTo create an API key for\ncrawling Soda\n:\nLog in to your Soda Cloud instance as an\nAdmin\n.\nIn the top right of your Soda Cloud account, click on your avatar, and from the dropdown, click\nProfile\n.\nUnder your profile name, click the\nAPI Keys\ntab.\nOn the\nAPI Keys\npage, click the\n+\nbutton to generate a new API key.\nIn the\nAPI Keys\ndialog, enter the following details:\nFor\nDescription\n, enter a meaningful description.\nFor\nOrganization\n, enter the name of your organization.\nClick\nCreate\nto finish setup.\nFrom the corresponding screen, copy the\nAPI Key ID\nand\nAPI Key Secret\nand store them in a secure location.\ndanger\nThe API secret cannot be retrieved later.\nTags:\ndata\napi\nauthentication\nPrevious\nSoda\nNext\nCrawl Soda\nCreate an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-tableau",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nGet Started\nSet up Tableau\nOn this page\nSet up Tableau\nWho can do this?\nYou will probably need your Tableau administrator to run these commands   -  you may not have access yourself.\nEnable the Tableau Metadata API\nâ\nTo enable the Tableau Metadata API, follow the\nsteps in Tableau documentation\n.\ndanger\nAtlan needs the Tableau Metadata API to crawl metadata. Please ensure you are running the latest version of Tableau Server or Tableau Online (2022.x with REST API version 3.14+). Learn more about the\npermissions\nused to access metadata through the Tableau Metadata API.\nPublish the worksheets you want to crawl\nâ\nEnsure you publish the worksheets in Tableau that you want to crawl in Atlan.\nTo publish Tableau worksheets, follow the\nsteps in Tableau documentation\n.\nChoose authentication mechanism\nâ\nAtlan supports the following authentication methods for fetching metadata from Tableau:\nBasic\n-  this method uses a username and password.\nPersonal access token\n-  this method uses a personal access token.\nJWT bearer\n-  this method uses a username and JWT client ID, secret ID, and secret value.\nBasic authentication\nâ\nDid you know?\nTo crawl assets and extract asset lineage from Tableau, the user must have the\nSite Administrator Explorer\nrole\n. Atlan requires the\nSite Administrator Explorer\nrole in Tableau to extract data source fields and calculated fields and create field-level assets and lineage. It is not possible to fetch either with the\nViewer\nrole in the current version of the Tableau Metadata API.\nAdd a user\nâ\nEnsure you add a user with the role\nSite Administrator Explorer\nto the site you want to crawl.\nTo add such a user, follow the\nsteps in Tableau documentation\n.\nGrant user permissions\nâ\nEnsure you grant the\nView\ncapability\nfor all the assets you want to crawl.\nTo grant the permission, follow the\nsteps in Tableau documentation\n.\nPersonal access token authentication\nâ\nIf you want to access Tableau using an access token, you can generate a personal access token.\nTo generate a personal access token, follow the\nsteps in Tableau documentation\n.\nJWT bearer authentication\nâ\ndanger\nTo access the\nTableau Metadata API using JWT bearer authentication\n, you must have Tableau Cloud October 2023 or Tableau Server 2023.3 version. In addition, JWT authorization currently does not support all\nREST API capabilities\n. Due to these limitations at source, Atlan will not be able to crawl\nTableau flows\nif you use the JWT bearer authentication method.\nConfigure a connected app\nâ\nIf you want to access Tableau using a JSON web token (JWT), you can configure a Tableau connected app. There are two types of connected apps that you can configure   -  direct trust or OAuth 2.0 trust.\nTo authenticate the Tableau connection in Atlan using this method, you will need the following:\nUsername   -  your Tableau Server username or Tableau Online email address, the user must have a\nSite Administrator Explorer\nrole\nConnected app ID   -  client ID generated for the connected app\nSecret ID   -  secret ID linked to the client ID of the connected app\nSecret value   -  secret value used to sign the token\nTo configure a connected app, follow the steps in Tableau documentation:\nDirect trust\nOAuth 2.0 trust\nAccess scopes for connected apps\nâ\nFor JWT authorization,\nscopes\ndefine access permissions granted to the token holder. Scopes control the specific actions that an application or user can perform in Tableau while accessing content through a connected app.\nThe Tableau connector in Atlan uses two\nread\nscopes to extract metadata from Tableau. Note that the Tableau connector is preconfigured to use these scopes, no action required.\nAtlan uses the following scopes for JWT authentication:\ntableau:content:read\n-  allows read access to your assets in Tableau, including:\nWorkbooks   -  can list, access, and retrieve metadata for workbooks.\nViews   -  can fetch specific views or dashboards within workbooks.\nData sources   -  can access published data sources and associated metadata.\nProjects   -  can retrieve project metadata.\nMetrics   -  can read metrics associated with workbooks or dashboards.\nTables and databases   -  can access metadata for tables and databases connected to Tableau.\ntableau:users:read\n-  allows read access to user details. This enables Atlan to display the source owner property for supported Tableau assets, including in the\nimpact analysis report\n.\nOptional)\ntableau:workbooks:download\nâ allows downloading a workbook (\n.twb\nor\n.twbx\n), enabling Atlan to display relationships for embedded Tableau dashboards.\nTags:\ndata\ncrawl\napi\nPrevious\nTableau\nNext\nSet up on-premises Tableau access\nEnable the Tableau Metadata API\nPublish the worksheets you want to crawl\nChoose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/references/integration-with-pingfederate-using-saml",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nReferences\nSSO integration with PingFederate using SAML\nSSO integration with PingFederate using SAML\nIf you have PingFederate as your IdP and are trying to integrate the SAML-based IdP using the metadata supplied from the page, you can use the following SAML assertion URL:\nFor identity provider initiated (IdP-initiated) SSO:\nhttps://{{instance}}/auth/realms/default/broker/{{alias}}/endpoint/clients/atlan-saml\nFor service provider initiated (SP-initiated) SSO:\nhttps://{{instance}}/auth/realms/default/broker/{{alias}}/endpoint\nTo use both IdP- and SP-initiated SSO, add both the URLs mentioned above.\nIf you encounter an\nInvalid signature\nerror, you must ensure that the certificate in the XML metadata file is of the SHA-256 or SHA-512 type.\nTags:\ndata\nintegration\nPrevious\nSet default user roles for SSO\nNext\nTroubleshooting SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/troubleshooting/troubleshooting-data-models",
    "content": "Configure Atlan\nData Models\nTroubleshooting\nTroubleshooting data models\nOn this page\nTroubleshooting data models\nWhat are the known limitations of data models in Atlan?\nâ\nFollowing are the known limitations of data models in Atlan:\nGeneralizations are not part of the entity diagram. These types of relationships are shown in a separate section of the\nRelations\ntab in the entity sidebar in a tree-like representation.\nAlthough attributes can be mapped to other attributes or columns, these mappings are currently not displayed on the Atlan UI. Atlan currently only displays entity mappings in the\nLayers\nsection of the entity profile and sidebar.\nThere is no asset filter or indicator on lineage graphs to help you identify crawled database assets linked to ER assets.\nAtlan does not distinguish between the optionality ends of an association. For example, an\nAccount\ncan place multiple\nOrders\nÂ (one-to-many). Atlan stores this information and visually represents it in the entity diagram. However, the below two variations that depict more details about the relationship are currently not supported:\nAn\nAccount\nshould place at least one\nOrder\n.\nAn\nAccount\nmay or may not place an\nOrder\n.\nInverse relationships need to be defined independently and are shown distinctly in the entity diagram. For example, a\nCustomer\n(entity) places (relationship) an\nOrder\n(entity). The inverse of the relationship is\nOrder\n(entity) is placed by a\nCustomer\n(entity). The entity diagram currently represents these two relationships distinctively and does not club them in a unified way.\nCan a single model contain entities of different types?\nâ\nNo, the entity type (conceptual, logical, or physical) is defined for a model and all associated objects such as entities and attributes within the model inherit the same type. You can create two different models with the same name but with different types and use them to populate entities.\nCan fine-grain mapping between two layers be done at an attribute level?\nâ\nThe backend supports attribute-level mapping and information can be stored via the\nData Model Ingestion package\nas well as retrieved via API/SDK. However, attribute-to-attribute mapping is currently not displayed on the Atlan UI.\nWhat is the relation between a data model connection in Atlan and data models present elsewhere?\nâ\nThere are no strict rules on how many data models or if a subset of a data model should form part of a single Atlan Data Model connection. For example, you may choose to include ER models belonging to the same domain/business function to be part of a single connection.\nAtlan functionalities like access control, asset deletion, and more operate at the connection level. These factors need to be taken into consideration when deciding what to ingest.\nSome guidelines:\nCorresponding models representing different levels of abstraction are clubbed into one connection.\nChange frequency and refresh requirement (schedules) can drive this decision, too.\nWhat is the difference between mapping and relation?\nâ\nWhile entities can be related to each other in two ways, mapping and relation, there is a fundamental difference between the two:\nEntity mapping   -  ties an entity across different layers of abstraction. For example,\nCustomer\nlogical entity can be mapped to\nCUST_DETAILS\nphysical entity.\nEntity relations   -  representation of a peer-to-peer relationship between entities. For example,\nOrder\ngenerates\nInvoice\n.\nRelated entities are of the same type (physical/logical/conceptual).\nGenerally materialized as a primary key-foreign key relationship at the database level.\nWhat ER modeling tools does Atlan support?\nâ\nAtlan currently does not support native integration with any specific ER modeling tool. Object information from an ER modeling tool can be exported to and transformed with the\nData Model Ingestion package\nand then ingested into Atlan.\nWhat ER assets can be linked to database assets crawled by Atlan?\nâ\nEntities can currently be linked to database tables or views. Although not required, entities of the physical type are generally mapped to database assets.\nIs it mandatory to create and map all three types of entities?\nâ\nNo, you can choose to create assets of any one, two, or all three types. The Atlan UI is optimized for entity mapping in the following order:\nConceptual to logical entity\nLogical to physical entity\nPhysical entity to database assets\nYou can skip a certain level of abstraction, as needed.\nTags:\ndata\ncrawl\nmodel\nPrevious\nWhat are data models?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nTroubleshooting\nTroubleshooting Jira\nOn this page\nTroubleshooting Jira\nWhat fields are supported when creating tickets or requesting access?\nâ\nAtlan currently only supports standard fields such as project, issue type, title, and description.\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nâ\nAtlan's\nJira Cloud\nand\nJira Data Center\nintegrations currently do not support assigning owners by default or configuring additional fields while\ncreating an issue\n. However, you can assign an owner or add any basic or required fields within Jira once the ticket has been created from Atlan.\nCan site renaming affect the Jira integration?\nâ\nAtlan currently stores and uses the organization URL to help you access your Jira workspace from Atlan, if required. Since Jira\nautomatically redirects\nthe old URL to the new one, site renaming will not impact the\nJira integration\n.\nDoes Atlan support multiple Jira accounts and boards?\nâ\nWhile Atlan's Jira integration supports multiple Jira projects, it currently does not support multiple Jira accounts.\nHow long are Jira tickets valid in Atlan?\nâ\nJira issues will remain in Atlan unless they are unlinked. To unlink a Jira issue, you can either:\nManually unlink the Jira issue from the asset in Atlan.\nDelete the issue in Jira.\nDoes Atlan support Jira Service Management?\nâ\nNo, Atlan currently does not support Jira Service Management. You can use Atlan's\nJira Cloud\nand\nJira Data Center\nintegrations.\nTags:\ndata\nintegration\nPrevious\nLink your Jira account\nNext\nWhat is included in the Jira integration?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/how-tos/view-data-models",
    "content": "Configure Atlan\nData Models\nGet Started\nHow to view data models\nOn this page\nview data models\nOnce you have\ningested your ER model assets in Atlan\n, you can:\nSearch, discover, and filter models, entities, attributes, relationships, and mappings, cataloged as native assets in Atlan.\nLink ER entities at the conceptual, logical, or physical layer to your crawled data assets. Note that the Atlan UI is optimized for the following linking design: Conceptual entity â Logical entity â Physical entity â Database table or view.\nTrace the object lifecycle across various layers of abstraction or implementation:\nBusiness glossary\nER models\nCrawled data assets   -  tables and views\nModels\nâ\nTo view an entityârelationship (ER) model:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nIn the\nFilters\nmenu on the left, click\nSource\n.\nClick\nChoose connection\nto filter for assets in a\nData Model\nconnection.\nUnder the search bar on the\nAssets\npage, click the\nAsset type\ndropdown.\nFrom the\nAsset type\ndropdown, select a\nModel\nto search by a specific asset type.\nSelect a model asset to view the asset sidebar or open the asset profile.\nAsset preview\nâ\nThe model asset preview includes basic information about the asset, including technical name,\nalias\n, model type, description, and total count of associated entities.\nAsset sidebar\nâ\nThe sidebar to the right of the asset preview provides high-level information about the asset. Here's what you can view specific to model assets:\nOverview\noffers a preview of the key characteristics of the asset, including model name, type, and description. You can add an\nalias\n, enrich metadata, link domains to your data models, and more from the asset sidebar.\nEntities\ndisplays a list of entities in a model, along with entity type and description. This tab also provides you with a search bar to search and sort entities.\nAsset profile\nâ\nThe model profile summarizes important details about the asset.\nOverview\ntab displays details such as technical name,\nalias\n, entity count, model type, description, certification status, and owners. The\nEntities\nsection offers a snapshot of all the entities in a model, listing the entity name and description.\nEntities\ntab allows you to update metadata such as tags and terms for your entities directly from the model profile.\nEntities\nâ\nTo view an entity:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nIn the\nFilters\nmenu on the left, click\nSource\n.\nClick\nChoose connection\nto filter for assets in a\nData Model\nconnection.\nUnder the search bar on the\nAssets\npage, click the\nAsset type\ndropdown.\nFrom the\nAsset type\ndropdown, select a\nEntity\nto search by a specific asset type.\nSelect an entity asset to view the asset sidebar or open the asset profile.\nAsset preview\nâ\nThe entity asset preview includes basic information about the asset, including technical name,\nalias\n, entity type, description, and total count of associated attributes.\nAsset sidebar\nâ\nThe sidebar to the right of the asset preview provides high-level information about the asset. Here's what you can view specific to entity assets:\nOverview\noffers a preview of the key characteristics of the asset, including entity name, type, and description. You can add an\nalias\n, enrich metadata, link domains to your entities, and more from the asset sidebar.\nAttributes\ndisplays a list of attributes in an entity, along with data type, description, and primary key indicator, if any. This tab also provides you with a search bar to search and sort attributes.\nRelations\nshows a list of entity relationships:\nAssociations\ndisplays relation name, associated entity, and cardinality.\nGeneralization\ndisplays parent-child relationships between entities.\nLayers\norganizes different levels of abstraction in a tree-like representation: Conceptual entity â Logical entity â Physical entity â Database table or view.\nAsset profile\nâ\nThe entity profile displays important details about the asset.\nOverview\ntab summarizes details such as technical name,\nalias\n, attribute count, entity type, associated model, description, certification status, and owners.\nThe\nLayers\nsection organizes different levels of abstraction in a tree-like representation: Conceptual entity â Logical entity â Physical entity â Database table or view. Click an entity name to view more details in the asset sidebar.\nThe\nAttributes\nsection offers a snapshot of all the attributes of an entity, including the attribute name, primary key indicator, data type, nullability, and description.\nAttributes\ntab allows you to update metadata such as tags and terms for your attributes directly from the entity profile.\nEntity Diagram\nprovides a\nvisual representation of entity relationships\n.\nEntity diagram\nâ\nAn entity diagram is a visual representation of entity relationships. It also allows you to navigate to the next set of related entities as you explore the graph. Note that the entity diagram in Atlan is not a replacement for ER modeling tools, which are optimized for editing objects and displaying an entire model.\nThe entity diagram in Atlan:\nIs entity-focused only\nUses crowâs foot notation to represent one-to-many relationships\nLists attributes that are part of an entity:\nAttributes that form part of a relationship are shown at the top of the list. Clicking on one highlights the attribute in the related entity that forms the basis of the association.\nThe association itself can be highlighted and has a sidebar with detailed information.\nLayers\nâ\nThe\nLayers\nsection of an entity or database table helps you navigate across different abstraction layers of entities. A layer is a mechanism to have multiple abstractions for a model or an entity.\nConceptual   -  most abstract\nLogical   -  more defined\nPhysical   -  well-defined and conformant to a target database system\nTags:\ndata\ncrawl\nmodel\nPrevious\nData Models\nNext\nWhat are data models?\nModels\nEntities\nEntity diagram\nLayers"
  },
  {
    "url": "https://docs.atlan.com/product/administration/logs/how-tos/view-query-logs",
    "content": "Configure Atlan\nAdministration\nMonitoring\nHow to view query logs\nView query logs\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to view query logs.\nThe query log helps you track all queries run in Atlan, including:\nsaved and unsaved queries in the Insights query editor\nqueries run through both the Atlan UI and API\nsample data previews from asset profiles\nYou can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization.\nTo view query logs:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder the\nLogs\nheading of your admin\nWorkspace\n, click\nQuery logs\n.\nOn the\nQuery logs\npage, you can view all the queries that your users have run or are running in Atlan.\n(Optional) Click the funnel icon to filter queries and then:\nClick\nStatus\nto filter queries by run status â\nSucceeded\n,\nFailed\n, or\nAborted\n.\nClick\nUsers\nto filter queries by Atlan users.\n(Optional) Use the search bar to search for queries using specific keywords.\nThe default date range is set to 30 days. Use the date filter to view query logs for the last 7 days, past 3 or 6 months, or a custom date range of your choice.\nFor any query listed in the query logs, you can view the query name, connection, execution details, user that run the query, and timestamp for when the query was run. (Optional) Click any query to view more details in the\nQuery details\nsidebar:\nIn the\nQuery details\nsidebar, you can view the full query, connection, database, schema, and asset name, query status, and query run time.\nClick the copy icon to copy the query and use it as a template for writing your own queries.\nClick the expand icon to see the full query.\nFor\nQuery Source\n, click\nCopy ID\nto copy the query ID.\nTags:\ndata\napi\nPrevious\nHow to view event logs\nNext\nCreate README templates"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/power-bi-lineage-processes",
    "content": "Use data\nLineage\nFAQ\nWhat are Power BI processes on the lineage graph?\nWhat are Power BI processes on the lineage graph?\nProcesses\nin general represent the movement and transformation of assets in Atlan. By default,\nprocess assets are hidden\non the assets page and reporting center.\nNote that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets.\nThese can be especially useful for SQL-first sources like Snowflake, in which case the process entities show the transformation SQL query that was parsed to generate lineage. This allows users to easily discover source code or scripts.\nTags:\ndata\nfaq\nfaq-lineage\nPrevious\nIs there a way to build lineage from NetSuite to Snowflake?\nNext\nWhat do the numbers in lineage view mean?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/references/what-does-atlan-crawl-from-amazon-athena",
    "content": "Connect data\nDatabases\nQuery Engines\nAmazon Athena\nReferences\nWhat does Atlan crawl from Amazon Athena?\nOn this page\nWhat does Atlan crawl from Amazon Athena?\nAtlan crawls and maps the following assets and properties from Amazon Athena.\nDatabases\nâ\nAtlan maps databases from Amazon Athena to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from Amazon Athena to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nCreateTime (via Glue)\nsourceCreatedAt\nTables\nâ\nAtlan maps tables from Amazon Athena to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nrecordCount (via Glue)\nrowCount\nsizeKey (via Glue)\nsizeBytes\nTABLE_TYPE\nsubType\nStorageDescriptor Location (via Glue)\nexternalLocation\nStorageDescriptor typeOfData (via Glue)\nexternalLocationFormat\nStorageDescriptor Columns and COLUMN_COUNT\ncertificateStatus (DEPRECATED) if Athena JDBC and Glue API do not have the same column count\nPartitionKeys (via Glue)\nisPartitioned\nPartitionData (via Glue)\npartitionCount\nPartitionData (via Glue)\npartitionList\nCreatedBy (via Glue)\nsourceCreatedBy\nCreateTime or CreationTime (via Glue)\nsourceCreatedAt\nUpdateTime or LastAccessTime (via Glue)\nsourceUpdatedAt\nViews\nâ\nAtlan maps views from Amazon Athena to its\nView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nVIEW_DEFINITION\ndefinition\nStorageDescriptor Columns and COLUMN_COUNT\ncertificateStatus (DEPRECATED) if Athena JDBC and Glue API do not have the same column count\nrecordCount (via Glue)\nrowCount\nPartitionKeys (via Glue)\nisPartitioned\nPartitionData (via Glue)\npartitionCount\nPartitionData (via Glue)\npartitionList\nCreatedBy (via Glue)\nsourceCreatedBy\nCreateTime or CreationTime (via Glue)\nsourceCreatedAt\nUpdateTime or LastAccessTime (via Glue)\nsourceUpdatedAt\nColumns\nâ\nAtlan maps columns from Amazon Athena to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\nTYPE_NAME\ndataType\nNULLABLE\nisNullable\nIS_PARTITION\nisPartition\nPARTITION_ORDER\npartitionOrder\nPRIMARY_KEY\nisPrimary\nDECIMAL_DIGITS\nprecision\nTags:\ndata\ncrawl\nPrevious\nCrawl Amazon Athena\nDatabases\nSchemas\nTables\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/references/what-does-atlan-crawl-from-amazon-dynamodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nAmazon DynamoDB\nReferences\nWhat does Atlan crawl from Amazon DynamoDB?\nOn this page\nWhat does Atlan crawl from Amazon DynamoDB?\nAtlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran.\nTables\nâ\nAtlan maps tables from Amazon DynamoDB to its\nDynamoDBTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTableName\nname\nasset profile and overview sidebar\nItemCount\nrowCount\nasset preview and profile, overview sidebar\nTableSizeBytes\nsizeBytes\nasset filter and overview sidebar\nAttributeDefinitions\nnoSQLSchemaDefinition\nasset profile\nHASH\nkey in\nKeySchema\ndynamoDBPartitionKey\nasset profile and overview sidebar\nRANGE\nkey in\nKeySchema\ndynamoDBSortKey\nasset profile and overview sidebar\nTableStatus\ndynamoDBStatus\nasset profile and overview sidebar\nCreationDateTime\nsourceCreatedAt\nasset profile and overview sidebar\nProvisionedThroughput.ReadCapacityUnits\ndynamoDBReadCapacityUnits\nasset profile and overview sidebar\nProvisionedThroughput.WriteCapacityUnits\ndynamoDBWriteCapacityUnits\nasset profile and overview sidebar\nGlobal secondary indexes\nâ\nAtlan maps global secondary indexes (GSI) from Amazon Dynamo DB to its\nDynamoDBGlobalSecondaryIndex\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nItemName\nname\nasset profile and overview sidebar\nIndexCount\nrowCount\nasset preview and profile, overview sidebar\nIndexSizeBytes\nsizeBytes\nasset filter and overview sidebar\nIndexStatus\ndynamoDBStatus\nasset profile and overview sidebar\nHASH\nkey in\nKeySchema\ndynamoDBPartitionKey\nasset profile and overview sidebar\nRANGE\nkey in\nKeySchema\ndynamoDBSortKey\nasset profile and overview sidebar\nProjection.ProjectionType\ndynamoDBSecondaryIndexProjectionType\nasset profile and overview sidebar\nProvisionedThroughput.ReadCapacityUnits\ndynamoDBReadCapacityUnits\nasset profile and overview sidebar\nProvisionedThroughput.WriteCapacityUnits\ndynamoDBWriteCapacityUnits\nasset profile and overview sidebar\nLocal secondary indexes\nâ\nAtlan maps local secondary indexes (LSI) from Amazon Dynamo DB to its\nDynamoDBLocalSecondaryIndex\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nIndexName\nname\nasset profile and overview sidebar\nItemCount\nrowCount\nasset preview and profile, overview sidebar\nIndexSizeBytes\nsizeBytes\nasset filter and overview sidebar\nHASH\nkey in\nKeySchema\ndynamoDBPartitionKey\nasset profile and overview sidebar\nRANGE\nkey in\nKeySchema\ndynamoDBSortKey\nasset profile and overview sidebar\nProjection.ProjectionType\ndynamoDBSecondaryIndexProjectionType\nasset profile and overview sidebar\nTags:\ndata\ncrawl\nPrevious\nCrawl Amazon DynamoDB\nNext\nTroubleshooting Amazon DynamoDB connectivity\nTables\nGlobal secondary indexes\nLocal secondary indexes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/references/what-does-atlan-crawl-from-amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nReferences\nWhat does Atlan crawl from Amazon Redshift?\nOn this page\nWhat does Atlan crawl from Amazon Redshift?\nAtlan crawls and maps the following assets and properties from Amazon Redshift.\nDatabases\nâ\nAtlan maps databases from Amazon Redshift to its\nDatabase\nasset type.\nSource property\nAtlan property\nDATABASE_NAME\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from Amazon Redshift to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nDATABASE_NAME\ndatabaseName\nOWNER\nsourceCreatedBy\nSCHEMA_TYPE\nsubType\nTables\nâ\nAtlan maps tables from Amazon Redshift to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nTABLE_TYPE (EXTERNAL TABLE)\nsubType\nLOCATION\nexternalLocation\nINPUT_FORMAT\nexternalLocationFormat\nTABLE_OWNER\nsourceCreatedBy\nCREATED\nsourceCreatedAt\nViews\nâ\nAtlan maps views from Amazon Redshift to its\nView\nand\nMaterialisedView\nasset types.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nVIEW_DEFINITION\ndefinition\nTABLE_OWNER\nsourceCreatedBy\nColumns\nâ\nAtlan maps columns from Amazon Redshift to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\nTYPE_NAME\ndataType\nNOTNULL\nisNullable\nCHARACTER_MAXIMUM_LENGTH\nmaxLength\nDECIMAL_DIGITS\nprecision\nCONSTRAINT_TYPE (PRIMARY KEY)\nisPrimary\nCONSTRAINT_TYPE (FOREIGN KEY)\nisForeign\nSORTKEY\nisSort\nDISKEY\nisDist\nTags:\ndata\ncrawl\nPrevious\nMine Amazon Redshift\nNext\nPreflight checks for Amazon Redshift\nDatabases\nSchemas\nTables\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/references/what-does-atlan-crawl-from-aws-glue",
    "content": "Connect data\nETL Tools\nAWS Glue\nReferences\nWhat does Atlan crawl from AWS Glue?\nOn this page\nWhat does Atlan crawl from AWS Glue?\nAtlan crawls and maps the following assets and properties from AWS Glue.\nDatabases\nâ\nAtlan maps databases from AWS Glue to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from AWS Glue to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nCOMMENTS\ndescription\nCreateTime\nsourceCreatedAt\nTables\nâ\nAtlan maps tables from AWS Glue to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nCOMMENTS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nobjectCount\ntableObjectCount\nBYTES\nsizeBytes\nParameters (recordCount)\nrowCount\nParameters (sizeKey)\nsizeBytes\nTABLE_TYPE\nsubType\nStorageDescriptor (Location)\nexternalLocation\nParameters (typeOfData, classification)\nexternalLocationFormat\nPartitionKeys\nisPartitioned\nPartitionData\npartitionCount\n,\npartitionList\nCreatedBy\nsourceCreatedBy\nCreateTime\n,\nCreationTime\nsourceCreatedAt\nUpdateTime\n,\nLastAccessTime\nsourceUpdatedAt\nViews\nâ\nAtlan maps views from AWS Glue to its\nView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nCOMMENTS\ndescription\nCOLUMN_COUNT\ncolumnCount\nVIEW_DEFINITION\ndefinition\nParameters (recordCount)\nrowCount\nPartitionKeys\nisPartitioned\nPartitionData\npartitionCount\n,\npartitionList\nCreatedBy\nsourceCreatedBy\nCreateTime\n,\nCreationTime\nsourceCreatedAt\nUpdateTime\n,\nLastAccessTime\nsourceUpdatedAt\nColumns\nâ\nAtlan maps columns from AWS Glue to its\nColumn\nasset type.\nAtlan also supports nested columns up to level 15 for AWS Glue to help you enrich your semi-structured data types:\nAtlan retrieves raw\nSTRUCT\nand\nARRAY\ntype objects for nested columns up to 15 levels.\nView nested columns in the column preview and overview sidebar for your table assets.\nColumn-level lineage is supported. Search, enrich metadata, and view lineage for nested columns.\nTag propagation is currently only supported from parent to nested columns.\nAtlan currently doesn't parse\nMAP\ntype objects for columns and nested columns.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nCOMMENTS\ndescription\nORDINAL_POSITION\n,\nCOLUMN_ID\norder\nTYPE_NAME\n,\nDATA_TYPE\ndataType\nIS_PARTITION\nisPartition\nPARTITION_ORDER\npartitionOrder\nTags:\ndata\ncrawl\nPrevious\nCrawl AWS Glue\nNext\nTroubleshooting AWS Glue connectivity\nDatabases\nSchemas\nTables\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/references/what-does-atlan-crawl-from-datastax-enterprise",
    "content": "Connect data\nDatabases\nNoSQL Databases\nDatastax Enterprise\nReferences\nWhat does Atlan crawl from DataStax Enterprise?\nOn this page\nWhat does Atlan crawl from DataStax Enterprise?\nAtlan integrates with DataStax Enterprise to crawl and map various asset types, helping you discover and understand your distributed data. This page outlines the DataStax Enterprise components that Atlan supports and how their properties are mapped.\nLineage support\nâ\nAtlan also supports the following lineage:\nAsset-level lineage for\nTables\nandÂ\nMaterialised Views\n.\nColumn-level lineage for\nTables\nand\nMaterialised Views\n.\nAssets\nâ\nAtlan crawls and maps the following assets and properties from DataStax Enterprise Cassandra.\nKeyspaces\nâ\nAtlan maps keyspaces from DataStax Enterprise Cassandra to its\nKeyspace\nasset type.\nSource property\nAtlan property\nkeyspace_name\nname\ndurable_writes\nschemaCount\nreplication\ncassandraKeyspaceReplication\nvirtual\ncassandraKeyspaceVirtual\nquery\ncassandraKeyspaceQuery\nTables\nâ\nAtlan maps tables from DataStax Enterprise Cassandra to its\nTable\nasset type.\nSource property\nAtlan property\ntable_name\nname\nbloom_filter_fp_chance\ncassandraTableBloomFilterFpChance\ncaching\ncassandraTableCaching\ncompaction\ncassandraTableCompaction\ncompression\ncassandraTableCompression\ncrc_check_chance\ncassandraTableCrcCheckChance\ndclocal_read_repair_chance\ncassandraTableDclocalReadRepairChance\ndefault_time_to_live\ncassandraTableDefaultTimeToLive\nextensions\ncassandraTableExtensions\nflags\ncassandraTableFlags\ncomment\ncassandraTableComment\ngc_grace_seconds\ncassandraTableGcGraceSeconds\nid\ncassandraTableId\nmax_index_interval\ncassandraTableMaxIndexInterval\nread_repair_chance\ncassandraTableReadRepairChance\nMaterialised Views\nâ\nAtlan maps tables from DataStax Enterprise Cassandra to its\nMaterialisedViews\nasset type.\nSource property\nAtlan property\ntable_name\nname\nbloom_filter_fp_chance\ncassandraViewBloomFilterFPChance\ncaching\ncassandraViewCaching\ncompaction\ncassandraViewCompaction\ncompression\ncassandraViewCompression\ncrc_check_chance\ncassandraViewCRCCheckChance\ndclocal_read_repair_chance\ncassandraViewDCLocalReadRepairChance\ndefault_time_to_live\ncassandraViewDefaultTTL\ngc_grace_seconds\ncassandraViewGCGraceSeconds\ninclude_all_columns\ncassandraViewIncludeAllColumns\ncomment\ndescription\ngc_grace_seconds\ncassandraTableGcGraceSeconds\nbase_table_id\ncassandraViewTableId\nmax_index_interval\ncassandrViewMaxIndexInterval\nread_repair_chance\ncassandraViewReadRepairInterval\nspeculative_retry\ncassandraViewSpeculativeRetry\nbase_table_name\ncassandraTableName\nquery\ncassandraViewQuery\nmemtable_flush_period_in_ms\ncassandraViewMembtableFlushPeriodInMS\nmin_index_interval\ncassandraViewMinIndexInterval\nkeyspace_name\ncassandraKeyspaceName\nIndexes\nâ\nAtlan maps views from DataStax Enterprise Cassandra to its\nIndexes\nasset type.\nSource property\nAtlan property\nindex_name\nname\nkind\ncassandraIndexKind\noptions\ncassandraIndexOptions\ntable_name\ncassandraTableName\nkeyspace_name\ncassandraKeyspaceName\nquery\ncassandraIndexQuery\nColumns\nâ\nAtlan maps columns from DataStax Enterprise Cassandra to its\nColumn\nasset type.\nSource property\nAtlan property\ncolumn_name\nname\ntable_name (if a view)\ncassandraViewName\ntable_name (if a table)\ncassandraTableName\nclustering_order\ncassandraColumnClusteringOrder\nkind\ncassandraColumnKind\nposition\ncassandraColumnPosition\ntype\ncassandraColumnType\nkeyspace_name\ncassandraColumnIsStatic\nTags:\ndata\ncrawl\nPrevious\nCrawl DataStax Enterprise\nNext\nPreflight checks for DataStax Enterprise\nLineage support\nAssets\nKeyspaces\nTables\nMaterialised Views\nIndexes\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/references/what-does-atlan-crawl-from-domo",
    "content": "Connect data\nBI Tools\nCloud-based BI\nDomo\nReferences\nWhat does Atlan crawl from Domo?\nOn this page\nWhat does Atlan crawl from Domo?\nAtlan supports lineage for the following asset types:\nDatasets\n-  upstream lineage to Google BigQuery and Snowflake data sources. Domo currently only supports upstream lineage for the following dataset types:\nGoogle BigQuery High Bandwidth Service connector\nSnowflake connector\nwith\nQuery Type\nas\nEnter Query\nGoogle BigQuery Service Connector\nwith report type as query. Note that this is currently only supported when\nqueryParameter\nis blank and\nqueryType\nis\nstandardSQL\n.\nSnowflake Federated Data\nwith basic authentication and Snowflake OAuth. To configure Snowflake OAuth-based access from Domo, please reach out to\nDomo support\n.\nCards\n-  upstream lineage to Domo datasets\nDashboards\n-  upstream lineage to Domo cards\nAtlan crawls and maps the following assets and properties from Domo.\ndanger\nCurrently, Atlan only represents the assets marked with ð in lineage.\nDatasets ð\nâ\nAtlan maps datasets from Domo to its\nDomoDataset\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset preview and profile, overview sidebar\ndescription\ndescription\nasset preview and profile, overview sidebar\nid\nDomoId\nAPI only\nowner.name\nsourceCreatedBy\noverview sidebar\nowner.id\ndomoOwnerId\nAPI only\nrows\ndomoDatasetRowCount\nasset profile and overview sidebar\ncolumns\ndomoDatasetColumnCount\nasset preview and profile, overview sidebar\ndataCurrentAt\ndomoDatasetLastRun\nAPI only\ncreatedAt\nsourceCreatedAt\nasset preview and profile, properties sidebar\nupdatedAt\nsourceUpdatedAt\nasset preview and profile, properties sidebar\ncalculated using dataset-card relationship API\ndomoDatasetCardCount\nasset preview and profile, overview sidebar\nDataset columns\nâ\nAtlan maps dataset columns from Domo to its\nDomoDatasetColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset preview and profile, overview sidebar\ntype\ndomoDatasetColumnType\noverview sidebar\nDashboards ð\nâ\nAtlan maps dashboards from Domo to its\nDomoDashboard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nname\nname\nasset preview and profile, overview sidebar\nid\ndomoId\nAPI only\ncalculated using card-dashboard relationship API\ndomoDashboardCardCount\nasset preview and profile, overview sidebar\nCards ð\nâ\nAtlan maps cards from Domo to its\nDomoCard\nasset type.\nSource property\nAtlan property\nWhere in Atlan\ncardTitle\nname\nasset preview and profile, overview sidebar\nid\ndomoId\nAPI only\ntype\ndomoCardType\noverview sidebar\nlastModified\nsourceUpdatedAt\nasset preview and profile, properties sidebar\ncalculated using card-dashboard relationship API\ndomoCardDashboardCount\nasset preview and profile, overview sidebar\nTags:\ndata\ncrawl\nauthentication\nPrevious\nCrawl Domo\nNext\nPreflight checks for Domo\nDatasets ð\nDataset columns\nDashboards ð\nCards ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/references/what-does-atlan-crawl-from-hive",
    "content": "Connect data\nDatabases\nQuery Engines\nHive\nReferences\nWhat does Atlan crawl from Hive?\nOn this page\nWhat does Atlan crawl from Hive?\nAtlan crawls and maps the following assets and properties from Hive.\nDatabase\nâ\nAtlan always creates a\nDatabase\nasset called\nHive\nto maintain the three-level hierarchy.\nSchemas\nâ\nAtlan maps schemas from Hive to its\nSchema\nasset type.\nSource property\nAtlan property\nDATABASE_NAME\nname\nCOMMENT\ndescription\nTables\nâ\nAtlan maps tables from Hive to its\nTable\nasset type.\nSource property\nAtlan property\ntableName\nname\nCOMMENT\ndescription\ncolumn_count\ncolumnCount\nnumRows\nrowCount\ntotalSize\nsizeBytes\nnumPartitions\nisPartitioned\nnumPartitions\npartitionCount\nlocation\nexternalLocation\ninputFormat\nexternalLocationFormat\nViews\nâ\nAtlan maps views from Hive to its\nView\nasset type.\nSource property\nAtlan property\ntableName\nname\nCOMMENT\ndescription\ncolumn_count\ncolumnCount\nviewExpandedText\ndefinition\nMaterialized views\nâ\nAtlan maps materialized views in Hive to its\nMaterialised View\nasset type.\nSource property\nAtlan property\ntableName\nname\nCOMMENT\ndescription\ncolumn_count\ncolumnCount\nviewExpandedText\ndefinition\nstaleSinceDate\nmaterializationTime\nnumRows\nrowCount\ntotalSize\nsizeBytes\nColumns\nâ\nAtlan maps columns from Hive to its\nColumn\nasset type.\nSource property\nAtlan property\nCOL_NAME\nname\nDATA_TYPE\ndataType\nCOMMENT\ndescription\nDetailed Table Information\norder\nConstraints (Primary Key)\nisPrimary\nConstraints (Foreign Key)\nisForeign\nTags:\ndata\ncrawl\nPrevious\nCrawl Hive\nNext\nPreflight checks for Hive\nDatabase\nSchemas\nTables\nViews\nMaterialized views\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/references/what-does-atlan-crawl-from-microsoft-azure-data-factory",
    "content": "Connect data\nETL Tools\nMicrosoft Azure Data Factory\nReferences\nWhat does Atlan crawl from Microsoft Azure Data Factory?\nOn this page\nWhat does Atlan crawl from Microsoft Azure Data Factory?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Data Factory.\nFor any currently unsupported linked services, datasets, and activities from Microsoft Azure Data Factory not listed below, Atlan will map them to the relevant asset type and only display asset name, type, and description.\nLinked services\nâ\nAtlan maps linked services from Microsoft Azure Data Factory to its\nAdfLinkedservice\nasset type.\nMicrosoft Azure Cosmos DB for MongoDB\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfLinkedserviceType\ndescription\ndescription\nannotations\nadfLinkedserviceAnnotations\ntypeProperties.database\nadfLinkedserviceDatabaseName\ntypeProperties.isServerVersionAbove32\nadfLinkedserviceVersionAbove\nversion\nadfLinkedserviceVersion\nAzure Data Lake Storage (ADLS)\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfLinkedserviceType\ndescription\ndescription\ntypeProperties.azureCloudType\nadfLinkedserviceAzureCloudType\ntypeProperties.servicePrincipalCredentialType\nadfLinkedserviceCredentialType\ntypeProperties.tenant\nadfLinkedserviceTenant\ntypeProperties.url\nadfLinkedserviceDomainEndpoint\nAzure Databricks\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfLinkedserviceType\ndescription\ndescription\nannotations\nadfLinkedserviceAnnotations\ntypeProperties.clusterId\nadfLinkedserviceClusterId\ntypeProperties.domain\nadfLinkedserviceDomainEndpoint\ntypeProperties.workspaceResourceId\nadfLinkedserviceResourceId\nSnowflake\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfLinkedserviceType\ndescription\ndescription\nannotations\nadfLinkedserviceAnnotations\ntypeProperties.accountIdentifier\nadfLinkedserviceDomainEndpoint\ntypeProperties.authenticationType\nadfLinkedserviceCredentialType\ntypeProperties.database\nadfLinkedserviceDatabaseName\ntypeProperties.user\nadfLinkedserviceUserName\ntypeProperties.warehouse\nadfLinkedserviceWarehouseName\nMicrosoft Azure SQL Database\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfLinkedserviceType\ndescription\ndescription\nannotations\nadfLinkedserviceAnnotations\ntypeProperties.authenticationType\nadfLinkedserviceCredentialType\ntypeProperties.database\nadfLinkedserviceDatabaseName\ntypeProperties.server\nadfLinkedserviceDomainEndpoint\nDatasets\nâ\nAtlan maps databases from Microsoft Azure Cosmos DB to its\nAdfDataset\nasset type.\nMicrosoft Azure Cosmos DB for MongoDB\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfDatasetType\ndescription\ndescription\nannotations\nadfDatasetAnnotations\nfolder\nadfDatasetFolderPath\nlinkedServiceName\nadfDatasetLinkedService\ntypeProperties.collection\nadfDatasetCollectionName\nAzure Data Lake Storage (ADLS)\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfDatasetType\ndescription\ndescription\nannotations\nadfDatasetAnnotations\nfolder\nadfDatasetFolderPath\nlinkedServiceName\nadfDatasetLinkedService\ntypeProperties.fileName\nadfDatasetFileName\ntypeProperties.folderPath\nadfDatasetFileFolderPath\ntypeProperties.format\nadfDatasetStorageType\ntypeProperties.container\nadfDatasetContainerName\nAzure Databricks\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfDatasetType\ndescription\ndescription\nannotations\nadfDatasetAnnotations\nfolder\nadfDatasetFolderPath\nlinkedServiceName\nadfDatasetLinkedService\ntypeProperties.database\nadfDatasetDatabaseName\ntypeProperties.table\nadfDatasetTableName\nSnowflake\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfDatasetType\ndescription\ndescription\nannotations\nadfDatasetAnnotations\nfolder\nadfDatasetFolderPath\nlinkedServiceName\nadfDatasetLinkedService\ntypeProperties.schema\nadfDatasetSchemaName\ntypeProperties.table\nadfDatasetTableName\nMicrosoft Azure SQL Database\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfDatasetType\ndescription\ndescription\nannotations\nadfDatasetAnnotations\nfolder\nadfDatasetFolderPath\nlinkedServiceName\nadfDatasetLinkedService\ntypeProperties.schema\nadfDatasetSchemaName\ntypeProperties.table\nadfDatasetTableName\nData flows\nâ\nAtlan maps data flows from Microsoft Azure Data Factory to its\nAdfDataflow\nasset type.\nSource property\nAtlan property\nname\nname\ndescription\ndescription\nfolder\nadfDataflowFolderPath\ntypeProperties.sources\nadfDataflowSources\ntypeProperties.sinks\nadfDataflowSinks\ntypeProperties.scriptLines\nadfDataflowScript\nActivities\nâ\nAtlan maps activities from Microsoft Azure Data Factory to its\nAdfActivity\nasset type.\nCopy activity\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\ndependsOn\nadfActivityPrecedingDependency\npolicyTimeout\nadfActivityPolicyTimeout\npolicyRetryInterval\nadfActivityPolictRetryInterval\nstate\nadfActivityState\nactivityRuns\nadfActivityRuns\ntypeProperties.sink\nadfActivitySinks\ntypeProperties.source\nadfActivitySources\ntypeProperties.sinkType\nadfActivitySinkType\ntypeProperties.sourceType\nadfActivitySourceType\nDatabricks notebooks\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\ndependsOn\nadfActivityPrecedingDependency\npolicyTimeout\nadfActivityPolicyTimeout\npolicyRetryInterval\nadfActivityPolictRetryInterval\nstate\nadfActivityState\nactivityRuns\nadfActivityRuns\ntypeProperties.source\nadfActivitySources\ntypeProperties.sourceType\nadfActivitySources\ntypeProperties.notebookPath\nadfActivityNotebookPath\nDatabricksSparkJar activity\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\ndependsOn\nadfActivityPrecedingDependency\npolicyTimeout\nadfActivityPolicyTimeout\npolicyRetryInterval\nadfActivityPolictRetryInterval\nstate\nadfActivityState\nactivityRuns\nadfActivityRuns\ntypeProperties.source\nadfActivitySources\ntypeProperties.sourceType\nadfActivitySources\ntypeProperties.mainClassName\nadfActivityMainClassName\nDatabricksSparkPython activity\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\ndependsOn\nadfActivityPrecedingDependency\npolicyTimeout\nadfActivityPolicyTimeout\npolicyRetryInterval\nadfActivityPolictRetryInterval\nstate\nadfActivityState\nactivityRuns\nadfActivityRuns\ntypeProperties.source\nadfActivitySources\ntypeProperties.sourceType\nadfActivitySources\ntypeProperties.pythonFile\nadfActivityPythonFilePath\nLookup activity\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\ndependsOn\nadfActivityPrecedingDependency\npolicyTimeout\nadfActivityPolicyTimeout\npolicyRetryInterval\nadfActivityPolictRetryInterval\nstate\nadfActivityState\nactivityRuns\nadfActivityRuns\ntypeProperties.firstRowOnly\nadfActivityFirstRowOnly\ntypeProperties.source\nadfActivitySources\nForEach activity\nâ\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\ndependsOn\nadfActivityPrecedingDependency\npolicyTimeout\nadfActivityPolicyTimeout\npolicyRetryInterval\nadfActivityPolictRetryInterval\nstate\nadfActivityState\nactivityRuns\nadfActivityRuns\ntypeProperties.activities\nadfActivitySubActivities\ntypeProperties.batchCount\nadfActivityBatchCount\ntypeProperties.isSequential\nadfActivityIsSequential\nPipelines\nâ\nAtlan maps pipelines from Microsoft Azure Data Factory to its\nAdfPipeline\nasset type.\nSource property\nAtlan property\nname\nname\ntype\nadfActivityType\ndescription\ndescription\nactivityCount\nadfPipelineActivityCount\npipelineRun\nadfPipelineRuns\ntypeProperties.folder\nadfPipelineFolderPath\nTags:\ndata\ncrawl\nPrevious\nCrawl Microsoft Azure Data Factory\nNext\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nLinked services\nDatasets\nData flows\nActivities\nPipelines"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/references/what-does-atlan-crawl-from-microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nReferences\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nOn this page\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources.\nDatabases\nâ\nAtlan maps databases from Microsoft Azure Synapse Analytics to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from Microsoft Azure Synapse Analytics to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nTables\nâ\nAtlan maps tables from Microsoft Azure Synapse Analytics to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nPARTITIONS\nisPartitioned\nPARTITION_COUNT\npartitionCount\nViews\nâ\nAtlan maps views from Microsoft Azure Synapse Analytics to its\nView\nasset type.\nSource property\nAtlan property\nVIEW_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nVIEW_DEFINITION (WITH SCHEMABINDING)\nisClustered\nVIEW_DEFINITION\ndefinition\nColumns\nâ\nAtlan maps columns from Microsoft Azure Synapse Analytics to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\nTYPE_NAME\ndataType\nCONSTRAINT_TYPE (PRIMARY KEY)\nisPrimary\nCONSTRAINT_TYPE (FOREIGN KEY)\nisForeign\nNULLABLE\nisNullable\nNUMERIC_SCALE\nnumericScale\nNUMERIC_PRECISION\nprecision\nRoutines\nâ\nAtlan maps routines in Microsoft Azure Synapse Analytics to its\nProcedure\nasset type.\nSource property\nAtlan property\nROUTINE_NAME\nname\nREMARKS\ndescription\nPROCEDURE_TYPE\nsubType\nROUTINE_DEFINITION\ndefinition\nTags:\ndata\ncrawl\nPrevious\nCrawl Microsoft Azure Synapse Analytics\nNext\nPreflight checks for Microsoft Azure Synapse Analytics\nDatabases\nSchemas\nTables\nViews\nColumns\nRoutines"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/references/what-does-atlan-crawl-from-microsoft-sql-server",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nReferences\nWhat does Atlan crawl from Microsoft SQL Server?\nOn this page\nWhat does Atlan crawl from Microsoft SQL Server?\nAtlan crawls and maps the following assets and properties from Microsoft SQL Server.\nDatabases\nâ\nAtlan maps databases from Microsoft SQL Server to its\nDatabase\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_CATALOG\nname\nasset preview, profile, and filter, overview sidebar\nSCHEMA_COUNT\nschemaCount\nAPI only\nSchemas\nâ\nAtlan maps schemas from Microsoft SQL Server to its\nSchema\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_SCHEMA\nname\nasset preview, profile, and filter, overview sidebar\nTABLE_COUNT\ntableCount\nasset preview and profile\nVIEW_COUNT\nviewsCount\nasset preview and profile\nTABLE_CATALOG\ndatabaseName\nasset preview and profile\nTables\nâ\nAtlan maps tables from Microsoft SQL Server to its\nTable\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview, profile, and filter, overview sidebar\nREMARKS\ndescription\nasset preview, profile, and filter, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview, profile, and filter, overview sidebar\nROW_COUNT\nrowCount\nasset preview, profile, and filter, overview sidebar\nBYTES\nsizeBytes\nasset preview, profile, and filter, overview sidebar\nPARTITIONS\nisPartitioned\nAPI only\nPARTITION_COUNT\npartitionCount\nAPI only\nViews\nâ\nAtlan maps views from Microsoft SQL Server to its\nView\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nTABLE_NAME\nname\nasset preview, profile, and filter, overview sidebar\nREMARKS\ndescription\nasset preview, profile, and filter, overview sidebar\nCOLUMN_COUNT\ncolumnCount\nasset preview, profile, and filter, overview sidebar\nVIEW_DEFINITION (WITH SCHEMABINDING)\nisClustered\nAPI only\nVIEW_DEFINITION\ndefinition\nasset profile and overview sidebar\nColumns\nâ\nAtlan maps columns from Microsoft SQL Server to its\nColumn\nasset type.\nSource property\nAtlan property\nWhere in Atlan\nCOLUMN_NAME\nname\nasset preview, profile, and filter, overview sidebar\nREMARKS\ndescription\nasset preview, profile, and filter, overview sidebar\nORDINAL_POSITION\norder\nasset profile\nTYPE_NAME\ndataType\nasset preview, profile, and filter, overview sidebar\nCONSTRAINT_TYPE (PRIMARY KEY)\nisPrimary\nasset preview, profile, and filter\nCONSTRAINT_TYPE (FOREIGN KEY)\nisForeign\nasset preview, profile, and filter\nNULLABLE\nisNullable\nasset profile and overview sidebar\nNUMERIC_SCALE\nnumericScale\nasset profile and overview sidebar\nNUMERIC_PRECISION\nprecision\nasset profile and overview sidebar\nRoutines\nâ\nAtlan maps routines in Microsoft SQL Server to its\nProcedure\nasset type. These assets are currently neither published nor discoverable in Atlan.\nSource property\nAtlan property\nWhere in Atlan\nROUTINE_NAME\nname\nAPI only\nREMARKS\ndescription\nAPI only\nPROCEDURE_TYPE\nsubType\nAPI only\nROUTINE_DEFINITION\ndefinition\nAPI only\nTags:\ndata\nintegration\ncrawl\napi\nPrevious\nSet up a private network link to Microsoft SQL Server on Amazon RDS\nNext\nPreflight checks for Microsoft SQL Server\nDatabases\nSchemas\nTables\nViews\nColumns\nRoutines"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/references/what-does-atlan-crawl-from-oracle",
    "content": "Connect data\nDatabases\nSQL Databases\nOracle\nReferences\nWhat does Atlan crawl from Oracle?\nOn this page\nWhat does Atlan crawl from Oracle?\nAtlan crawls and maps the following assets and properties from Oracle.\nDatabases\nâ\nAtlan maps databases from Oracle to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from Oracle to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nTables\nâ\nAtlan maps tables from Oracle to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nTABLE_TYPE\nsubType\nHAS_PARTITIONS\nisPartitioned\nPARTITION_STRATEGY\npartitionStrategy\nPARTITION_COUNT\npartitionCount\nTEMPORARY\nisTemporary\nALIAS\ndisplayName\nViews\nâ\nAtlan maps views from Oracle to its\nView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nVIEW_DEFINITION\ndefinition\nHAS_PARTITIONS\nisPartitioned\nPARTITION_COUNT\npartitionCount\nTEMPORARY\nisTemporary\nALIAS\ndisplayName\nColumns\nâ\nAtlan maps columns from Oracle to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\nTYPE_NAME\ndataType\nCONSTRAINT_TYPES (PRIMARY KEY)\nisPrimary\nCONSTRAINT_TYPES (FOREIGN KEY)\nisForeign\nIS_NULLABLE\nisNullable\nNUMERIC_SCALE\nnumericScale\nCHARACTER_MAXIMUM_LENGTH\nmaxLength\nTags:\ndata\ncrawl\nPrevious\nCrawl Oracle\nNext\nPreflight checks for Oracle\nDatabases\nSchemas\nTables\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/references/what-does-atlan-crawl-from-prestosql",
    "content": "Connect data\nDatabases\nQuery Engines\nPrestoSQL\nReferences\nWhat does Atlan crawl from PrestoSQL?\nOn this page\nWhat does Atlan crawl from PrestoSQL?\nAtlan crawls and maps the following assets and properties from PrestoSQL.\nDid you know?\nAtlan currently only supports PrestoSQL until version 349. PrestoDB is not supported at present.\nDatabases\nâ\nAtlan maps databases from PrestoSQL to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from PrestoSQL to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nTables\nâ\nAtlan maps tables from PrestoSQL to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nViews\nâ\nAtlan maps views from PrestoSQL to its\nView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nEXTRA_INFO (CREATE VIEW)\ndefinition\nColumns\nâ\nAtlan maps columns from PrestoSQL to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\nTYPE_NAME\ndataType\nNULLABLE\nisNullable\nDECIMAL_DIGITS\nprecision\nNUMERIC_SCALE\nnumericScale\nStored procedures\nâ\nAtlan maps stored procedures in PrestoSQL to its\nProcedure\nasset type.\nSource property\nAtlan property\nPROCEDURE_NAME\nname\nREMARKS\ndescription\nPROCEDURE_TYPE\nsubType\nROUTINE_DEFINITION\ndefinition\nTags:\ndata\ncrawl\nPrevious\nCrawl PrestoSQL\nNext\nPreflight checks for PrestoSQL\nDatabases\nSchemas\nTables\nViews\nColumns\nStored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/references/what-does-atlan-crawl-from-sap-ecc",
    "content": "Connect data\nERP\nSAP ECC\nReferences\nWhat does Atlan crawl from SAP ECC?\nOn this page\nWhat does Atlan crawl from SAP ECC?\nPrivate Preview\nAtlan integrates with SAP ECC to crawl and map various asset types, helping you gain insights into structured business data. This page outlines the SAP ECC components that Atlan supports and how their properties are mapped.\nLineage\nâ\nAtlan supports lineage for the following lineage:\nAsset Lineage - Table to View\nColumn Level Lineage - Table Columns to View Columns Assets\nAssets\nâ\nAtlan extracts metadata from SAP ECC across different asset types, including:\nComponents\n:\nSoftware modules providing specific functionalities.\nTables\n:\nStructured storage for master data, transactions, and configurations.\nViews\n:\nLogical representations of data for efficient access.\nColumns\n:\nData attributes within tables and views.\nABAP programs\n:\nCustom scripts written for automation and processing.\nFunction modules\n:\nReusable logic blocks for ABAP programs and remote function calls.\nTransaction Codes\n:\nShortcuts for executing SAP functions.\nThe following sections detail how each asset type is mapped in Atlan.\nComponents\nâ\nSAP ECC components are modular software units that deliver specific business functions. These components form the foundation of SAP's enterprise applications, enabling functionalities such as finance, logistics, and human resources. Atlan maps Components from SAP ECC to its\nSapErpComponent\nasset type.\nSource property\nAtlan property\nTEXT\nname\nNAME\nsapComponentName\nTables\nâ\nTables in SAP ECC store structured business data, including master records, transactional details, and configuration settings. These tables form the foundation of SAP's data storage and retrieval system. Atlan maps Table from SAP ECC to its\nSapErpTable\nasset type.\nSource property\nAtlan property\nTABNAME\nname\nDDTEXT\ndescription\nTABCLASS\nsapErpTableType\nDEVCLASS\nsapPackageName\nColumn Count\nsapFieldCount\nCONTFLAG\nsapErpTableDeliveryClass\nAS4USER\nsourceUpdatedBy\nAS4DATE\nsourceUpdatedAt\nViews\nâ\nViews provide a logical representation of data by combining information from one or more tables. They simplify data access and reporting by allowing users to work with pre-defined, structured datasets. Atlan maps View from SAP ECC to its\nSapErpView\nasset type.\nSource property\nAtlan property\nVIEWNAME\nname\nDDTEXT\ndescription\nVIEWCLASS\nsapErpViewType\nDEVCLASS\nsapPackageName\nColumn Count\nsapFieldCount\nAS4USER\nsourceUpdatedBy\nAS4DATE\nsourceUpdatedAt\nColumns\nâ\nColumns define individual data attributes within tables and views. Each column has a specific data type, length, and constraints, ensuring accurate data representation and integrity. Atlan maps Column from SAP ECC to its\nSapErpColumn\nasset type.\nSource property\nAtlan property\nFIELDNAME\nname\nTABNAME\nsapErpTableName or sapErpViewName\nROLLNAME\nsapErpColumnDataElement\nDATATYPE\nsapDataType\nINTTYPE\nsapErpColumnLogicalDataType\nLENG\nsapErpColumnLength\nDECIMALS\nsapErpColumnDecimals\nKEYFLAG\nsapErpColumnIsPrimary\nCHECKTABLE\nsapErpColumnIsForeign\nNOTNULL\nsapErpColumnIsMandatory\nDEVCLASS\nsapPackageName\nPOSITION\nsapFieldOrder\nABAP programs\nâ\nAdvanced Business Application Programming (ABAP) programs are scripts used to automate processes, manipulate data, and extend SAP functionalities. These programs are written in SAP's proprietary programming language. Atlan maps ABAP Programs from SAP ECC to its\nSapErpAbapProgram\nasset type.\nSource property\nAtlan property\nPROGNAME\nname\nTEXT\ndescription\nSUBC\nsapErpAbapProgramType\nDEVCLASS\nsapPackageName\nCNAM\nsourceCreatedBy\nCDAT\nsourceCreatedAt\nUNAM\nsourceUpdatedBy\nUDAT\nsourceUpdatedAt\nFunction modules\nâ\nFunction modules are reusable code blocks that perform predefined operations in SAP ECC. They can be called within ABAP programs or accessed remotely to execute business logic efficiently. Atlan maps Function Modules from SAP ECC to its\nSapErpFunctionModule\nasset type.\nSource property\nAtlan property\nFUNCNAME\nname\nSTEXT\ndescription\nFUNC_GROUP\nsapErpFunctionModuleGroup\nImport Parameters\nsapErpFunctionModuleImportParams\nImport Parameters Count\nsapErpFunctionModuleImportParamsCount\nExport Parameters\nsapErpFunctionModuleExportParams\nExport Parameters Count\nsapErpFunctionModuleExportParamsCount\nException List\nsapErpFunctionExceptionList\nException List Count\nsapErpFunctionExceptionListCount\nDEVCLASS\nsapPackageName\nTransaction Codes\nâ\nTransaction codes (T-codes) provide quick access to specific SAP functions or screens. Users enter T-codes in the SAP command field to navigate directly to related operations, improving workflow efficiency. Atlan maps Transaction Code from SAP ECC to its\nSapErpTransactionCode\nasset type.\nSource property\nAtlan property\nFUNCNAME\nname\nSTEXT\ndescription\nDEVCLASS\nsapPackageName\nTags:\ndata\ncrawl\nconfiguration\nPrevious\nCrawl SAP ECC\nLineage\nAssets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/references/what-does-atlan-crawl-from-sap-s4hana",
    "content": "Connect data\nERP\nSAP S/4HANA\nReferences\nWhat does Atlan crawl from SAP S/4HANA?\nOn this page\nWhat does Atlan crawl from SAP S/4HANA?\nPrivate Preview\nAtlan integrates with SAP S/4HANA to crawl and map various asset types, helping you gain insights into structured business data. This page outlines the SAP S/4HANA components that Atlan supports and how their properties are mapped.\nLineage\nâ\nAtlan supports the following lineage in SAP S/4HANA:\nAsset lineage:\nTracks relationships from Tables to Views.\nTracks relationships from Tables, Views and CDS Views to CDS Views.\nColumn-level lineage:\nTracks mappings from Table Columns to View Columns.\nAssets\nâ\nAtlan extracts metadata from SAP S/4HANA across different asset types, including:\nComponents\n:\nSoftware modules providing specific functionalities.\nTables\n:\nStructured storage for master data, transactions, and configurations.\nViews\n:\nLogical representations of data for efficient access.\nCDS views\n:\nVirtual data models in SAP S/4HANA that define and consume structured data efficiently.\nColumns\n:\nData attributes within tables and views.\nABAP programs\n:\nCustom scripts written for automation and processing.\nFunction modules\n:\nReusable logic blocks for ABAP programs and remote function calls.\nTransaction Codes\n:\nShortcuts for executing SAP functions.\nThe following sections detail how each asset type is mapped in Atlan.\nComponents\nâ\nSAP S/4HANA components are modular software units that deliver specific business functions. These components form the foundation of SAP's enterprise applications, enabling functionalities such as finance, logistics, and human resources. Atlan maps Components from SAP S/4HANA to its\nSapErpComponent\nasset type.\nSource property\nAtlan property\nTEXT\nname\nNAME\nsapComponentName\nTables\nâ\nTables in SAP S/4HANA store structured business data, including master records, transactional details, and configuration settings. These tables form the foundation of SAP's data storage and retrieval system. Atlan maps Table from SAP S/4HANA to its\nSapErpTable\nasset type.\nSource property\nAtlan property\nTABNAME\nname\nDDTEXT\ndescription\nTABCLASS\nsapErpTableType\nDEVCLASS\nsapPackageName\nColumn Count\nsapFieldCount\nCONTFLAG\nsapErpTableDeliveryClass\nAS4USER\nsourceUpdatedBy\nAS4DATE\nsourceUpdatedAt\nViews\nâ\nViews provide a logical representation of data by combining information from one or more tables. They simplify data access and reporting by allowing users to work with pre-defined, structured datasets. Atlan maps View from SAP S/4HANA to its\nSapErpView\nasset type.\nSource property\nAtlan property\nVIEWNAME\nname\nDDTEXT\ndescription\nVIEWCLASS\nsapErpViewType\nDEVCLASS\nsapPackageName\nColumn Count\nsapFieldCount\nAS4USER\nsourceUpdatedBy\nAS4DATE\nsourceUpdatedAt\nCDS views\nâ\nCDS (Core Data Services) views in SAP S/4HANA are virtual data models that define and consume structured data efficiently. Atlan maps CDS View from SAP S/4HANA to its\nSapErpCdsView\nasset type.\nSource property\nAtlan property\nDDLNAME\nname\nDDTEXT\ndescription\nDEVCLASS\nsapPackageName\nColumn Count\nsapFieldCount\nPS_POSID\nsapComponentName\nAS4DATE\nsourceUpdatedAt\nAS4USER\nsourceUpdatedBy\nColumns\nâ\nColumns define individual data attributes within tables and views. Each column has a specific data type, length, and constraints, ensuring accurate data representation and integrity. Atlan maps Column from SAP S/4HANA to its\nSapErpColumn\nasset type.\nSource property\nAtlan property\nFIELDNAME\nname\nTABNAME\nsapErpTableName or sapErpViewName\nROLLNAME\nsapErpColumnDataElement\nDATATYPE\nsapDataType\nINTTYPE\nsapErpColumnLogicalDataType\nLENG\nsapErpColumnLength\nDECIMALS\nsapErpColumnDecimals\nKEYFLAG\nsapErpColumnIsPrimary\nCHECKTABLE\nsapErpColumnIsForeign\nNOTNULL\nsapErpColumnIsMandatory\nDEVCLASS\nsapPackageName\nPOSITION\nsapFieldOrder\nABAP programs\nâ\nAdvanced Business Application Programming (ABAP) programs are scripts used to automate processes, manipulate data, and extend SAP functionalities. These programs are written in SAP's proprietary programming language. Atlan maps ABAP Programs from SAP S/4HANA to its\nSapErpAbapProgram\nasset type.\nSource property\nAtlan property\nPROGNAME\nname\nTEXT\ndescription\nSUBC\nsapErpAbapProgramType\nDEVCLASS\nsapPackageName\nCNAM\nsourceCreatedBy\nCDAT\nsourceCreatedAt\nUNAM\nsourceUpdatedBy\nUDAT\nsourceUpdatedAt\nFunction modules\nâ\nFunction modules are reusable code blocks that perform predefined operations in SAP S/4HANA. They can be called within ABAP programs or accessed remotely to execute business logic efficiently. Atlan maps Function Modules from SAP S/4HANA to its\nSapErpFunctionModule\nasset type.\nSource property\nAtlan property\nFUNCNAME\nname\nSTEXT\ndescription\nFUNC_GROUP\nsapErpFunctionModuleGroup\nImport Parameters\nsapErpFunctionModuleImportParams\nImport Parameters Count\nsapErpFunctionModuleImportParamsCount\nExport Parameters\nsapErpFunctionModuleExportParams\nExport Parameters Count\nsapErpFunctionModuleExportParamsCount\nException List\nsapErpFunctionExceptionList\nException List Count\nsapErpFunctionExceptionListCount\nDEVCLASS\nsapPackageName\nTransaction codes\nâ\nTransaction codes (T-codes) provide quick access to specific SAP functions or screens. Users enter T-codes in the SAP command field to navigate directly to related operations, improving workflow efficiency. Atlan maps Transaction Code from SAP S/4HANA to its\nSapErpTransactionCode\nasset type.\nSource property\nAtlan property\nFUNCNAME\nname\nSTEXT\ndescription\nDEVCLASS\nsapPackageName\nTags:\ndata\ncrawl\nconfiguration\nPrevious\nCrawl SAP S/4HANA\nLineage\nAssets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/references/what-does-atlan-crawl-from-teradata",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nReferences\nWhat does Atlan crawl from Teradata?\nOn this page\nWhat does Atlan crawl from Teradata?\nAtlan crawls and maps the following assets and properties from Teradata.\nSchemas\nâ\nAtlan maps\nschemas\nfrom Teradata to its\nSchema\nasset type.\nSource property\nAtlan property\ndbc.databases.DatabaseName\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\ndbc.databases.CreatorName\nsourceCreatedBy\ndbc.databases.CreateTimeStamp\nsourceCreatedAt\ndbc.databases.LastAlterName\nsourceUpdatedBy\ndbc.databases.LastAlterTimeStamp\nsourceUpdatedAt\ndbc.databases.CommentString\ndescription\nTables\nâ\nAtlan maps tables from Teradata to its\nTable\nasset type.\nSource property\nAtlan property\ndbc.Tables.TableName\nname\ndbc.TablesV.CommentString\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nCurrentPerm\nsizeBytes\ndbc.TablesV.CreatorName\nsourceCreatedBy\ndbc.TablesV.CreateTimeStamp\nsourceCreatedAt\ndbc.TablesV.LastAlterName\nsourceUpdatedBy\ndbc.TablesV.LastAlterTimeStamp\nsourceUpdatedAt\ndbc.Tables.DataBaseName\nschema name\nViews\nâ\nAtlan maps views from Teradata to its\nView\nasset type.\nSource property\nAtlan property\ndbc.Tables.TableName\nname\ndbc.TablesV.CommentString\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nCurrentPerm\nsizeBytes\ndbc.TablesV.CreatorName\nsourceCreatedBy\ndbc.TablesV.CreateTimeStamp\nsourceCreatedAt\ndbc.TablesV.LastAlterName\nsourceUpdatedBy\ndbc.TablesV.LastAlterTimeStamp\nsourceUpdatedAt\ndbc.Tables.DataBaseName\nschema name\nColumns\nâ\nAtlan maps columns from Teradata to its\nColumn\nasset type.\nSource property\nAtlan property\ndbc.ColumnsV.ColumnName\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\ndbc.ColumnsV.ColumnType\ndataType\ndbc.ColumnsV.Nullable\nisNullable\ndbc.IndicesV.TableName\nisPrimary\ndbc.IndicesV.ChildDB\nisForeign\ndbc.IndicesV.DecimalTotalDigits\nprecision\ndbc.IndicesV.DecimalFractionalDigits\nnumericScale\ndbc.ColumnsV.DataBaseName\nschema name\nTags:\ndata\ncrawl\nPrevious\nSet up on-premises Teradata miner access\nNext\nPreflight checks for Teradata\nSchemas\nTables\nViews\nColumns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/references/what-does-atlan-crawl-from-trino",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nReferences\nWhat does Atlan crawl from Trino?\nOn this page\nWhat does Atlan crawl from Trino?\nAtlan crawls and maps the following assets and properties from Trino.\nDatabases\nâ\nAtlan maps databases from Trino to its\nDatabase\nasset type.\nSource property\nAtlan property\nTABLE_CATALOG\nname\nSCHEMA_COUNT\nschemaCount\nSchemas\nâ\nAtlan maps schemas from Trino to its\nSchema\nasset type.\nSource property\nAtlan property\nTABLE_SCHEMA\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTABLE_CATALOG\ndatabaseName\nTables\nâ\nAtlan maps tables from Trino to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nBYTES\nsizeBytes\nViews\nâ\nAtlan maps views from Trino to its\nView\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nREMARKS\ndescription\nCOLUMN_COUNT\ncolumnCount\nEXTRA_INFO (CREATE VIEW)\ndefinition\nColumns\nâ\nAtlan maps columns from Trino to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nREMARKS\ndescription\nORDINAL_POSITION\norder\nTYPE_NAME\ndataType\nNULLABLE\nisNullable\nDECIMAL_DIGITS\nprecision\nNUMERIC_SCALE\nnumericScale\nStored procedures\nâ\nAtlan maps stored procedures in Trino to its\nProcedure\nasset type.\nSource property\nAtlan property\nPROCEDURE_NAME\nname\nREMARKS\ndescription\nPROCEDURE_TYPE\nsubType\nROUTINE_DEFINITION\ndefinition\nTags:\ndata\ncrawl\nPrevious\nSet up a private network link to Trino\nNext\nPreflight checks for Trino\nDatabases\nSchemas\nTables\nViews\nColumns\nStored procedures"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/faq/jira-integration",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nFAQ\nWhat is included in the Jira integration?\nOn this page\nWhat is included in the Jira integration?\nWith two of your most important workspaces connected, you can save time and improve the way you track issues for your data.\nOnce you integrate Jira with Atlan you can do all of the following   -  all without leaving Atlan!\nCreate new Jira issues on assets\nLink existing Jira issues to assets\nSee the complete history of Jira issues for each asset\nDid you know?\nTo use all of the features outlined, each user needs to\nconnect their individual Jira account\nto their Atlan profile.\nCreate new Jira issues on assets\nâ\ndanger\nYou will first need to\nadd a labels field\nto your\nCreate issue\nform in Jira to be able to create Jira issues on your assets in Atlan.\nTo create issues in Jira, without leaving Atlan:\nFrom any asset, on the right of the screen, click the\nJira\nsidebar icon.\nClick theÂ\nAdd Issue\nbutton, and then\nCreate Issue\n.\nFrom the\nCreating issue\ndialog:\n(Optional) ForÂ\nProject\nselect the project for the issue. (By default, the project\nconfigured during integration\nwill be selected.)\n(Optional) ForÂ\nIssue type\nselect the type of issue being created in Jira.\nForÂ\nTitle\nenter a brief summary of the issue.\nForÂ\nDescription\nenter the detailed explanation of the issue.\nClick theÂ\nCreate\nbutton to submit your issue to Jira.\nYour issue is now on Jira and linked to the asset for future reference! ð\nLink existing Jira issues to assets\nâ\nEver determine that an issue in Jira is related to multiple data assets? It's important to bring that context back to your assets.\nTo add existing issues in Jira into Atlan:\nFrom the asset, on the right of the screen, click the\nJira\nsidebar icon.\nClick theÂ\nAdd Issue\nbutton, and then\nLink Issue\n.\nFrom theÂ\nLink Issues\ndialog:\n(Optional) Search for any issues in Jira to link.\nCheck each issue that should be added to the asset.\nAt the top of the dialog, click the\nLink\nbutton.\nYour existing issue on Jira is now linked to the asset for future reference! ð\nTags:\ndata\nintegration\nfaq\nfaq-integrations\nPrevious\nTroubleshooting Jira\nNext\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nCreate new Jira issues on assets\nLink existing Jira issues to assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/glossary-default-permissions",
    "content": "Build governance\nGlossary\nFAQ\nWhat is the default permission for a glossary?\nWhat is the default permission for a glossary?\nBy default, users can search and discover\nglossaries\nin Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a\nglossary policy\nto control what users can do with glossary metadata and\ncreate a persona\nto curate edit access.\nIn addition, admins can\nturn off the default behavior\n, so that your member and guest users will only have access to the glossaries curated through glossary policies in their personas.\nTags:\ndata\nfaq\nfaq-governance\nPrevious\nCan I add duplicate glossary terms?\nNext\nUse personas to update a term in a glossary"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/personal-data-processor",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nFAQ\nWhen does Atlan become a personal data processor or subprocessor?\nWhen does Atlan become a personal data processor or subprocessor?\nAtlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.\nFor this purpose, where Atlan customers intend to crawl any personal data into the platform, Atlan enters into a data protection agreement with its customers, and as applicable: a\nStandard Contractual Clauses (SCC)\nunder EU GDPR, an\nInternational data transfer agreement (IDTA)\nunder UK GDPR, or a\nHIPAA Business Associate Agreement (BAA)\nwhen dealing with personal health information governed by HIPAA regulations.\nTags:\ndata\ncrawl\nfaq-integrations\nPrevious\nWhat type of user provisioning does Atlan support for SSO integrations?\nNext\nWhy did my users not receive an invite email from Atlan?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/join-options-in-visual-query",
    "content": "Use data\nInsights\nFAQ\nWhy do I only see tables from the same schema to join from in a visual query?\nWhy do I only see tables from the same schema to join from in a visual query?\nWhen\ncreating a visual query\n, Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the\nVisual Query Builder\n.\nTags:\ndata\nfaq\nfaq-insights\nPrevious\nWhat controls the frequency of queries?\nNext\nTroubleshooting bring your own credentials"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/table-level-lineage-only",
    "content": "Use data\nLineage\nFAQ\nWhy is lineage available for table level but not column level?\nWhy is lineage available for table level but not column level?\nThe home icon on top of any asset on the\nlineage graph\nindicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for\nsupported sources\n, click\nview columns\nand then select a column to view data flows for that particular asset.\nTags:\ndata\nfaq\nfaq-lineage\nPrevious\nWhat lineage do you support?\nNext\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?"
  },
  {
    "url": "https://docs.atlan.com/faq/workflows-and-data-processing",
    "content": "Configure Atlan\nFrequently Asked Questions\nWorkflows and Data Processing\nOn this page\nWorkflows and Data Processing\nEverything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan.\nHow do I configure custom cron schedules?\nâ\nYou can use cron expressions to create custom cron schedules for your workflows in Atlan. A cron expression helps you specify the date and time for when a scheduled task must be executed.\nCron expressions consist of five date and time fields separated by white space - there shouldn't be any white spaces within a field value.\nCron expressions in Atlan include the following five fields and corresponding values:\nField name\nAllowed values\nAllowed special characters\nMinutes\n0-59\n,\n-\n*\n/\nHours\n0-23\n,\n-\n*\n/\nDay of the month\n1-31\n,\n-\n*\n/\nMonth\n0-12\n,\n-\n*\n/\nDay of the week\n0-7\n,\n-\n*\n/\n,\n- comma specifies a list of values\n-\n- dash specifies a range of values\n*\n- asterisk specifies all possible values for a field\n/\n- slash is used to skip a given number of values\nExamples of cron expressions and their respective meanings:\n0 0 1 * *\n- Run at midnight on day 1 of every month\n0 0 * * *\n- Run once a day at midnight\n0 */3 * * *\n- Run at minute 0 past every 3rd hour or run every 3 hours\n0 0 1,15 * *\n- Run at midnight on day 1 and 15 of every month\n30 14 * * 1,3\n- Run at 14:30 on Monday and Wednesday\n0 6,18 * * *\n- Run at minute 0 past hours 6 and 18 or run at 6 AM and then 6 PM.\n0 0 1 3,6,9,12 *\n- Run at midnight on day 1 of March, June, September, and December\nWhy is the first miner run taking so long to finish?\nâ\nTypically, the first run of the miner takes longer than usual. This is likely because it's parsing through queries beginning from the date chosen during setup.\nIt's recommended that the start date be no further back than a week. As long as the miner is scheduled to run and running, it continuously picks up and builds lineage as data flows run.\nSubsequent runs must be much quicker in relation to the first run - especially if the miner is set up to run daily. In that case, it only parses through new queries as opposed to historic ones. Keep in mind that the number of queries or transformations running daily can also be a factor in the time it takes for the miner to run.\nTo learn more about miner logic, see\nhere\n.\nAre there any extra steps required for rerunning the miner?\nâ\nThe miner can rerun without any additional steps involved. However, it's possible that the miner errors out when running after a few weeks. If this happens, consider changing the start date of the miner config to be no further back than a week.\nWhy do some workflows take longer?\nâ\nYou can take the following reasons into consideration when accounting for longer workflow runtimes:\nExtracting a high volume of assets from the source may increase the time a workflow takes to complete.\nAtlan workflows run\ndifferential crawls\n, bringing in only delta changes from the source. This speeds things up, and further parallelisation helps optimise runtime.\nOn some days the runtime can exceed the usual duration:\nThere may be more transformations to process, so more delta changes need to be synced to Atlan.\nIf one of those transformations includes a\ndelete\noperation, Atlan archives the removed assets. Archival can take longer and therefore extend the overall runtime.\nFor general guidelines, see\nHow to order workflows\n.\nWhy is the workflow config or new workflow button not working?\nâ\nIf the workflow config page is blank or the\nNew workflow\nbutton doesn't proceed to the next step, try these checks:\nOpen Atlan in an incognito / private-browsing window and see whether the page loads.\nIf it loads, verify whether your browser has any ad-blockers enabled.\nEither disable the ad-blocker or add\n*.atlan.com\nto the allowlist.\nWorkflow is failing with: Delete percentage is more than 80.0. Exiting.\nâ\nAtlan has added guardrails to the workflow execution to make sure that assets don't get archived accidentally. A circuit breaker logic is triggered when 80% of existing assets are missing in the current workflow run. This logic aborts the workflow and prevents it from committing any changes.\nThis situation commonly arises if:\nPermissions for the Atlan integration user are revoked or updated in the source system.\nInclude and exclude metadata filters in the\nworkflow configuration are modified\n.\nAssets are removed from the source system.\nIn case the mass deletion is intentional, please\nreach out to Atlan support\nto disable the circuit breaker. The next run of the workflow proceeds with archiving assets that aren't part of the run. However, note that any metadata updates (tags, descriptions, and more) on these assets are lost.\nHow's lineage from procedures deduced in Atlan?\nâ\nLineage from procedures is inferred indirectly from the\nquery history\ngenerated when the procedure runs.\nCan offline extraction fail if there are spaces in the path?\nâ\nAtlan currently doesn't support spaces in folder names for S3. The offline extraction workflow fails if you include any spaces in the folder name in S3. To follow documented guidelines for safe characters, refer to\nAmazon S3 documentation\n.\nIs the existing file in the bucket overwritten when uploading the JSON files?\nâ\nYes. For dbt Core workflows the recommended approach is to replace the folder with the new\nmanifest.json\nand\nrun_results.json\nfiles. The workflow uses file names to locate its inputs and doesn't check timestamps. (The\ncatalog.json\nfile is no longer required.)\nCan I configure a Snowflake workflow using account usage and then switch to information schema?\nâ\nWhen you\nmodify an existing Snowflake connection\nand change the extraction method, Atlan\ndeletes and recreates\nall assets in that connection. If you need to switch from\nAccount usage\nto\nInformation schema\n, please\ncontact Atlan support\nso the change can be applied safely.\nCan I receive notifications when workflows fail?\nâ\nAtlan can send failure alerts to\nSlack\nand\nMicrosoft Teams\n.\nIn\nAdmin â Integrations\n, open the Slack (or Microsoft Teams) tile and enable\nReceive failure alerts only\nto get a notification whenever a workflow fails.\nCan I create a multi-step approval workflow in Atlan?\nâ\nYes. Atlan integrates with tools such as\nJira\n, enabling you to build multi-step approval workflows that match your organisation's processes.\nIs the PII tagging of data or metadata automated?\nâ\nAtlan propagates\ntags\nbased on hierarchy and lineage. For example, if you\nattach a tag\nnamed\nPII\nto a table and tag propagation is enabled, the tag is copied to downstream columns.\nAtlan doesn't automatically detect PII. Propagation only occurs if you enable it manually or automate it using playbooks. For details, see\nWhy does tag propagation take time to apply?\n.\nAre there any dbt assets that cannot be viewed in dbt?\nâ\nAtlan shows the\nView in dbt\nlink only for dbt models, sources, and tests that include a valid\ntarget_url\n. Assets without a target URL won't display the link.\nCan I follow the background processes of workflows in Argo?\nâ\nIf you have cluster-level access, you can open the built-in Argo UI at\nhttps://your-atlan-domain/argo\nto watch each workflow's DAG, pod logs, and retry status in real time. Otherwise, use the\nHistory\ntab inside the workflow sidebar in Atlan or the run-level logs downloadable from the\nRuns\ntable.\nHow does Atlan work with dbt single-tenant vs multi-tenant?\nâ\nFor dbt Cloud single-tenant projects, Atlan authenticates with the project-scoped API key you provide. For multi-tenant workspaces, Atlan uses your account-level service token and the project ID to pull lineage and documentation. Behaviour in Atlan is identical; the difference is only in where the credentials are scoped in dbt Cloud.\nCloud logging and monitoring\nâ\nAtlan sends application and access logs to your cloud provider's native logging service: CloudWatch (AWS), Stackdriver (GCP), or Azure Monitor. You can ingest these logs into your SIEM for central monitoring. Contact Atlan support to enable log shipping for your tenant.\nTags:\ndata\nconfiguration\nfaq-automation\nPrevious\nUser Management and Access Control"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/how-tos/crawl-gcs",
    "content": "Connect data\nStorage\nGoogle GCS\nCrawl GCS Assets\nCrawl GCS assets\nOn this page\nCrawl GCS assets\nThe Google Cloud Storage crawler fetches assets from Google Cloud Storage and publishes them to Atlan for discovery. The crawler catalogs buckets and objects from your GCS environment.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nCompleted the\nGCS setup guide\n.\nGCS credentials (Project ID and Service Account JSON key) ready.\nDetermined which GCS buckets and prefixes you want to catalog.\nCreate crawler workflow\nâ\nStart by creating a new GCS Assets workflow:\nNavigate to the bottom right of any screen and select\nWorkflow\n.\nSelect\nMarketplace\nfrom the top if you are creating a new workflow, or select\nManage\nif you want to use an existing workflow.\nSelect\nGoogle Cloud Storage\nfrom the package list.\nSelect\nSetup Workflow\n.\nConfigure extraction method\nâ\nChoose how to connect to your GCS environment. Connect directly to GCS using Atlan's credential store:\nAdd the\nProject ID\n(Google Cloud project ID that contains the buckets).\nAdd the\nService Account JSON key\nthat you created in the\nGCS setup guide\n.\nSelect\nTest Authentication\nto verify connectivity.\nSelect\nNext\n.\nConfigure metadata filters\nâ\nSet up filters to control which buckets and objects get cataloged:\nBucket prefix\n: Publish to Atlan only the buckets that start with the specified prefix. Leave empty if you need all buckets.\nObject prefix\n: Publish to Atlan only the objects that start with the specified prefix. Leave empty if you need all objects.\nObject delimiter\n(applicable only if inventory report isn't selected): Use this to list all blobs in a \"folder,\" for example \"public/.\" The delimiter argument restricts results to only the \"files\" in the given \"folder.\" Without the delimiter, the entire tree under the prefix is returned.\nFor example, given these blobs:\na/1.txt\na/b/2.txt\nIf prefix = 'a/', without a delimiter, the following blobs are published to Atlan:\na/1.txt\na/b/2.txt\nHowever, if prefix = 'a/' and delimiter = '/', only the file directly under 'a/' is published to Atlan:\na/1.txt\nBucket exclusion list\n: List of buckets (comma separated) to be excluded.\nConfigure ingestion method\nâ\nChoose how to ingest data from GCS:\nDirect crawling\nInventory report\nConfigure direct crawling options:\nBuild abstraction layer\n: Whether to build abstraction layer on top of files (default: No).\nPublish as-is patterns\n: List of comma-separated patterns to be published as-is (without abstraction layer). Applicable only if Build abstraction layer = Yes.\nRegex to match characters to replace\n: Regular expression to match characters to replace. It acts on the file full name (without bucket prefix).\nRegex with replacement characters\n: Regular expression with replacement characters. It acts on the file full name (without bucket prefix).\nConfigure inventory report options:\nInventory bucket name\n: Bucket where the inventory is stored.\nInventory prefix\n: Prefix within the inventory bucket where the inventory is located.\nInventory file format\n: File format used to generate the inventory report (CSV or Parquet).\nThe following permissions must be granted to the role assigned to the Service Account:\nstorage.buckets.list\n,\nstorage.objects.list\n, and\nroles/storage.objectViewer\n.\nBuild abstraction layer\n: Whether to build abstraction layer on top of files (default: No).\nPublish as-is patterns\n: List of comma-separated patterns to be published as-is (without abstraction layer). Applicable only if Build abstraction layer = Yes.\nRegex to match characters to replace\n: Regular expression to match characters to replace. It acts on the file full name (without bucket prefix).\nRegex with replacement characters\n: Regular expression with replacement characters. It acts on the file full name (without bucket prefix).\nConfigure asset handling\nâ\nControl how assets are created and updated:\nInput handling\n: How to handle assets in the CSV file that don't exist in Atlan:\nCreate full\n: Create a full-fledged asset that can be discovered and maintained like other assets in Atlan.\nCreate partial\n: Create a \"partial\" asset. These are only shown in lineage and can't be discovered through search. These are useful when you want to represent a placeholder for an asset that you lack full context about, but also don't want to ignore completely.\nUpdate only\n: Only update assets that already exist in Atlan, and don't create any asset of any kind. Note: READMEs and links in Atlan are technically separate assetsâthese are still created, even in Update only mode.\nDelta handling\n: Whether to treat the input file as an initial load, full replacement (deleting any existing assets not in the file), or only incremental (no deletion of existing assets).\nRemove attributes\n: How to delete any assets not found in the latest file.\nReload which assets\n: Which assets to reload from the latest input CSV file. Changed assets only calculates which assets have changed between the files and only attempts to reload those changes.\nConfigure connection\nâ\nSet up the connection details:\nConnection\n: Name of the connection that's created in Atlan. The connection name must be unique across all Google Cloud Storage connections.\nNeed help\nâ\nIf you run into issues during the GCS crawling process:\nGCP documentation\n: Refer to the\nGoogle Cloud Storage documentation\nfor detailed information about buckets and objects.\nContact Atlan support\n: For issues related to Atlan integration,\ncontact Atlan support\n.\nSee also\nâ\nWhat does Atlan crawl from GCS\n: Learn about the GCS metadata that Atlan discovers and catalogs.\nTags:\ngcs\ngoogle-gcs\ncrawl\ndata-catalog\nstorage\nPrevious\nSet up Google Cloud Storage\nNext\nWhat does Atlan crawl from Google GCS\nPrerequisites\nCreate crawler workflow\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/crawl-s3",
    "content": "Connect data\nStorage\nAmazon S3\nCrawl S3 Assets\nCrawl S3 assets\nOn this page\nCrawl S3 assets\nCatalog your Amazon S3 buckets and objects in Atlan using the\nS3 Assets\nworkflow. This guide walks you through setting up authentication and running your first crawl.\nPrerequisites\nâ\nBefore you begin, make sure the you have:\nCompleted\nS3 setup\nwith IAM credentials.\nYour AWS credentials (Access Key ID and Secret Access Key, or Role ARN) ready.\nInformation about S3 buckets and prefixes you want to catalog.\nVerified that the AWS account is allowlisted to assume the role when using IAM role-based authentication\nSet up the\ndestination bucket structure\n, required only if you plan to use\ninventory-based ingestion\n.\nSet up workflow\nâ\nCreate a new\nS3 Assets\nworkflow:\nIn the top right, select\nNew\n>\nNew Workflow\n.\nFrom the package list, select\nS3 Assets\n.\nSelect\nSetup Workflow\n.\nConfigure extraction method\nâ\nChoose how to connect to your S3 environment:\nDirect extraction\nAgent extraction\nSelect\nDirect\nfor the extraction method.\nChoose your authentication type:\nIAM User\n: Enter your Access Key ID and Secret Access Key.\nIAM Role\n: Enter your Role ARN.\nSelect the AWS\nRegion\nwhere your buckets are located.\nSelect\nTest Authentication\nto verify the connection.\nSelect\nNext\n.\nSelect\nAgent\nfor the extraction method.\nAdd the secret keys for your secret store configuration.\nFollow the\nSecure Agent configuration guide\n.\nSelect\nNext\n.\nChoose ingestion method\nâ\nSelect your ingestion method:\nDirect ingestion\n: Recommended for fewer than 1 million objects. This method crawls S3 buckets and objects directly.\nInventory ingestion\n: Recommended for large-scale use (more than 1 million objects). Uses inventory reports for efficiency.\nFor inventory ingestion, provide:\nS3 Bucket Name\n: Bucket holding the inventory reports (without the\ns3://\nprefix).\nS3 Bucket Prefix\n: Prefix used in the report configuration. Include a trailing slash (\n/\n). Leave empty if no prefix was used.\nnote\nThe region for the inventory report is picked from the credentials used in the\nextraction method\n.\nConfigure bucket filters\nâ\nChoose which buckets and prefixes to include or exclude. Exclude filters override include filters if both match.\nFor a single bucket\n:\nInclude Bucket\n: Exact bucket name (e.g.,\nmy-data-bucket\n)\nInclude Prefix\n: Specific prefix to crawl (e.g.,\nprocessed/2024/\n)\nLeave all other filters empty.\nFor multiple buckets\n:\nInclude Bucket\n: Regex pattern (e.g.,\nprod-.* | analytics-.*\n)\nExclude Bucket\n: Regex pattern (e.g.,\n.*-temp | .*-backup\n)\nInclude Prefix\n: Prefixes to include (e.g.,\ndata/ | reports/\n)\nExclude Prefix\n: Prefixes to exclude (e.g.,\narchive/ | tmp/\n)\nConfigure connection details\nâ\nEnter a\nConnection Name\nto identify your S3 environment.\nExamples:\nproduction-s3\n,\nanalytics-lake\n,\nraw-data-store\nAssign\nConnection Admins\nto manage access. At least one admin is required.\nRun crawler\nâ\nYou can now start cataloging your assets:\nRun now\n: Select\nRun\nto start a one-time crawl.\nSchedule runs\n: Select\nSchedule & Run\nto automate recurring crawls.\nMonitor crawl progress in the activity log. Once complete, your S3 buckets and objects will appear in Atlan.\nTroubleshooting\nâ\nPermissions\n: Confirm all required IAM permissions are set. See the\nS3 setup guide\nfor details.\nNeed help\nâ\nContact Atlan support\nfor integration issues or assistance.\nSee also\nâ\nWhat Atlan crawls from S3\n: Full list of assets and metadata included in the crawl.\nTags:\ns3\namazon-s3\ncrawl\ndata-catalog\nstorage\nPrevious\nSet up Inventory reports\nNext\nS3 Inventory Report Structure\nPrerequisites\nSet up workflow\nTroubleshooting\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/how-tos/crawl-sap-ecc",
    "content": "Connect data\nERP\nSAP ECC\nCrawl SAP ECC Assets\nCrawl SAP ECC\nOn this page\nCrawl SAP ECC\nOnce you have configured the\nSAP ECC access permissions\n, you can establish a connection between Atlan and your SAP ECC system.\nTo crawl metadata from your SAP ECC system, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select SAP ECC as your source:\nIn the top right corner of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nSAP ECC Assets\n, and click\nSetup Workflow\n.\nProvide credentials\nâ\nAtlan supports using a Secure Agent for fetching metadata from SAP ECC. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nAdd secret keys for your SAP ECC credentials in the linked secret store.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nThe SAP ECC agent configuration requires the following parameters:\nSAP Host Name\n: The hostname or IP address of your SAP ECC system\nSAP User Name\n: The dedicated user account for metadata extraction\nSAP Password\n: The password for the user account\nConnection Type\n: Choose between Application Server or Message Server. If you have chosen Message server, provide below details:\nPort\n: Port number (required if using Message Server)\nGroup\n: Group number (required if using Message Server)\nSAP System Number\n: Two-digit system identifier (00-99)\nSAP Client Number\n: Three-digit client identifier (001-999)\nUse SAP Router\n: Choose\nYes\n, if using a SAP Router, and provide the\nSAP Router String\n.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the SAP ECC connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you don't specify any user or group, nobody can manage the connection - not even admins.\nUse Secure Network Connection\n: Choose\nYes\nto use secure communication. When using a secure connection, prvoide below details:\nSNC Name\n: Your SNC name\nSNC Partner\n: Partner SNC name\nSNC Security Level\n: Security level\nSNC Library Path\n: Path to SNC library\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the SAP ECC crawler, you can configure which components to include or exclude:\nTo select the components you want to include in crawling, click\nInclude Components\nand use the dropdown to select specific SAP components and their subcomponents. You can search for components using the search bar or use the advanced search option.\nTo select the components you want to exclude from crawling, click\nExclude Components\nand use the dropdown to select specific SAP components and their subcomponents to exclude from the crawl.\nThe component selection supports hierarchical browsing, allowing you to expand and collapse component categories to select specific subcomponents.\nYou may choose to configure the crawler to use advance settings by providing below details:\nSAP Language\n: Language code (default: EN)\nSAP Codepage\n: Character encoding (default: 0)\nSAP Unicode System\n: Choose\nYes\nto use unicode\nUnicode Support\n: Choose\nYes\nto enable Unicode support\nSAP Trace\n: Choose\nYes\nto enable trace logging for debugging\nConnection Pool Size\n: Maximum number of connections (default: 5)\nRun the crawler\nâ\nFollow these steps to run the SAP ECC crawler:\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you see the assets in Atlan's asset page! ð\nTags:\nerp\ncrawl\nsetup\nPrevious\nSet up SAP ECC\nNext\nWhat does Atlan crawl from SAP ECC?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/how-tos/crawl-sap-s4hana",
    "content": "Connect data\nERP\nSAP S/4HANA\nCrawl SAP S/4HANA Assets\nCrawl SAP S/4HANA\nOn this page\nCrawl SAP S/4HANA\nOnce you have configured the\nSAP S/4HANA access permissions\n, you can establish a connection between Atlan and your SAP S/4HANA system.\nTo crawl metadata from your SAP S/4HANA system, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select SAP S/4HANA as your source:\nIn the top right corner of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nSAP S/4HANA Assets\n, and click\nSetup Workflow\n.\nProvide credentials\nâ\nAtlan supports using a Secure Agent for fetching metadata from SAP S/4HANA. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nAdd secret keys for your SAP S/4HANA credentials in the linked secret store.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nThe SAP S/4HANA agent configuration requires the following parameters:\nSAP Host Name\n: The hostname or IP address of your SAP S/4HANA system\nSAP User Name\n: The dedicated user account for metadata extraction\nSAP Password\n: The password for the user account\nConnection Type\n: Choose between Application Server or Message Server. If you have chosen Message server, provide below details:\nPort\n: Port number (required if using Message Server)\nGroup\n: Group number (required if using Message Server)\nSAP System Number\n: Two-digit system identifier (00-99)\nSAP Client Number\n: Three-digit client identifier (001-999)\nUse SAP Router\n: Choose\nYes\n, if using a SAP Router, and provide the\nSAP Router String\n.\nClick\nNext\nafter completing the configuration.\nConfigure the connection\nâ\nTo complete the SAP S/4HANA connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might want to use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you don't specify any user or group, nobody can manage the connection - not even admins.\nUse Secure Network Connection\n: Choose\nYes\nto use secure communication. When using a secure connection, prvoide below details:\nSNC Name\n: Your SNC name\nSNC Partner\n: Partner SNC name\nSNC Security Level\n: Security level\nSNC Library Path\n: Path to SNC library\nAt the bottom of the screen, click the\nNext\nbutton to proceed.\nConfigure the crawler\nâ\nBefore running the SAP S/4HANA crawler, you can configure which components to include or exclude:\nTo select the components you want to include in crawling, click\nInclude Components\nand use the dropdown to select specific SAP components and their subcomponents. You can search for components using the search bar or use the advanced search option.\nTo select the components you want to exclude from crawling, click\nExclude Components\nand use the dropdown to select specific SAP components and their subcomponents to exclude from the crawl.\nThe component selection supports hierarchical browsing, allowing you to expand and collapse component categories to select specific subcomponents.\nYou may choose to configure the crawler to use advance settings by providing below details:\nSAP Language\n: Language code (default: EN)\nSAP Codepage\n: Character encoding (default: 0)\nSAP Unicode System\n: Choose\nYes\nto use unicode\nUnicode Support\n: Choose\nYes\nto enable Unicode support\nSAP Trace\n: Choose\nYes\nto enable trace logging for debugging\nConnection Pool Size\n: Maximum number of connections (default: 5)\nRun the crawler\nâ\nFollow these steps to run the SAP S/4HANA crawler:\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you see the assets in Atlan's asset page! ð\nTags:\nerp\ncrawl\nsetup\nPrevious\nSet up SAP S/4HANA\nNext\nWhat does Atlan crawl from SAP S/4HANA?\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/set-up-looker",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nGet Started\nSet up Looker\nOn this page\nSet up Looker\nWho can do this?\nYou will probably need your Looker administrator to run these commands   -  you may not have access yourself.\nAtlan supports two options for user permissions in Looker. You should choose one of these methods to set up Looker:\nAdmin role\nâ\nThis role is required for Atlan to automatically generate lineage across Looker objects. When using this role, the crawler can access all folders in Looker including personal folders.\nTo set up this role:\nLog in to your Looker instance and ensure that you are an Admin user.\nFrom the menu in the upper left, click the\nAdmin\nitem.\nUnder the\nUsers\nsection, click the\nUsers\nitem.\nIn the table, find the user you're logged in as. Click the\nEdit\nbutton to the right of your user's row.\nNext to\nAPI3 Keys\n, click the\nEdit Keys\nbutton.\nOn the resulting\nEdit User API3 Keys\npage, click the\nNew API3 Key\nbutton.\nSave the generated credentials for\ncrawling Looker\n.\nCustom role\nâ\ndanger\nWhen using this approach, Atlan will not automatically generate lineage across Looker objects. You will need to individually allow access to each folder to be included in lineage.\nCreate role\nâ\nTo create a custom role for Atlan to access Looker:\nLog in to your Looker instance.\nFrom the menu in the upper left, click the\nAdmin\nitem.\nUnder the\nUsers\nsection, click the\nRoles\nitem.\nAt the top of the page, click the\nNew Permission Set\nbutton.\nEnter a name for the new permission set.\nFor the permissions, select the following:\naccess_data\nallows access to the other permissions below.\nsee_lookml_dashboards\nallows Atlan to crawl LookML dashboards.\nsee_looks\nallows Atlan to crawl Looks.\nsee_user_dashboards\nallows Atlan to crawl user-defined dashboards.\nexplore\nallows Atlan to fetch from the Explore page.\nsee_sql\nallows Atlan to fetch the SQL of a query or Look, to generate lineage.\nsee_lookml\nallows Atlan to fetch model information from LookML.\ndevelop\nallows Atlan to fetch connection names from models, to generate lineage.\nsee_datagroups\nallows Atlan to fetch all connection names, to generate lineage.\nAt the bottom of the permissions list, click the\nNew Permission Set\nbutton.\nBack on the\nRoles\npage, at the top click the\nNew Role\nbutton.\nEnter a name for the new role.\nFor\nPermission Set\n, select the permission set you created in the previous step.\nFor\nModel Set\n, select the models that you want to give access to.\nAt the bottom of the page click the\nNew Role\nbutton.\nCreate user\nâ\nTo create a user through which Atlan can access Looker:\nOpen the\nAdmin\nmenu in Looker.\nUnder the\nUsers\nsection, click the\nUsers\nitem.\nAt the top of the page, click the\nAdd Users\nbutton.\nFor\nEmail addresses\nenter the email address for the user.\nFor\nSend setup emails\nuncheck the setting.\nFor\nRoles\ncheck the box next to the role you created above.\nAt the bottom of the page, click the\nAdd Users\nbutton.\nOn the resulting page, click the\nDone\nbutton.\nGenerate API key for user\nâ\nTo generate an API key for the user:\nOpen the\nAdmin\nmenu in Looker.\nUnder the\nUsers\nsection, click the\nUsers\nitem.\nIn the table, find the user created above. Click the\nEdit\nbutton to the right of that user's row.\n(Optional) Consider entering a\nFirst Name\nand\nLast Name\nfor the user to make it easier to recognize and find in the future.\nNext to\nAPI3 Keys\n, click the\nEdit Keys\nbutton.\nOn the resulting\nEdit User API3 Keys\npage, click the\nNew API3 Key\nbutton.\nSave the generated credentials for\ncrawling Looker\n.\nInclude folders for lineage\nâ\nTo include folders when using a custom role, give permission using the following steps:\nFrom the Looker menu in the upper left, click the\nAdmin\nitem.\nUnder the\nUsers\nsection, click the\nContent Access\nitem.\nIn the resulting page next to\nFolders\nselect the folder and then click on the\nManage Access...\nbutton.\nIn the blank box at the bottom of the table, select the user created above from the list.\nTo allow Atlan to crawl only dashboards, enable the\nView\npermission for this user.\nTo allow Atlan to crawl tiles and queries for dashboards, enable the\nManage Access, Edit\npermission for this user.\nTo the right of the row for that user, click the\nAdd\nbutton.\nAt the lower-right of the dialog, click the\nSave\nbutton.\ndanger\nYou will need to repeat these steps for\nevery\nfolder you want Atlan to be able to access.\nSSH key for lineage\nâ\nWho can do this?\nAny user with access to the Looker project files in GitHub can set up this part. You will need to share the generated private key with whoever\nsets up the Looker crawler\nin Atlan. If your organization uses single sign-on (SSO) on GitHub, you must first authorize the SSH key for use with SSO. Refer to\nAuthorizing an SSH key for use with SAML single sign-on\nto complete the process.\nIn addition to the role, you also need to set up access to your project files in GitHub for the following:\nGenerate field-level and cross-project lineage from Looker\nCrawl Looker views and build upstream lineage for views and explores\nTo configure an SSH key for access to GitHub project files:\nCreate a new SSH key on your local computer\n. For example, run the following command and enter a passphrase when prompted (or leave blank for no passphrase):\nssh-keygen -t ed25519 -C \"\n[email protected]\n\" -f ~/.ssh/atlan_looker_lineage\nCopy the generated keys from your local computer\n. For example:\nTo copy the public key, run this command and copy the output:\ncat ~/.ssh/atlan_looker_lineage.pub\nTo copy the private key, run this command and copy the output:\ncat ~/.ssh/atlan_looker_lineage\nIn the upper-right corner of any GitHub page, click your profile photo, then click\nSettings\n.\nUnder the\nAccess\nsection of the left sidebar, clickÂ\nSSH and GPG keys\n.\nIn the upper-right, click the\nNew SSH key\nbutton:\nFor\nTitle\nenter a descriptive label for the new key. For example,\nAtlan Lineage\n.\nFor\nKey\npaste in the public key you copied above.\nAt the bottom of the form, click theÂ\nAdd SSH key\nbutton.\nIf prompted, enter your GitHub password and clickÂ\nConfirm password\n.\nTags:\ncrawl\napi\nPrevious\nLooker\nNext\nSet up on-premises Looker access\nAdmin role\nCustom role\nSSH key for lineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/set-up-trino",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nGet Started\nSet up Trino\nOn this page\nSet up Trino\nWho can do this?\nYou will probably need your Trino administrator to run these commands   -  you may not have access yourself.\nCurrently we only support basic (username and password) authentication for Trino. We recommend creating a separate user for Atlan with read-only access. Please ensure you are using frontend password authentication over HTTPS for clients.\nCreate user in Trino\nâ\nTo create a new user with password file authentication follow\nthe steps in the official Trino documentation\n.\nGrant read-only access\nâ\nTo grant read-only access to the user created above follow\nthe steps in the official Trino documentation\n. This includes adding a list of catalogs you wish to crawl to your\nrules.json\nfile, for example:\n{\n\"catalogs\"\n:\n[\n{\n\"user\"\n:\n\"atlan\"\n,\n\"catalog\"\n:\n\"postgresql\"\n,\n\"allow\"\n:\n\"read-only\"\n}\n,\n{\n\"user\"\n:\n\"atlan\"\n,\n\"catalog\"\n:\n\"mysql\"\n,\n\"allow\"\n:\n\"read-only\"\n}\n,\n...\n]\n}\nTags:\ncrawl\nauthentication\nPrevious\nTrino\nNext\nCrawl Trino\nCreate user in Trino\nGrant read-only access"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/references/what-does-atlan-crawl-from-s3",
    "content": "Connect data\nStorage\nAmazon S3\nReferences\nWhat does Atlan crawl from Amazon S3\nOn this page\nWhat does Atlan crawl from Amazon S3\nThis document provides complete details on the S3 assets and properties that Atlan crawls and maps during the S3 cataloging process. Atlan supports two ingestion modes with different property coverage.\nBuckets\nâ\nAtlan maps buckets from S3 to its\nS3Bucket\nasset type.\nSource Property\nAtlan Property\nIngestion Mode\nBucket Name\nname\nDirect, Inventory\nObject Count\ns3ObjectCount\nDirect, Inventory\nConsole URL\nsourceURL\nDirect\nRegion\nawsRegion\nDirect\nServerSideEncryptionConfiguration\ns3Encryption\nDirect\nVersioning Status\ns3BucketVersioningEnabled\nDirect\nObjects\nâ\nAtlan maps objects from S3 to its\nS3Object\nasset type.\nSource Property\nAtlan Property\nIngestion Mode\nObject Name\nname\nDirect, Inventory\nObject Key\ns3ObjectKey\nDirect, Inventory\nBucket Name\ns3BucketName\nDirect, Inventory\nObject Size (bytes)\ns3ObjectSize\nDirect, Inventory\nLastModifiedDate\ns3ObjectLastModifiedTime\nDirect, Inventory\nStorageClass\ns3ObjectStorageClass\nDirect, Inventory\nETag\ns3ETag\nDirect, Inventory\nConsole URL\nsourceURL\nDirect\nRegion\nawsRegion\nDirect\nContent Type\ns3ObjectContentType\nDirect\nContent Disposition\ns3ObjectContentDisposition\nDirect\nVersion ID\ns3ObjectVersionId\nDirect\nSee also\nâ\nSet up S3\n: Configure AWS permissions and environment\nCrawl S3 assets\n: Step-by-step guide for setting up S3 crawling\nTags:\ns3\namazon-s3\ncrawl\ndata-catalog\nreference\nassets\nproperties\nPrevious\nS3 Inventory Report Structure\nBuckets\nObjects\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/references/what-does-atlan-crawl-from-gcs",
    "content": "Connect data\nStorage\nGoogle GCS\nReferences\nWhat does Atlan crawl from Google GCS\nOn this page\nWhat does Atlan crawl from Google GCS\nThis document provides complete details on the GCS assets and properties that Atlan crawls and maps during the GCS cataloging process. Atlan supports two ingestion modes with different property coverage.\nBuckets\nâ\nAtlan maps buckets from GCS to its\nGCSBucket\nasset type.\nSource Property\nAtlan Property\nBucket Name\nname\nObject Count\ngcsObjectCount\nConsole URL\nsourceURL\nObjects\nâ\nAtlan maps objects from GCS to its\nGCSObject\nasset type.\nSource Property\nAtlan Property\nObject Name\nname\nObject Key\ngcsObjectKey\nObject Size (bytes)\ngcsObjectSize\nObject updated\nsourceUpdatedAt\nObject created\nsourceCreatedAt\nGcs object content type\ngcsObjectContentType\nObject URL\nsourceURL\nSee also\nâ\nSet up GCS\n: Configure GCS permissions and environment\nCrawl GCS assets\n: Step-by-step guide for setting up GCS crawling\nTags:\ngcs\ngoogle-gcs\ncrawl\ndata-catalog\nreference\nassets\nproperties\nPrevious\nCrawl GCS assets\nBuckets\nObjects\nSee also"
  },
  {
    "url": "https://docs.atlan.com/tags/databricks",
    "content": "6 docs tagged with \"databricks\"\nView all tags\nDatabricks\nIntegrate, catalog, and govern Databricks assets in Atlan.\nDatabricks Data Quality Studio\nSet up and configure Databricks for data quality monitoring through Atlan.\nEnable data quality on connection\nEnable and configure data quality for your Databricks connection in Atlan.\nSet up cross-workspace extraction\nConfigure a single service principal to crawl metadata from all workspaces within a Databricks metastore using system tables\nSet up Databricks\nConfigure Databricks to enable data quality monitoring through Atlan.\nSetup and configuration\nCommon questions about Databricks data quality setup and configuration."
  },
  {
    "url": "https://docs.atlan.com/tags/connector",
    "content": "63 docs tagged with \"connector\"\nView all tags\nAiven Kafka\nIntegrate, catalog, and govern Aiven Kafka assets in Atlan.\nAlteryx\nIntegrate, catalog, and govern Alteryx assets in Atlan using OpenLineage.\nAmazon Athena\nIntegrate, catalog, and govern Amazon Athena assets in Atlan.\nAmazon DynamoDB\nIntegrate, catalog, and govern Amazon DynamoDB assets in Atlan.\nAmazon MSK\nIntegrate, catalog, and govern Amazon MSK assets in Atlan.\nAmazon MWAA OpenLineage\nIntegrate, catalog, and visualize Amazon MWAA lineage in Atlan.\nAmazon QuickSight\nIntegrate, catalog, and govern Amazon QuickSight assets in Atlan.\nAmazon Redshift\nIntegrate, catalog, and govern Amazon Redshift assets in Atlan.\nAnomalo\nIntegrate, catalog, and govern Anomalo assets in Atlan.\nApache Airflow OpenLineage\nIntegrate, catalog, and visualize Apache Airflow lineage in Atlan.\nApache Kafka\nIntegrate, catalog, and govern Apache Kafka assets in Atlan.\nApache Spark OpenLineage\nIntegrate, catalog, and visualize Apache Spark lineage in Atlan.\nAstronomer OpenLineage\nIntegrate, catalog, and visualize Astronomer lineage in Atlan.\nAWS Glue\nIntegrate, catalog, and govern AWS Glue assets in Atlan.\nBigID\nIntegrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata.\nCloudera Impala\nIntegrate, catalog, and govern Cloudera Impala assets in Atlan.\nConfluent Kafka\nIntegrate, catalog, and govern Confluent Kafka assets in Atlan.\nConfluent Schema Registry\nIntegrate, catalog, and govern Confluent Schema Registry assets in Atlan.\nCrateDB\nIntegrate, catalog, and govern CrateDB assets in Atlan.\nDagster\nIntegrate, catalog, and visualize Dagster lineage in Atlan.\nDatabricks\nIntegrate, catalog, and govern Databricks assets in Atlan.\nDataStax Enterprise\nCatalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data.\ndbt\nIntegrate, catalog, and govern dbt assets in Atlan.\nDomo\nIntegrate, catalog, and govern Domo assets in Atlan.\nFivetran\nIntegrate, catalog, and govern Fivetran assets in Atlan.\nGoogle BigQuery\nIntegrate, catalog, and govern Google BigQuery assets in Atlan.\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan.\nGoogle Cloud Storage\nIntegrate, catalog, and govern Google Cloud Storage assets in Atlan.\nHive\nCatalog and govern Hive assets in Atlan for discovery and governance.\nIBM Cognos Analytics\nIntegrate, catalog, and govern IBM Cognos Analytics assets in Atlan.\nInformatica CDI\nIntegrate, catalog, and govern Informatica Cloud Data Integration assets in Atlan.\nLooker\nIntegrate, catalog, and govern Looker assets in Atlan.\nMatillion\nIntegrate, catalog, and govern Matillion assets in Atlan.\nMetabase\nIntegrate, catalog, and govern Metabase assets in Atlan.\nMicrosoft Azure Cosmos DB\nCatalog and govern Cosmos DB assets in Atlan for discovery and governance.\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan.\nMicrosoft Azure Event Hubs\nIntegrate, catalog, and govern Azure Event Hubs assets in Atlan.\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan.\nMicrosoft Power BI\nIntegrate, catalog, and govern Power BI assets in Atlan.\nMicrosoft SQL Server\nIntegrate, catalog, and govern Microsoft SQL Server assets in Atlan.\nMicroStrategy\nIntegrate, catalog, and govern MicroStrategy assets in Atlan.\nMode\nIntegrate, catalog, and govern Mode assets in Atlan.\nMongoDB\nCatalog and govern MongoDB assets in Atlan for discovery and governance.\nMonte Carlo\nIntegrate, catalog, and govern Monte Carlo assets in Atlan.\nMySQL\nIntegrate, catalog, and govern MySQL assets in Atlan.\nOracle\nIntegrate, catalog, and govern Oracle assets in Atlan.\nPostgreSQL\nIntegrate, catalog, and govern PostgreSQL assets in Atlan.\nPrestoSQL\nIntegrate, catalog, and govern PrestoSQL assets in Atlan.\nQlik Sense Cloud\nIntegrate, catalog, and govern Qlik Sense Cloud assets in Atlan.\nQlik Sense Enterprise (Windows)\nIntegrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan.\nRedash\nIntegrate, catalog, and govern Redash assets in Atlan.\nRedpanda Kafka\nIntegrate, catalog, and govern Redpanda Kafka assets in Atlan.\nSalesforce\nIntegrate, catalog, and govern Salesforce assets in Atlan.\nSAP ECC\nIntegrate, catalog, and govern SAP ECC assets in Atlan.\nSAP HANA\nCatalog and govern SAP HANA assets in Atlan for discovery and governance.\nSAP S/4HANA\nIntegrate, catalog, and govern SAP S/4HANA assets in Atlan.\nSigma\nIntegrate, catalog, and govern Sigma assets in Atlan.\nSisense\nIntegrate, catalog, and govern Sisense assets in Atlan.\nSoda\nIntegrate, catalog, and govern Soda assets in Atlan.\nTableau\nIntegrate, catalog, and govern Tableau assets in Atlan.\nTeradata\nCatalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage.\nThoughtSpot\nIntegrate, catalog, and govern ThoughtSpot assets in Atlan.\nTrino\nIntegrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-warehouse",
    "content": "4 docs tagged with \"data warehouse\"\nView all tags\nAmazon Redshift\nIntegrate, catalog, and govern Amazon Redshift assets in Atlan.\nDatabricks\nIntegrate, catalog, and govern Databricks assets in Atlan.\nGoogle BigQuery\nIntegrate, catalog, and govern Google BigQuery assets in Atlan.\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/cross-workspace-extraction",
    "content": "One doc tagged with \"cross-workspace-extraction\"\nView all tags\nSet up cross-workspace extraction\nConfigure a single service principal to crawl metadata from all workspaces within a Databricks metastore using system tables"
  },
  {
    "url": "https://docs.atlan.com/tags/rest-api",
    "content": "25 docs tagged with \"rest-api\"\nView all tags\nAPI authentication\nLearn about api authentication.\nAtlan's open API\nLearn about atlan's open api.\nConnectors and capabilities\nLearn about connectors and capabilities.\nGenerate HAR files and console logs\nAtlan is built on [REST APIs](https://apidocs.atlan.com/), so you can see the requests being sent by the UI to the API gateway through your browser's developer console.\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nSet up an AWS private network link to Databricks\nFor all details, see [Databricks documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\nSet up Qlik Sense Cloud\n:::warning Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps - you may not have access yourself.\nSet up Qlik Sense Enterprise on Windows\n:::warning Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps - you may not have access yourself.\nSet up Sigma\n:::warning Who can do this? You will probably need your Sigma administrator to complete these steps - you may not have access yourself.\nSoftware development kits (SDKs)\nLearn about software development kits (sdks).\nSupport and Technical Help\nComplete guide to getting support, understanding API limits, and accessing technical assistance for Atlan.\nSupported sources\nLearn about supported sources.\nTroubleshooting Databricks connectivity\nLearn about troubleshooting databricks connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Matillion connectivity\nLearn about troubleshooting matillion connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting Sigma connectivity\nLearn about troubleshooting sigma connectivity.\nTroubleshooting Trino connectivity\nLearn about troubleshooting trino connectivity.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/graphql",
    "content": "25 docs tagged with \"graphql\"\nView all tags\nAPI authentication\nLearn about api authentication.\nAtlan's open API\nLearn about atlan's open api.\nConnectors and capabilities\nLearn about connectors and capabilities.\nGenerate HAR files and console logs\nAtlan is built on [REST APIs](https://apidocs.atlan.com/), so you can see the requests being sent by the UI to the API gateway through your browser's developer console.\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nProvider package versions for OpenLineage\nLearn about provider package versions for openlineage.\nSet up an AWS private network link to Databricks\nFor all details, see [Databricks documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\nSet up Qlik Sense Cloud\n:::warning Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps - you may not have access yourself.\nSet up Qlik Sense Enterprise on Windows\n:::warning Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps - you may not have access yourself.\nSet up Sigma\n:::warning Who can do this? You will probably need your Sigma administrator to complete these steps - you may not have access yourself.\nSoftware development kits (SDKs)\nLearn about software development kits (sdks).\nSupport and Technical Help\nComplete guide to getting support, understanding API limits, and accessing technical assistance for Atlan.\nSupported sources\nLearn about supported sources.\nTroubleshooting Databricks connectivity\nLearn about troubleshooting databricks connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Matillion connectivity\nLearn about troubleshooting matillion connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting Sigma connectivity\nLearn about troubleshooting sigma connectivity.\nTroubleshooting Trino connectivity\nLearn about troubleshooting trino connectivity.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/troubleshooting/troubleshooting-usage-and-popularity-metrics",
    "content": "Use data\nUsage & Popularity\nTroubleshooting\nTroubleshooting usage and popularity metrics\nOn this page\nTroubleshooting usage and popularity metrics\nAtlan currently supports\nusage and popularity metrics\nfor the following connectors:\nAmazon Redshift\n-  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source.\nDatabricks\n-  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the\nDatabricks APIs\n.\nGoogle BigQuery\n-  tables, views, and columns\nMicrosoft Power BI\n-  reports and dashboards\nSnowflake\n-  tables, views, and columns\nCan any user set this up?\nâ\nThere is no separate setup required. It is bundled with the\nAmazon Redshift\n,\nDatabricks\n,\nGoogle BigQuery\n,\nMicrosoft Power BI\n, and\nSnowflake\nminer packages. As long as the miner is set up, popularity will be calculated.\nWhich editions of Snowflake are supported?\nâ\nThe following Snowflake editions are supported:\nStandard Edition\nEnterprise Edition\nBusiness Critical Edition\nVirtual Private Snowflake (VPS)\nDo account-level permissions need to be modified for setup?\nâ\nNo extra permissions are required.\nFor enterprise customers, Atlan will use the\nACCESS_HISTORY\ntable to determine which objects have been accessed   -  keeping the data source as the source of truth.\nFor other customers, Atlan will use internal logic to determine the same.\nHow can I set up popularity on my instance?\nâ\nFirst-time setup:\nHead over to the Atlan marketplace and set up the miner package for a\nsupported connector\nwith a daily frequency (recommended).\nOn the first run, popularity will be calculated from the start date of the miner with a default window of 14 days.\nFor all subsequent runs, the popularity window will be increased to a maximum limit of 30 days.Â\nFor preconfigured miners:\nThe next run of the miner will migrate the last 30 days worth of query data and calculate popularity for 30 days (if available).\nSubsequent runs will work as expected.\nHow is the popularity score calculated?\nâ\nThe computation of popularity score is based on the number of read queries that used the data asset and the number of distinct users executing the read queries. Values are collected over a period of 30 days.\nPopularity score = number of distinct readers * log (total number of read queries)\nWhat asset types have popularity scores?\nâ\nPopularity scores are currently available for all\nAmazon Redshift\n,\nDatabricks\n,\nGoogle BigQuery\n, and\nSnowflake\ntables, views, and columns, and\nMicrosoft Power BI\nreports and dashboards.\nHow is the compute cost estimated per asset?\nâ\nAmazon Redshift\nâ\nCompute cost for Amazon Redshift assets are currently unavailable due to limitations at source.\nDatabricks\nâ\nCompute cost for Databricks assets are currently unavailable due to limitations of the\nDatabricks APIs\n.\nGoogle BigQuery\nâ\nIf the\nGoogle BigQuery miner\nis configured to use:\nOn-demand pricing   -  the\nbytes\nfield will indicate the total bytes billed for the query.\nFlat-rate pricing   -  the\nslot-ms\n(slot-milliseconds) field will indicate the total slot utilization for the query. For example, if a 20-second query is continuously consuming 4 slots, then the query will have utilized 80,000 slot-ms.\nIn Atlan, the compute cost of a Google BigQuery asset is estimated based on each individual query. For example:Â\nQuery A ran on asset x = 10 slot-ms or bytes\nQuery B ran on asset x = 15 slot-ms or bytes\nEstimated compute cost for asset x = 10 + 15 = 25 slot-ms or bytes\nLearn more about\nGoogle BigQuery pricing\n.\nSnowflake\nâ\nIn Atlan, the compute cost of a Snowflake asset is estimated based on each individual query. For example, if warehouse\nX-small\ncosts 1 credit per hour:Â\nQuery A ran from 1 p.m. to 5 p.m. on warehouse\nX-small\n= 4 credits\nQuery B ran from 11 a.m. to 3 p.m. on warehouse\nX-small\n= 4 credits\nEstimated compute cost: 4 credits + 4 credits = 8 credits\nCan we exclude queries run by ETL users?\nâ\nAtlan supports excluding queries by users. You can exclude users while configuring the miner behavior for all supported connectors   -\nAmazon Redshift\n,\nDatabricks\n,\nGoogle BigQuery\n,\nMicrosoft Power BI\n, and\nSnowflake\n.\nWhy are metrics missing even after running the miner?\nâ\nUsage and popularity metrics are computed using both the number of queries and the number of users who have queried that asset in the last 30 days. If an asset has not been queried in the last 30 days, there will be no usage and popularity metrics to report for that asset.\nIf an asset has been queried and you've run the miner but metrics are still missing, then it may be due to the miner logic at work.\nWhen setting up the miner for the first time, you will need to provide a start date   -  ranging from the last two days up to past two weeks of query history. If an asset has not been queried during the selected time period, usage and popularity metrics will be unavailable.\nFor subsequent runs, the miner will fetch query history based on the following logic:\nSTART_TIME\nâ¤\nCURRENT_DATE\n-\nINTERVAL '1 DAY'\nFor example, the miner logic for January 23 will be:\nJan 22 5 p.m. â¤ Jan 23 00:00 - 1 day\nJan 22 5 p.m. â¤ Jan 22 00:00\nThe miner will not fetch the data for the previous day (January 22) on the current day (January 23). Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a query session.\nWhat type of queries do you show for popular, slow, or expensive queries?\nâ\nOnly read queries or\nSELECT\nstatements are shown for popular, slow, and expensive queries. DDL and DML statements are not supported.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nHow to interpret usage metrics"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity",
    "content": "Use data\nUsage & Popularity\nOn this page\nUsage and Popularity\nOverview:\nUsage and popularity metrics in Atlan help you understand how your data assets are being used. Track asset usage, identify popular assets, and make data-driven decisions about your data landscape.\nGet started\nâ\nHow to interpret usage metrics\nGuides\nâ\nUsage analysis\nâ\nHow to find assets by usage\n: Discover assets based on their usage patterns.\nTroubleshooting\nâ\nTroubleshooting usage metrics\n: Solutions for common usage tracking issues.\nTags:\nusage\npopularity\nmetrics\ncapabilities\nNext\nHow to find assets by usage\nGet started\nGuides\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/authentication",
    "content": "Secure Agent 2.0\nConcepts\nAuthentication\nOn this page\nAuthentication\nSecure Agent 2.0 applications use OAuth 2.0 client credentials flow for communicating with the Atlan SaaS deployment.\nEach deployed application uses an independent Client ID and Client Secret combination. The Client ID and Secret is used to generate short-lived tokens which are used for Secure Agent to Atlan SaaS interactions.\nHow authentication works\nâ\nThe authentication process follows a continuous token refresh cycle:\nApplication uses Client ID and Client Secret to request a JWT token from Atlan tenant's authentication service\nThe JWT token is valid for 15 minutes and is used for all Atlan service communications\nBefore token expiration, application requests a new token automatically and uses it for further interactions.\nWithout a valid token, application can't communicate with the Atlan SaaS deployment\nWhat tokens enable\nâ\nThe authentication system provides these capabilities:\nUses OAuth 2.0 client credentials flow per\nOAuth 2.0 Security - RFC 9700\nClient credentials can be rotated on-demand\nApplication registration with Atlan SaaS tenant\nConnecting to Atlan's hosted Temporal orchestrator service\nTransferring application outcome (for example metadata) to Atlan\nSee also\nâ\nSecret management\n: How OAuth 2.0 credentials are stored and retrieved from enterprise secret stores.\nSecurity\n: Overall security architecture including network isolation and container hardening.\nTags:\nsecure-agent\nauthentication\noauth\ntokens\nPrevious\nConfigure network security\nNext\nData transfer and observability\nHow authentication works\nWhat tokens enable\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda/how-tos/create-an-aws-lambda-trigger",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAWS Lambda\nCreate an AWS Lambda trigger\nOn this page\nCreate an AWS Lambda trigger\nOnce you have configured the\nAWS Lambda permissions\n, you can run an AWS Lambda function.\nTo run a Lambda function, complete the following s\nteps.\nSelect the utility\nâ\nTo select the AWS Lambda trigger utility:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the filters along the top, click\nUtility\n.\nFrom the list of packages, select\nAWS Lambda Trigger\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nTo enter your AWS credentials:\nFor\nAuthentication\nchoose the method to authenticate with AWS:\nFor\nIAM User\nauthentication\n, enter the\nAWS Access Key\n,\nAWS Secret Key\n, and\nRegion\nof AWS to use.\nFor\nIAM Role\nauthentication\n, enter theÂ\nRegion\nof AWS and (optional)\nAWS Role ARN\nto use.\nClick\nTest Authentication\nto confirm connectivity to AWS using these details.\nWhen successful, at the bottom of the screen click\nNext\n.\nConfigure the Lambda function\nâ\nTo configure the Lambda function:\nUnder\nFunction ARN\nenter the ARN for the Lambda function to call.\n(Optional) Under\nQualifier\nenter a specific version of the Lambda function to call. (Or leave this as\n$LATEST\nto always run the latest version of the function.)\n(Optional) Under\nPayload\nenter a minimized (compact) form of any\nJSON payload\nto pass to the Lambda function. (Leave this as an empty JSON object\n{}\nif you have nothing to pass to the Lambda function.)\nUnder\nInvocation Type\nselect how you would like call the Lambda function:\nUse\nSynchronously\nto use\nsynchronous invocation\n. With this approach, the response body and headers include details about the response, including errors.\nUse\nAsynchronously\nto use\nasynchronous invocation\n. With this approach, AWS queues events and they could be skipped or processed more than once.\nRun the Lambda function\nâ\nYou can now run the Lambda function.\nAt the bottom of the screen, click\nRun\nto run the function once, immediately.\nClick\nSchedule & Run\nto scheduled the function to run hourly, daily, weekly, or monthly.\nTags:\nintegration\nauthentication\nsetup\nPrevious\nSet up AWS Lambda\nNext\nAlways On\nSelect the utility\nProvide credentials\nConfigure the Lambda function\nRun the Lambda function"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/pingfederate-404-error",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nTroubleshooting\nPingFederate SSO 404 error\nOn this page\nPingFederate SSO 404 error\nWhy do I get a 404 error when using PingFederate SSO?\nâ\nIf you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion.\nTo\nreconfigure the signature policy\n:\nNavigate to the\nSignature Policy\ntab in PingFederate and uncheck the\nAlways Sign Assertion\nbox.\nBy disabling the\nAlways Sign Assertion\nsetting, the authentication request will be able to proceed without requiring a signed assertion.\nIf you encounter any further issues or have questions, refer to the\nPingFederate documentation\nor\ncontact Atlan support\nfor assistance.\nTags:\nintegration\nauthentication\nfaq-integrations\nPrevious\nTroubleshooting connector-specific SSO authentication\nNext\nOkta first-time login authentication error"
  },
  {
    "url": "https://docs.atlan.com/tags/power-bi",
    "content": "One doc tagged with \"power bi\"\nView all tags\nMicrosoft Power BI\nIntegrate, catalog, and govern Power BI assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/business-intelligence",
    "content": "15 docs tagged with \"business intelligence\"\nView all tags\nAmazon QuickSight\nIntegrate, catalog, and govern Amazon QuickSight assets in Atlan.\nDomo\nIntegrate, catalog, and govern Domo assets in Atlan.\nIBM Cognos Analytics\nIntegrate, catalog, and govern IBM Cognos Analytics assets in Atlan.\nLooker\nIntegrate, catalog, and govern Looker assets in Atlan.\nMetabase\nIntegrate, catalog, and govern Metabase assets in Atlan.\nMicrosoft Power BI\nIntegrate, catalog, and govern Power BI assets in Atlan.\nMicroStrategy\nIntegrate, catalog, and govern MicroStrategy assets in Atlan.\nMode\nIntegrate, catalog, and govern Mode assets in Atlan.\nQlik Sense Cloud\nIntegrate, catalog, and govern Qlik Sense Cloud assets in Atlan.\nQlik Sense Enterprise (Windows)\nIntegrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan.\nRedash\nIntegrate, catalog, and govern Redash assets in Atlan.\nSigma\nIntegrate, catalog, and govern Sigma assets in Atlan.\nSisense\nIntegrate, catalog, and govern Sisense assets in Atlan.\nTableau\nIntegrate, catalog, and govern Tableau assets in Atlan.\nThoughtSpot\nIntegrate, catalog, and govern ThoughtSpot assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/upstream-dependencies",
    "content": "13 docs tagged with \"upstream-dependencies\"\nView all tags\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nData and metadata persistence\nLearn about data and metadata persistence.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nLineage Generator (no transformations)\nLearn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting Microsoft Power BI connectivity\nLearn about troubleshooting microsoft power bi connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?."
  },
  {
    "url": "https://docs.atlan.com/tags/data-sources",
    "content": "13 docs tagged with \"data-sources\"\nView all tags\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nConnectors\nLearn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management.\nData and metadata persistence\nLearn about data and metadata persistence.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nHow can Atlan generate upstream lineage from the data warehouse layer?\nLearn about how can atlan generate upstream lineage from the data warehouse layer?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting Microsoft Power BI connectivity\nLearn about troubleshooting microsoft power bi connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nTroubleshooting Qlik Sense Cloud connectivity\nLearn about troubleshooting qlik sense cloud connectivity.\nTroubleshooting ThoughtSpot connectivity\nLearn about troubleshooting thoughtspot connectivity.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?."
  },
  {
    "url": "https://docs.atlan.com/tags/model",
    "content": "13 docs tagged with \"model\"\nView all tags\nAdd impact analysis in GitHub\nLearn about add impact analysis in github.\nAdd impact analysis in GitLab\nLearn about add impact analysis in gitlab.\nAtlan AI security\nAtlan uses [Azure OpenAI Service](https://azure.microsoft.com/en-in/products/cognitive-services/openai-service) to power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model.\nData Models\nData models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network.\nMigrate from dbt to Atlan action\nThe dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/.\nSet up dbt Cloud\n:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.\nSet up Local MCP Server\nThe Atlan Model Context Protocol (MCP) server enables seamless interaction between external tools and Atlan services. It defines the data structures and conventions used to prepare and transmit context for model execution, making it easier to integrate Atlan into your existing workflows.\nTroubleshooting data models\nWhat are the known limitations of data models in Atlan?\nview data models\nOnce you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:.\nWhat does Atlan crawl from Looker?\nAtlan crawls and maps the following assets and properties from Looker.\nWhat does Atlan crawl from Sisense?\nAtlan crawls and maps the following assets and properties from Sisense.\nWhat is the difference between a Power BI data source and dataflow?\nLearn about what is the difference between a power bi data source and dataflow?.\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?\nAtlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/releases",
    "content": "One doc tagged with \"releases\"\nView all tags\nProduct release stages\nLearn about product release stages."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-integrations",
    "content": "17 docs tagged with \"faq-integrations\"\nView all tags\nCan Atlan integrate with multiple Azure AD tenants within a single instance?\nLearn about can atlan integrate with multiple azure ad tenants within a single instance?.\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nRefer to our [troubleshooting Jira documentation](/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira) to learn more.\nCan site renaming affect the Jira integration?\nLearn about can site renaming affect the jira integration?.\nCan we use a Microsoft SSO login?\nLearn about can we use a microsoft sso login?.\nGoogle Dashboard login error\nLearn about why do i get an error while logging in via google dashboard?.\nHow are product updates deployed?\nLearn about how are product updates deployed?.\nHow do I send messages or search assets from Slack?\nSending messages and searching assets from Slack are disabled. Refer to [Troubleshooting Slack](/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack) to learn more.\nOkta first-time login authentication error\nLearn about why do i get an authentication error when logging in via okta for the first time?.\nPingFederate SSO 404 error\nIf you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion.\nWhat does Atlan do with each Slack permission?\nLearn about what does atlan do with each slack permission?.\nWhat is included in the Jira integration?\nWith two of your most important workspaces connected, you can save time and improve the way you track issues for your data.\nWhat is included in the Microsoft Teams integration?\nWith two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.\nWhat is included in the Slack integration?\nLearn about the features and capabilities of the Slack integration with Atlan.\nWhat is the difference between Copy Link and Share on Slack or Teams?\nLearn about what is the difference between copy link and share on slack or teams?.\nWhat type of user provisioning does Atlan support for SSO integrations?\nAtlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:.\nWhen does Atlan become a personal data processor or subprocessor?\nAtlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.\nWhy did my users not receive an invite email from Atlan?\nIf you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid",
    "content": "Connect data\nPrivacy & Security\nBigID\nOn this page\nBigID\nOverview:\nCrawl BigID for a specified set of data sources and import classifications and policy-violation metadata for assets in Atlan in the form of tags, custom metadata and announcements.\nGet started\nâ\nFollow these steps to connect and catalog BigID metadata in Atlan:\nSet up the connector\nCrawl BigID metadata\nReferences\nâ\nWhat does Atlan crawl from BigID\n: Learn about the BigID metadata that Atlan discovers and catalogs.\nTags:\nbigid\nconnector\nprivacy\nsecurity\nconnectivity\nNext\nSet up BigID\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/how-tos/configure-network-security",
    "content": "Secure Agent 2.0\nManage agent\nConfigure network security\nOn this page\nConfigure network security\nConfigure network security for Secure Agent 2.0 to permit only required encrypted traffic between the agent and Atlan services. This ensures secure communication while blocking unauthorized access.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdministrative access to your firewall or network security groups\nConfigure firewall rules\nâ\nAccess your firewall management interface\n: Log into your firewall management system (AWS Security Groups, Azure NSGs, enterprise firewall console, or iptables for Linux).\nPermit outbound connections\n: Configure your firewall to permit the following outbound connections from your Secure Agent deployment:\nfirewall_rules\n:\noutbound_allowed\n:\n-\ndestination\n:\n\"*.atlan.com\"\nport\n:\n443\nprotocol\n:\nHTTPS\n-\ndestination\n:\n\"*.atlan.com\"\nport\n:\n443\nprotocol\n:\ngRPC/TLS\nBlock all inbound traffic\n: Configure your firewall to deny all inbound connections to the Secure Agent:\nfirewall_rules\n:\ninbound_blocked\n:\n-\nall_traffic\n:\nDENY\n# No inbound connections to agent\nNeed help\nâ\nIf you are still facing issues and need help, contact\n[email protected]\nfor assistance.\nSee also\nâ\nSecurity\n: Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0.\nVerify container images\n: Confirm image authenticity and integrity with Cosign before deployment.\nTags:\nsecure-agent\nnetwork\nsecurity\nfirewall\nPrevious\nVerify container images\nNext\nAuthentication\nPrerequisites\nConfigure firewall rules\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/best-practices/customer-environment-security",
    "content": "Secure Agent 2.0\nBest practices\nCustomer environment security\nOn this page\nCustomer environment security\nThis document outlines customer environment security best practices and minimum security baselines for deploying and operating Secure Agent 2.0 in your environment.\nSecurity assessment process\nâ\nBefore deployment, customers must complete a security assessment that includes:\nNetwork architecture review\n: Validate network security controls and configurations\nCredential management validation\n: Verify secret management practices and policies\nThis assessment ensures your environment meets the minimum security baselines required for secure deployment.\nInfrastructure security\nâ\nYour infrastructure must meet these minimum security baselines:\nContainer runtime security\n: Kubernetes 1.24+ or Docker 20.10+ with security features enabled\nNetwork segmentation\n: Agent isolated from production systems\nEndpoint protection\n: Anti-malware and endpoint detection solutions deployed\nVulnerability management\n: Regular patching and vulnerability scanning implemented\nNetwork security controls\nâ\nSecure Agent 2.0 uses outbound-only communication and never accepts inbound connections. Your network must implement these controls:\nOutbound-only communication\n: Agent never accepts inbound connections\nTLS 1.2 minimum\n: All external communication uses TLS 1.2 or higher\nFirewall requirements\n: Customer firewall rules for agent communication\nRequired network configuration\nâ\nfirewall_rules\n:\noutbound_allowed\n:\n-\ndestination\n:\n\"*.atlan.com\"\nport\n:\n443\nprotocol\n:\nHTTPS\n-\ndestination\n:\n\"*.atlan.com\"\nport\n:\n443\nprotocol\n:\ngRPC/TLS\ninbound_blocked\n:\n-\nall_traffic\n:\nDENY\n# No inbound connections to agent\nFor detailed configuration steps, see\nConfigure network security\n.\nIdentity and access management\nâ\nApply the principle of least privilege with these requirements:\nPrinciple of least privilege\n: Agent credentials limited to required data sources only\nRegular credential rotation\n: Scheduled rotation of agent access credentials\nAudit logging\n: Complete audit trail for agent-related activities\nOngoing security requirements\nâ\nMaintain security with these ongoing requirements:\nVulnerability management\n: Prompt application of security patches\nIncident response coordination\n: Participation in security incident response\nNeed help\nâ\nIf you need help, contact\n[email protected]\nfor assistance.\nTags:\nsecure-agent\nsecurity\ndeployment\nPrevious\nDeployment and security\nSecurity assessment process\nInfrastructure security\nNetwork security controls\nIdentity and access management\nOngoing security requirements\nNeed help"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/faq/deployment-and-security-faq",
    "content": "Secure Agent 2.0\nFAQ\nDeployment and security\nOn this page\nDeployment and security\nFind answers to common questions about supported deployment platforms, available methods, and security incident handling for Secure Agent 2.0.\nWhat platforms are supported for running OCI images?\nâ\nSecure Agent 2.0 images run on any OCI-compatible runtime, including Podman, Docker, ECS, and Kubernetes.\nDocker Compose and Helm charts are provided for easier deployment.\nHow does Secure Agent 2.0 handle security incidents?\nâ\nIf you suspect a security incident, contact\n[email protected]\n. The Atlan security team coordinates the response and investigation process.\nHow do I raise a security incident response?\nâ\nIf you suspect a security incident, follow this process:\nImmediate isolation\n: Stop agent processing immediately\nPreserve evidence\n: Capture logs and system state\nContact security\n: Email\n[email protected]\nwith incident details\nDocument timeline\n: Record all incident-related activities and timestamps\nCoordinate response\n: Work with Atlan security team for investigation\nSee also\nâ\nCustomer environment security best practices\n: Security baselines and requirements for deploying Secure Agent 2.0 in your environment.\nSecurity\n: Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0.\nTags:\nsecure-agent\nfaq\ndeployment\ntroubleshooting\nsecurity\nPrevious\nSecret management\nNext\nCustomer environment security\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/manage-user-authentication",
    "content": "Configure Atlan\nAccess control\nManage users and groups\nManage user authentication\nOn this page\nManage user authentication\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to configure user authentication settings.\nWhen users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication.\nAtlan currently only supports configuring session settings up to a maximum value of 30 days. You must also enter a minimum value of 1 minute.\nYou can configure the following parameters:\nSession idle timeout\n-  the total length of time that a session is allowed to be idle before it expires.\nSession max timeout\n-  the maximum amount of time before a session expires.\nRemember me session idle timeout\n-  the total length of time that a Remember Me session is allowed to be idle before it expires. If you choose not to set this parameter, Atlan will use the standard SSO session idle value, 7 days.\nRemember me session max timeout\n-  the maximum amount of time before a session expires if a user has enabled the Remember Me session option. If you choose not to set this parameter, Atlan will use the standard SSO session max value, 4,745 days.\nNote that tokens and browser sessions become invalid when a session expires.\nConfigure session settings\nâ\nTo configure user authentication settings in Atlan:\nFrom the left menu in\nAtlan\n, click\nAdmin\n.\nUnder\nWorkspace\n, click\nAuthentication\n.\nUnder\nAuthentication\n, to the right of\nSession\nsettings\n, click the\nEdit\nbutton.\nAtlan currently only supports setting a maximum value of up to 30 days. In the\nChange session settings\ndialog, you can configure the following:\nSet a numeric value for the following fields (You must enter a minimum value of 1 minute.):\nSession idle timeout\nSession max timeout\n_  - _ you must enter a value same as or higher than\nSession idle timeout\n.\nRemember me session idle timeout\nRemember me session max timeout\n_  - _ you must enter a value same as or higher than\nRemember me session idle timeout\n.\nClick the\nDays\ndropdown to specify the validity period. You can define parameters in the form of\nMinutes\n,\nHours\n, or\nDays\n.\nClick\nSave changes\nto save your configuration.\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nAdd users to groups\nNext\nDelegate administration\nConfigure session settings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/troubleshooting/microsoft-defender-sso-error",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nTroubleshooting\nMicrosoft Defender SSO error\nMicrosoft Defender SSO error\nIf you are getting an error message from Microsoft Defender that the Atlan login page cannot be loaded, it's possible that the wrapping URL prefix   -  for example, _\nhttps://nam02.safelinks.protection.outlook.com/\n- _ is causing the error.\nTo get around this, you should\nadd a Safe Links policy\nto whitelist Atlan's invitation emails and URLs. Otherwise, the Safe Links function will need to be turned off.\nTags:\nsecurity\naccess-control\npermissions\nPrevious\nGoogle Dashboard login error\nNext\nCan Atlan integrate with multiple Azure AD tenants within a single instance?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/faq/roles-and-permissions",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nFAQ\nRoles and permissions\nOn this page\nRoles and permissions\nThis document answers common questions about the roles Atlan creates or requires in Snowflake, why elevated permissions such as\ndq_admin\nare necessary, and how Snowflakeâs built-in controls keep your data safe.\nWhy does the\ndq_admin\nrole need table owner privileges?\nâ\nSnowflake's security model restricts data metric function management to table owners only. According to\nSnowflake's documentation\n, only the role that owns a table can schedule and manage data metric functions on that table.\nTo support data quality operations across your tables, the\ndq_admin\nrole must be granted access to the table owner roles. This permission lets it manage data metric functions on your behalf.\nHow does Atlan access these elevated privileges?\nâ\nAtlan maintains security through a controlled access pattern:\nAtlan\nnever\nreceives the\ndq_admin\nrole or table ownership directly\nAll operations execute through the\nMANAGE_DMF\nstored procedure\nThis procedure runs with\ndq_admin\nprivileges but only exposes specific, predefined data quality operations\nEvery operation remains within Snowflake's secure execution context\nTags:\nsnowflake\ndata-quality\nsecurity\npermissions\nfaq\nPrevious\nOperations\nWhy does the\ndq_admin\nrole need table owner privileges?\nHow does Atlan access these elevated privileges?"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/references/security",
    "content": "Secure Agent 2.0\nArchitecture & Security\nSecurity\nOn this page\nSecurity\nSecure Agent follows a zero-trust model where every component, connection, and operation is verified, minimized, and observable. This model starts with hardened containers, extends through verified supply chains, and encompasses all network communications and secret management. Critically, agents run entirely within your environment where you control the encryption keys and security policies. Each layer of security builds upon the previous, creating defense in depth.\nContainer security\nâ\nThe foundation of Secure Agent's security begins with how containers are built and run. Every container starts from a minimal base image and runs with strict limitations that prevent exploitation even if compromised.\nHardened execution environment\nâ\nAll Secure Agent containers run with a strict security context to reduce attack surface:\nsecurityContext\n:\nrunAsNonRoot\n:\ntrue\n# Never run as root user\nrunAsUser\n:\n1000\n# Dedicated non-privileged user\nallowPrivilegeEscalation\n:\nfalse\ncapabilities\n:\ndrop\n:\n[\n\"ALL\"\n]\nreadOnlyRootFilesystem\n:\ntrue\nThis configuration creates the first security boundaryâeven if an attacker compromises the application code, they can't escalate privileges, modify the filesystem, or access system capabilities. The use of distroless base images further limits what's available to exploit, containing only the application and its runtime dependencies.\nRuntime protections\nâ\nBuilding on the hardened base, additional runtime controls keep containers within strict boundaries:\nNo shell access in containers eliminates common attack vectors\nMinimal base images (distroless) reduce the attack surface to essential components only\nResource limits prevent any single container from exhausting system resources\nNetwork policies restrict each container to communicate only with required endpoints\nThese protections work togetherâa compromised container can't spawn shells, consume unlimited resources, or communicate with unauthorized systems.\nSupply chain integrity\nâ\nBefore any container reaches your environment, it passes through multiple verification stages that verify both security and authenticity.\nContinuous vulnerability scanning\nâ\nThe security of container images is continuously validated through automated scanning:\nAll images are scanned with Snyk and Trivy on every change and at regular intervals. These scans detect:\nKnown vulnerabilities (CVEs) and outdated dependencies that may be exploited\nSecurity misconfigurations that deviate from best practices\nAccidentally embedded secrets that may provide unauthorized access\nLicense compliance to meet legal requirements\nOnly images free of Critical and High severity vulnerabilities are released, establishing a baseline security standard before deployment.\nImage repository options\nâ\nContainer images required by the Secure Agent are hosted on Atlan using the\nHarbor\nopen source registry. Harbor provides enterprise-grade security features including vulnerability scanning, image signing verification, and access control.\nIf required, these images can be mirrored or pulled to your private container registry to meet organizational compliance and security requirements. This flexibility ensures the Secure Agent can operate within your existing container management and security infrastructure.\nImage signing and verification\nâ\nBeyond scanning, every container is cryptographically signed to verify authenticity:\nAll containers are signed with Cosign, creating entries in Sigstore's transparency log. This immutable record lets you verify that images haven't been tampered with since they left Atlan's build pipeline. The signing process uses keyless signing via GitHub Actions, ensuring traceability and security without requiring long-lived private keys.\nVerification capabilities:\nImage signature validation with Sigstore's transparency log\nGitHub workflow identity verification\nCertificate chain validation through GitHub's OpenID Connect (OIDC) provider\nFor complete verification steps and examples, see\nVerify container images\nguide.\nNetwork security\nâ\nWith containers secured and verified, the next layer protects how they communicate. Secure Agent's network model ensures that even if other controls fail, unauthorized network access remains impossible.\nOutbound-only communication\nâ\nSecure Agent operates with an outbound-only communication modelâit initiates all connections and never accepts inbound connections. This fundamental design choice eliminates entire categories of attacks:\nNo exposed ports that can be scanned or exploited\nNo services that can be overwhelmed with requests\nNo authentication challenges from external sources\nAll traffic to Atlan uses TLS 1.2 or higher, which means the content remains protected even if network traffic is intercepted.\nFirewall and routing controls\nâ\nThe outbound-only model is enforced through firewall rules that explicitly block all inbound traffic while permitting only specific outbound connections:\nUse firewall enforcement to permit authorized outbound traffic to Atlan endpoints and block all inbound traffic to the agent. For configuration steps, see\nConfigure network security\n.\nRecommended network configuration (example)\nfirewall_rules\n:\noutbound_allowed\n:\n-\ndestination\n:\n\"*.atlan.com\"\nport\n:\n443\nprotocol\n:\nHTTPS\n-\ndestination\n:\n\"*.atlan.com\"\nport\n:\n443\nprotocol\n:\ngRPC/TLS\ninbound_blocked\n:\n-\nall_traffic\n:\nDENY\nAdvanced networking practices\nâ\nFor environments requiring additional security, these practices layer on top of the basic network controls:\nEgress-only endpoints\nrestrict outbound connections to an approved list of external endpoints\nPrivate routing\nthrough VPC/VNet peering or private endpoints keeps traffic within cloud provider networks\nProxy support\nroutes all outbound connections through enterprise proxies for inspection\nZero-trust implementation\nlimits communications to required services using IP allowlists or service-specific endpoints\nRate limiting\non egress prevents data exfiltration if other controls are bypassed\nTLS certificate pinning\nprevents man-in-the-middle attacks even with compromised CAs\nSegregated workloads\nisolate agent instances handling different sensitivity levels\nNetwork monitoring\nâ\nContinuous monitoring provides visibility into all network activity:\nNetwork logs\ncapture all traffic for anomaly detection and audit trails\nConnection health checks\ncontinuously verify that outbound connections to Atlan endpoints remain secure and functional\nThis monitoring feeds into the broader observability system, creating a complete picture of agent behavior.\nAuthentication and secret management\nâ\nWith secure containers communicating over protected networks, the next critical layer manages how the agent authenticates and handles secrets. This system ensures credentials never leave your control.\nAuthentication model\nâ\nSecure Agent uses OAuth 2.0 client credentials flow for all authentication to Atlan services. Each deployed application receives its own unique Client ID and Client Secret, ensuring credential isolation between applications.\nThis per-application credential model provides critical security benefits: if one application's credentials are compromised, the impact is limited to that specific application rather than affecting all Secure Agent deployments. This follows OAuth 2.0 best practices as defined in\nRFC 9700\n.\nKey security features:\nIsolated credentials\n: Each application has unique credentials\nRotation support\n: Client credentials can be rotated on-demand\nSecure storage\n: Credentials retrieved from your secret manager, never hardcoded\nAudit trail\n: All authentication events logged for security monitoring\nThis authentication integrates with your existing identity infrastructure, enabling you to apply your standard rotation and governance policies.\nSecret management\nâ\nRather than storing any credentials itself, Secure Agent integrates with enterprise secret managers through Dapr's secret store abstraction:\nJust-in-time retrieval\nmeans secrets are fetched only when needed for a specific operation\nMemory-only storage\nensures secrets exist only in application memory during use\nNo transmission to Atlan\nkeeps all credentials within your security perimeter\nComprehensive audit logging\ntracks every secret access for compliance\nMulti-vault support\nworks with AWS Secrets Manager, Azure Key Vault, HashiCorp Vault, and others\nThis design ensures that even if an agent container is compromised, no credentials can be extracted from disk or logs. All secret access follows your existing security policies and appears in your vault's audit logs.\nLogging, monitoring, and incident response\nâ\nAll these security controls generate observable events, creating a comprehensive audit trail and enabling rapid incident response.\nObservability\nâ\nEvery security-relevant event is captured and made available through multiple channels:\nStructured logs\ncapture all operations with correlation IDs for tracing\nMetrics\nexpose performance and security indicators through Prometheus format\nHealth endpoints\nvia the FastAPI server provide real-time status\nTrace data\nshows the complete flow of operations across components\nThese observability streams integrate with your existing monitoring infrastructure, whether that's Splunk, Datadog, CloudWatch, or other platforms.\nIncident response\nâ\nWhen security events occur, the comprehensive logging enables rapid response:\nEmergency contact\n:\n[email protected]\nprovides 24/7 security team access\nLog preservation\nautomatically retains all incident-related logs\nCoordinated notification\nensures customer teams are informed of any impacts\nAutomated responses\ncan revoke credentials or terminate connections based on detected anomalies\nThe incident response process leverages all the security controlsânetwork isolation limits blast radius, credential rotation stops unauthorized access, and comprehensive logs enable thorough investigation.\nSecurity alerts\nâ\nCustomers receive automatic notifications for:\nCritical/High vulnerabilities\ndiscovered in deployed images\nSecurity patches\navailable for immediate deployment\nUnusual security events\ndetected in agent behavior\nCompliance violations\nor configuration drift\nThese proactive alerts enable rapid remediation before issues can be exploited.\nCompliance and audit\nâ\nAll these security controls align with enterprise compliance requirements and provide the evidence needed for audits.\nRegulatory alignment\nâ\nThe security architecture supports compliance with major frameworks:\nSOC 2 Type II\nvalidates security controls and their operating effectiveness\nISO 27001\naligns with information security management system requirements\nGDPR\nensures appropriate data protection and privacy controls\nHIPAA\nprovides necessary safeguards for healthcare data (when applicable)\nPCI DSS\nmeets requirements for payment card data security (when applicable)\nEach control maps to specific requirements within these frameworks, simplifying compliance documentation.\nAudit capabilities\nâ\nThe layered security model generates comprehensive audit evidence:\nImmutable audit logs\nwith cryptographic timestamps prove when events occurred\nLog integration\nwith SIEM and centralized logging solutions enables enterprise-wide visibility\nConfigurable retention\nfrom 30â365 days aligns with your policy requirements\nChain of custody\ntracks data from extraction through delivery to Atlan\nThis audit trail spans all security layersâfrom container startup through network connections, authentication events, secret access, and data operations.\nSee also\nâ\nSecret management\n: How credentials are protected and never leave your environment.\nCustomer environment security best practices\n: Security baselines and requirements for deploying Secure Agent 2.0 in your environment.\nArchitecture\n: Component interactions and how security controls map to the system design\nTags:\nsecure-agent\nsecurity\nPrevious\nArchitecture\nNext\nVerify container images\nContainer security\nSupply chain integrity\nNetwork security\nAuthentication and secret management\nLogging, monitoring, and incident response\nCompliance and audit\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity",
    "content": "Connect data\nCRM\nSalesforce\nTroubleshooting\nTroubleshooting Salesforce connectivity\nOn this page\nTroubleshooting Salesforce connectivity\nWhy does Atlan require an admin user in Salesforce?\nâ\nAtlan recommends a Salesforce administrator for setting up a\nconnection between Atlan and Salesforce\n.\nOnce connected, Atlan can extract all Salesforce objects   -  including corresponding fields, folders and child folders, along with dashboards and reports   -  without having to enable object-level permissions and field-level security (FLS) for each object addition in the profile or permission sets.\nAlthough it's possible to enable these permissions for non-admin users, only admins have the special permissions to oversee all newly added custom objects. This is regardless of which non-admin users created those custom objects or the permissions that were imposed on them.\nWhy is Atlan unable to crawl some system-generated objects in Salesforce?\nâ\nAtlan operates in read-only mode and doesn't make any API calls that modify your Salesforce instance. With the limited permissions typically granted during setup, Atlan may not be able to crawl certain system-generated objects, such as:\nContentDocumentSubscription\nContentNotification\nContentTagSubscription\nContentUserSubscription\nContentVersionComment\nContentVersionRating\nContentWorkspaceSubscription\nCorsWhitelistEntry\nEmailCapture\nFeedPollChoice\nFeedPollVote\nOrgDeleteRequest\nPlatformStatusAlertEvent\nPromptError\nPromptAction\nSetupAssistantStep\nTopicUserEvent\nDoes Atlan collect formula fields from Salesforce?\nâ\nYes, Atlan collects formula fields from Salesforce. However, if the formula fields are brought into your data warehouse via Fivetran,Â they aren't reflected in Atlan as assets from your data warehouse. This is because Fivetran\ndoes not sync formula fields\nfrom Salesforce.\nWhy do I get an \"sObject type 'Organization' isn't supported\" error message?\nâ\nTo pass the preflight check for\norganization count\n, make sure that you've added the\nModify All Data\nÂ permission while\nsetting up Salesforce\n. This\nobject permission\nenables the user to access all shared and public folders, regardless of sharing settings.\nTags:\ndashboards\nvisualization\nanalytics\nsecurity\naccess-control\npermissions\nPrevious\nPreflight checks for Salesforce\nNext\nDoes Atlan require an admin user in Salesforce?"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/how-tos/verify-container-images",
    "content": "Secure Agent 2.0\nManage agent\nVerify container images\nOn this page\nVerify container images\nContainer image verification prevents malicious or modified images from entering your environment. Before you deploy a Secure Agent container, verify its signature so you can be confident the image was built by Atlanâs official CI/CD pipeline, hasn't been modified since signing, and is traceable to a specific GitHub workflow. This guide provides step-by-step instructions to verify images using\nCosign\n.\nPrerequisites\nâ\nCosign\nis installed on your system. If not, follow the\nofficial Cosign installation guide\n.\nYou can access the container image you want to verify.\nYou know the image name and tag (for example:\npublic.ecr.aws/atlanhq/redshift-app:2.0\n).\nVerify container image\nâ\nRun the verification command\n: Run the following command, replacing\n<image-name>\nand\n<full-image-path>\nwith your values. For example,\npublic.ecr.aws/atlanhq/redshift-app:2.0\n:\nCOSIGN_EXPERIMENTAL=1 cosign verify \\\n--allow-insecure-registry \\\n--certificate-identity=\"https://github.com/atlanhq/<image-name>/.github/workflows/test-image-sign.yaml@refs/heads/image-signing-test\" \\\n--certificate-oidc-issuer=\"https://token.actions.githubusercontent.com\" \\\n<full-image-path>\nExample: verify Redshift connector image\nCOSIGN_EXPERIMENTAL=1 cosign verify \\\n--allow-insecure-registry \\\n--certificate-identity=\"https://github.com/atlanhq/connector-auth/.github/workflows/test-image-sign.yaml@refs/heads/image-signing-test\" \\\n--certificate-oidc-issuer=\"https://token.actions.githubusercontent.com\" \\\npublic.ecr.aws/atlanhq/redshift-app:2.0\nInterpret the results\n: Verify the results produced by the command:\nIf the verification is\nsuccessful\n, Cosign returns a verified signature along with signer details.\nExample: verification successful\nVerification successful!\n- Image Digest (SHA256): abcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890\n- Signed by: atlanhq/<image-name>/.github/workflows/image-sign.yaml\n- OIDC Issuer: https://token.actions.githubusercontent.com\nIf the verification\nfails\n, it means the image is either unsigned or has been modified. Don't proceed with deployment until you obtain a valid signed image.\nExample: verification failed\nVerification failed!\n- Error: Signature verification failed\n- Reason: The signature doesn't match the expected digest.\n- Suggested Action: Check the image signature and ensure it was signed correctly.\nTroubleshooting\nâ\nIf verification fails:\nMake sure you are using the correct\nimage path and tag\n.\nVerify the\ncertificate identity\n(the\n--certificate-identity\nvalue) matches the repository/workflow that signed the image.\nConfirm network connectivity to Sigstore (Cosign uses transparency and registry services).\nNeed help\nâ\nIf you are still facing issues and need help, contact\n[email protected]\nfor assistance.\nSee also\nâ\nSecurity\n: Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0.\nConfigure network security\n: Set firewall rules, proxies, and Kubernetes policies to control agent traffic.\nTags:\nsecure-agent\nsecurity\ncontainer-images\nPrevious\nSecurity\nNext\nConfigure network security\nPrerequisites\nVerify container image\nTroubleshooting\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/auto-assign-roles-by-group",
    "content": "Configure Atlan\nAccess control\nManage users and groups\nAutomatically assign roles\nOn this page\nAutomatically assign roles based on group names\nApp\nYou can automatically assign roles in Atlan based on user group memberships. This helps streamline onboarding, enforce consistent access control, and reduce manual effort by mapping groups to roles such as Admin, Member, or Guest.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAccess to the User Role Sync app. If you don't have access,\ncontact Atlan support\nor your Atlan customer team to request it.\nAdmin permissions in Atlan to configure workflows and assign roles. Learn more about\nadmin roles\n.\nA\npersonal API token\ngenerated from your Atlan profile for workflow authentication.\nAll required user groups created and available in Atlan, as described in\nCreate groups\n.\nSetup workflow\nâ\nIn your Atlan workspace, go to the homepage and click\nNew workflow\nin the top navigation bar.\nSearch for\nUser Role Sync\n, and then select\nSet up workflow\n.\nIn the\nWorkflow name\nfield, enter a descriptive name that clearly identifies the purpose of this workflow.\nIn the\nAuthentication\nsection, provide your\nAPI token\n. This token is required for the workflow to authenticate with Atlan and perform user role updates.\nDefine role mapping rules\nâ\nAfter authentication is configured and tested, set up how group memberships correspond to specific roles in Atlan. This configuration also determines the order of priority when a user belongs to multiple groups and can optionally include sub-role assignments for more granular control.\nIn the\nSelection\nmode, select\nList\nto define a fixed set of Atlan group names that are explicitly mapped to roles, such as assigning the admin role to\ndata-admins\nand\ndata-leads\n(both part of the existing Atlan groups in your workspace).\nIf you need to match multiple groups that follow a naming pattern, such as all groups ending in\n-admins\n, use\nRegex\nmode instead. For more details, see\nSelection mode\n.\nIn the\nRole hierarchy\n, choose how Atlan resolves conflicts when a user belongs to multiple groups mapped to different roles. You can select the default\nGuest â Member â Admin\n, which prioritizes the most restrictive role.\nIf a user belongs to both the\nguests\nand\ndata-admins\ngroups, the\nguest\nrole is assigned. To view other hierarchy options, see\nRole hierarchy options\n.\nIn the\nAdmin group\nfield, enter a comma-separated list of group names whose members are assigned the\nadmin\nrole. Use names that are identical to the display names of the groups in Atlan.\ndata-admins,data-leads\nTo match group names using patterns, see\nRegex matching\n.\nTo assign more granular responsibilities under the admin role, select\nStatic\nunder the\nAdmin Sub Role\nOption. This assigns fixed sub-roles such as\nworkflow-admin\nand\ngovernance-admin\nto specific groups and sets their order of precedence.\nIn the\nWorkflow Admin (sub-role) Group\nfield, enter the group names whose members receive the\nworkflow-admin\nsub-role. Use comma-separated values, such as\nengineering-ops-admins\n.\nIn the\nGovernance Admin (sub-role) Group\nfield, enter the group names whose members receive the\ngovernance-admin\nsub-role. Use comma-separated values, such as\ndata-governance-leads\n.\nIn the\nAdmin Sub Role\nhierarchy field, define the order of precedence between\nsub-roles\n. If a user qualifies for multiple sub-roles, the one listed first is assigned.\nworkflow-admin,governance-admin\nIf you need to define flexible, custom admin sub-roles instead of fixed ones, see\nConfigure dynamic sub-roles\n.\nIn the\nMember group\nfield, add group names to assign the\nmember\nrole for users who work with metadata, glossaries, or queries but donât need admin access.\nIn the\nGuest group\nfield, enter group names for users who only need limited or read-only access to Atlan, such as\nviewers\nor\ncontractors\n.\nSchedule and run\nthe workflow. Run the workflow manually or set a recurring schedule to keep role assignments up to date.\nNeed help?\nâ\nIf you have any issues related to configuring the app, contact\nAtlan support\n.\nSee also\nâ\nUser Role Sync: Reference\n: Detailed explanation of each configuration property, including valid values, examples, and behavior.\nTags:\naccess control\nroles\nuser groups\nautomation\napp\ngovernance\nPrevious\nDelegate administration\nNext\nWhat are personas?\nPrerequisites\nSetup workflow\nDefine role mapping rules\nNeed help?\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/requests",
    "content": "Use data\nRequests\nOn this page\nRequests\nOverview:\nUse Atlan's requests capabilities to suggest changes to assets that you cannot directly modify. Enable collaboration and governance by allowing users to request updates while maintaining proper access control.\nGet started\nâ\nHow to manage requests\nGuides\nâ\nHow to manage requests\n: Handle incoming change requests and manage the request workflow.\nConcepts\nâ\nWhat are requests\n: Learn about requests and their role in governance.\nPolicy-based permissions\n: Understand how policies affect request capabilities.\nOverridden permissions\n: Learn about permission overrides.\nTags:\nrequests\ngovernance\naccess-control\ncapabilities\nNext\nManage requests\nGet started\nGuides\nConcepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/user-role",
    "content": "Configure Atlan\nAccess control\nReferences\nUser Role Sync\nOn this page\nUser role sync\nApp\nThe User Role Sync app enables automated role and sub-role assignment in Atlan by mapping user groups to roles like Admin, Member, or Guest. It supports both exact and pattern-based matching and helps scale access control by reducing manual configuration.\nThis reference provides complete configuration details for the User Role Sync app. Use this page to look up specific property definitions, valid values, and configuration formats when setting up automated role assignments based on group memberships.\nAccess\nâ\nThe User Role Sync app isn't enabled by default. To use this app,\ncontact Atlan support\nand request it be added to your tenant.\nCredentials\nâ\nThis section defines the fields required for workflow authentication and identification.\nWorkflow name\nâ\nSpecifies a unique and descriptive name to identify the workflow configuration in the Atlan interface. This name appears in the workflow list and helps distinguish it from other workflows, especially in environments with multiple automated role management setups.\nExample:\natlan\n-\nprod\n-\nuser\n-\nrole\n-\nsync\nAtlan API token\nâ\nSpecifies the API token used to authenticate this workflow with your Atlan tenant.\nGenerate a personal API\ntoken from your user profile in Atlan. The token must belong to an admin user with permission to view groups and manage user roles. You can click\nTest\nauthentication in the workflow UI to validate the token before proceeding.\nSelection mode\nâ\nDetermines how group names are matched to roles during role assignment. Use this setting to choose whether to provide exact group names or define a pattern that matches multiple group names.\nThe value selected here affects how group inputs are interpreted for all role and sub-role mappings in the workflow.\nList\n: Use this option when you have a known set of group names. Each group must be entered explicitly.\nExample:\nTo assign the admin role to the data-admins and analytics-leads groups, enter:\ndata-admins,analytics-leads\nRegex\n: Use this option when group names follow a naming pattern. All groups matching the provided regular expression are included.\nExample:\nTo assign the member role to all groups that start with\nteam-\n, use:\n^\nteam\n-\n.\n*\nExample:\nTo target groups ending in\n-admins\n(for example,\ndata-admins\n,\ncloud-admins\n,\nsecurity-admins\n), use:\n.\n*\n-\nadmins$\nExample:\nTo include only groups with exactly three characters followed by\n-ops\n(for example,\neng-ops\n,\ndev-ops\n), use:\n^\n[\na\n-\nz\n]\n{\n3\n}\n-\nops$\nRole hierarchy\nâ\nControls how role precedence is resolved when a user belongs to multiple groups that are mapped to different roles. The selected hierarchy determines which role is ultimately assigned based on predefined priority logic.\nThis setting is especially useful in large organizations where users may be part of multiple functional teams with overlapping access needs.\nAvailable options:\nGuest â Member â Admin\n(default)\nAssigns the most restrictive role in cases of conflict. Recommended when security and access limitation are the primary concerns.\nExample\n: A user belongs to both\nguests\n(mapped to guest role) and\ndata-admins\n(mapped to admin role). The assigned role is\nguest\n.\nGuest â Admin â Member\nPrioritizes the\nguest\nrole first, followed by\nadmin\n, then\nmember\n. This ensures that users in temporary, external, or read-only groups don't receive unintended contributor access.\nExample\n: A user belongs to\nguests\n(mapped to guest role),\ndata-admins\n(mapped to admin role), and\nreporting-team\n(mapped to member role). The assigned role is\nguest\n.\nMember â Guest â Admin\nAssigns the\nmember\nrole over others. Use this when contributor-level access must take priority, even if the user is also in limited-access or admin groups.\nExample\n: A user belongs to\nteam-product\n(mapped to member role) and\nguests\n(mapped to guest role). The assigned role is\nmember\n.\nMember â Admin â Guest\nPrefers the\nmember\nrole first, then\nadmin\n, followed by\nguest\n. This is useful when users who actively work with data must always retain contributor access, even if they're admins or in guest groups.\nExample\n: A user belongs to\nfinance-team\n(mapped to member role),\ndata-admins\n(mapped to admin role), and\ncontractors\n(mapped to guest role). The assigned role is\nmember\n.\nAdmin â Guest â Member\nAssigns the\nadmin\nrole when present. Suitable for teams where users with admin responsibilities must retain their elevated access regardless of membership in guest or contributor groups.\nExample\n: A user belongs to\ndata-admins\n(mapped to admin role),\nguests\n(mapped to guest role), and\nteam-marketing\n(mapped to member role). The assigned role is\nadmin\n.\nAdmin â Member â Guest\nThe most permissive hierarchy. Always assigns the\nadmin\nrole if available. Use this when users who have administrative responsibilities must never be downgraded, even if they belong to other groups.\nExample\n: A user belongs to\ndata-admins\n(mapped to admin role),\nteam-analytics\n(mapped to member role), and\nviewers\n(mapped to guest role). The assigned role is\nadmin\n.\nAdmin\nâ\nThis section defines how users are assigned the admin role in Atlan. The admin role is intended for users who require elevated privileges to manage system configurations, workflows, users, and platform-level settings.\nAdmin group\nâ\nSpecifies the groups whose members are assigned the\nadmin\nrole in Atlan. The values for this field depend on the selection made in\nSelection mode\n.\nIf\nList\nmode is selected: Provide exact, comma-separated names of Atlan groups.\nExample\n:\ndata-admins,analytics-leads\nIf\nRegex\nmode is selected: Provide a valid regular expression to match group names dynamically.\nExample\n:\n.*admins\nUsers who belong to any of the specified groups are assigned the\nadmin\nrole, subject to the logic defined in the\nRole hierarchy\nsetting.\nAdmin sub-role option\nâ\nDetermines how sub-roles under the\nadmin\nrole are assigned to users. This property supports two modes:\nStatic\n:  Choose this when sub-roles and their corresponding groups are predefined and don't change often.\nYou can configure fixed sub-roles such as\nworkflow-admin\nor\ngovernance-admin\nand assign them to specific groups. Use the\nAdmin sub-role hierarchy\nfield to define precedence.\nDynamic\n: Use this mode when sub-roles are more varied or need to be configured on a per-workflow basis. You can define up to five sub-roles and map each one to one or more groups.\nThe order in which the sub-roles are configured determines their precedence (first = highest). Choose the mode that best aligns with your organization's administrative structure.\nWorkflow admin (sub-role) group\nâ\nDefines the Atlan groups whose members can be assigned the\nworkflow-admin\nsub-role. This field is available only when\nStatic\nis selected under the\nAdmin Sub Role\noption.\nThe expected input depends on the configured\nSelection mode\n:\nIf\nList\nmode is selected: Enter one or more exact group names, separated by commas.\nExample:\nengineering-admins,project-ops-leads\nIf\nRegex\nmode is selected: Provide a valid regular expression pattern to match group names dynamically.\nExample:\n.*workflow-admins\nGovernance admin (sub role) group\nâ\nSpecifies the groups whose members can be assigned the governance-admin sub-role. This field appears only when\nStatic\nis selected for the\nAdmin sub-role\noption.\nIf\nList\nmode is selected: Enter one or more exact group names, separated by commas.\nExample:\ndata-governance-leads,compliance-admins\nIf\nRegex\nmode is selected: Enter a regular expression to dynamically match group names.\nExample:\ngovernance-.*\nAdmin sub role hierarchy\nâ\nSets the precedence between admin sub-roles when users are mapped to multiple sub-role groups. This field is displayed only when\nStatic\nis selected for the\nAdmin sub-role\noption.\nList the sub-role names in the desired order of priority. The first sub-role listed is considered the highest in precedence.\nAdmin sub-role 1-5\nâ\nAvailable only when Dynamic is selected for the Admin sub-role option. These fields enable you to define up to five custom admin sub-roles and map them to corresponding Atlan user groups.\nEach sub-role entry contains:\nSub-role name\n: A unique label to identify the custom admin sub-role (for example,\ndata-platform-admin\n).\nGroups\n: One or more group names to which this sub-role are applied.\nSub-role precedence is determined by the order in which the sub-roles are configured. The first sub-role listed is considered the highest in priority.\nExamples:\nSub-role 1:\nSub-role name:\ndata-governance-lead\nGroups\n:\ngovernance-team\n,\ndata-leads\nSub-role 2:\nSub-role name:\nplatform-admin\nGroups\n:\ninfra-core\n,\nplatform-eng\nMember\nâ\nThis section defines how users are assigned the member role in Atlan. The member role is intended for users who contribute to data work, such as exploring metadata, managing glossaries, or running queries, without requiring administrative privileges.\nMember group\nâ\nDefines the group or groups whose members can be assigned the member role in Atlan. Depending on the\nSelection mode\n, group names can be matched directly or via pattern.\nIn\nList\nmode: Enter exact, comma-separated group names.\nExample:\ndata-analysts,reporting-team\nIn\nRegex\nmode: Enter a valid regular expression to match one or more group names dynamically.\nExample:\n.*-members\nIf a user belongs to multiple groups with different role assignments, the final role is determined based on the\nRole hierarchy\nsetting.\nGuest\nâ\nThis section defines how users are assigned the guest role in Atlan. The guest role is suited for individuals who require read-only access or limited interaction with metadata, such as external users, contractors, or stakeholders.\nGuest role\nâ\nSpecifies the group or groups whose members can be assigned the guest role in Atlan. The input format depends on the\nSelection mode\n:\nIn\nList\nmode: Provide exact, comma-separated group names.\nExample:\nviewers,contractors\nIn\nRegex\nmode: Provide a valid regular expression to dynamically match group names.\nExample:\n.*-guests\nIf users belong to multiple groups, the final assigned role depends on the configuration in the Role hierarchy property.\nSee also\nâ\nAutomatically assign roles based on group names\nCreate groups\nTags:\naccess control\nroles\nuser groups\nautomation\napp\ngovernance\nreference\nPrevious\nWhat are the sidebar tabs?\nAccess\nCredentials\nAdmin\nMember\nGuest\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/references/data-quality-permissions",
    "content": "Build governance\nData Quality Studio\nWhat is Data Quality Studio\nReferences\nData quality permissions\nOn this page\nData quality permissions\nTo grant users in your organization access to set up data quality rules, you must assign the necessary permissions via metadata policies in personas.\nPermission scopes\nâ\nScope\nDescription\nCreate Rule\nGrants users permission to create new rules on tables and add or update schedules\nUpdate Rule\nGrants users permission to modify existing rules\nDelete Rule\nGrants users permission to delete rules\nRead Rule\nGrants users permission to view the run values for a rule. Without this permission, users can still see the rule's run status (whether it passed or failed) and other metadata, such as the test's expected outcome, but the actual values remain hidden\nConfiguration\nâ\nAssign permissions via metadata policies in personas using the\nData Quality (DQ)\nscopes.\nSee also\nâ\nSet up Databricks\n- Configure Databricks for data quality monitoring\nSet up Snowflake\n- Configure Snowflake for data quality monitoring\nTags:\nsnowflake\ndata-quality\npermissions\nscopes\nreference\nPrevious\nWhat's Data Quality Studio\nNext\nRules and dimensions\nPermission scopes\nConfiguration\nSee also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/how-tos/set-up-sap-ecc",
    "content": "Connect data\nERP\nSAP ECC\nGet Started\nSet up SAP ECC\nOn this page\nSet up SAP ECC\nThis guide explains how to create a dedicated service user in SAP ECC and grant the necessary permissions for Atlan to extract metadata.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdministrative access to SAP ECC.\nSAP system details, including:\nHost\nSystem number\nClient number\nCreate communication user for metadata extraction\nâ\nIn the SAP GUI command field, enter\nSU01\nand press\nEnter\nto open\nUser Maintenance\n.\nIn the\nUser\nfield, enter a name for the new service user and click\nCreate\n.\nOn the\nAddress\ntab, provide the required contact information.\nOpen the\nLogon Data\ntab and:\nSet an initial password (enter it twice).\nSet\nUser Type\nto\nC (Communications Data)\n.\nSwitch to the\nRoles\ntab and assign roles that enable:\nRemote function call (RFC) execution\nTable-level read access for metadata tables\nAccess to system-level information\nVerify that the assigned roles enable execution of these RFC modules:\nSTFC_CONNECTION\n: Verifies connectivity between Atlan and SAP ECC.\nRFC_SYSTEM_INFO\n: Retrieves system metadata such as SYSID, operating system, and release version.\nRFC_READ_TABLE\n: Enables table-level reads for metadata extraction.\nClick\nSave\nto confirm the changes.\nnote\nThe user must change the password on first login.\nNext steps\nâ\nCrawl SAP ECC\n: Follow the instructions to extract metadata from SAP ECC using the configured service user.\nTags:\nerp\nsetup\npermissions\nsap-ecc\nPrevious\nSAP ECC\nNext\nCrawl SAP ECC\nPrerequisites\nCreate communication user for metadata extraction\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/how-tos/set-up-sap-s4hana",
    "content": "Connect data\nERP\nSAP S/4HANA\nGet Started\nSet up SAP S/4HANA\nOn this page\nSet up SAP S/4HANA\nThis guide explains how to create a dedicated service user in SAP S/4HANA and grant the necessary permissions for Atlan to extract metadata.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdministrative access to SAP S/4HANA.\nSAP system details, including:\nHost\nSystem number\nClient number\nCreate communication user for metadata extraction\nâ\nIn the SAP GUI command field, enter\nSU01\nand press\nEnter\nto open\nUser Maintenance\n.\nIn the\nUser\nfield, enter a name for the new service user and click\nCreate\n.\nOn the\nAddress\ntab, provide the required contact information.\nOpen the\nLogon Data\ntab and:\nSet an initial password (enter it twice).\nSet\nUser Type\nto\nC (Communications Data)\n.\nSwitch to the\nRoles\ntab and assign roles that enable:\nRemote function call (RFC) execution\nTable-level read access for metadata tables\nAccess to system-level information\nVerify that the assigned roles enable execution of these RFC modules:\nSTFC_CONNECTION\n: Verifies connectivity between Atlan and SAP S/4HANA.\nRFC_SYSTEM_INFO\n: Retrieves system metadata such as SYSID, operating system, and release version.\nRFC_READ_TABLE\n: Enables table-level reads for metadata extraction.\nDD_DDL_DEPENDENCY_GET\n: Fetches dependencies used in CDS views and table lineage.\nClick\nSave\nto confirm the changes.\nnote\nThe user must change the password on first login.\nNext steps\nâ\nCrawl SAP S/4HANA\n: Follow the instructions to extract metadata using the configured service user.\nTags:\nerp\nsetup\npermissions\nsap-s4hana\nPrevious\nSAP S/4HANA\nNext\nCrawl SAP S/4HANA\nPrerequisites\nCreate communication user for metadata extraction\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/tags/lambda",
    "content": "One doc tagged with \"lambda\"\nView all tags\nAutomation Integrations\nIntegrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On."
  },
  {
    "url": "https://docs.atlan.com/tags/webhooks",
    "content": "4 docs tagged with \"webhooks\"\nView all tags\nAutomation Integrations\nIntegrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On.\nConnections Integration\nIntegrate Atlan with Connections to create webhooks and automate notifications.\nCreate webhooks\nIf your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or [submit a request](/support/submit-request).\nWebhooks Integration\nIntegrate Atlan with Webhooks to automate actions and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/browser-extension",
    "content": "3 docs tagged with \"browser-extension\"\nView all tags\nAutomation Integrations\nIntegrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On.\nBrowser Extension\nIntegrate Atlan with the Browser Extension to enhance your data catalog experience.\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/aws-lambda",
    "content": "One doc tagged with \"aws lambda\"\nView all tags\nAWS Lambda\nIntegrate Atlan with AWS Lambda to automate workflows and triggers."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/always-on/references/tag-propagation",
    "content": "Configure Atlan\nIntegrations\nAutomation\nAlways On\nTag propagation\nOn this page\nTag propagation\nTag propagation is disabled by default in Atlan.\nWhen\ntagging an asset\n, you can enable tag propagation to\nchild\nassets.\nTo downstream assets\nâ\nDownstream assets are derived through the\ndata transformations\nof an asset.\nAtlan can propagate tags through lineage. For example:\nYou\ntag a column\nas\nPII\n.\nThat column has downstream columns (in lineage).\nAtlan tags the downstream columns as\nPII\n, too, if propagation is enabled.\nThis is particularly useful when you want to tag data for protection reasons. When combined with\npurposes\n, Atlan protects all downstream copies of the data automatically.\nTo child assets\nâ\nChild assets are linked to parent assets through a parent-child hierarchical relationship.\nAtlan can propagate tags from parent to child assets. For example:\nYou\ntag a database table\nas\nMarketing Analysis\n.\nAtlan tags all the columns in that table as\nMarketing Analysis\n, too, if propagation is enabled.\nYou can also configure the propagation of your tags:\nHierarchy & lineage\nenables tag propagation to child and downstream assets.\nHierarchy only (no lineage)\nenables tag propagation to child assets only.\nLineage only (no hierarchy)\nenables tag propagation to downstream assets only.\nNo propagation\ndisables any tag propagation.\nThis is particularly useful when you want to tag data for a business domain or project use. Atlan includes all assets under the level you tag in that business domain or project.\nTo linked assets\nâ\nAtlan can propagate tags to all\nlinked assets\nfor\nterms\n. For example:\nYou\ntag a glossary term\nas\nPublic\n.\nAtlan tags all the linked assets for that term as\nPublic\n, too, if propagation is enabled.\nYou can also configure the propagation of your tags:\nHierarchy & lineage\nenables tag propagation to child and downstream assets.\nHierarchy only (no lineage)\nenables tag propogation to child assets only.\nLineage only (no hierarchy)\nenables tag propogation to downstream assets only.\nNo propagation\ndisables any tag propagation.\nThis is particularly useful when you want to tag data en masse for use across all teams. If propagation is enabled, Atlan includes all linked assets for the tagged term.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nSuggestions from similar assets\nNext\nBrowser Extension\nTo downstream assets\nTo child assets\nTo linked assets"
  },
  {
    "url": "https://docs.atlan.com/tags/always-on",
    "content": "One doc tagged with \"always on\"\nView all tags\nAlways On\nIntegrate Atlan with Always On to enable continuous automation and suggestions."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nOn this page\nTableau\nOverview:\nCatalog dashboards, workbooks, and data sources in Atlan. Gain visibility into lineage, usage, and governance for your BI assets.\nGet started\nâ\nFollow these steps to connect and catalog Tableau assets in Atlan:\nSet up the connector\nCrawl Tableau assets\nGuides\nâ\nSet up on-premises access\n: Configure Atlan to connect to Tableau environments that are isolated from the public internet.\nCrawl on-premises Tableau\n: Extract metadata from on-premises Tableau instances.\nSet up a private network link to Tableau Server\n: Establish a secure, private network connection to Tableau Server for metadata extraction.\nReferences\nâ\nWhat does Atlan crawl from Tableau\n: Learn about the Tableau assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Tableau\n: Verify prerequisites before setting up the Tableau connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Tableau connection issues and errors.\nTags:\ntableau\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Tableau\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/tags/tableau",
    "content": "2 docs tagged with \"tableau\"\nView all tags\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance.\nTableau\nIntegrate, catalog, and govern Tableau assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/embedded",
    "content": "One doc tagged with \"embedded\"\nView all tags\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/how-to",
    "content": "One doc tagged with \"how-to\"\nView all tags\nEnable embedded metadata in Tableau\nLearn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/webhooks/how-tos/create-webhooks",
    "content": "Configure Atlan\nIntegrations\nAutomation\nWebhooks\nCreate webhooks\nOn this page\nCreate webhooks\nWho can do this?\nYou will need to be an admin user in Atlan to create webhooks.\ndanger\nIf your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or\nsubmit a request\n.\nWebhooks allow you to monitor events happening in Atlan, receive notifications for these events to a URL of your choice, and take action right away. For example, you can create a webhook to send notifications to your email address or messaging app when a\nterm\nis updated or an asset is\ntagged\n.\nWebhooks send the payload in a specific format that cannot be customized. This is meant for consumption by a programmatic entity down the line   -  for example, AWS Lambda or a microservice. For a webhook to be consumed directly, Atlan will need to customize the payload, which is currently not supported. Alternatively, you can explore out-of-the-box integrations such as\nSlack\nand\nMicrosoft Teams\n.\nAtlan currently supports creating webhooks for the following event types:\nAsset creation, deletion, and metadata update\nCustom metadata update for assets\nTag attachment or removal from assets\nCreate a webhook\nâ\nTo create a webhook:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nWebhooks\n.\nOn the\nWebhooks\npage, click\n+ New Webhook\nto create a new webhook.\nIn the\nNew Webhook\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for your webhook.\nFor\nWebhook URL\n, enter the URL for where you want to receive event notifications.\nFor\nAsset type\n, select the asset types for which you'd like to receive notifications. (This will default to all asset types, if none are specified.)\nFor\nEvent type\n, under\nAssets\n, select all the event types for which you'd like to receive notifications:\nCreate\n-  to process notifications for\nasset creation\n.\nUpdate\n-  to process notifications for when\nassets are updated\n.\nDelete\n-  to process notifications for\nasset deletion\n.\nUpdate Custom Metadata\n-  to process notifications for when\ncustom metadata is updated\nfor assets.\nAdd Tags\n-  to process notifications for when\ntags areÂ attached\nto an asset.\nDelete Tags\n-  to process notifications when\ntags areÂ removed\nfrom an asset.\nTo validate the URL you've entered, in the upper right, click the\nValidate\nbutton.\ndanger\nAtlan will send a sample payload to test if the webhook URL is correct. You will need to respond with a\n2xx\nstatus for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure.\nClick\nSave\nto finish creating your webhook.\nFrom the\nWebhook successfully created\ndialog, under\nSecret Key\n, click the clipboard icon to copy the secret key and store it in a secure location to\nverify requests from Atlan\n.\nClick\nDone\nto complete setup.\nVerify requests from Atlan\nâ\nAtlan signs its webhooks using a secret that is unique to your app. With the help of signing secrets, you can verify the authenticity of such requests with confidence.\nEach HTTP request sent from Atlan will include an\nx-atlan-signing-secret\nHTTP header. You can use the secret key for your webhook to validate requests from Atlan.\nTags:\nwebhooks\nautomation\nnotifications\nPrevious\nWebhooks Integration\nNext\nCollaboration Integrations\nCreate a webhook\nVerify requests from Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/connections",
    "content": "One doc tagged with \"connections\"\nView all tags\nConnections Integration\nIntegrate Atlan with Connections to create webhooks and automate notifications."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nMicrosoft Teams\nOn this page\nMicrosoft Teams\nOverview:\nConnect Atlan with Microsoft Teams to enable seamless collaboration, notifications, and sharing of data assets within your organization.\nGet started\nâ\nFollow these steps to connect and integrate Microsoft Teams with Atlan:\nIntegrate Microsoft Teams\nLink your Microsoft Teams account\nGuides\nâ\nIntegrate Microsoft Teams\n: Step-by-step instructions to connect Atlan with Microsoft Teams.\nLink your Microsoft Teams account\n: How to link your Microsoft Teams account to Atlan for seamless collaboration.\nTroubleshooting\nâ\nTroubleshooting Microsoft Teams integration\n: Solutions for common issues encountered when integrating Atlan with Microsoft Teams.\nFAQ\nâ\nWhat is included in the Microsoft Teams integration?\n: Learn about the features and capabilities of the Microsoft Teams integration in Atlan.\nTags:\nmicrosoft teams\nintegration\ncollaboration\nPrevious\nCollaboration Integrations\nNext\nHow to integrate Microsoft Teams\nGet started\nGuides\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nOn this page\nSlack\nOverview:\nConnect Atlan with Slack to enable seamless collaboration, notifications, and sharing of data assets within your organization.\nGet started\nâ\nFollow these steps to connect and integrate Slack with Atlan:\nIntegrate Slack\nLink your Slack account\nGuides\nâ\nIntegrate Slack\n: Step-by-step instructions to connect Atlan with Slack.\nLink your Slack account\n: How to link your Slack account to Atlan for seamless collaboration.\nTroubleshooting\nâ\nTroubleshooting Slack integration\n: Solutions for common issues encountered when integrating Atlan with Slack.\nFAQ\nâ\nHow do I send messages or search assets from Slack?\nWhat does Atlan do with each Slack permission?\nWhat is included in the Slack integration?\nWhat is the difference between Copy Link and Share on Slack or Teams?\nTags:\nslack\nintegration\ncollaboration\nPrevious\nWhat is included in the Microsoft Teams integration?\nNext\nHow to integrate Slack\nGet started\nGuides\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/setup-workflow-alerting",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSend alerts for workflow events\nOn this page\nSend alerts for workflow events\nApp\nYou can send workflow alerts to\nEmail\nand\nGoogle Chat\nto notify your team when workflows in Atlan complete or fail. You can also send alerts to\nSlack\nand\nMicrosoft Teams\nusing Atlanâs built-in notification capabilities.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAccess to the Workflow Alerting app. You can verify this by searching for Workflow Alerting in the Atlan marketplace. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nSetup workflow\nâ\nIn your Atlan workspace, go to the homepage and click\nNew workflow\nin the top navigation bar.\nSearch for\nWorkflow Alerting\n, and then select\nSet up workflow\n.\nIn the\nMode\noption, select the alert delivery method:\nEmail\nGoogle Chat\nSends workflow alerts as a tabular report to one or more email addresses, including details of the selected workflow runs.\nEnter one or more email addresses in the\nEmail IDs\nfield, separated by commas. Use valid addresses for recipients who need to receive workflow alerts.\n[email protected]\n,\n[email protected]\nIn the\nEmail subject\nfield, enter a clear, concise subject line for the alert email so recipients can quickly identify the notification.\nAtlan Workflow Failure Alert\nSends workflow alerts to a Google Chat space using a webhook URL. Alerts appear as structured Google Chat cards, which include the workflow name, status (for example,\nFailed\nor\nCompleted\n), and other run details.\nEnter the Google Chat webhook URL in the\nWebhook URL\nfield. You can generate a webhook URL by navigating to your\nGoogle Chat\nspace, selecting\nSettings\nâ\nApps & integrations\n, and adding an incoming webhook. For more information, see the\nGoogle Chat documentation\n.\nhttps://chat.googleapis.com/v1/spaces/AAAA123456/messages?key=abc123&token=xyz456\nIn\nWorkflows Created By\n, select the users whose workflows trigger alerts. Leave blank to monitor workflows from all users.\nSelect\nFailed\nto receive alerts only for failed runs, or\nAll Workflows\nto receive alerts for every workflow execution.\ninfo\nWorkflows that are still running are ignored. The package only captures workflows that have finished, either successfully or with a failure.\nIn\nScheduling Status\n, choose one of the following options to specify which workflows to monitor:\nScheduled\n: Includes only workflows with a defined run schedule.\nUnscheduled\n: Includes only workflows without a set schedule.\nAll\n: Includes both scheduled and unscheduled workflows.\nIn\nWorkflow Type\n, select the categories of workflows you want to monitor. Multiple categories can be selected if needed.\nIn\nMonitoring interval for workflow completions\n, select the time range to track workflow completions. Atlan evaluates workflows completed within the chosen interval and triggers alerts if they meet the configured criteria, enabling timely detection of issues and faster response.\nIf\nLast 24 Hours\nis selected, the alert includes all workflows that completed or failed within the past day, listed together in a single notification.\nSchedule and run\nthe workflow. Run the workflow manually or set a recurring schedule to send alerts at regular intervals.\nNeed help?\nâ\nIf you have any issues related to configuring the app, contact\nAtlan support\n.\nTags:\nautomation\nalerts\nworkflows\napp\nPrevious\nTroubleshooting spreadsheets\nNext\nCommunication Integrations\nPrerequisites\nSetup workflow\nNeed help?"
  },
  {
    "url": "https://docs.atlan.com/tags/collaboration",
    "content": "4 docs tagged with \"collaboration\"\nView all tags\nCollaboration Integrations\nIntegrate Atlan with collaboration tools like Microsoft Teams and Slack.\nMicrosoft Teams\nIntegrate Atlan with Microsoft Teams to enable collaboration and notifications.\nSlack\nIntegrate Atlan with Slack to enable collaboration and notifications.\nWhat is included in the Slack integration?\nLearn about the features and capabilities of the Slack integration with Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/teams",
    "content": "One doc tagged with \"teams\"\nView all tags\nCollaboration Integrations\nIntegrate Atlan with collaboration tools like Microsoft Teams and Slack."
  },
  {
    "url": "https://docs.atlan.com/tags/slack",
    "content": "5 docs tagged with \"slack\"\nView all tags\nCollaboration Integrations\nIntegrate Atlan with collaboration tools like Microsoft Teams and Slack.\nSlack\nIntegrate Atlan with Slack to enable collaboration and notifications.\nWhat does Atlan do with each Slack permission?\nLearn about what does atlan do with each slack permission?.\nWhat is included in the Slack integration?\nLearn about the features and capabilities of the Slack integration with Atlan.\nWhat is the difference between Copy Link and Share on Slack or Teams?\nLearn about what is the difference between copy link and share on slack or teams?."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements",
    "content": "Configure Atlan\nIntegrations\nCommunication\nSMTP and Announcements\nOn this page\nSMTP and Announcements Integration\nOverview:\nConnect Atlan with SMTP to send system announcements and notifications, keeping your teams up to date with important information.\nGuides\nâ\nHow to configure SMTP\n: Step-by-step instructions to configure SMTP for sending notifications.\nHow to create announcements\n: How to create and manage announcements for your data assets.\nHow to manage system announcements\n: How to add and remove system-wide announcements in Atlan.\nTags:\nintegrations\ncommunication\nsmtp\nannouncements\nPrevious\nCommunication Integrations\nNext\nConfigure SMTP\nGuides"
  },
  {
    "url": "https://docs.atlan.com/tags/communication",
    "content": "2 docs tagged with \"communication\"\nView all tags\nCommunication Integrations\nIntegrate Atlan with communication tools like SMTP and Announcements.\nSMTP and Announcements Integration\nIntegrate Atlan with SMTP to send announcements and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/smtp",
    "content": "2 docs tagged with \"smtp\"\nView all tags\nCommunication Integrations\nIntegrate Atlan with communication tools like SMTP and Announcements.\nSMTP and Announcements Integration\nIntegrate Atlan with SMTP to send announcements and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/announcements",
    "content": "2 docs tagged with \"announcements\"\nView all tags\nCommunication Integrations\nIntegrate Atlan with communication tools like SMTP and Announcements.\nSMTP and Announcements Integration\nIntegrate Atlan with SMTP to send announcements and notifications."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSCIM\nOn this page\nSCIM Integration\nOverview:\nConnect Atlan with SCIM to automate user provisioning and streamline identity management.\nGet started\nâ\nHow to configure SCIM provisioning\n: Learn how to set up SCIM provisioning in Atlan.\nGuides\nâ\nHow to enable Azure AD for SCIM provisioning\n: Step-by-step instructions to enable SCIM provisioning with Azure AD.\nHow to enable Okta for SCIM provisioning\n: Step-by-step instructions to enable SCIM provisioning with Okta.\nTroubleshooting & FAQ\nâ\nTroubleshooting SCIM provisioning\n: Answers to common questions and troubleshooting tips for SCIM provisioning.\nTags:\nintegrations\nidentity management\nscim\nPrevious\nIdentity Management Integrations\nNext\nConfigure SCIM provisioning\nGet started\nGuides\nTroubleshooting & FAQ"
  },
  {
    "url": "https://docs.atlan.com/tags/identity-management",
    "content": "3 docs tagged with \"identity management\"\nView all tags\nIdentity Management Integrations\nIntegrate Atlan with identity management tools like SCIM and SSO.\nSCIM Integration\nIntegrate Atlan with SCIM to automate user provisioning.\nSSO Integration\nIntegrate Atlan with SSO to enable secure authentication and access control."
  },
  {
    "url": "https://docs.atlan.com/tags/scim",
    "content": "2 docs tagged with \"scim\"\nView all tags\nIdentity Management Integrations\nIntegrate Atlan with identity management tools like SCIM and SSO.\nSCIM Integration\nIntegrate Atlan with SCIM to automate user provisioning."
  },
  {
    "url": "https://docs.atlan.com/tags/sso",
    "content": "2 docs tagged with \"sso\"\nView all tags\nIdentity Management Integrations\nIntegrate Atlan with identity management tools like SCIM and SSO.\nSSO Integration\nIntegrate Atlan with SSO to enable secure authentication and access control."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nOn this page\nJira\nOverview:\nConnect Atlan with Jira to automate ticket creation, link your Jira account, and manage data-related tasks directly from your data catalog.\nGet started\nâ\nFollow these steps to connect and integrate Jira with Atlan:\nIntegrate Jira Cloud\nIntegrate Jira Data Center\nLink your Jira account\nGuides\nâ\nIntegrate Jira Cloud\n: Step-by-step instructions to connect Atlan with Jira Cloud.\nIntegrate Jira Data Center\n: Step-by-step instructions to connect Atlan with Jira Data Center.\nLink your Jira account\n: How to link your Jira account to Atlan for seamless ticket management.\nTroubleshooting\nâ\nTroubleshooting Jira integration\n: Solutions for common issues encountered when integrating Atlan with Jira.\nFAQ\nâ\nWhat is included in the Jira integration?\nConfigure additional fields or auto-assign owners to Jira tickets created from Atlan\nCan site renaming affect the Jira integration?\nTags:\njira\nintegration\nproject management\nPrevious\nProject Management Integrations\nNext\nHow to integrate Jira Cloud\nGet started\nGuides\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow",
    "content": "Configure Atlan\nIntegrations\nProject Management\nServiceNow\nOn this page\nServiceNow\nOverview:\nConnect Atlan with ServiceNow to automate ticket creation, link your ServiceNow account, and manage data-related tasks directly from your data catalog.\nGet started\nâ\nFollow these steps to connect and integrate ServiceNow with Atlan:\nIntegrate ServiceNow\nLink your ServiceNow account\nGuides\nâ\nIntegrate ServiceNow\n: Step-by-step instructions to connect Atlan with ServiceNow.\nLink your ServiceNow account\n: How to link your ServiceNow account to Atlan for seamless ticket management.\nTroubleshooting\nâ\nTroubleshooting ServiceNow integration\n: Solutions for common issues encountered when integrating Atlan with ServiceNow.\nTags:\nservicenow\nintegration\nproject management\nPrevious\nCan site renaming affect the Jira integration?\nNext\nHow to integrate ServiceNow\nGet started\nGuides\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/tags/project-management",
    "content": "3 docs tagged with \"project management\"\nView all tags\nJira\nIntegrate Atlan with Jira to automate ticket creation and link your Jira account.\nProject Management Integrations\nIntegrate Atlan with project management tools like Jira and ServiceNow.\nServiceNow\nIntegrate Atlan with ServiceNow to automate ticket creation and link your ServiceNow account."
  },
  {
    "url": "https://docs.atlan.com/tags/jira",
    "content": "2 docs tagged with \"jira\"\nView all tags\nJira\nIntegrate Atlan with Jira to automate ticket creation and link your Jira account.\nProject Management Integrations\nIntegrate Atlan with project management tools like Jira and ServiceNow."
  },
  {
    "url": "https://docs.atlan.com/tags/servicenow",
    "content": "2 docs tagged with \"servicenow\"\nView all tags\nProject Management Integrations\nIntegrate Atlan with project management tools like Jira and ServiceNow.\nServiceNow\nIntegrate Atlan with ServiceNow to automate ticket creation and link your ServiceNow account."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight",
    "content": "Connect data\nBI Tools\nCloud-based BI\nAmazon QuickSight\nOn this page\nAmazon QuickSight\nOverview:\nCatalog Amazon QuickSight dashboards, analyses, and datasets in Atlan. Gain visibility into lineage, usage, and governance for your AWS-based analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Amazon QuickSight assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Amazon QuickSight?\n: Detailed list of QuickSight asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Amazon QuickSight\n: Technical checks and requirements needed for a successful QuickSight integration.\nTags:\namazon\nquicksight\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Amazon QuickSight\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/tags/salesforce",
    "content": "10 docs tagged with \"salesforce\"\nView all tags\nCrawl Salesforce\nOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.\nDoes Atlan require an admin user in Salesforce?\nNo. However, it is recommended that a Salesforce administrator establishes a [connection between Atlan and Salesforce](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce). To learn more, see [here](/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity).\nPreflight checks for Salesforce\nBefore [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co.\nSet up client credentials flow\nConfigure Salesforce for OAuth 2.0 client credentials authentication in Atlan.\nSet up JWT bearer flow\nConfigure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan.\nSet up Salesforce\nLearn about setting up Salesforce authentication for Atlan.\nSet up username-password flow\nConfigure Salesforce username-password flow for Atlan integration.\nWhat does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.\nWhat does Atlan crawl from Salesforce?\nAtlan only performs GET requests on these five endpoints:.\nWhy does the description from Salesforce not show up in Atlan?\nAtlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce)."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt",
    "content": "Connect data\nETL Tools\ndbt\nOn this page\ndbt\nOverview:\nCatalog dbt models, sources, tests, and documentation in Atlan. Gain visibility into lineage, transformations, and governance for your data build tool assets.\ndbt core\nâ\nGet started\nâ\nSet up the connector\nCrawl dbt Core assets\nGuides\nâ\nManage dbt tags\n: Manage and sync tags from dbt Core.\nAdd impact analysis in GitHub\n: Enable impact analysis for dbt Core projects in GitHub.\nAdd impact analysis in GitLab\n: Enable impact analysis for dbt Core projects in GitLab.\nMigrate from dbt to Atlan Action\n: Migrate your dbt Core integration to Atlan Action.\nEnrich Atlan through dbt\n: Update and enrich Atlan metadata using dbt Core.\nReferences\nâ\nWhat does Atlan crawl from dbt Core\n: Learn about the dbt Core assets and metadata that Atlan discovers and catalogs.\nPreflight checks for dbt\n: Verify prerequisites before setting up the dbt Core connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common dbt Core connection issues and errors.\ndbt cloud\nâ\nGet started\nâ\nSet up the connector\nCrawl dbt Cloud assets\nGuides\nâ\nManage dbt tags\n: Manage and sync tags from dbt Cloud.\nAdd impact analysis in GitHub\n: Enable impact analysis for dbt Cloud projects in GitHub.\nAdd impact analysis in GitLab\n: Enable impact analysis for dbt Cloud projects in GitLab.\nMigrate from dbt to Atlan Action\n: Migrate your dbt Cloud integration to Atlan Action.\nEnrich Atlan through dbt\n: Update and enrich Atlan metadata using dbt Cloud.\nReferences\nâ\nWhat does Atlan crawl from dbt Cloud\n: Learn about the dbt Cloud assets and metadata that Atlan discovers and catalogs.\nPreflight checks for dbt\n: Verify prerequisites before setting up the dbt Cloud connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common dbt Cloud connection issues and errors.\nTags:\ndbt\nconnector\netl\ndata transformation\nconnectivity\nNext\nSet up dbt Cloud\ndbt core\ndbt cloud"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/troubleshooting/troubleshooting-dbt-connectivity",
    "content": "Connect data\nETL Tools\ndbt\nTroubleshooting\nConnection issues\nOn this page\nConnection issues\nThis guide helps you resolve common connection and authentication issues when setting up the dbt connector in Atlan.\nWhat are the known limitations of the dbt connector?\nâ\nFollowing are the known limitations:\nFor\ndbt Core\n, Atlan currently only processes the status of dbt tests   -  passed, failed, and warning   -  but not the job name and execution time. However, this information is available for\ndbt Cloud\n. You can hover over the test\nStatus\nin the asset sidebar to view this information.\nColumn-level lineage for dbt models is currently unavailable at source. However, Atlan applies a custom SQL-parsing algorithm to the materialized SQL tables to generate column-level lineage for dbt models. Note that due to the limitations of SQL parsing, Atlan doesn't guarantee generating lineage for all columns.\nEphemeral dbt models don't materialize tables at source. Hence, these doesn't create lineage for the materialized layer or publish dbt model columns in Atlan. This is an expected behavior of\nephemeral dbt models\n.\nA dbt source can include multiple SQL warehouse tables. Lineage between dbt sources and models isn't supported. Instead, Atlan builds lineage between dbt models and SQL tables when dbt models have an explicit dependency on specific SQL tables of a dbt source.\nWhy are my dbt Cloud models not showing up in Atlan after setup?\nâ\nAtlan only crawls dbt assets that are in the âappliedâ (built) state in dbt Cloud. Models must be part of a successful run to be picked up during crawling; models that are only defined in your project files but havenât been executed wonât be included. For more information about project state, see\nProject states in dbt Cloud.\nDoes Atlan support data types for dbt model columns?\nâ\nYes, Atlan supports column data types for dbt models:\ndbt Cloud\n: Make sure there is at least one job run with the\nGenerate docs on run\noption\nenabled for every environment where dbt models are executed.\ndbt Core\n-  upload the\nmanifest.json\nand\ncatalog.json\nfiles generated by the\ndbt docs generate\ncommand for every dbt Core project. Refer to\ndbt Core documentation\nto learn how to structure the bucket while uploading your files.\nWhy are columns for dbt models missing?\nâ\nIf you've crawled your dbt models but columns are missing:\nDefine the columns in the\ncolumns\nattribute of the model's\n.yml\ndefinition in the\nmanifest.json\nfile.\nCheck if the dbt model is materializing a table or view asset, without which columns for a dbt model don't show up.\nIs dbt source metadata available for materialized columns?\nâ\nIf\nmetadata enrichment is enabled\nfor assets that dbt materializes, only table assets are updated with the\nmaterialized by\ndbt source or model metadata. Materialized columns are only updated with dbt model metadata   -  dbt source metadata is currently not supported for column assets.\nCan I map the Atlan GitHub action to multiple dbt projects?\nâ\nYes, you can configure the\nAtlan GitHub action\nfor multiple dbt projects.\nDoes Atlan support syncing terms from dbt?\nâ\nYes, you can update terms from dbt to Atlan. For more details, refer to the\ndeveloper documentation\n.\nWhy are some dbt tests missing?\nâ\nAtlan doesn't support crawling dbt tests with an auto-generated\nunique_id\nthat exceeds the character limit of 32,000 characters. If you want to catalog such dbt tests in Atlan, you need to\ndefine a custom name\nfor your dbt tests within the character limit.\nWhy is there a discrepancy in dbt test count between dbt and Atlan?\nâ\nAtlan fetches dbt models, sources, and tests from the\napplied state\nof each dbt environment. This is the most recent project state for each environment. The asset count on Atlan may differ from whatâs present in the manifest files for individual job runs. Refer to\ndbt documentation\nto learn more about dbt project states.\nWhy does Atlan link the dbt model description to my SQL source table?\nâ\nIf a materialized table is linked to multiple dbt assets, Atlan applies the description from the linked dbt model to the materialized asset. In case this is unavailable, Atlan then applies the description from the linked dbt source to the asset.\nWhy does Atlan link the dbt seed description to my SQL source table?\nâ\nIf a materialized table is linked to multiple dbt assets, Atlan applies the description from the linked dbt seed to the materialized asset. In case this is unavailable, there is no description available.\nPrevious\nWhat does Atlan crawl from dbt Core?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery",
    "content": "Connect data\nData Warehouses\nGoogle BigQuery\nOn this page\nGoogle BigQuery\nOverview:\nCatalog Google BigQuery projects, datasets, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your cloud data warehouse assets.\nGet started\nâ\nFollow these steps to connect and catalog Google BigQuery assets in Atlan:\nSet up the connector\nCrawl Google BigQuery assets\nGuides\nâ\nMine Google BigQuery\n: Extract query history and build lineage for your BigQuery assets.\nEnable SSO for Google BigQuery\n: Set up SSO authentication for BigQuery connections.\nManage Google BigQuery tags\n: Configure and manage tags and policy tags in BigQuery.\nReferences\nâ\nWhat does Atlan crawl from Google BigQuery\n: Learn about the BigQuery assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Google BigQuery\n: Verify prerequisites before setting up the Google BigQuery connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Google BigQuery connection issues and errors.\nTags:\ngoogle\nbigquery\nconnector\ndata warehouse\nconnectivity\nNext\nSet up Google BigQuery\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nOn this page\nIBM Cognos Analytics\nOverview:\nCatalog reports, dashboards, and data modules from IBM Cognos Analytics in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog IBM Cognos Analytics assets in Atlan:\nSet up the connector\n: Configure Atlan to connect to your IBM Cognos Analytics environment.\nCrawl assets\n: Extract and catalog IBM Cognos Analytics reports, dashboards, and data modules.\nGuides\nâ\nSet up on-premises access\n: Configure Atlan to connect to IBM Cognos Analytics environments that are isolated from the public internet\nCrawl on-premises IBM Cognos Analytics\n: Step-by-step instructions for extracting metadata from on-premises IBM Cognos Analytics instances.\nReferences\nâ\nWhat does Atlan crawl from IBM Cognos Analytics?\n: Detailed list of IBM Cognos Analytics asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to IBM Cognos Analytics, including permissions and network problems.\nTags:\nibm cognos\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up IBM Cognos Analytics\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/troubleshooting/troubleshooting-ibm-cognos-analytics-connectivity",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nIBM Cognos Analytics\nTroubleshooting\nTroubleshooting IBM Cognos Analytics connectivity\nOn this page\nTroubleshooting IBM Cognos Analytics connectivity\nWhat are the known limitations of the IBM Cognos Analytics connector?\nâ\nAtlan currently does not support the following:\nField-level lineage for IBM Cognos Analytics assets.\nWhy are some assets missing?\nâ\nDue to issues with the IBM Cognos Analytics API response, it is possible that the connection may only catalog a partial number of assets. If you notice any inconsistencies among your cataloged assets in Atlan, raise a support ticket with IBM Cognos Analytics and\ncontact Atlan support\nas well to triage the issue.\nTags:\nlineage\ndata-lineage\nimpact-analysis\napi\nrest-api\ngraphql\ncatalog\nmetadata\ndiscovery\nPrevious\nWhat does Atlan crawl from IBM Cognos Analytics?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nOn this page\nLooker\nOverview:\nCatalog explores, dashboards, and models from Looker in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Looker assets in Atlan:\nSet up the connector\nCrawl Looker assets\nGuides\nâ\nSet up on-premises access\n: Configure Atlan to connect to Looker environments that are isolated from the public internet.\nCrawl on-premises Looker\n: Extract metadata from on-premises Looker instances.\nReferences\nâ\nWhat does Atlan crawl from Looker\n: Learn about the Looker assets and metadata that Atlan discovers and catalogs.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Looker connection issues and errors.\nTags:\nlooker\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Looker\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/troubleshooting/troubleshooting-looker-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nTroubleshooting\nTroubleshooting Looker connectivity\nOn this page\nTroubleshooting Looker connectivity\nWhat are the known limitations of the Looker connector?\nâ\nFollowing are the known limitations of the Looker connector:\nPersistent derived tables (PDTs) and Liquid parameterized tables are currently not supported for\nLooker views\n.\nWhat is the timeframe for calculating\nsourceViewCount\nfor Looks and dashboards?\nâ\nThe Atlan property\nsourceViewCount\nis based on the source property\nview_count\n:\nview_count\ndisplays a total count of views for a Look or dashboard since creation of the asset.\nAtlan currently does not support a timestamp for views due to limitations of the Looker APIs.\nWhy do I get a 403 forbidden error after clicking the test authentication button?\nâ\nIf you receive a 403 forbidden error when testing authentication, confirm the following:\nEnsure you have provided the API host URL for Looker. (This may be different from the Looker instance URL.)\nEnsure your Looker instance is running on port 433. If not, add the port number explicitly to the URL.\nEnsure your client ID and secret are correct.\nEnsure you have allowlisted the Atlan IP in your Looker instance. (To get details of your Atlan IP, please\nraise a support ticket\n.)\nOnce you have confirmed or corrected these, retry the authentication.\nWhy is lineage missing from an Explore to a Tile or Look?\nâ\nIf lineage is missing from an Explore to a Tile or Look, this is usually because you have not\ngiven content access to a folder\n.\nOnce you have corrected the access in Looker, rerun the Looker crawler in Atlan.\nWhy is lineage missing between data sources and Looker assets?\nâ\nIf lineage is missing from data sources to Looker assets, confirm the following:\nEnsure you have given the\nsee_sql\npermission to the role for which you\ngenerated the API3 credentials\n.\nEnsure you have crawled the data sources' assets in Atlan,\nbefore\ncrawling the Looker assets.\nOnce you have confirmed or corrected these, rerun the Looker crawler in Atlan.\nWhy is lineage missing from tables to models?\nâ\nIf lineage is missing from tables to models, this is usually because you have excluded a folder or project that contains these lineage details.\nOnce you have corrected this in the crawler configuration, rerun the Looker crawler in Atlan.\nWhy is relationship missing between models and queries, or Looks and models?\nâ\nIf a relationship is missing between some Looker assets, this is usually because you have excluded a project that contains these details.\nOnce you have corrected this in the crawler configuration, rerun the Looker crawler in Atlan.\nCan I use deploy keys for setup?\nâ\nDeploy keys\nonly grant access to a single repository. For more complex projects where Looker connects to multiple repositories, Atlan does not support using deploy keys.\nWhy am I seeing a failed to clone project error?\nâ\nException: ('Failed to clone project %s. Git URL: %s', '<project>', '<git_url>')\nIf you encounter the above error, please ensure that the SSH key youâre using has access to all the GitHub project files included for crawling. Refer to the\nLooker setup documentation\nto learn more.\nTags:\ndashboards\nvisualization\nanalytics\nPrevious\nPreflight checks for Looker\nNext\nTroubleshooting on-premises Looker connectivity"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMode\nOn this page\nMode\nOverview:\nCatalog reports, queries, and dashboards from Mode in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Mode assets in Atlan:\nSet up the connector\n: Configure Atlan to connect to your Mode environment.\nCrawl assets\n: Extract and catalog Mode reports, queries, and dashboards.\nReferences\nâ\nWhat does Atlan crawl from Mode?\n: Detailed list of Mode asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Mode, including permissions and network problems.\nTags:\nmode\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Mode\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud",
    "content": "Connect data\nBI Tools\nCloud-based BI\nQlik Sense Cloud\nOn this page\nQlik Sense Cloud\nOverview:\nCatalog apps, sheets, and data sources from Qlik Sense Cloud in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Qlik Sense Cloud assets in Atlan:\nSet up the connector\n: Configure Atlan to connect to your Qlik Sense Cloud environment.\nCrawl assets\n: Extract and catalog Qlik Sense Cloud apps, sheets, and data sources.\nReferences\nâ\nWhat does Atlan crawl from Qlik Sense Cloud?\n: Detailed list of Qlik Sense Cloud asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Qlik Sense Cloud, including permissions and network problems.\nTags:\nqlik sense\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Qlik Sense Cloud\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud",
    "content": "Connect data\nBI Tools\nCloud-based BI\nQlik Sense Cloud\nGet Started\nSet up Qlik Sense Cloud\nOn this page\nSet up Qlik Sense Cloud\nWho can do this?\nYou will need your Qlik Sense Cloud tenant administrator to complete these steps   -  you may not have access yourself.\nInvite a new user\nâ\nTo\ninvite a new user\nfor Atlan to use when integrating with Qlik Sense Cloud:\nLog in to your Qlik Sense Cloud instance.\nFrom the upper right corner of your Qlik Sense Cloud instance, click the\nmenu icon\nand then click\nManagement Console\n.\nIn the left menu under\nGovernance\nin the\nManagement Console\n, click the\nUsers\ntab.\nFrom the top right of the\nUsers\npage, click\nInvite\nto invite a new user.\nIn the\nInvite users\ndialog, for\nEmail addresses\n, enter the email address of the new user that you want to invite and click\nInvite\n.Â\nYou can also set up roles and groups for robust access management.\nSet permissions\nâ\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the objects in your Qlik Sense Cloud instance.\nOnce you've invited a new user, ensure that the new user has the following minimum permissions for\ncrawling Qlik Sense Cloud\n:\nAssign\ndeveloper\nrole\nto the new user to allow the creation of API keys.\nEnsure that the new user has\nList\n,\nRead\n, and\nOpen\npermissions to spaces, apps, data, sheets, charts, and connections in Qlik Sense Cloud.\nIn order to ensure that the new user has the above permissions to Qlik Sense Cloud objects, the new user can either be the creator of the objects, have access to the\nshared or managed spaces\nwhere the objects are located, or the\nobjects need to be made public\n.Â\nEnable API key creation\nâ\nOnce you have created a new user and ensured the necessary permissions, you will need to enable API key creation for that user.\nTo enable API key creation for the new user:\nFrom the upper left corner of your Qlik Sense Cloud instance, click the menu icon and then click\nManagement Console\n.\nIn the left menu under\nManagement Console\n, click the\nSettings\ntab.\nFrom\nSettings\n, scroll down to\nAPI keys\nand enter the following details:\nFor\nEnable API keys\n, toggle the slider on to enable the creation of API keys.\nFor\nChange maximum token expiration\n, enter a value for allowed maximum token age.\nFor\nChange maximum of active API keys per user\n, enter a value for the maximum number of active API keys that a user may have in the tenant.\nRefresh the\nManagement Console\npage and navigate to the left menu to view a new tab for\nAPI keys\n.\nGenerate API key\nâ\nOnce API key creation has been enabled, the new user can\ngenerate API credentials\nfor crawling Qlik Sense Cloud.\nTo generate an API key for\ncrawling Qlik Sense Cloud\n:\nLog in to your Qlik Sense Cloud instance as the new user. (If you just completed the steps above, you'll need to log out from the tenant admin account first.)\nFrom the top right of your Qlik Sense Cloud instance, click your\nprofile avatar\nand then click\nProfile settings\n.\nFrom the left menu under\nManagement\n, click\nAPI keys\n.\nFrom the upper right of the\nAPI keys\npage, click\nGenerate new key\n.Â\nIn the\nGenerate a new API key\ndialog, enter an API key description and select an expiration time within the allowed maximum token age.Â\nClick\nGenerate\nto generate an API key.\nFrom the resulting dialog, copy the generated API key value and save it in a temporary location.\ndanger\nThe API key is only displayed once. You need to copy and save the value since there is no way to see that specific key again. You will have to generate a new key if you do not copy it.\nTags:\napi\nrest-api\ngraphql\nPrevious\nQlik Sense Cloud\nNext\nCrawl Qlik Sense Cloud\nInvite a new user\nSet permissions\nEnable API key creation\nGenerate API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/troubleshooting/troubleshooting-qlik-sense-cloud-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nQlik Sense Cloud\nTroubleshooting\nTroubleshooting Qlik Sense Cloud connectivity\nOn this page\nTroubleshooting Qlik Sense Cloud connectivity\nWhat are the known limitations of the Qlik Sense Cloud connector?\nâ\nFollowing are the known limitations of the Qlik Sense Cloud connector, related to lineage:\nLineage can be missing in the following scenarios:\nData joins or concatenations made in the Data manager within a Qlik App.\nCalculated fields created in the Data manager within a Qlik App.\nJoins within QVD files.\nAdvanced usage\nAtlan currently does not support upstream lineage for Qlik applications that use:\nother apps as sources\ndatasets loaded from the file system via SMTP or RESTful web services\nDue to limitations at source, Atlan does not get the requisite metadata to generate upstream lineage to data sources.\nTo learn more about lineage errors in general, refer to\nQlik documentation\n.\nWhy is the count of charts in Atlan much higher than at source?\nâ\nAtlan catalogs only charts that include fields. Charts without fields are not cataloged, which may result in a higher chart count in Atlan compared to the source. For more details, see\nWhat Atlan crawls from Qlik Sense Cloud\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nupstream-dependencies\ndata-sources\napi\nrest-api\ngraphql\nPrevious\nPreflight checks for Qlik Sense Cloud"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nRedash\nOn this page\nRedash\nOverview:\nCatalog queries, dashboards, and visualizations from Redash in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Redash assets in Atlan:\nSet up the connector\n: Configure Atlan to connect to your Redash environment.\nCrawl assets\n: Extract and catalog Redash queries, dashboards, and visualizations.\nReferences\nâ\nWhat does Atlan crawl from Redash?\n: Detailed list of Redash asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Redash, including permissions and network problems.\nTags:\nredash\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Redash\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce",
    "content": "Connect data\nCRM\nSalesforce\nOn this page\nSalesforce\nOverview:\nAtlan's Salesforce connector enables you to discover, catalog, and govern your CRM data assets for improved data visibility and governance.\nGet started\nâ\nFollow these steps to connect and catalog Salesforce assets in Atlan:\nSet up the connector\n: Configure authentication and connection settings for Salesforce\nCrawl Salesforce assets\n: Discover and catalog your Salesforce objects, fields, and metadata\nReferences\nâ\nWhat does Atlan crawl from Salesforce\n: Understand the metadata and assets that Atlan discovers from your Salesforce instance\nPreflight checks for Salesforce\n: Verify your Salesforce configuration before connecting to Atlan\nTroubleshooting\nâ\nTroubleshooting Salesforce connectivity\n: Resolve common connection and authentication issues\nFAQ\nâ\nAdmin user requirements for Salesforce\n: Understand the admin user requirements for connecting Salesforce to Atlan\nSalesforce description not showing\n: Troubleshoot issues with Salesforce descriptions not appearing in Atlan\nTags:\nconnector\ncrm\nconnectivity\nNext\nSet up Salesforce\nGet started\nReferences\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSigma\nOn this page\nSigma\nOverview:\nCatalog Sigma workbooks, pages, and elements in Atlan. Gain visibility into lineage, usage, and governance for your Sigma analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Sigma assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Sigma?\n: Detailed list of Sigma asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Sigma\n: Technical checks and requirements needed for a successful Sigma integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Sigma, including permissions and network problems.\nTags:\nsigma\nconnector\nbusiness intelligence\nhybrid bi\nconnectivity\nNext\nSet up Sigma\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSigma\nGet Started\nSet up Sigma\nOn this page\nSet up Sigma\nWho can do this?\nYou will probably need your Sigma administrator to complete these steps   -  you may not have access yourself.\nIdentify your organization's cloud\nâ\nYou will need your organization's cloud information to determine the endpoint while authenticating in Atlan.\nTo identify your organization's cloud:\nOpen your Sigma account.\nIn the top right of the screen, click your\nprofile avatar\nand then click\nAdministration\nto open your\nAccount\npage.\nOn the\nAccount\npage, under\nSite\nin\nGeneral Settings\n, view the cloud information.\nCreate an API token and client ID\nâ\nTo create an API token and client ID:\nOpen your Sigma account.\nIn the top right of the screen, click your profile avatar and then click\nAdministration\n.\nFrom the left menu of the\nAdministration\npage, click\nDeveloper Access\n.\nIn the top right of the\nDeveloper Access\npage, click the\nCreate New\nbutton.\nIn the\nCreate client credentials\ndialog, for\nSelect privileges\n, click the\nRest API\ncheckbox, and then enter the following details:\nFor\nName\n, enter a meaningful name.\nFor\nOwner\n, select the user you would like to associate with the token.\nClick\nCreate\nto finish creating the API token.\nOnce prompted, click\nCopy\nto copy and paste your API key secret in a secure location.\ndanger\nThe secret cannot be retrieved once this popover is closed.\nYour newly created API token will be listed in the\nDeveloper Access\nÂ page. Hover over the tokenâs\nClient ID\nand click\nCopy\n.\nBoth the API token and the client ID are required for authentication in Atlan.\nVerify necessary permissions\nâ\nEnsure that the owner associated with the API token has the following permissions:\nCan View\npermission for all the Sigma workbooks and datasets you want to crawl.\nCan Use\npermission for all the Sigma connections used in the workbooks and datasets.\nGrant permissions\nâ\nTo grant permissions, follow the instructions in these links:\nWorkbooks and datasets\nConnections\nTags:\napi\nrest-api\ngraphql\nPrevious\nSigma\nNext\nCrawl Sigma\nIdentify your organization's cloud\nCreate an API token and client ID\nVerify necessary permissions\nGrant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/troubleshooting/troubleshooting-sigma-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSigma\nTroubleshooting\nTroubleshooting Sigma connectivity\nOn this page\nTroubleshooting Sigma connectivity\nWhy is the SQL query only visible for Sigma data elements and not Sigma datasets?\nâ\nAtlan gets the SQL query only for Sigma data elements. SQL queries for Sigma datasets are currently unavailable from the Sigma APIs.\nWhy am I not seeing columns for Sigma datasets?\nâ\nAtlan currently does not support column metadata for Sigma datasets due to limitations of the Sigma APIs.\nWhy am I not seeing Sigma datasets in lineage?\nâ\nAtlan currently does not support showing lineage for Sigma datasets due to limitations of the Sigma APIs.\nWhy is lineage missing between Sigma data elements?\nâ\nAtlan currently does not support field-level lineage between Sigma data elements due to limitations of the Sigma Workbook APIs. However, to maintain field-level lineage, all Sigma data elements will have direct upstream lineage to SQL assets and field-level lineage to its column assets.\nCan users who do not have access to a workbook still see the preview?\nâ\nUsers can only see asset previews if the following conditions are met:\nThey have the necessary permissions in both Sigma and Atlan.\nThey are logged into Atlan and Sigma on the same browser.\nTherefore, if a user lacks the permission to view a workbook in Sigma, they will not be able to see the workbook preview in Atlan. Even if they do have the necessary permissions, they will need to be logged into Sigma on the same browser as their Atlan instance for asset previews to work.\nWhy can I not see previews for my Sigma assets?\nâ\nYour Sigma assets will be updated with previews during the next run of your Sigma workflow. If you have run the workflow and still do not see the previews, we suggest you rerun the workflow. Once you've rerun the workflow, the previews should be visible to all eligible users.\nTags:\nlineage\ndata-lineage\nimpact-analysis\napi\nrest-api\ngraphql\nPrevious\nPreflight checks for Sigma"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nTableau\nTroubleshooting\nTroubleshooting Tableau connectivity\nOn this page\nTroubleshooting Tableau connectivity\nWhat are the known limitations of the Tableau connector?\nâ\nAtlan currently does not support the following:\nCrawling\nTableau flows\nwhen using the\nJWT bearer authentication method\n, due to limitations at source.\nCrawling\ntags\nfrom Tableau.\nCataloging Tableau Pulse, stories, and views.\nParsing custom SQL queries, where all tables referenced (whether in the main query or subqueries) are considered as upstream lineage for the asset.\nWhy does Atlan require the Site Administrator Explorer role in Tableau?\nâ\nAtlan requires the\nSite Administrator Explorer\nrole in Tableau to extract data source fields and calculated fields. It is not possible to fetch data source fields and calculated fields with the\nViewer\nrole in the current version of the Tableau Metadata API. Atlan uses this data to generate granular column-level lineage across data sources and SQL assets. To extract lineage for assets in Tableau, the user must have the\nSite Administrator Explorer\nrole.\nIs lineage available for Tableau custom SQL data sources?\nâ\nYes, Atlan can parse custom SQL queries in Tableau to generate lineage between the data source and tables. Lineage is available for tables from all SQL sources. However, column-level lineage is currently not supported.\nWhy is upstream lineage missing for Tableau data sources?\nâ\nIf your\nTableau data source\nis in a\npaused state\n, the Tableau Metadata API may fail to provide the requisite metadata on source databases and tables for Atlan to generate upstream lineage. Restart your Tableau data source and ensure that it remains active while\ncrawling Tableau\n. This will allow Atlan to fetch the requisite metadata to generate upstream lineage for data sources.\nWhy is there a discrepancy in asset count between Tableau and Atlan?\nâ\nDashboards   -  the Tableau UI does not display a unique count of dashboards. Dashboards in Tableau are represented in collections of one or more views. These may have same names as the views but are independent objects. Hence, the total count of these views in Tableau does not match the dashboard count in Atlan. Atlan sources the dashboard count from the Tableau API, which is the only reliable way to fetch the dashboard count.\nData sources   -  embedded data sources are not reported on the Tableau UI. However, in Atlan, data sources can be filtered to show only published data sources, which should match the count of data sources on the Tableau UI.\nCan users who do not have access to a dashboard still see the preview?\nâ\nUsers can only see asset previews if the following conditions are met:\nThey have the necessary permissions in both Tableau and Atlan.\nThey are logged into Atlan and Tableau on the same browser.\nTherefore, if a user lacks the permission to view a dashboard in Tableau, they will not be able to view the dashboard preview in Atlan. Even if they do have the necessary permissions, they will need to be logged into Tableau on the same browser as their Atlan instance for asset previews to work.\nWhy can I not see previews for my Tableau assets?\nâ\nYour Tableau assets will be updated with previews during the next run of your Tableau workflow. If you have run the workflow and still do not see the previews, we suggest you rerun the workflow. Once you've rerun the workflow, the previews should be visible to all eligible users.\nIf you're using Tableau Server with clickjack protection enabled and your Tableau instance URL is of a different origin than the Atlan instance URL, the asset previews will not load due to a same-origin error from the browser. You will need to\ndisable clickjack protection\nto allow the Tableau asset previews to load.\nIs the certified status in Tableau mapped to the certificates field in Atlan?\nâ\nYes, the\nisCertified status\nfor published data sources in Tableau is mapped to the\ncertificates\nfield in Atlan.\nIs the owner field in Tableau mapped to the owners field in Atlan?\nâ\nNo, the asset owner in Tableau is displayed as the source owner in the\nOverview\nsection of the\nasset sidebar\nin Atlan. This is also only available for Tableau\nprojects\n,\nflows\n,\nworkbooks\n, and published\ndata sources\n.\nTableau has\nretired metrics methods in API 3.22\n, hence source owner attribute for\nmetrics\nis not supported in Atlan.\nWhy am I getting a \"still creating the Metadata API store\" error?\nâ\nError message:\nStill creating the Metadata API Store. Results from the query might be incomplete at this time. BACKFILL-RUNNING\nIf your Tableau workflow is failing with the above error message, this is because the\nTableau Metadata API is being re-indexed\nafter a quarterly release. The re-indexing of the Metadata API after quarterly releases can take up to a week, depending on the size of your instance.\nSince\nAtlan uses the Tableau Metadata API\nto fetch metadata, your Tableau workflows in Atlan may fail if the re-indexing has not been completed. You can check the backfill status of the Tableau Metadata API Store following this\nguide\n.\nLearn more about\ncommon errors\nin your Metadata API query.\nHow to debug test authentication and preflight check errors?\nâ\nIncorrect hostname\nUnable to connect to the specified host. Please verify that the host details are correct and retry.\nEnsure that you have entered the hostname for your Tableau Online or Tableau Server instance correctly.\nIf you're using a domain name, verify that the DNS name correctly resolves to the corresponding IP address.\nConnection timed out\nUnable to connect to Tableau instance. Please verify server port or check if your server is up and running.\nEnsure that you're using the correct port number, especially if using a custom port for Tableau Server.\nVerify network connectivity and DNS resolution   -  you can also test from a different network or device.\nSSL error\nUnable to connect. Please check your SSL setting.\nEnsure that the server URL uses\nhttps\nif\nSSL is enabled\n. If the connection does not require an SSL, use\nhttp\ninstead.\nThe ssl details provided are incorrect. Please provide correct ssl certs.\nIf your Tableau Server instance uses a self-signed or an internal CA SSL certificate, enter the\nSSL certificate correctly in the recommended format\n.\nIncorrect port number\nUnable to connect to Tableau instance. Please verify server port and retry.\nEnsure that you're using the correct port number, especially if using a custom port for Tableau Server.\nInvalid personal access token\nThe personal access token you provided is invalid. Please check your PAT name and token value.\nEnsure that you have entered the token name correctly and it matches the token generated in Tableau:\nToken name is case-sensitive.\nEnsure that there are no extra spaces or characters.\nIf the token you provided is invalid, you can\ncreate a new token\n.Â\nIncorrect site details\nThe site details provided are incorrect. Please provide correct site details.\nConfirm that the site name in the URL matches the exact case and spelling of the site you are trying to access. Site names in Tableau are case-sensitive.\nIncorrect username\nProvided username is incorrect. Please check.\nConfirm that the username is present in Tableau. Otherwise, you can\nadd a new user for basic authentication\n.\nIncorrect client ID\nThe client id provided is incorrect or site is empty or connected app in tableau is deleted. Please check and try again.\nEnsure that you have specified the site name if using JWT bearer authentication.\nEnsure that the\nconnected app\nis present in Tableau and verify the client ID.\nIncorrect secret ID or value\nThe secret id provided is incorrect or the secret value is deleted. Please check and try again.\nor\nThe secret value provided is incorrect. Please check your secret value and try again.\nVerify the secret ID of the\nconnected app\n.\nEnsure that the secret value of the\nconnected app\nhas not been deleted.\nTags:\ncatalog\nmetadata\ndiscovery\nPrevious\nPreflight checks for Tableau"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot",
    "content": "Connect data\nBI Tools\nCloud-based BI\nThoughtSpot\nOn this page\nThoughtSpot\nOverview:\nCatalog ThoughtSpot liveboards, visualizations, and worksheets in Atlan. Gain visibility into lineage, usage, and governance for your ThoughtSpot analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog ThoughtSpot assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from ThoughtSpot?\n: Detailed list of ThoughtSpot asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to ThoughtSpot, including permissions and network problems.\nTags:\nthoughtspot\nconnector\nbusiness intelligence\nhybrid bi\nconnectivity\nNext\nSet up ThoughtSpot\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nMicroStrategy\nOn this page\nMicroStrategy\nOverview:\nCatalog MicroStrategy reports, documents, and dossiers in Atlan. Gain visibility into lineage, usage, and governance for your MicroStrategy analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog MicroStrategy assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from MicroStrategy?\n: Detailed list of MicroStrategy asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for MicroStrategy\n: Technical checks and requirements needed for a successful MicroStrategy integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to MicroStrategy, including permissions and network problems.\nTags:\nmicrostrategy\nconnector\nbusiness intelligence\nhybrid bi\nconnectivity\nNext\nSet up MicroStrategy\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/getting-started-with-the-apis",
    "content": "Get Started\nQuick Start Guides\nDevelopers\nSoftware development kits (SDKs)\nSoftware development kits (SDKs)\nYou can integrate with Atlan in several ways. We recommend using one of our\nsoftware development kits (SDKs)\n, if possible, which:\nencode best practices,\nare built for performance,\nand add validations and simplifications not found in the raw REST APIs themselves.\nDid you know?\nOur SDKs are nothing more than client libraries that wrap our underlying REST APIs. They prevent you from needing to reinvent the wheel by wrapping the REST APIs yourself.\nTags:\napi\nrest-api\ngraphql\nPrevious\nCustom solutions\nNext\nAtlan's open API"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/delegate-administration",
    "content": "Configure Atlan\nAccess control\nManage users and groups\nDelegate administration\nOn this page\nDelegate administration\nAtlan allows you to define granular access controls and delegate administrative functions with admin subroles. Atlan currently supports the following built-in admin subroles:\nWorkflow admin\nÂ   -  the workflow admin subrole allows Atlan admins to:\nGrant administrative access to users to manage\nconnectors\nand\nconnection workflows\nonly.\nRestrict access to admin capabilities in the admin center and governance capabilities in the governance center.\nGovernance admin\n-  the governance admin subrole allows Atlan admins to:\nGrant administrative access to users to manage governance capabilities only.\nRestrict access to admin capabilities in the admin center and\nconnectors\nand\nconnection workflows\nin the workflow center.\nAssign a subrole\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to assign an admin subrole.\nTo assign an admin subrole:\nFrom the left menu of any screen in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nUsers\n.\nTo assign an admin subrole, you can either:\nTo assign the subrole to an existing user, navigate to any user and click the\nRole\ndropdown. In the\nSelect Role\ndialog, click\nWorkflow Admin\nor\nGovernance Admin\nand then click\nUpdate\n.\nTo assign the subrole to a new user, follow the steps in\nHow to invite new users\nwithout SSO. Change the role of the user to\nWorkflow Admin\nor\nGovernance Admin\nand then click the\nSend Invite\nbutton.\nWorkflow admin\nâ\nThe workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows.\nPermissions\nâ\nA workflow admin has the following permissions and capabilities:\nConnections\n:\nCreate a new connection for\nsupported sources\nView all connections\nManage all connections from the\nConnections\ntab in the\nGovernance center\nEdit an existing connection   -  the user must also be a\nconnection admin\nfor that specific connection or have a\npolicy\ngranting them access to the connection.\nWorkflows\n:\nCreate and manage workflows from the\nWorkflow center\nView all workflows and workflow runs\nEdit or delete any workflow credentials   -\nconnection admin\naccess not required\nRun any workflow\nAdd, remove, or edit schedules for any workflow\nThe following capabilities work exactly as that of a\nmember user\n:\nAsset search and discovery\n-  can update metadata for assets in a connection that the workflow admin either created or was added to as a connection admin.\nGlossary\n-  can view all glossaries but will require edit access through\nglossary policies\n. If\nglossary restrictions are in place\n, then the workflow admin will only be able to view the glossaries as per their glossary policies.\nInsights\n-  requires\ndata policies\nto query data and preview sample data.\nReporting center\n-  if\nenabled by admins\n, can view the assets, glossary, Insights, and usage and cost dashboards.\nData products\n-  requires\ndomain policies\nto access domains and products.\nRestrictions\nâ\nA workflow admin has the following explicit restrictions:\nCan only access the\nConnections\ntab in the\nGovernance center\n.\nCannot delete any existing connections using the\nConnection Delete\nworkflow\n.\nCannot access or perform any actions in the\nAdmin center\n.\nIs excluded from the default\nAll Admins\ngroup in any workflow configuration.\n(Optional) Restrict workflow visibility\nâ\nBy default, all workflow admins can see the existence of all workflows. However, you may want to limit specific teams from being able to see all workflows in Atlan. You can optionally turn off the default behavior to restrict workflow visibility.\nOnce you have turned off the default behavior, in the\nWorkflow center\n:\nThe\nMonitor\ntab will no longer be visible to workflow admins.\nThe\nManage\ntab will display only the workflows created by workflow admins themselves.\nIf there are no existing workflows, a workflow admin will only have access to the\nMarketplace\ntab to create a new one.\nTo restrict workflow visibility:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAccess Control\nheading of the\nLabs\npage, turn off\nAllow workflow admins to access all workflows\n.\nYour workflow admins will now only have access to the workflows they created by default.\nIf you'd like to restore the default behavior, follow the steps above and then turn it on.\nGovernance admin\nâ\nThe governance admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for managing the governance center.\nPermissions\nâ\nA governance admin has the following permissions and capabilities:\nPersonas\n:\nCreate and manage personasÂ from the\nGovernanceÂ center\nView all personas\nEdit users and policies for existing personas   -  the user must either also be a\nconnection admin\nor have a\npolicy\ngranting them access to the persona.\nPurposes\n:\nCreate and manage purposes from the\nGovernance center\nView all purposes\nEdit users and policies for existing purposes   -  the user must either also be a\nconnection admin\nÂ or have a\npolicy\ngranting them access to the purpose.\nGovernance workflows\n-  create and manage governance workflows\nPlaybooks\n-  create and run playbooks\nPolicy center\n-  create and manage data governance policies\nREADME templates\n-  create and manage README templates\nTags\n-  create and manage tags\nDomains\n-  only manage domains, cannot create them\nCustom metadata\n,\nbadges\n, and\noptions\n-  create and manage custom metadata and associated properties\nThe following capabilities work exactly as that of a\nmember user\n:\nAsset search and discovery\n-  can update metadata for assets in a connection that the governance admin was added to as a connection admin.\nGlossary\n-  can view all glossaries but will require edit access through\nglossary policies\n. If\nglossary restrictions are in place\n, then the governance admin will only be able to view the glossaries as per their glossary policies.\nInsights\n-  requires\ndata policies\nto query data and preview sample data.\nReporting center\n-  if\nenabled by admins\n, can view the assets, glossary, Insights, and usage and cost dashboards.\nData products\n-  requires\ndomain policies\nto access domains and products.\nRestrictions\nâ\nA governance admin has the following explicit restrictions:\nCannot access or perform any actions in the\nAdmin center\nor\nWorkflow center\n.\nCannot access metadata and data policies if the user is neither a\nconnection admin\nnor has a\npolicy\ngranting them access to a persona or purpose.\nCannot access the\nConnections\ntab in the\nGovernance center\n.\nIs excluded from the default\nAll Admins\ngroup in any workflow configuration.\nTags:\nworkflow\nautomation\norchestration\nPrevious\nManage user authentication\nNext\nAutomatically assign roles\nAssign a subrole\nWorkflow admin\nGovernance admin"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso",
    "content": "Configure Atlan\nIntegrations\nIdentity Management\nSSO\nTroubleshooting\nTroubleshooting SSO\nOn this page\nTroubleshooting SSO\nCan I change the username of a provisioned user in Atlan?\nâ\nNo, once you have\nintegrated SSO in Atlan\n, the usernames of provisioned users will be dependent on your SSO provider. For example, if a username has changed due to an automation at source or in the case of a migration from one provider to another, you will not be able to update usernames in Atlan.\nUsernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. Ensure that your username in the SSO provider matches that in Atlan.\nHow does group mapping sync affect my current SSO setup?\nâ\nIf your\ngroups\nin Atlan are mapped to their corresponding groups in the SSO provider, then any changes in group membership at source will be synced to Atlan:\nFor any groups in the SSO provider not mapped to Atlan or vice versa, there will be no changes to your current SSO setup.\nIf a user is part of a mapped group in Atlan but not in the SSO provider, the user will be removed from the mapped group in Atlan.\nIf a user is part of a mapped group in the SSO provider but not in Atlan, the user will be added to the mapped group in Atlan.\nTags:\nintegration\nsetup\nPrevious\nSSO integration with PingFederate using SAML\nNext\nTroubleshooting connector-specific SSO authentication"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-associated-terms",
    "content": "Configure Atlan\nAdministration\nFeature Management\nHow to enable associated terms\nOn this page\nEnable  associated terms\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to enable\nassociated terms\n.\nTo enable associated terms, follow these steps.\nEnable associated terms\nâ\nTo enable associated terms for your users:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nGlossary\nÂ heading of the\nLabs\npage, for\nTerm attributes\n, click the\nNo attributes applied\ndropdown.Â\nFrom the terms menu, select the\nassociated terms\nyou'd like to enable for your users.Â\nYour users will now be able to\nadd associated terms\nto their glossaries! ð\nIf you'd like to turn off any of the associated terms, follow the steps above and then deselect the corresponding checkboxes.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nDisable user activity\nNext\nHow to enable discovery of process assets\nEnable associated terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-governance-workflows",
    "content": "Build governance\nStewardship\nWorkflow Management\nCreate governance workflows\nOn this page\nCreate governance workflows\nWho can do this?\nYou must be an\nadmin user\nin Atlan to\nenable\n, create, and\nmanage\ngovernance workflows.\nAtlan provides no-code governance workflow templates with predefined steps. To create a\ngovernance workflow\n, complete the following steps.\nCreate a governance workflow\nâ\nTo create a governance workflow:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nGovernance workflows\n.\nClick the\n+ Workflow\nbutton to create a new governance workflow.\nSelect a workflow template\nâ\nTo select a workflow template:\nFrom the\nCreate new workflow\nmenu, select the workflow template best suited to your use case:\nChange management\nNew entity creation\nAccess management\nPolicy approval\n-  if\npolicy center module is enabled\nIn the upper right of the screen, click the\nUse template\nbutton to begin.\nIn the\nNew workflow\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for your workflow.\n(Optional) For\nDescription\n, enter a brief description of your workflow.\n(Optional) Select an icon to represent your workflow.\nClick\nCreate\nto create your workflow.\nSelect the scope of workflow\nâ\nGovernance workflows must either be associated with assets, including data products, or certain actions in Atlan. If a user submits an access or update request, your workflow will be triggered to provision access or approve the update request, respectively.Â\nTo select assets for the scope of your workflow:\nFor\nWhen assets match rules\n, define the scope of your workflow to specific assets.\nTo set a matching condition for the filters, select\nMatch all\nor\nMatch any\n.\nMatch all\nwill logically\nAND\nthe criteria, while\nMatch any\nwill logically\nOR\nthe criteria.\nFor\nAttributes\n, select a relevant option:\nClick\nConnection\nand then select an existing connection. (Optional) To further refine your asset selection:\nClick\nAll databases\nto filter by databases in a selected connection.\nClick\nAll schemas\nto filter by schemas in a selected connection.\nClick\nConnector\nto filter assets by\nsupported connectors\n.\nClick\nAsset type\nto filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more.\nClick\nCertificate\nto filter assets by\ncertification status\n.\nClick\nOwners\nto filter assets by\nasset owners\n.\nClick\nTags\nto filter assets by your\ntags\nin Atlan, including imported\nSnowflake\nand\ndbt\ntags.Â\nClick\nGlossary, terms, & categories\nto filter by a specific\nglossary\nor\ncategory\nto bulk update all the nested terms or by multiple glossaries and categories.\nClick\nLinked terms\nto filter assets by\nlinked terms\n.\nClick\nSchema qualified Name\nto filter assets by the qualified name of a given schema.\nClick\nDatabase qualified Name\nto filter assets by the qualified name of a given database.\nClick\ndbt\nto filter assets by dbt-specific filters and then select a\ndbt Cloud\nor\ndbt Core\nfilter.\nClick\nProperties\nto filter assets by\ncommon asset properties\n.\nClick\nUsage\nto filter assets by\nusage metrics\n.\nClick\nMonte Carlo\nto filter assets by\nMonte Carlo-specific filters\n.\nClick\nSoda\nto filter assets by\nSoda-specific filters\n.\nClick\nTable/View\nto filter tables or views by row count, column count, or size.\nClick\nColumn\nto filter columns by\ncolumn-specific filters\n, including parent asset type or name, data type, or\ncolumn keys\n.\nClick\nProcess\nto filter\nlineage processes\nby the SQL query.\nClick\nQuery\nto filter assets by associated\nvisual queries\n.\nClick\nMeasure\nto filter\nMicrosoft Power BI measures\nusing the external measures filter.\nFor\nOperator\n, select\nIs one of\nfor values to include or\nIs not\nfor values to exclude. Depending on the selected attribute(s), you can also choose from\nadditional operators\n:\nSelect\nEquals (=)\nor\nNot Equals (!=)\nto include or exclude assets through exact match search.\nSelect\nStarts With\nor\nEnds With\nto filter assets using the starting or ending sequence of values.\nSelect\nContains\nor\nDoes not contain\nto find assets with or without specified values contained within the attribute.\nSelect\nPattern\nto filter assets using supported\nElastic DSL regular expressions\n.\nSelect\nIs empty\nto filter assets with null values.\nFor\nValues\n, select the relevant values. The values will vary depending on the selected attributes.\n(Optional) To add more filters, click\nAdd filter\nand select\nFilter\nto add individual filters or\nFilter\nGroup\nto nest more filters in a group.\n(Optional) To view all the assets that match your rules, click\nView\nÂ for a preview.\nAt the bottom of the form, click the\nSave &\nContinue\nbutton.\n(Optional) Conditional Branching\nâ\nConditional branching enables you to define dynamic paths within governance workflows based on specific rules. Each branch can have its own set of conditions, approvers, forms, and auto-approval settings. Conditional branching is currently supported only for the\nChange Management\nand\nNew Entity Creation\nworkflow templates.\nChange Management Template\nâ\nCondition based on Metadata Attribute Type\nApply approval logic only when specific metadata attributes are changed (e.g., tags, owners, certificates).\nEnable up to 10 conditional branches\nSupported attributes include:\nName\nAlias\nAnnouncement\nDescription\nCertificate\nDomain\nOwners\nTags\nTerms\nReadme\nCustom metadata\nEach branch can include multiple conditions â if any condition is satisfied, the branch will trigger. Conditions cannot be reused across branches.\nFallback Path (Change Management)\nTriggered when no other branch conditions match\nAllows exclusions â you can uncheck specific attributes that should not trigger approval\nMetadata changes grouped as:\nAll asset metadata changes (common attributes)\nGlossary-specific changes\nAI asset-specific changes\nProduct and domain-specific changes\nOnly select applicable change types based on the asset universe. For example, if glossary assets aren't in scope, glossary-specific changes won't be triggered.\nNew Entity Creation Template\nâ\nUse this template to route requests based on where a new term, category, or product is being created.\nDefine routing conditions based on the entity's destination.\nYou can also set conditions at the category level.\nEach branch can include multiple conditions; if any condition is met, that branch is triggered.\nAttach different input forms to each branchâor reuse the same form, to collect context-specific information from requesters.\nThis enables personalized context collection depending on the route taken.\nIf no conditions are met, the fallback path is triggered automatically.\n(Optional) Collect information from requesters\nâ\nLinking\nforms\nto your governance workflows allows you to collect more information from requesters as they raise requests, thus enabling approvers to make informed decisions.Â\nYou can currently embed\nforms\nwithin the following governance workflow templates:\nAccess management\nNew entity creation\nTo enable collecting information from requesters:\nFor\nCollect information from requester\n, toggle on the\nRequire requestor information\nslider.\nTo link a form to your governance workflow, you can either:\nClick\n+ Create new\nto\ncreate a new form\n.\nFor\nInput form\n, click the dropdown to select an existing form. You can optionally preview or edit your selected form.\nAt the bottom of the form, click the\nSave &\nContinue\nbutton.\n(Optional) Enable auto-approval\nâ\nYou can set up specific conditions for auto-approval of requests to reduce the need for human intervention. For example, you can enable auto-approval of:\nData access requests from new users in your team to facilitate faster onboarding.\nMetadata update requests for assets with no restrictions.\nTo enable auto-approval of requests:\nFor\nIf auto approval is enabled\n, toggle on the\nAuto approve request\nslider.\nOnce you have turned on auto-approval, you can specify a subset of assets or a list of approved users that qualify for automated approval. You can either:\nFor\nFilter assets for auto approval\n, create a subset of assets for auto-approval. Follow the steps in Select the scope of workflow to filter your selections.\nFor\nAuto-approve eligible users requests\n, configure the following:\nClick\n+ Add users/groups\nto select individual users or groups whose requests can be cleared for auto-approval.\nClick\n+ Add owners\nto select\nasset owners\nwhose requests can be cleared for auto-approval.\nAt the bottom of the form, click the\nSave &\nContinue\nbutton.\nSet up manual approval\nâ\nYou can set up the approval process for requests and identify approvers. This ensures that each request is reviewed and authorized by designated approvers. Approvers can be individual users, user groups, or a combination of both.\nTo set up the manual approval process:\nFor\nSelect approvers and process\n, determine the approval strategy from the following options:\nTo enable any one approver from a list of approvers to approve requests, click\nAnyone approves\n. This means if any one approver approves or rejects a request, the workflow will be completed.\nTo enable all selected approvers to approve requests in no particular order or simultaneously, click\nAll approve - Parallel\n. This means that requests will go to all designated approvers and must be approved by all.\nTo enable all selected approvers to approve requests in a predefined order, click\nAll approve - Sequential\n. This means that the request will go through a particular order for approval and must be approved by all.\nFor\nWho can approve requests\n, designate approvers:\nClick\n+ Add users/groups\nÂ to select individual users or groups as approvers. If you have selected the sequential option, you can drag and sort approvers to create a custom order for approval.\nClick\n+ Add owners\nto select\nasset owners\nas approvers. If you have selected the sequential option, you can drag and sort approvers to create a custom order for approval.\nClick\n+ Add users/groups\nto add at least one additional approver other than the selected asset owner(s). This is a mandatory step if you have designated asset owners as approvers.\nFor\nRequest expiry period\n, set the minimum or maximum number of days during which the approval window will be open. Non-approval will lead to automatic rejection of the request.\nAt the bottom of the form, click the\nSave\nContinue\nbutton.\nDid you know?\nAtlan recommends that you limit your group selection for automated and manual approval to groups with fewer than 100 users. This will ensure that your governance workflows and approvals run smoothly.\nReview and publish workflow\nâ\nIf you'd like to continue working on your workflow, you can save it as a draft. If your workflow is ready, you can proceed to publishing it.\nReview all your selections, and then to publish your governance workflow:\nIn the upper right of the screen, click the\nPublish\nbutton.\nCongratulations, your governance workflow is now active! ð\nAny requests on assets within the scope of your workflow will be immediately routed through the workflow you just created. Requesters will be notified about the outcome of their requests through the\ntask inbox\n.\nDid you know?\nFor governed assets, you can open the\nactivity log\nto view whether it was updated using governance workflows, an approval timeline, who requested the change, and approvers that approved it.\nTags:\nworkflow\nautomation\norchestration\nPrevious\nAutomate data governance\nNext\nManage governance workflows\nCreate a governance workflow\nSelect a workflow template\nSelect the scope of workflow\n(Optional) Conditional Branching\n(Optional) Collect information from requesters\n(Optional) Enable auto-approval\nSet up manual approval\nReview and publish workflow"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/revoke-data-access",
    "content": "Build governance\nStewardship\nAccess Management\nRevoke data access\nRevoke data access\nWho can do this?\nYou must be an\nadmin user\nto\nrevoke data access on governed assets\nin Atlan.\nAs an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the\nscope of governance workflows\n.\nTo revoke data access, complete the following steps.\nTo revoke data access:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nClick on an asset to open the asset sidebar.\nIn the right menu of the\nOverview\nsidebar, click the\nAccess\ntab.\nIn the\nData access in Atlan\ntab, you can view the list of users that have data access to query data and preview sample data. Hover over the username to revoke data access and then click\nRevoke\n.\nIn the\nRevoke data access\ndialog, you can:\nFor\nReview and modify how (username) can access this asset in Atlan\n, you can revoke access provided through the following access control mechanisms:\nPersonas\nPurposes\nGovernance workflows\nConnection admin\nClick\nRaise Jira ticket to revoke data access on source\nto revoke data access through Jira. You will need to\nintegrate with Jira Cloud\nand\ninstall or register a webhook\nto use this option.\nClick\nRaise ServiceNow request to revoke data access on source\nto revoke data access through ServiceNow. You will need to\nintegrate with ServiceNow\nand\nlink your ServiceNow account in Atlan\nto use this option.\nIf a webhook has been configured for revoking data access in a source tool, you can optionally add a comment before proceeding with your revocation request.\nClick\nRevoke\nto confirm.\n(Optional) Click\nView\nto view your request(s) in the\ninbox\n.\nTags:\nworkflow\nautomation\norchestration\nPrevious\nManage policies\nNext\nCreate forms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-governance-workflows",
    "content": "Build governance\nStewardship\nWorkflow Management\nManage governance workflows\nOn this page\nManage governance workflows\nWho can do this?\nYou must be an\nadmin user\nin Atlan to\nenable\n,\ncreate\n, and manage governance workflows.\nOnce you have created governance workflows, you can manage and modify your workflows and monitor requests from the\nGovernance workflows\ndashboard.\nManage governance workflows\nâ\nTo manage governance workflows:\nFrom the left menu of any screen in Atlan, click\nGovernance\n.\nUnder the\nGovernance\nheading of the\nGovernance center\n, click\nGovernance workflows\n.\nFrom the\nOverview\ntab, you can view the following:\nIn the\nActivity\nsection, monitor governance workflows by status and type.\nIn the\nRequests\nsection, monitor requests on assets within the scope of your workflows. The default date range for requests is set to 14 days. You can also view requests for the last 7, 30, or 45 days, or a custom date range of your choice.\nIn the\nWorkflows\nsection, view the governance workflows you created or recently viewed.\nChange to the\nDefinitions\ntab to modify your workflows:\nFilter your existing workflows by\nPublished\n,\nDraft\n, or\nDisabled\nstatus.\nTo edit a workflow, click the name of your workflow and edit it.\nTo disable a workflow, hover over a workflow and then click the horizontal 3-dot icon. From the dropdown, click\nDisable workflow\nto disable it.\nChange to the\nMonitor\ntab to view all requests on governed assets:\nFilter requests on governed assets by\nOpen\n,\nApproved\n,\nRejected\n, or\nArchived\nstatus.\nClick the\nRequest by\nfilter to filter requests from specific users.\nClick any request to track the progress on that request and view the governance trail in detail. You can also view all other requests on a specific workflow.\nTags:\nworkflow\nautomation\norchestration\nPrevious\nCreate governance workflows\nNext\nManage tasks\nManage governance workflows"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-tasks",
    "content": "Build governance\nStewardship\nWorkflow Management\nManage tasks\nOn this page\nManage tasks\nWho can do this?\nAnyone with access to Atlan   -  admin, member, or guest user   -  can use the inbox.\nThe inbox in Atlan allows you to take action on and monitor requests waiting for your approval. Additionally, you can track any requests that you may have raised.\nFor requesters, the task can help you:\nTrack the progress of your requests as soon as you have raised them.\nManage your alerts and requests all in one place.\nView requests listed in order of when the approval timeline expires.\nFor approvers, the inbox can help you:\nGet notified immediately for requests that require your approval.\nManage your alerts, approvals, and tasks all in one place.\nView tasks listed in order of when the approval timeline expires.\nApprove or reject requests, along with comments.\nCompleted requests and tasks in your inbox are retained throughout the lifecycle of the Atlan instance for your organization.\nManage requests\nâ\nTo manage requests:\nFrom the top right of any screen in Atlan, click the\nInbox\nicon.\nIn the inbox, switch to the\nCreated by me\ntab to track your requests.\n(Optional) Filter requests by\nOpen\n,\nApproved\n,\nRejected\n, or\nArchived\nÂ status.\n(Optional) To withdraw your request, in the top right of your request page, click the\nWithdraw\nbutton.\nManage tasks\nâ\nTo manage tasks:\nFrom the top right of any screen in Atlan, click the\nInbox\nicon.\nIn the inbox, from the\nAssigned to me\ntab, select a task to review.\n(Optional) Filter tasks by status   -\nOpen\n,\nApproved, Rejected\n, or\nArchived\n.\nTo take action on a task, from the top right of the task page, you can either:\nClick\nApprove\nto approve the request. (Optional) In the\nApprove\ndialog, for\nAdd comment\n, add a comment and then click\nApprove\n.\nClick\nReject\nto reject the request and send back for revision. (Optional) In the\nReject\ndialog, for\nAdd comment\n, add a comment and then click\nReject\n.\nGet notified on Slack\nâ\nIf your organization's\nSlack account is integrated with Atlan\n, you will receive Slack notifications for your approvals and requests.\nTo receive Slack notifications on your approvals and requests:\nIntegrate Slack and Atlan\n-  for\nRequest notifications\n, toggle on the slider to receive Slack notifications when requests are raised in Atlan and approve or reject them directly from Slack.\nThe email address used for Slack and Atlan should be the same, even if you haven't personally integrated the accounts.\nThe Slack app should have been installed before August 12, 2022. If installed later, you'll need to update Slack.\nIf different email addresses were used for Slack and Atlan, you'll first need to\nlink your Slack account with Atlan\n.\nTags:\nalerts\nmonitoring\nnotifications\nPrevious\nManage governance workflows\nNext\nAutomate policy compliance\nManage requests\nManage tasks\nGet notified on Slack"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/set-up-glossaries",
    "content": "Build governance\nGlossary\nGet Started\nSet up glossaries\nOn this page\nSet up glossaries\nThe Atlan\nglossary\nallows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy.\nSet up a glossary\nâ\nTo define the relevant terms and categories for your data assets, you will first need to set up a glossary.\nTo create a glossary:\nFrom the left menu of any screen in Atlan, click\nGlossary\nand then click\nGet started\n.\nIn the\nCreate new glossary\ndialog, enter the following details:\nFor\nGlossary name\n, enter a name for your glossary   -  for example,\nFinance\n. The character limit for a glossary name is 80 characters.\nIn the top right,\nDraft\nis set as the default certificate. To change the certificate, click the dropdown arrow and select the\ncertificate\nyou'd like to apply to your glossary.\nClick the glossary icon to personalize the icon for your glossary.\nFor\nDescription\n, write a short or detailed description for your glossary   -  size limit for\ndescription\nvalues is 32766 bytes.\nFor\nAdd owners\n, add yourself or anyone else in your team as\nowners\nof the glossary.\nClick\nCreate\nto add your glossary.\n(Optional) From the top right of the glossary profile:\nClick the user avatars to view a list of recently visited users, total views on your glossary, total number of unique visitors, and total views by user.\nUse the days filter to filter glossary views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your glossary\n.\nClick the clipboard icon to copy the link for your glossary.\nClick the pencil icon to edit the glossary name, description, and icon.\nClick the\nSlack\nor\nTeams\nicon to share directly on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the bell icon to enable\nSlack\nor\nMicrosoft Teams\nnotifications for glossary updates in Atlan.\nClick the 3-dot icon and then:\nClick\nAdd announcement\nto\nadd an announcement\nto your glossary.\nClick\nBulk upload terms\nto\nbulk upload terms\nto your glossary.\nClick\nExport\nto\nexport nested categories and terms\nwithin a glossary to spreadsheets.\nClick\nArchive\nto archive the glossary.\nYour glossary is now ready for you to start adding\nterms\nand\ncategories\n! ð\nAdd new glossary terms\nâ\nTerms\nare the building blocks of your glossary. While defining a new glossary term, add as much information as possible for your term so that your team fully understands how to use it.\nTo add a new glossary term:\nOn the\nGlossary\npage, click the\n+\nicon next to\nAll glossaries\nand then click\nAdd term\nfrom the dropdown.\nIn the\nCreate new term\ndialog, enter the following details:\nFor\nSelect glossary\n, select a glossary for your term. In this example, we'll select the\nFinance\nglossary.\nFor\nTerm name\n, enter a name for your term   -  for example,\nCredit Score\n. The character limit for a term name is 80 characters.\nIn the top right,\nDraft\nis set as the default certificate. To change the certificate, click the dropdown arrow and select the\ncertificate\nyou'd like to apply to your term.\n(Optional) For\nAlias\n,\nadd an alias\nto your term.\nFor\nDescription\n, write a short or detailed description for your term   -  size limit for\ndescription\nvalues is 32766 bytes.\nFor\nAdd owners\n, add yourself or anyone else in your team as\nowners\nof the glossary.\n(Optional) Turn on\nCreate multiple\nto create more terms from the same dialog.\nClick\nCreate\nto add your term.\n(Optional) From the top right of the term profile:\nClick the user avatars to view a list of recently visited users, total views on your term, total number of unique visitors, and total views by user.\nUse the days filter to filter views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your term\n.\nClick the clipboard icon to copy the link for your term.\nClick the pencil icon to edit the term name and description or\nadd an alias\nto your term.\nClick the\nSlack\nor\nTeams\nicon to share directly on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the bell icon to enable\nSlack\nor\nMicrosoft Teams\nnotifications for glossary updates in Atlan.\nClick the 3-dot icon to\nadd an announcement\nor archive the term.\nDid you know?\nOnce you've added terms to your glossaries, you can also\nlink them to your assets\n.\nUpdate your glossary terms\nâ\nYou can also add a term to your glossary without attaching a certificate or adding an owner at first. Once you have completed adding a term, navigate to the sidebar next to the term profile:\nClick\n+\nunder\nOwners\nto assign\nowners\nfor a term.\nClick\nDraft\nto update the certificate for a term. Choose from four\ncertificate\noptions   -\nDraft\n,\nVerified\n,\nDeprecated\n, and\nNo certificate\n.\nClick\n+\nunder\nTags\nto\nclassify\nthe key characteristics of your term and configure\ntag propagation\nfor\nlinked assets\n.\nClick\n+\nunder\nCategories\nto assign a term to a particular category.\nClick\n+\nunder any of the\nassociated term options\nto create relationships between terms.\nDid you know?\nAdding an\nowner\nto your term can help your teammates figure out who is an expert on a glossary term. This is the person they should reach out to if they have any questions about the term or would like to collaborate on updating it.\nAdd new glossary categories\nâ\nYou can add\ncategories\nto your glossary to better organize your terms and create a hierarchy of information.\nTo add a category to your glossary:\nOn the\nGlossary\npage, next to the name of your glossary in the left, click the three horizontal dots icon and then click\nAdd category\n.\nIn the\nCreate new category\ndialog, enter the following details:\nFor\nCategory name\n, enter a name for your category   -  for example,\nPersonal Finance\n. The character limit for a category name is 80 characters.\nIn the top right,\nDraft\nis set as the default certificate. To change the certificate, click the dropdown arrow and select the\ncertificate\nyou'd like to apply to your category.\nFor\nDescription\n, write a short or detailed description for your category   -  size limit for\ndescription\nvalues is 32766 bytes.\nFor\nAdd owners\n, add yourself or anyone else in your team as\nowners\nof the glossary.\n(Optional) Turn on\nCreate multiple\nto create more categories from the same dialog.\nClick\nCreate\nto add your category.\nNext to the category name in the left menu, click the three horizontal dots icon and then add new terms or subcategories to your category.\n(Optional) From the top right of the category profile:\nClick the user avatars to view a list of recently visited users, total views on your category, total number of unique visitors, and total views by user.\nUse the days filter to filter views and user activity in the last 7, 30, and 90 days.\nThis feature is turned on by default   -  admins can\nturn off user activity\n.\nClick the star button to\nstar your category\n.\nClick the clipboard icon to copy the link for your term.\nClick the pencil icon to edit the category name and description.\nClick the\nSlack\nor\nTeams\nicon to share directly on a\nSlack\nor\nMicrosoft Teams\nchannel.\nClick the bell icon to enable\nSlack\nor\nMicrosoft Teams\nnotifications for glossary updates in Atlan.\nClick the 3-dot icon and then:\nClick\nAdd announcement\nto\nadd an announcement\nto your category.\nClick\nExport\nto\nexport nested terms\nwithin a category to spreadsheets.\nClick\nArchive\nto archive the category.\nMove terms and categories\nâ\nYou can move terms and categories within and across glossaries to better organize your business context. Move terms to a different category or create subcategories within the same glossary or across your glossaries in Atlan.\nYou will need the following permissions:\nMoving a term or category from one glossary to another   -  read, update, and delete permissions on both glossaries.\nMoving a term or category within the same glossary   -  update permission on the glossary you want to reorganize.\nTo move an existing term or category:\nFrom the left menu of any screen in Atlan, click\nGlossary\n.\nIn the left menu of the\nGlossary\npage, you can either:\nDrag and drop a term or category into the relevant category within the same or a different glossary. In the popup, click\nConfirm\nto confirm the changes.\nTo the right of the term or category name, click the three dots icon and then click\nMove to\n. In the\nMove to\ndialog, select a relevant category within the same or a different glossary and then click\nMove\nto confirm the changes.\nSearch for glossary terms\nâ\nThere are two ways to search for glossary terms:\nIn the left panel of the\nGlossary\npage, type the name of your term in the search bar and select your preferred option from the search results.\nClick the\n>\nicon preceding the name of a category to expand the full list of nested terms in that category.\nAdd READMEs to your assets\nâ\nFor glossaries, terms, and categories, the asset profile provides a helpful summary. For example, the\nLinked assets\nsection displays all the data assets that are linked to a particular term. This is also where you can\nadd a README\n.\nThe secret to making your glossary really useful is to provide as much information as possible. Adding a README will allow you to state your objectives for defining a glossary unit in greater detail.\nInspect glossary terms and categories\nâ\nThe navigation bar to the right of the asset profile provides high-level information about the glossary item you are looking at. Here's what you can view:\nOverview\nshows key characteristics of a glossary term or category and helps you understand its relationship to other items in a glossary.\nActivity\ndisplays the\nchangelog\nfor your glossary items. For instance, you can find out who updated a term and when.\nResources\nare links to\ninternal or external URLs\nthat can help your team better understand your glossary items. You can add links from GitHub, Google Docs, Google Sheets, or more as resources to your glossary item to provide additional context.\nRequests\nfor a particular glossary item can be filtered by their status, such as\nPending\n,\nApproved\n, and\nRejected\n.\nProperties\nshow the unique identification number of a glossary item and other properties.\nIntegrations\nshow\nSlack\nor\nTeams\nmessages and\nJira\ntickets pertaining to a particular glossary item.\nAdd associated terms\nâ\nWho can do this?\nYou will need your Atlan administrator to\nenable associated terms\n-  except related terms.\nIn order to\ninter-relate your terms\n, you will first need to\nset up a glossary\nand then\nadd terms\n.\nTo add relationships between terms:\nFrom the left menu on any screen, click\nGlossary\n.\nUnder\nGlossary\nin the left menu, click the name of your glossary.\nUnder the glossary name, click the category in which your term is nested and then click the term you would like to enrich with an associated term.\nIn the\nOverview\ntab of the term sidebar to the right, under\nAssociated terms\n, click\n+\nto add relationships to your term. In the\nAssociate terms\ndialog, configure the following:\nTo select a term relationship:\nClick\nRelated to\nto add a\nterm that is related in some way\n.\nClick\nRecommended\nto add a\nstandard form of use for the term\n.\nClick\nSynonyms\nto add a\nterm that is similar in meaning\n.\nClick\nAntonyms\nto add a\nterm that is opposite in meaning\n.\nClick\nTranslates to\nto add a\ntranslated version of the term\n.\nClick\nValid values for\nto add\napplicable values for the term\n.\nClick\nClassifies\nto add an\numbrella category for the term\n.\nClick\nClassified By\nto add a\nterm that falls under the purview of the term in use\n.\nFor\nSelect terms\n, select existing terms to associate.\nClick\nAssociate terms\nto confirm your selections. The interrelated terms will reflect the relationships automatically in the term profile and sidebar.\n(Optional) Under\nAssociated terms\nin the term profile, you can view a visual representation of your term relationships:\nClick any term attribute to focus on that specific term relationship.Â\nClick the minus or plus icons to zoom out or zoom in on the graph, respectively.\nClick the expand icon to enlarge the graph.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nGlossary\nNext\nBulk upload terms in the glossary\nSet up a glossary\nAdd new glossary terms\nUpdate your glossary terms\nAdd new glossary categories\nMove terms and categories\nSearch for glossary terms\nAdd READMEs to your assets\nInspect glossary terms and categories\nAdd associated terms"
  },
  {
    "url": "https://docs.atlan.com/tags/alerts",
    "content": "9 docs tagged with \"alerts\"\nView all tags\nAutomate data governance\nYou can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nConfigure alerts\nSet up real-time notifications for data quality rule failures via Slack or Microsoft Teams.\nLink your Microsoft Teams account\nTo get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users.\nManage tasks\n:::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can use the inbox.\nSecurity monitoring\nLearn about security monitoring.\nSend alerts for workflow events\nLearn how to configure alerts for workflow events in Atlan via email or Google Chat.\nSupported sources\nLearn about supported sources.\nTroubleshooting Fivetran connectivity\nLearn about troubleshooting fivetran connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity."
  },
  {
    "url": "https://docs.atlan.com/tags/notifications",
    "content": "9 docs tagged with \"notifications\"\nView all tags\nAutomate data governance\nYou can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution.\nConfigure alerts\nSet up real-time notifications for data quality rule failures via Slack or Microsoft Teams.\nCreate webhooks\nIf your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or [submit a request](/support/submit-request).\nLink your Microsoft Teams account\nTo get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users.\nManage tasks\n:::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can use the inbox.\nSecurity monitoring\nLearn about security monitoring.\nSupported sources\nLearn about supported sources.\nTroubleshooting Fivetran connectivity\nLearn about troubleshooting fivetran connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/export-assets",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSpreadsheets\nHow-tos\nHow to export assets\nOn this page\nExport Assets\nWho can do this?\nBefore you can export assets to spreadsheets, you will first need your Atlan admin to\nenable asset export\n.\nAtlan enables you to export all your assets or a\nfiltered subset of assets\nto spreadsheets. Atlan currently supports exporting assets to:\nGoogle Sheets\nMicrosoft Excel online\nOnce your\nAtlan admin\nhas integrated a supported tool, you will be able to export your assets and asset metadata to spreadsheets. Your existing\npermissions\nand\naccess policies\nin Atlan will determine whether you can export assets, but at a minimum you'll require read permission on the assets you want to export.\nFor example, you can:\nExport a list of assets that need enrichment and share the spreadsheet widely to crowdsource documentation.\nExport your business glossary and bring everyone up to speed on commonly used terminology across the organization.\nTo export impacted assets, see\nHow to download and export lineage\n.\nDid you know?\nAtlan currently limits the total number of assets you can export to 150,000 rows. Reach out to your customer success manager if you'd like to increase the limit for your organization.\nSupported assets for export\nâ\nYou can export assets from the following tabs:\nAssets tab\nâ\nAll assets\nFiltered subset of assets\nChild assets such as columns or fields from the\nparent asset profile\nGlossary tab\nâ\nGlossary profile\n-  all categories and terms within a glossary\nCategory profile\n-  all direct subcategories and terms within a category\nTerm profile\n-\nlinked assets for terms\nfrom the\nLinked Assets\ntab\nReporting tab\nâ\nAssets\ndashboard\n-\ntotal assets\n, archived assets, assets by certificates, SQL assets,\nassets without enrichment\n,\nassets with lineage\n, and assets with announcements\nGlossary\ndashboard\n-\nlinked assets for terms\nInsights\ndashboard\n-\nquery assets with certificate\nUsage & Cost\ndashboard\n-\nsuggested assets for deprecation\nEnable asset export\nâ\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to integrate a supported tool   -  Google Sheets and Microsoft Excel   -  and enable your users to export assets. For Atlan admins, you do not have to set the following permissions manually. You can simply sign in to your Google or Microsoft account while integrating the supported tool, which in turn will automatically set the permissions and grant appropriate access to your users.\nThe export icon or button will only be visible if your Atlan admin has integrated a supported tool. If you cannot see the export icon or button, reach out to your Atlan admin to integrate Google Sheets or Microsoft Excel.\nAtlan uses the following permissions to integrate supported tools:\nGoogle Sheets:\nhttps://www.googleapis.com/auth/drive.file\n-  allows the app to see, edit, create, and delete only specific Google Drive files used with the Google Sheets app. This enables Atlan to create and update spreadsheets with exported assets. Refer to\nGoogle documentation\n.\nMicrosoft Excel:\nFiles.ReadWrite\n-  allows the app to read, create, update, and delete the signed-in user's OneDrive files. This enables Atlan to create and update spreadsheets with exported assets. Note that access is restricted to the signed-in user's files   -  Atlan does not access any shared files. Refer to\nMicrosoft documentation\n.\noffline_access\n-  allows the app to access resources on behalf of the users, even when users are not currently using the app. Refer to\nMicrosoft documentation\n.\nUser.Read\n-  allows users to sign in to the app, and allows the app to read the profile of signed-in users. This enables Atlan to authenticate the user. Atlan only supports authenticating with an organizational account. Refer to\nMicrosoft documentation\n.\nDid you know?\nFor Microsoft users, if your global administrator has enabled the\nadmin consent workflow\n, you will be prompted to request admin approval while attempting to integrate Microsoft Excel. Reach out to your admin to\napprove the admin consent request\nfrom the Microsoft Entra admin center. Additionally, if there is any\nexpiry date\nset for such requests, ensure that the request is approved within that period of time, otherwise you will need to request approval again.\nTo integrate a supported tool for exporting assets:\nFrom the left menu of any screen, click\nAdmin\n.\nUnder the\nWorkspace\nheading, click\nIntegrations\n.\nTo connect Atlan to a supported tool for exporting assets:\nIn the\nGoogle Sheets\ntile, click the\nConnect\nbutton. A sign-in dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click\nAllow\nto integrate Atlan with Google Sheets.\nIn the\nMicrosoft Excel\ntitle, click the\nConnect\nbutton. A sign-in dialog will appear and you will be redirected to sign in with your Microsoft account to integrate Atlan with Microsoft Excel.\nYour users can now export assets from Atlan! ð\nExport assets\nâ\nWho can do this?\nOnce an Atlan admin has integrated a supported tool, any\nadmin, member, or guest user\nin Atlan with read permission on assets can export assets to spreadsheets.\ndanger\nAtlan recommends that you avoid exporting assets during workflow runs. Exporting assets while you have workflows running in the background may lead to duplicate assets on the spreadsheet.\nAtlan allows you to export your assets to spreadsheets and view asset metadata in bulk.\nAt a tenant level, Atlan supports running five exports concurrently for each supported tool. If both Google Sheets and Microsoft Excel have been integrated, a total of 10 concurrent exports is supported. At an individual user level, only one export is allowed for each supported tool at a time, the rest will be auto-queued for execution.\nTo export your assets:\nFrom the left menu of any screen in Atlan, click\nAssets\n.\nTo export assets, in the\nAssets\npage, you can either:\nNext to the search bar, click the 3-dot icon and then click\nExport\nto export all assets.\nApply any filters\n, click the 3-dot icon, and then click\nExport\nto export a list of filtered assets.\nThe\nExport\ndialog displays a total count of assets available for export:\nClick\nGoogle Sheets\nto export your assets to a Google Sheets spreadsheet.\nClick\nMicrosoft Excel\nto export your assets to a Microsoft Excel spreadsheet.\nA sign-in dialog will appear and you will be redirected to sign in with your Google or Microsoft account. From the corresponding screen, click\nAllow\nto connect to Google Sheets or Microsoft Excel.\nTo track the progress of the export, you can either:\nIn the\nExport in progress\npopup, click\nOpen Sheets\nor\nOpen Excel\nto navigate to the spreadsheet. The\nQueued\nstatus will change to\nSuccess\nonce assets have been exported.\nClick the 3-dot icon and then click\nExport\nto view asset export in progress, along with an estimated time of completion. (Optional) To cancel the export, hover over\nIn queue\nand click the\nStop\nbutton.\nOn the spreadsheet, you will be able to view the following details and asset metadata:\nStatus\n-  export status and total count of assets exported\nTitle\n-  asset name and link\nType\n-  asset type\nConnector\n-  name of\nsupported source\nBusiness Name (Alias)\n-  business-oriented\nalias\nof assets, if any\nCreated By\nand\nCreated At\n-  username for user who created the asset and when it was created, only applicable to glossary exports\nDescription\n-\nasset description\n, if any\nOwner Users\nand\nOwner Groups\n-\nasset owners\n, if any\nCertification Status\nand\nCertification Message\n-\ncertification status of asset\n, if any\nAnnouncement Type\n,\nAnnouncement Title\n, and\nAnnouncement Message\n-\nannouncements on assets\n, if any\nTags\nand\nPropagated Tags\n-\ntags\ndirectly attached or\npropagated\nto an asset, if any\nTerms\n-\nlinked assets\nQualified Name\n-  fully qualified name of the asset\nGUID\n-  globally unique identifier of the asset\nCustom Metadata\n-\ncustomized metadata\nwith organizational context, if any\n(Optional) To view your asset export history, click the 3-dot icon and then click\nExport\n. From the\nExport\ndialog, expand the\nHistory\ndropdown to view your last 10 exports. Note that only\nyou\ncan currently view your own export history.\nThat's it, you've successfully exported your assets from Atlan! ð\nUpdate metadata in spreadsheets and sync to Atlan\nâ\nOnce you've exported your assets, you can use the Atlan extension for spreadsheets to update metadata for exported assets and sync your updates to Atlan. Atlan currently supports updating metadata and syncing updates for Google Sheets and Microsoft Excel.\nGoogle Sheets\nâ\nMicrosoft Excel\nâ\nUpdate metadata\nâ\nWho can do this?\nAny\nnon-guest user with edit access to an asset's metadata\ncan update metadata for exported assets and sync changes to Atlan. This only includes admin and member users.\nAtlan also recommends noting the following while updating metadata:\nIf asset import from Atlan to a spreadsheet is in progress, avoid making any updates until the import is completed.\nYou cannot make any changes to columns marked read-only or grayed out. Any changes to those columns will not be synced to Atlan and you will receive an error message.\nYou can remove or change the order of assets in the spreadsheet, but you cannot add or replace it with an entirely new set of assets.\nYou cannot add extra columns to the spreadsheet   -  sync to Atlan will fail.\nAvoid having any empty rows between assets.\nAlthough you can make changes in the spreadsheet while a sync is in progress, avoid starting a new sync until the current one has ended.\nYou can easily collaborate with other users on the spreadsheet. However, sync to Atlan will only be recorded for the username that initiated the sync.\nAtlan uses hidden sheets to create a snapshot of the changes you've made. Refrain from making any changes in the hidden sheets.\nTo update metadata for exported assets in a spreadsheet:\nInstall the Atlan extension for:\nGoogle Sheets\nMicrosoft Excel\nConnect your Atlan instance to:\nGoogle Sheets\nMicrosoft Excel\nTo update metadata for your exported assets:\nYou can only make changes to the metadata in the following columns:\nBusiness Name (Alias)\nDescription\nOwner Users\nand\nOwner Groups\n-  when adding an owner user or group, they must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple owners or groups as comma-separated values.\nCertification Status\n-  select valid values from the dropdown.\nCertification Message\nAnnouncement Type\n-  select valid values from the dropdown.\nAnnouncement Title\nAnnouncement Message\nTags\n-  when adding a tag, it must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags as comma-separated values.\nTerms\n-  when adding a term, it must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple terms as comma-separated values.\nCustom Metadata\n-  enter custom metadata values for any properties you want to enrich based on the description and expected data type and format. Atlan uses two placeholder values,\nN/A\nindicates that the custom metadata property is not applicable on a specific asset and\nNot Permitted\nindicates that the user lacks permission to make metadata updates. Atlan recommends that you do not make any changes to these values. Neither of these values will be synced to Atlan. Additionally, Atlan currently only allows you to select a single value for\nOptions\ndata type when updating from spreadsheets.\nYou\ncannot\nmake the following changes:\nEdit headers for any of the columns.\nEdit the metadata in the following columns:\nTitle\nType\nConnector\n,\nGlossary\nand\nCategories\nPropagated Tags\nQualified Name\n_GUID\n_\nCreated By\nand\nCreated At\nDelete any columns or rows.\ndanger\nAny of these changes will not be pushed to Atlan and you'll receive an error message.\nSync to Atlan\nâ\ndanger\nIf you do not have the\npermissions\nto update asset metadata in Atlan, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in the spreadsheet. Ensure that you have the requisite permissions to update an asset before syncing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access.\nTo sync your metadata updates to Atlan:\nTo sync your metadata updates:\nFor Google Sheets, in the menu bar of your Google Sheets spreadsheet, click\nExtensions\n. From the dropdown menu, click\nAtlan\nand then click\nSync to Atlan\n.\nFor Microsoft Excel, in the menu bar of your Microsoft Excel workbook, click\nAtlan\n. From the\nAtlan\ntab, click\nSync to Atlan\n.\ndanger\nAtlan recommends that you do not change the name of your Microsoft Excel workbook if you want to sync metadata updates to Atlan, the sync will fail otherwise. Additionally, Microsoft Excel may take longer to autosave changes on the workbook. If you notice that zero changes were detected, retry syncing after a few seconds.\nOnce the changes have synced, the Atlan sidebar will indicate completion of the sync.\n(Optional) At the bottom of the spreadsheet, switch to the\nSync-Logs\ntab to view a record of your changes:\nView all changes synced to Atlan   -  the latest sync details will be displayed at the top of the spreadsheet.\nChanges are displayed on an asset level. For example, even if you have updated three attributes for an asset, it will be recorded as one change per asset.\nView the status of each update   -\nSynced\nor\nFailed\n. For\nFailed\n, Atlan will display a failure message:\nInvalid value\n-  for unsupported values.\nUnauthorized\n-  you do not have the permissions to update asset metadata.\nNot synced due to conflict\n-  if any attributes were updated in Atlan since the assets were last exported, Atlan will signal a conflict. In that case, you will need to export your assets once again with the latest changes to proceed.\nIf the error message does not match any of the above options,\nreach out to Atlan support\n.\nClick any asset to verify the changes directly in Atlan.\nIn Atlan, when viewing the activity log:\nAn\nUpdated via Google Sheets\nstamp will appear in the\nactivity log\nfor updated assets. Click the\nGoogle Sheets\nlink to view the source spreadsheet from Atlan.\nAn\nUpdated via Microsoft Excel\nstamp will appear in the\nactivity log\nfor updated assets. Click the\nExcel\nlink to view the source spreadsheet from Atlan.\nTags:\nintegrations\nspreadsheets\nassests\nPrevious\nDownload impacted assets in Microsoft Excel\nNext\nHow to integrate Atlan with Google Sheets\nSupported assets for export\nEnable asset export\nExport assets\nUpdate metadata in spreadsheets and sync to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/bulk-upload-terms-in-the-glossary",
    "content": "Build governance\nGlossary\nTerm Management\nBulk upload terms in the glossary\nOn this page\nBulk upload terms in the glossary\nWho can do this?\nCurrently, only member users with\nedit access on glossaries\nand admin users in Atlan can start a bulk glossary upload   -  you may not have access yourself.\nYou can upload multiple terms and categories into a glossary using the bulk upload option:\nDownload the template\nâ\nA dynamic template can be generated per glossary. The template contains drop-down values for Type, Categories, Owner Users, Owner Groups and Tags.\nTo download the template from within Atlan:\nClick on the desired glossary, choose\nBulk Upload\ntab and proceed to download the\nsample template\n.\nOr to download the glossary bulk terms template from within Atlan:\nFrom the left menu, click\nGlossary\n.\nTerms and categories can only be added/updated to an existing\nglossary\n.\nClick\nAll glossaries\nand then click the glossary you want to add terms to. To the right of the glossary's name in the glossary tree, click the\nBulk Upload\ntab and proceed to download the\nsample template\n.\nRead the notes on each column before you start to edit the template.\nA file named\n{glossary_name}.xlsx\ndownloads. You can edit this file with your existing terms and definitions in Microsoft Excel or Google Sheets.\nPopulate the template\nâ\nPopulate the downloaded template following the template field structure. You can find the same details below in the comments embedded in the\n{glossary_name}.xlsx\nfile.\nColumn explanations\nâ\ndanger\nExcept for the terms and category, these objects must already exist: users, groups, and tags.\nTerm name\nâ\nEnter the name of the term under the\nName\ncolumn.\nThis is a mandatory field.\nEnter the term or category name. Avoid using @ in the name. If the name already exists, it updates the asset metadata based on the rest of the columns.\nTerm names must be unique within a glossary, categories with same name can exist only at different levels in a glossary.\nAdd all categories if term exists already.\nAvoid having empty rows in between.\nDid you know?\nYou can use bulk upload to bulk edit term or category information.\nType\nâ\nSelect the type of the entry in the\nType\ncolumn.\nThis is a mandatory field.\nSpecify whether the entry is a term or a category. Only Term or Category are valid options.\nUse drop-down to fill the value.\nCategories\nâ\nEnter the categories to which to add the term in the\nCategories\ncolumn.\nMultiple categories can be mentioned, as comma-separated values. You can manually add categories or select from drop-down options.\nCategories that don't exist get auto created. Even sub categories can get auto created.\nUse\n@\nto define hierarchy (for example, Sales@USA), where sub-category USA is present/created under category Sales.\nCategories with same name can exist only at different levels. For example, one can add Term \"Customer\" at level 2 category with path\nUsers@Users\n.\nDescription\nâ\nEnter the definition of the entry in the\nDescription\ncolumn.\nBusiness name\nâ\nEnter the alternative name of the entry in the\nBusiness name\ncolumn.\nUser owners\nâ\nEnter the usernames of owners in the\nUser owners\ncolumn.\nThe users must already exist in Atlan.\nUser owners are responsible for maintaining the data asset.\nYou can manually add usernames or select from drop-down options. Enter valid Atlan usernames to assign ownership.\nSeparate multiple owners with commas.\nGroup owners\nâ\nEnter the group names of owners in the\nGroup owners\ncolumn.\nThe groups must already exist in Atlan.\nYou can manually add group names or select from drop-down options. Enter valid group names responsible for the term.\nSeparate multiple groups with commas.\nCertification\nâ\nEnter the certification status in the\nCertification\ncolumn.\nThe value must be one of\nVerified\n,\nDraft\n,\nDeprecated\n, or leave empty.\nKeeping the cell empty removes the certificate status if it exists already.\nCertification message\nâ\nEnter any additional information related to the certification in the\nCertification message\ncolumn.\nCertification message is added only when Certification column contains non-empty value.\nTags\nâ\nEnter any tags to apply to the term in the\nTags\ncolumn.\nYou can manually add tags or select from drop-down options.\nTags must already exist in Atlan before being used. Comma-separated values can add multiple tags.\nDid you know?\nTag propagation is disabled by default in Atlan. You can\nenable tag propagation\nto any assets linked to the term.\nExample\nâ\nFollowing is a simple example of a filled in template, pivoted for readability:\nColumn name\nExample value\nName*\nMonthly Active Users\nType*\nTerm\nDescription\nThe number of users who are active over a month, often abbreviated MAU.\nCategories\nMetrics@Critical Metrics\nUser Owners\njsmith,jdoe\nGroup Owners\nfinance,ops\nCertification\nVerified\nCertification Message\nAgreed by governance council on May 10, 2021.\nTags\nPII, Confidential\nSave as\nCSV\nor\nXLSX\nfile\nâ\nBefore uploading the completed template to Atlan, save it as a CSV (\n.csv\n) or an XLSX (\n.xlsx\n) file.\nYou may also need to check the following:\nAtlan requires the file to be comma-separated. If you are in a region where your preferred spreadsheet editor saves CSV or XLSX files with a separator other than a comma\n,\nyou need to modify your spreadsheet's settings to use commas as separators.\nIf your file contains special characters   -  for example,\nÃ±\n,\nÃ§\n,\nÃµ\n, and others   -  Atlan recommends uploading the completed template in an XLSX file to retain the special characters. If uploaded in a CSV file format, the bulk upload and decoding of special characters may fail.\nImportant!\nFor optimal results when converting XLSX files to CSV format, it's highly advised to use Google Spreadsheets, as this approach ensures better compatibility and accuracy during the conversion process.\nUpload the file\nâ\nTo upload your CSV or XLSX file to Atlan:\nFrom the left menu, click\nGlossary\n.\nUnder\nAll Glossaries\n, click the glossary you want to add terms to. (Terms can only be added to an existing\nglossary\n.)\nTo the right of the glossary's name, click the\nBulk Upload\ntab, and then use\nClick to upload or drag it here.\n.\nClick\nSelect a CSV or XLSX file to upload\n* or drag and drop the file into the dialog.\nReview the summary of changes displayed and proceed .\nMonitor the upload\nâ\nTo track the status of your upload, from the glossary:\nView the history from the same\nBulk Upload\ntab.\nIf there are errors in your file, you see a message indicating\nFailed to upload\n.\nIf you see the link\nDownload file with errors\n, click it to download a copy of the file with notations on what the errors are. If you don't see this link, retry the process or reach out to Atlan support.\nUpdate your file, save your changes, and upload the edited file following the steps earlier.\nIf your upload is successful, you see a message indicating\nSuccess\n. You can hover on the record to see\nView Summary\n.\nThat's it, your terms and categories are now available in Atlan! ð\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nSet up glossaries\nNext\nLink terms to assets\nDownload the template\nPopulate the template\nSave as\nCSV\nor\nXLSX\nfile\nUpload the file\nMonitor the upload"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/glossary-approval-issue",
    "content": "Build governance\nGlossary\nTroubleshooting\nGlossary update request approval issue\nOn this page\nGlossary update request approval issue\nWhy am I unable to approve a glossary update request?\nâ\nIf you encounter either of the following error messages while attempting to approve a glossary update request   -\nRequest modification failed, try again\nor\nresponse status code does not match any response statuses defined for this endpoint in the swagger spec (status 409): {}\n, following may be the reasons why:\nThe term or category to be created is already present in the glossary.\nThe term or category to be updated has been archived.\nIf neither of the above is true, reach out to\nAtlan support\nfor help with debugging the issue.\nTags:\nglossary\nbusiness-terms\ndefinitions\nfaq-governance\nPrevious\nWhat is a glossary?\nNext\nCan I add duplicate glossary terms?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/add-duplicate-glossary-terms",
    "content": "Build governance\nGlossary\nFAQ\nCan I add duplicate glossary terms?\nCan I add duplicate glossary terms?\nEach\nterm\nin a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need.\nHowever, you can\ncreate terms\nwith the same name and add them to separate glossaries. This can be helpful for terms that are common across different domains or teams   -  for example, the term\nMetrics\ncan belong to the\nFinance\n,\nMarketing\n, and\nProduct\nglossaries. In this case, when a user searches for the term\nMetrics\n, the glossary name will provide additional context, such as whether it belongs to the finance, marketing, or product team glossary.\nTags:\nglossary\nbusiness-terms\ndefinitions\nfaq-governance\nPrevious\nGlossary update request approval issue\nNext\nWhat is the default permission for a glossary?"
  },
  {
    "url": "https://docs.atlan.com/tags/glossary",
    "content": "18 docs tagged with \"glossary\"\nView all tags\nA Glossary\nA glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like `cost`, `P&L`, and `revenue` can be used to group and search all financial data assets.\nBulk upload terms in the glossary\nLearn about bulk upload terms in the glossary.\nCan I add duplicate glossary terms?\nEach [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary#term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need.\nEnable  associated terms\nTo enable associated terms, follow these steps.\nGlossary\nLearn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization.\nGlossary update request approval issue\nLearn about why am i unable to approve a glossary update request?.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nLink terms to assets\nOnce you've [set up a glossary](/product/capabilities/governance/glossary/how-tos/set-up-glossaries), you can link terms from your glossary to your data assets in Atlan.\nReport on assets\nLearn about report on assets.\nReport on automations\nYou can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions.\nReport on glossaries\nThe [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further.\nReport on governance\nYou can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup.\nReport on queries\nThe Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs.\nReport on usage and cost\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nRestrict asset visibility\nNote that [glossary access works slightly differently](/product/administration/labs/how-tos/restrict-glossary-visibility).\nRestrict glossary visibility\nRestrict glossary visibility <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nSet up glossaries\nThe Atlan [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy.\nSummarize metadata\nThe reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more."
  },
  {
    "url": "https://docs.atlan.com/tags/business-terms",
    "content": "17 docs tagged with \"business-terms\"\nView all tags\nA Glossary\nA glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like `cost`, `P&L`, and `revenue` can be used to group and search all financial data assets.\nBulk upload terms in the glossary\nLearn about bulk upload terms in the glossary.\nCan I add duplicate glossary terms?\nEach [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary#term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need.\nEnable  associated terms\nTo enable associated terms, follow these steps.\nGlossary update request approval issue\nLearn about why am i unable to approve a glossary update request?.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nLink terms to assets\nOnce you've [set up a glossary](/product/capabilities/governance/glossary/how-tos/set-up-glossaries), you can link terms from your glossary to your data assets in Atlan.\nReport on assets\nLearn about report on assets.\nReport on automations\nYou can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions.\nReport on glossaries\nThe [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further.\nReport on governance\nYou can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup.\nReport on queries\nThe Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs.\nReport on usage and cost\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nRestrict asset visibility\nNote that [glossary access works slightly differently](/product/administration/labs/how-tos/restrict-glossary-visibility).\nRestrict glossary visibility\nRestrict glossary visibility <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nSet up glossaries\nThe Atlan [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy.\nSummarize metadata\nThe reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more."
  },
  {
    "url": "https://docs.atlan.com/tags/definitions",
    "content": "18 docs tagged with \"definitions\"\nView all tags\nA Glossary\nA glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like `cost`, `P&L`, and `revenue` can be used to group and search all financial data assets.\nBulk upload terms in the glossary\nLearn about bulk upload terms in the glossary.\nCan I add duplicate glossary terms?\nEach [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary#term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need.\nEnable  associated terms\nTo enable associated terms, follow these steps.\nGlossary\nLearn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization.\nGlossary update request approval issue\nLearn about why am i unable to approve a glossary update request?.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nLink terms to assets\nOnce you've [set up a glossary](/product/capabilities/governance/glossary/how-tos/set-up-glossaries), you can link terms from your glossary to your data assets in Atlan.\nReport on assets\nLearn about report on assets.\nReport on automations\nYou can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions.\nReport on glossaries\nThe [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further.\nReport on governance\nYou can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup.\nReport on queries\nThe Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs.\nReport on usage and cost\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nRestrict asset visibility\nNote that [glossary access works slightly differently](/product/administration/labs/how-tos/restrict-glossary-visibility).\nRestrict glossary visibility\nRestrict glossary visibility <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nSet up glossaries\nThe Atlan [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy.\nSummarize metadata\nThe reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/generate-lineage-between-assets",
    "content": "Use data\nLineage\nManage lineage\nGenerate lineage between assets App\nOn this page\nGenerate lineage between assets App\nApp\nYou can use the Lineage Generator (no transformations) app to automatically create lineage between assets across two systems when their names match or follow a consistent pattern. This is useful for keeping track of how data flows between databases, warehouses, or storage systems without needing to configure every connection manually.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAccess to the Lineage Generator (No Transformations) app. You can verify this by searching for Workflow Alerting in the Atlan marketplace. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nAt least one source connection and one target connection set up in Atlan with assets already crawled.\nSetup workflow\nâ\nIn your Atlan workspace, go to the homepage and click\nNew workflow\nin the top navigation bar.\nSearch for\nLineage Generator (no transformations)\n, and then select\nSet up workflow\n.\nIn the\nWorkflow name\nfield, enter a descriptive name such as:\nStaging to Reporting Lineage\nIn the\nSource asset type\nproperty, select the type of asset from which lineage originates. For example, choose\nTable\nwhen working with a Snowflake staging schema. This limits scanning and matching to tables in the source prefix.\nFor details on other supported asset types such as View, Materialized View, Column, or file-based objects, see the\nSource asset type\n.\nIn the\nSource qualified name prefix\nproperty, enter the prefix that identifies the source assets. This narrows lineage generation to only those assets whose qualified names begin with the specified prefix.\nFor example, if your Snowflake staging schema is\nstg\n, set the prefix to:\ndefault/snowflake/.../database_name/stg\nIn the\nTarget asset type\nproperty, select the type of asset to connect as the downstream target. For example, choose\nTable\nwhen linking Snowflake staging tables to reporting tables. This directs the workflow to search for target tables that match the source tables.\nFor information on other supported asset types, including BI datasets and dashboards, see the\nTarget asset type\n.\nIn the\nTarget qualified name prefix\nproperty, enter the prefix that identifies the target assets. This restricts lineage generation to assets whose qualified names begin with the specified prefix.\nFor example, if your Snowflake reporting schema is\nrpt\n, set the prefix to:\ndefault/snowflake/.../database_name/rpt\nBy default, the\nCase sensitive match\nproperty is set to Yes, while\nIgnore circular lineage\nand\nMatch on schema\nare set to No. These defaults provide straightforward matching for a first run. For more information about how each option changes lineage generation, see the\nLineage Generator reference\n.\nIn the\nOutput type\nproperty, select\nPreview lineage\nto generate a CSV preview of proposed matches without creating lineage in Atlan. Use this option to validate the results before making changes. For detailed explanations of each option, see the\nOutput type\n.\nRun the workflow with\nOutput type\nset to\nPreview lineage\n. This generates a CSV file showing the proposed matches between source and target assets.\nIf the preview looks correct, update the\nOutput type\nproperty to\nGenerate lineage\nand run the workflow again. This applies the matches as lineage relationships in Atlan, making them visible in the lineage graph.\nYou can configure advanced options such as regex transformations, schema-based matching, or child-asset lineage. These options help handle complex naming patterns, prevent false matches across schemas, and extend lineage to column-level detail. For more details, see the\nLineage Generator reference\n.\nNeed help?\nâ\nIf you have any issues related to configuring the app, contact\nAtlan support\n.\nSee also\nâ\nLineage Generator (no transformations)\n: Detailed explanation of each configuration property, including valid values, examples, and behavior.\nTags:\nlineage\nautomation\napp\ndata-lineage\nPrevious\nDownload and export lineage\nNext\nWhat is column-level lineage?\nPrerequisites\nSetup workflow\nNeed help?\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/snowflake-crawl-frequency",
    "content": "Connect data\nConnectivity Framework\nConnector Framework\nFAQ\nHow often does Atlan crawl Snowflake?\nHow often does Atlan crawl Snowflake?\nYou can decide how often Atlan crawls Snowflake when\nsetting up your Snowflake workflow\nin Atlan.\nIf the Snowflake workflow is already set up and you'd like to know the current run schedule, you can click the\nManage\ntab of the\nWorkflow Run History\nfor the last Snowflake run. The frequency will be on the top of the tab, next to the workflow name.\nTags:\nworkflow\nautomation\norchestration\nfaq-connections\nPrevious\nCan the Hive crawler connect to an independent Hive metastore?\nNext\nWhat column keys does Atlan crawl?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/troubleshooting/troubleshooting-anomalo-connectivity",
    "content": "Connect data\nData Quality & Observability\nAnomalo\nTroubleshooting\nTroubleshooting Anomalo connectivity\nOn this page\nTroubleshooting Anomalo connectivity\nWhat are the known limitations of the Anomalo connector?\nâ\nFollowing are the known limitations of the Anomalo connector:\nIf any of the checks that you've crawled in Atlan are deleted from your Anomalo instance, these will not be automatically archived in Atlan. You may have to\nmanually archive your Anomalo checks\nfrom Atlan.\nIf you exclude a previously included warehouse from an existing Anomalo workflow configuration, your Anomalo checks will not be automatically archived in Atlan.\nAtlan receives connector type and schema and table name metadata from Anomalo. If there are multiple assets from\nsupported SQL sources\nin Atlan with the same schema or table name, your Anomalo checks on those assets will not be cataloged in Atlan.\nTags:\nfaq\ntroubleshooting\nworkflow\nautomation\norchestration\nPrevious\nPreflight checks for Anomalo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx",
    "content": "Connect data\nETL Tools\nAlteryx\nOn this page\nAlteryx\nPrivate preview\nOverview:\nConnect your Alteryx environment with Atlan to automatically capture and catalog workflow metadata using OpenLineage. Discover Alteryx assets, visualize lineage, and drive data governance at scale.\nGet started\nâ\nFollow these steps to configure the Alteryx integration and begin cataloging workflows in Atlan:\nSet up the Alteryx integration\nReferences\nâ\nWhat does Atlan crawl from Alteryx\n:  Learn what metadata, workflows, and lineage Atlan ingests from Alteryx through OpenLineage.\nTroubleshooting\nâ\nTroubleshooting Alteryx connectivity\n:\nResolve common issues with connection setup, token validation, or missing assets.\nTags:\nalteryx\nconnector\nlineage\nworkflows\nopenlineage\netl-tools\nNext\nSet up Alteryx\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/troubleshooting/troubleshooting-amazon-dynamodb-connectivity",
    "content": "Connect data\nDatabases\nNoSQL Databases\nAmazon DynamoDB\nTroubleshooting\nTroubleshooting Amazon DynamoDB connectivity\nOn this page\nTroubleshooting Amazon DynamoDB connectivity\nWhat are the known limitations of the Amazon DynamoDB connector?\nâ\nAtlan currently does not support cataloging attributes for Amazon DynamoDB tables as native assets in Atlan. Hence, column-level lineage between Amazon DynamoDB as a source and supported data warehouses is currently not supported.\nWhy are some tables and indexes missing?\nâ\nCheck that the\nIAM user\ndefined for the crawler has been granted the\ndynamodb:DescribeTable\npermission on the missing tables.\nTo grant permission on all DynamoDB tables in your selected AWS region, add the following to the IAM policy to\nResource\nin the\ndynamodb:DescribeTable\npermission:\narn:aws:dynamodb:<region>:<account_id>:table/*\nCheck that the names of the missing tables do not match the table names in the\nExclude tables regex\nfilter\n, if you have specified tables to exclude.\nWhy are some attributes missing from the table asset profile?\nâ\nThe\nAttributes\nsection in the table asset profile of an Amazon DynamoDB table lists all the attributes that are either used as a partition key or a sort key in the table itself or its global or local secondary indexes.\nFor example:\nThe\nOrders\ntable has a partition key\nOrderID\nand a sort key\nProductID\n.\nThis table also has a global secondary index with the partition key\nCustomerID\n.\nKeeping this in mind, the\nAttributes\nsection in the table asset profile will display the following attributes only:\nOrderID\nProductID\nCustomerID\nHow to debug test authentication and preflight check errors?\nâ\nInvalid region\nInvalid region. Please provide a valid region.\nEnsure that you have specified the correct\nAWS region in your configuration\n. The region must match the location of your Amazon DynamoDB tables.\nInvalid secret key\nInvalid secret key. Please provide a valid secret key.\nEnsure that the\nAWS access key ID matches the secret key\n. These must be paired correctly and belong to the same IAM user.\nInvalid access key\nInvalid access key. Please provide a valid access key.\nEnsure that the\nAWS access key ID matches the secret key\n. These must form a valid pair.\nEnsure that the IAM user associated with the access key is active and neither disabled nor deleted.\nNo tables detected in the selected region\nNo tables detected in the selected region. Please check the selected region.\nEnsure that you have specified the correct\nAWS region in your configuration\n. The region must match the location of your Amazon DynamoDB tables.\nConfirm that the IAM user or role you are using has the\nnecessary permissions\n(\ndynamodb:ListTables\n) to view Amazon DynamoDB tables in the specified region.\nAccess denied: 'ListTables' permission required\nAccess denied: 'ListTables' permission required. Please reach out to your AWS administrator to request the permissions.\nEnsure that the IAM policy attached to your role includes the\ndynamodb:ListTables\npermission.\nConfirm that the role has the permissions to list tables.\nIf you do not have the permission to update IAM policies, reach out to your AWS administrator to request the necessary permissions.\nAccess denied: 'AssumeRole' permission required\nAccessÂ denied: 'AssumeRole' permission required. Please reach out to your AWS administrator to request the permissions.\nEnsure that the\nIAM policy\nattached to your user or role includes the\nsts:AssumeRole\npermission for the role you are trying to assume.\nConfirm that the IAM policy allows access to the specific role ARN you are trying to assume.\nEnsure that the role you are trying to assume has a trust policy allowing your user or role to assume it.\nTags:\nlineage\ndata-lineage\nimpact-analysis\ncatalog\nmetadata\ndiscovery\nPrevious\nWhat does Atlan crawl from Amazon DynamoDB?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/troubleshooting/why-is-my-databricks-lineage-api-not-working",
    "content": "Use data\nLineage\nTroubleshooting\nWhy is my Databricks lineage API not working?\nWhy is my Databricks lineage API not working?\nYour Databricks workspace must be\nUnity Catalog-enabled\nto successfully extract lineage using APIs. The data lineage API is not supported if you're using Hive metastore. You will need to\nmigrate your Hive tables and views\nto Unity Catalog.\nAtlan recommends extracting lineage for your Databricks assets using\nsystem tables\n. You must have a Unity Catalog-enabled workspace to use system tables. See\nHow to set up Databricks\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\napi\nrest-api\ngraphql\ncatalog\nmetadata\ndiscovery\nPrevious\nTroubleshooting lineage"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models",
    "content": "Configure Atlan\nData Models\nOn this page\nData Models\nOverview:\nData models in Atlan provide a framework for structuring and organizing your data assets. Use data models to create entity-relationship diagrams (ERDs), define relationships between assets, and maintain data consistency.\nGet started\nâ\nHow to view data models\nGuides\nâ\nHow to view data models\n: View and explore your data models in Atlan.\nConcepts\nâ\nWhat are data models\n: Understand the fundamentals of data modeling in Atlan.\nTroubleshooting\nâ\nTroubleshooting data models\n: Solutions for common data model problems.\nTags:\ndata-models\nerd\ndata-modeling\ncapabilities\nNext\nHow to view data models\nGet started\nGuides\nConcepts\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting",
    "content": "Use data\nReporting\nOn this page\nReporting\nOverview:\nAtlan's reporting capabilities help you generate insights about your data landscape. Create reports on governance, usage, queries, glossaries, and automations to track progress and make data-driven decisions.\nGet started\nâ\nHow to report on governance\nGuides\nâ\nHow to report on usage and cost\n: Generate reports on data usage and associated costs.\nHow to report on queries\n: Track and analyze query patterns and performance.\nHow to report on glossaries\n: Monitor glossary usage and coverage.\nHow to report on automations\n: Track automation workflows and their impact.\nHow to summarize metadata\n: Create concise summaries of your metadata.\nTags:\nreporting\nanalytics\nmetrics\ncapabilities\nNext\nReport on glossaries\nGet started\nGuides"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-asset-visibility",
    "content": "Configure Atlan\nAdministration\nFeature Management\nRestrict asset visibility\nOn this page\nRestrict asset visibility\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to restrict asset visibility.\nNote that\nglossary access works slightly differently\n.\nTo build effective data governance, you need to control data access across your organization. Atlan handles this through\naccess control policies\n.\nBy default in Atlan, all users can see the existence of all assets. The\nAll assets\nview includes all assets in your Atlan data estate.\nTo change this default setting, see\nDisable all assets view\n.\nGenerally, you may want to limit specific teams from being able to see all assets in Atlan.\nAll you need to do is turn off the default behavior, so that your\nmember\nand\nguest\nusers will only have access to the assets curated through their\npersonas\nand\npurposes\n. Admin users will still have full access to all assets, even when this default behavior is turned off.\nSummary\nâ\nView all assets in Labs is OFF: To view any assets, a persona is required. Personas are necessary to grant access for viewing.\nView all assets in Labs is ON: All assets are visible by default, even without a persona. However, only the default metadata will be displayed and the rest will be locked. Personas are needed to restrict access to view specific assets.\nExample\nâ\nImagine a user who belongs to an\nInsurance Member\nÂ\npurpose\n. Once you turn off the default behavior, this user will have access to:\nView details only about the curated assets in the\nInsurance Member\npurpose.\nSearch and discover only the curated assets in the\nInsurance Member\npurpose.\nView details only about the related assets in the\nInsurance Member\npurpose.\nView only the\nlinked assets\nin the\nInsurance Member\npurpose for glossary terms.\nView all assets in the lineage graph, but only view details in the sidebar forÂ assets in the\nInsurance Member\npurpose.\nIf the user is a member of multiple purposes and personas, the assets they can access will be a superset of assets across all those purposes and personas.\ndanger\nTurning off the default visibility of all assets currently does not apply to the\nExplorer\nsection in\nInsights\nÂ or the requests widget. Member and guest users will still be able to view all assets in\nInsights\nand the requests widget.\nDisable all assets view\nâ\nThe\nAll assets\nview on the\nAssets\npage is enabled for all users by default. To turn it off for your member and guest users, complete the following steps.\nTo disable all assets view:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAccess Control\nheading of the\nLabs\npage, turn off\nView \"All assets\" in Assets Discovery\n.\nYour member and guest users will now only have access to the curated assets for their persona or purpose by default! ð\nIf you'd like to enable the all assets view, follow the steps above and then turn it on.\nDid you know?\nIf a user does not belong to any persona or purpose and the\nAll assets\nview is disabled, then the user will be prompted to reach out to their Atlan administrator and request to be added to a persona or purpose. For more questions on restricting asset visibility, head over\nhere\n.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nHow to enable scheduled queries\nNext\nRestrict glossary visibility\nSummary\nExample\nDisable all assets view"
  },
  {
    "url": "https://docs.atlan.com/tags/assets",
    "content": "3 docs tagged with \"assets\"\nView all tags\nDiscovery FAQs\nFrequently asked questions about Atlan's Discovery capabilities.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-discovery",
    "content": "One doc tagged with \"faq-discovery\"\nView all tags\nDiscovery FAQs\nFrequently asked questions about Atlan's Discovery capabilities."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo",
    "content": "Connect data\nData Quality & Observability\nAnomalo\nOn this page\nAnomalo\nOverview:\nCatalog Anomalo checks, results, and metrics in Atlan. Gain visibility into data quality information, alerts, and governance for your data assets.\nGet started\nâ\nFollow these steps to connect and catalog Anomalo assets in Atlan:\nSet up the connector\nGuides\nâ\nIntegrate Anomalo with Atlan\n: Configure the integration between Anomalo and Atlan for data quality monitoring.\nReferences\nâ\nWhat does Atlan crawl from Anomalo\n: Learn about the Anomalo assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Anomalo\n: Verify prerequisites before setting up the Anomalo connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Anomalo connection issues and errors.\nTags:\nanomalo\nconnector\nobservability\ndata quality\nconnectivity\nNext\nSet up Anomalo\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/how-tos/configure-alerts",
    "content": "Build governance\nData Quality Studio\nAdvanced configuration\nConfigure alerts\nOn this page\nConfigure alerts\nSet up real-time notifications for data quality rule failures in Atlan. Receive immediate alerts via Slack or Microsoft Teams when rules fail, enabling quick response to data quality issues. This guide shows how to configure organization-level alert destinations and set rule-specific alert priorities.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAdministrative access to Atlan (for organization-level configuration)\nAccess to a public Slack or Microsoft Teams channel\nData quality rules already configured in your environment\nConfigure organization-level alerts\nâ\nSet up the alert destination for all data quality rules in your organization. This configuration applies to all rules and determines where alerts are sent. Only instance administrators can access this setting.\nIMPORTANT\nOnly public channels are supported. Alerts can't be routed to private channels or Direct Messages at this time.\nNavigate to the\nAdmin\npanel from the profile menu\nIn the left-hand menu, select\nIntegrations\nChoose your preferred messaging platform:\nSlack\nor\nMicrosoft Teams\nWithin the selected platform, scroll to the\nData Quality\nsection\nEnter the name of the public channel where rule failure alerts are delivered\nClick\nUpdate\nto activate the integration\nOnce saved, the alerting configuration is in effect for all data quality rules based on their priority settings.\nConfigure rule-level alert priority\nâ\nSet alert priorities during rule creation or editing. This determines how frequently alerts are sent for each specific rule.\nNavigate to the data quality rule you want to configure\nOpen the rule for editing or create a new rule\nIn the rule creation workflow, scroll to the\nAdditional Settings\nsection\nAll rules default to Normal priority unless explicitly changed by the user. Under\nAlerts\n, select the desired priority level:\nNormal\n(default): Alerts are sent up to three times per failure, then suppressed until the rule passes. Use this for most data quality rules.\nUrgent\n: Alerts are sent every time the rule fails. Use this for critical business rules.\nLow\n: No alerts are sent. Failures are silently logged. Use this for non-critical monitoring.\nNext steps\nâ\nAfter completing these steps:\nRule failures trigger alerts in the designated public Slack or Teams channel, based on priority\nEach alert includes full context - rule name, asset, severity, and relevant metadata - to aid quick triage and action\nRule-level alert priority settings can be modified at any time by editing the rule\nNeed help\nâ\nIf you have questions or need assistance with configuring alerts, reach out to Atlan Support by\nsubmitting a support request\n.\nSee also\nâ\nSet up Databricks\n- Configure Databricks for data quality monitoring\nSet up Snowflake\n- Configure Snowflake for data quality monitoring\nTags:\ndata-quality\nalerts\nnotifications\nNext\nWhat's auto re-attachment\nPrerequisites\nConfigure organization-level alerts\nConfigure rule-level alert priority\nNext steps\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/enable-auto-re-attachment",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nConfigure data quality\nEnable auto re-attachment of rules\nOn this page\nEnable auto re-attachment of rules\nThis guide explains how to configure your Snowflake environment to enable automatic reattachment of rules in Atlan.\nPrerequisites\nâ\nMake sure the following conditions are met:\nThe recreated asset has the same name and similar structure.\nThe original rule is still present and not deleted.\nThe feature is enabled for your tenant.\nGrant privileges\nâ\nBelow are the minimum privileges the\natlan_dq_service_role\nrole needs to reattach rules automatically. Replace\n<database-name>\nand similar placeholders with your own object names.\nDatabase and schema usage\n: Enables Atlan to discover and reference objects.\nGRANT\nUSAGE\nON\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nALL\nSCHEMAS\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nTable and view reference\n: Required for rules that reference columns in these objects.\nGRANT\nREFERENCES\nON\nALL\nTABLES\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nREFERENCES\nON\nALL\nVIEWS\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nDMF schema and function usage\n: Enables execution of Snowflake Data Metric Functions created by Atlan.\n-- Grant usage on the helper database / schema holding DMF functions\nGRANT\nUSAGE\nON\nDATABASE\nATLAN_DQ_DQ\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\nATLAN_DQ_DQ\n.\nDMFS\nTO\nROLE atlan_dq_service_role\n;\n-- Grant usage on all existing and future functions within the DMF schema\nGRANT\nUSAGE\nON\nALL\nFUNCTIONS\nIN\nSCHEMA\nATLAN_DQ_DQ\n.\nDMFS\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nFUTURE FUNCTIONS\nIN\nSCHEMA\nATLAN_DQ_DQ\n.\nDMFS\nTO\nROLE atlan_dq_service_role\n;\nFuture-proofing privileges\n: Ensures newly created objects are covered without manual grants.\nGRANT\nUSAGE\nON\nFUTURE SCHEMAS\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nREFERENCES\nON\nFUTURE\nTABLES\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nREFERENCES\nON\nFUTURE VIEWS\nIN\nDATABASE\n<\ndatabase\n-\nname\n>\nTO\nROLE atlan_dq_service_role\n;\nAfter these grants are applied, any table or view that's recreated with the same name automatically regains its attached rules, keeping your data quality checks continuous.\nWhatâs next\nâ\nOnce privileges are configured, rule reattachment happens automatically whenever matching assets are recreated in Snowflake. You can now continue monitoring your data quality workflows without needing to manually reapply rules.\nSee also\nâ\nAuto re-attachment rules\n: Understand how Atlan automatically reattaches rules to recreated Snowflake assets to maintain continuous data quality enforcement.\nTags:\nsnowflake\ndata-quality\nauto-re-attachment\nPrevious\nEnable data quality on connection\nNext\nUpgrade to Snowflake data quality studio\nPrerequisites\nGrant privileges\nWhatâs next\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/references/operations",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nReferences\nOperations\nOn this page\nOperations\nAtlan crawls and manages the following data quality operations and results from Snowflake.\nOnce you have\nenabled data quality on your connection\n, Atlan can perform data quality operations and retrieve results from your Snowflake environment.\nCUD operations for rules\nâ\nWhen creating, updating, or deleting data quality rules:\nComponent\nDetails\nProcedure\nAtlan invokes the\nMANAGE_DMF\nstored procedure in Snowflake\nUser\nAtlan DQ User\nRole\natlan_dq_service_role\nSupported action types\nâ\nAction\nDescription\nATTACH_DMF\nAttach a data metric function to a table\nDETACH_DMF\nRemove a data metric function from a table\nSUSPEND_DMF\nPause execution of a data metric function\nRESUME_DMF\nResume execution of a data metric function\nUPDATE_SCHEDULE\nModify the execution schedule\nCREATE_DMF\nCreate a new data metric function\nProcedure response\nâ\nMANAGE_DMF\nprovides only an acknowledgment response indicating successful execution\nTest SQL operations\nâ\nFor custom SQL rules, Atlan validates and tests SQL statements:\nComponent\nDetails\nProcedure\nAtlan triggers the\nMANAGE_DMF\nprocedure for test SQL operations\nUser\nAtlan DQ User\nRole\natlan_dq_service_role\nSupported action types\nâ\nAction\nDescription\nEXECUTE_SQL\nRun custom SQL for testing\nVALIDATE_SQL_PERMISSIONS\nVerify SQL permissions\nProcedure responses\nâ\nAction\nResponse\nVALIDATE_SQL_PERMISSIONS\nReturns acknowledgment confirming permission validity\nEXECUTE_SQL\nReturns a scalar numeric result from executing the custom user-defined SQL statement, following rigorous validation\nCrawling\nâ\nAtlan periodically crawls data from Snowflake to update rule results.\nTarget tables\nâ\nTable\nPurpose\nATLAN_DQ.STORE.DQ_RULE\nRule definitions\nATLAN_DQ.STORE.DQ_RULE_RESULT\nExecution results\nOutcome\nâ\nCrawled data populates Atlan's internal representation of rule execution results.\nTags:\nsnowflake\ndata-quality\noperations\nreference\nPrevious\nUpgrade to Snowflake data quality studio\nNext\nRoles and permissions\nCUD operations for rules\nTest SQL operations\nCrawling"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/references/data-quality-rules",
    "content": "Build governance\nData Quality Studio\nWhat is Data Quality Studio\nReferences\nRules and dimensions\nOn this page\nRules and dimensions\nThis document lists the data quality rules and classification dimensions available in Snowflake Data Quality Studio.\nPredefined data quality rules\nâ\nDuring the private preview, Atlan provides a set of predefined data quality rules, including:\nBlank & Null Checks\nBlank count\nBlank percentage\nNull count\nNull percentage\nVolume Checks\nRow count\nFreshness Metrics\nData freshness tracking\nStatistical Insights\nAverage value\nMinimum value\nMaximum value\nStandard deviation\nUniqueness & Duplicates\nDuplicate count\nUnique count\nString Validations\nRegex\nString Length\nValid Values\nData quality dimensions\nâ\nTo provide better context and insights, Atlan classifies results into key data quality dimensions:\nâ\nAccuracy\n: Verifying correctness and reliability\nâ³\nTimeliness\n: Validating data freshness and latency\nð\nValidity\n: Checking data formats and constraints\nð\nCompleteness\n: Measuring missing or incomplete data\nð\nConsistency\n: Maintaining data follows the same format and standards across datasets\nð¢\nUniqueness\n: Verifying data records are distinct and free from duplicates\nð\nVolume\n: Measuring data quantity and row counts\nSee also\nâ\nSet up Databricks\n- Configure Databricks for data quality monitoring\nSet up Snowflake\n- Configure Snowflake for data quality monitoring\nTags:\nsnowflake\ndata-quality\nrules\ndimensions\nreference\nPrevious\nData quality permissions\nPredefined data quality rules\nData quality dimensions\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/how-tos/set-up-databricks",
    "content": "Build governance\nData Quality Studio\nDatabricks Data Quality\nGet Started\nSet up Databricks\nOn this page\nSet up Databricks\nPrivate Preview\nThis guide walks through configuring Databricks to work with Atlan's data quality studio by creating the required service principal, setting up authentication, and granting the necessary privileges. Atlan recommends using serverless SQL warehouses for instant compute availability.\nSystem requirements\nâ\nBefore setting up the integration, make sure you meet the following requirements:\nDatabricks Premium or Enterprise edition\nServerless Compute for Jobs & Notebooks enabled\nDedicated SQL warehouse for running DQ-related queries\nOutbound network access permitted from Serverless Compute (Enterprise tier only)\nPrerequisites\nâ\nBefore you begin, complete the following steps:\nObtain Workspace admin and Metastore Admin or CREATE CATALOG privilege\nIdentify your dedicated SQL warehouse for DQ operations\nCreate an API token in Atlan\nthat's stored in Databricks for authentication\nReview\nData Quality permissions\nto understand required privileges\nCreate service principal\nâ\nCreate the service principal that Atlan uses to perform Data Quality (DQ) operations within your Databricks workspace.\nFollow the appropriate guide based on your Databricks deployment environment:\nCreating a Service Principal in AWS based Databricks Accounts\nCreating a Service Principal in Azure based Databricks Accounts\nStore the following credentials securely:\nclient_id\nclient_secret\ntenant_id\n(Azure only)\nService principal name\nAtlan recommends naming it:\natlan-dq-service-principal\nSet up authentication: Choose one of the following authentication methods for your service principal:\nOAuth (Recommended)\n:\nUse the\nclient_id\n,\nclient_secret\n, and\ntenant_id\n(Azure only) from the service principal created in the previous step\nNo additional configuration required\nPersonal access token (pat)\n:\nFollow the\nDatabricks Personal Access Token guide\nto generate a token for the service principal\nStore the token securely for use in the next steps\nGrant warehouse access: Grant the service principal access to a SQL warehouse that's used to run Data Quality queries.\nGo to your Databricks workspace UI\nNavigate to\nSQL\n>\nSQL Warehouses\nClick on the warehouse you want Atlan to use\nClick on the\nPermissions\nbutton\nSelect the Service Principal (\natlan-dq-service-principal\n) from the list\nAssign the\nCan Use\npermission\nClick\nAdd\nOnce access is granted, Atlan can use this warehouse to run SQL queries related to Data Quality operations.\nSet up Databricks objects\nâ\nCreate the required Databricks objects needed for the functioning of the Atlan Data Quality Studio.\nCreate the atlan_dq catalog\nâ\nThe\natlan_dq\ncatalog is used by Atlan to store metadata, DQ rule execution results, and internal processing tables.\nRun the following SQL command in a Databricks notebook or SQL editor:\nCREATE\nCATALOG\nIF\nNOT\nEXISTS\natlan_dq\n;\nSet up secret scope and secret\nâ\nCreate a Databricks Secret Scope to securely store the Atlan API token. This token enables the service principal to authenticate and interact with Atlan's APIs.\nnote\nSecret scopes and secret ACLs can only be managed using the Databricks CLI or REST API. These operations aren't supported through SQL.\nCreate a new Secret Scope named\natlan_dq\n:\ndatabricks secrets create-scope atlan_dq\nSave the Atlan API token in a secret named\napi_token\nin the scope:\ndatabricks secrets put-secret --json '{\n\"scope\": \"atlan_dq\",\n\"key\": \"api_token\",\n\"string_value\": \"<ATLAN_API_TOKEN>\"\n}'\nReplace\n<ATLAN_API_TOKEN>\nwith the API token value you created in Atlan.\nGrant privileges\nâ\nGrant the following privileges to\natlan-dq-service-principal\nso it can create internal objects, read the Atlan API token, and query data for quality checks. Replace placeholders with real values.\nManage the\natlan_dq\ncatalog\nGRANT\nUSE\nCATALOG\nON\nCATALOG atlan_dq\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nCREATE\nSCHEMA\nON\nCATALOG atlan_dq\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nRead the API token stored in the\natlan_dq\nsecret scope\ndatabricks secrets put-acl atlan_dq <SERVICE_PRINCIPAL_CLIENT_ID> READ\nAccess data for quality checks\n(choose one scope)\nCatalog level\nGRANT\nUSE\nCATALOG\nON\nCATALOG\n<\nCATALOG\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nUSE\nSCHEMA\nON\nCATALOG\n<\nCATALOG\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nSELECT\nON\nCATALOG\n<\nCATALOG\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nSchema level\nGRANT\nUSE\nCATALOG\nON\nCATALOG\n<\nCATALOG\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nUSE\nSCHEMA\nON\nSCHEMA\n<\nSCHEMA\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nSELECT\nON\nSCHEMA\n<\nSCHEMA\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nTable level\nGRANT\nUSE\nCATALOG\nON\nCATALOG\n<\nCATALOG\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nUSE\nSCHEMA\nON\nSCHEMA\n<\nSCHEMA\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nGRANT\nSELECT\nON\nTABLE\n<\nTABLE\n>\nTO\n'<SERVICE_PRINCIPAL_CLIENT_ID>'\n;\nThese grants let Atlan create its internal schemas, fetch the API token securely, and run SELECT queries needed for rule execution.\nNext steps\nâ\nEnable data quality on connection\n- Configure your Databricks connection for data quality monitoring\nNeed help\nâ\nIf you have questions or need assistance with setting up Databricks for data quality, reach out to Atlan Support by\nsubmitting a support request\n.\nSee also\nâ\nConfigure alerts for data quality rules\n- Set up real-time notifications for rule failures\nTags:\ndatabricks\ndata-quality\nsetup\ngovernance\nPrevious\nDatabricks Data Quality Studio\nNext\nEnable data quality on connection\nSystem requirements\nPrerequisites\nCreate service principal\nSet up Databricks objects\nCreate the atlan_dq catalog\nSet up secret scope and secret\nGrant privileges\nNext steps\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/set-up-snowflake",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nGet Started\nSet up Snowflake\nOn this page\nSet up Snowflake\nPrivate Preview\nThis guide walks through configuring Snowflake to work with Atlan's data quality studio by creating the required roles, setting up database objects, and granting the necessary privileges.\nSystem requirements\nâ\nBefore setting up the integration, make sure you meet the following requirements:\nSnowflake Enterprise or Business Critical edition\nDedicated Snowflake warehouse for running DQ-related queries\nPrerequisites\nâ\nBefore you begin, complete the following steps:\nObtain\nACCOUNTADMIN\nrole or equivalent administrative privileges in Snowflake\nIdentify your dedicated warehouse name for DQ operations\nHave access to create a new Snowflake user for Atlan\nReview\nData Quality permissions\nto understand required privileges\nCreate roles\nâ\nCreate two roles for the integration:\n-- Create DQ Admin Role\nCREATE\nROLE\nIF\nNOT\nEXISTS\ndq_admin\n;\nGRANT\nOPERATE\n,\nUSAGE\nON\nWAREHOUSE\n\"<warehouse-name>\"\nTO\nROLE dq_admin\n;\n-- Create Atlan Service Role\nCREATE\nROLE\nIF\nNOT\nEXISTS\natlan_dq_service_role\n;\nGRANT\nOPERATE\n,\nUSAGE\nON\nWAREHOUSE\n\"<warehouse-name>\"\nTO\nROLE atlan_dq_service_role\n;\nCreate user\nâ\nCreate a dedicated Snowflake user for Atlan following your organization's standards, then grant the service role:\nGRANT\nROLE atlan_dq_service_role\nTO\nUSER\n<\natlan_dq_user\n>\n;\nSet up database objects\nâ\nCreate the database, schemas, and stored procedure required for Atlan data quality operations.\nCreate the required database structure:\n-- Create database\nCREATE\nDATABASE\nATLAN_DQ\n;\n-- Create schemas\nCREATE\nSCHEMA\nATLAN_DQ\n.\nSHARED\n;\nCREATE\nSCHEMA\nIF\nNOT\nEXISTS\nATLAN_DQ\n.\nDMFS\n;\nThe\nATLAN_DQ\ndatabase serves as a container for all objects related to Atlan Data Quality. The\nATLAN_DQ.SHARED\nschema provides a separate namespace for shared procedures and functions, while the\nATLAN_DQ.DMFS\nschema is required for custom Data Metric Functions (DMFs).\nCreate stored procedure for DMF management:\nView procedure code\n/**\n* Manages Data Metric Functions (DMF) operations for Snowflake tabular entities.\n* This procedure handles various DMF operations including:\n* - Creating and managing DMFs (CREATE_DMF)\n* - Attaching/detaching DMFs to entities (ATTACH_DMF, DETACH_DMF)\n* - Managing DMF schedules (UPDATE_SCHEDULE)\n* - Executing SQL expressions (EXECUTE_SQL)\n* - Validating SQL permissions (VALIDATE_SQL_PERMISSIONS)\n*\n* The procedure runs with the privileges of the procedure owner and includes comprehensive\n* validation of all inputs and permissions before executing any operations.\n*\n*\n@param\n{\nstring\n}\nACTION\n- Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE, CREATE_DMF, EXECUTE_SQL, VALIDATE_SQL_PERMISSIONS)\n*\n@param\n{\nstring\n}\nENTITY_TYPE\n- Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE)\n*\n@param\n{\nstring\n}\nENTITY_NAME\n- Fully qualified name of the entity (database.schema.name)\n*\n@param\n{\nstring\n}\n[\nDMF_NAME\n=\nnull\n]\n- Fully qualified name of the DMF (database.schema.name)\n*\n@param\n{\nstring\n}\n[DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations\n*\n@param\n{\nstring\n}\n[\nSCHEDULE_TYPE\n=\nnull\n]\n- Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED)\n*\n@param\n{\nstring\n}\n[\nSCHEDULE_VALUE\n=\nnull\n]\n- Schedule value based on type\n*\n@param\n{\nstring\n}\n[\nDMF_DEFINITION\n=\nnull\n]\n- SQL expression defining the DMF\n*\n@param\n{\nstring\n}\n[\nROLE_TO_CHECK\n=\nnull\n]\n- Role to check permissions for\n*\n@param\n{\nstring\n}\n[\nDATABASES_TO_CHECK\n=\nnull\n]\n- Comma-separated list of databases to validate permissions for\n*\n@param\n{\nstring\n}\n[\nSCHEMAS_TO_CHECK\n=\nnull\n]\n- Comma-separated list of schemas to validate permissions for\n*\n@param\n{\nstring\n}\n[\nTABLES_TO_CHECK\n=\nnull\n]\n- Comma-separated list of tables to validate permissions for\n*\n@returns\n{\nstring\n}\n- JSON string with operation status and result message\n*/\nCREATE\nOR\nREPLACE\nSECURE\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nACTION\nSTRING\n,\nENTITY_TYPE\nSTRING\nDEFAULT\nNULL\n,\nENTITY_NAME\nSTRING\nDEFAULT\nNULL\n,\nDMF_NAME\nSTRING\nDEFAULT\nNULL\n,\nDMF_ARGUMENTS_JSON\nSTRING\nDEFAULT\n'[]'\n,\nSCHEDULE_TYPE\nSTRING\nDEFAULT\nNULL\n,\nSCHEDULE_VALUE\nSTRING\nDEFAULT\nNULL\n,\nDMF_DEFINITION\nSTRING\nDEFAULT\nNULL\n,\nROLE_TO_CHECK\nSTRING\nDEFAULT\nNULL\n,\nDATABASES_TO_CHECK\nSTRING\nDEFAULT\nNULL\n,\nSCHEMAS_TO_CHECK\nSTRING\nDEFAULT\nNULL\n,\nTABLES_TO_CHECK\nSTRING\nDEFAULT\nNULL\n)\nRETURNS\nSTRING\nLANGUAGE\nJAVASCRIPT\nEXECUTE\nAS\nOWNER\nAS\n$$\n// -----------------------------------------------------UTILITY FUNCTIONS-----------------------------------------------------\n/**\n* Executes a SQL query with parameters\n*\n@param\n{\nstring\n}\nsqlText\n- SQL statement to execute\n*\n@param\n{\nArray\n}\n[binds=[]] - Array of bind parameters for the query\n*\n@param\n{\nboolean\n}\n[\nreturnFirstRow\n=\nfalse\n]\n- Whether to return only the first row\n*\n@returns\n{\nObject\n}\nObject containing execution result or error information\n*/\nfunction\nexecuteQuery\n(\nsqlText\n,\nbinds\n=\n[\n]\n,\nreturnFirstRow\n=\nfalse\n)\n{\ntry\n{\nif\n(\n!\nsqlText\n)\nreturn\n{\nisErrored\n:\ntrue\n,\nmessage\n:\n\"SQL Text is required\"\n,\nresult\n:\nnull\n,\n}\n;\nconst\nstatement\n=\nsnowflake\n.\ncreateStatement\n(\n{\nsqlText\n,\nbinds\n}\n)\n;\nconst\nresult\n=\nstatement\n.\nexecute\n(\n)\n;\nconst\nresponse\n=\n{\nisErrored\n:\nfalse\n,\nmessage\n:\n\"\"\n,\nresult\n:\nnull\n,\n}\n;\nif\n(\nreturnFirstRow\n)\n{\nresponse\n.\nresult\n=\nresult\n.\nnext\n(\n)\n?\nresult\n:\nnull\n;\nreturn\nresponse\n;\n}\nresponse\n.\nresult\n=\nresult\n;\nreturn\nresponse\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\n{\nisErrored\n:\ntrue\n,\nmessage\n:\n`\n${\nerr\n.\ncode\n}\n-\n${\nerr\n.\nmessage\n}\n-\n${\nsqlText\n}\nwith binds:\n${\nbinds\n.\njoin\n(\n\", \"\n)\n}\n`\n,\nresult\n:\nnull\n,\n}\n;\n}\n}\n/**\n* Safely parses a JSON string\n*\n@param\n{\nstring\n}\njsonString\n- JSON string to parse\n*\n@returns\n{\nObject\n}\nParsed JSON object or null if invalid\n*/\nfunction\nsafelyParseJSON\n(\njsonString\n)\n{\ntry\n{\nreturn\nJSON\n.\nparse\n(\njsonString\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\nnull\n;\n}\n}\n/**\n* Validates a number within a range\n*\n@param\n{\nstring\n}\nvalue\n- Number to validate\n*\n@param\n{\nnumber\n}\nmin\n- Minimum value\n*\n@param\n{\nnumber\n}\nmax\n- Maximum value\n*\n@returns\n{\nboolean\n}\nTrue if number is valid\n*\n@returns\n{\nboolean\n}\nFalse if number is invalid\n*/\nfunction\nisNumberValid\n(\nvalue\n,\nmin\n,\nmax\n)\n{\nconst\nnum\n=\nparseInt\n(\nvalue\n,\n10\n)\n;\nreturn\n!\nisNaN\n(\nnum\n)\n&&\nnum\n>=\nmin\n&&\nnum\n<=\nmax\n;\n}\n/**\n* Escapes and quotes a Snowflake identifier\n*\n@param\n{\nstring\n}\nidentifier\n- Raw identifier to normalize\n*\n@returns\n{\nstring\n}\nProperly quoted identifier safe for SQL\n*/\nfunction\nnormalizeIdentifier\n(\nidentifier\n)\n{\nreturn\n`\n\"\n${\nidentifier\n.\nreplace\n(\n/\n\"\n/\ng\n,\n'\"\"'\n)\n}\n\"\n`\n;\n}\n/**\n* Retrieves all columns for a given entity. Validates that the entityexists and procedure owner has access to it.\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified entity name\n*\n@returns\n{\nArray\n}\nArray of column objects with name and dataType properties\n*\n@throws\n{\nError\n}\nIf entity doesn't exist or is inaccessible\n*/\nfunction\ngetAllColumnsForEntity\n(\nentityName\n)\n{\nconst\nsqlText\n=\n\"SHOW COLUMNS IN IDENTIFIER(?)\"\n;\nconst\nbinds\n=\n[\nentityName\n]\n;\nconst\n{\nresult\n,\nisErrored\n,\nmessage\n}\n=\nexecuteQuery\n(\nsqlText\n,\nbinds\n)\n;\nif\n(\nisErrored\n)\n{\n// Validates that the entity exists and procedure owner has access to it\nthrow\nnew\nError\n(\nmessage\n)\n;\n}\nconst\ncolumns\n=\n[\n]\n;\nwhile\n(\nresult\n.\nnext\n(\n)\n)\n{\nconst\ncolumn\n=\n{\nname\n:\nresult\n.\ngetColumnValue\n(\n\"column_name\"\n)\n,\ndataType\n:\nJSON\n.\nparse\n(\nresult\n.\ngetColumnValue\n(\n\"data_type\"\n)\n)\n.\ntype\n,\n}\n;\nif\n(\ncolumn\n.\ndataType\n===\n\"FIXED\"\n)\ncolumn\n.\ndataType\n=\n\"NUMBER\"\n;\ncolumns\n.\npush\n(\ncolumn\n)\n;\n}\nreturn\ncolumns\n;\n}\n/**\n* Validates that the DMF is valid and exists\n*\n@param\n{\nstring\n}\ndmfName\n- Fully qualified name of the DMF\n*\n@param\n{\nstring\n}\ndmfArguments\n- Arguments for the DMF\n*\n@returns\n{\nboolean\n}\nWhether the DMF is valid\n*\n@throws\n{\nError\n}\nIf DMF is invalid\n*/\nfunction\nisDMFValid\n(\ndmfName\n,\ndmfArguments\n)\n{\nconst\n{\nisErrored\n,\nmessage\n}\n=\nexecuteQuery\n(\n`\nDESCRIBE FUNCTION IDENTIFIER(?)(\n${\ndmfArguments\n}\n)\n`\n,\n[\ndmfName\n]\n,\ntrue\n)\n;\nif\n(\nisErrored\n)\nthrow\nnew\nError\n(\nmessage\n)\n;\nreturn\ntrue\n;\n}\n/**\n* Checks if a timezone is valid\n*\n@param\n{\nstring\n}\ntimezone\n- Timezone to validate\n*\n@returns\n{\nboolean\n}\nTrue if timezone is valid\n*\n@returns\n{\nboolean\n}\nFalse if timezone is invalid\n*/\nfunction\nisTimezoneValid\n(\ntimezone\n)\n{\nconst\nresult\n=\nexecuteQuery\n(\n`\nSELECT CONVERT_TIMEZONE(?, CURRENT_TIMESTAMP())\n`\n,\n[\ntimezone\n]\n,\ntrue\n)\n;\nreturn\n!\nresult\n.\nisErrored\n;\n}\n/**\n* Generates a DMF type signature based on the arguments and entity columns\n*\n@param\n{\nArray\n}\ndmfArguments\n- Array of DMF arguments\n*\n@param\n{\nObject\n}\nentityColumnsMap\n- Map of entity names to column objects in the format\n{\n<ENTITY_NAME>: [\n{\nname: <COLUMN_NAME> , dataType: <DATA_TYPE>\n}\n]\n}\n*\n@param\n{\nstring\n}\nbaseEntityName\n- Name of the base entity\n*\n@returns\n{\nstring\n}\nDMF type signature\n*\n@throws\n{\nError\n}\nIf entity not found in the cache\n*/\nfunction\ngenerateDMFTypeSignature\n(\ndmfArguments\n,\nentityColumnsMap\n,\nbaseEntityName\n)\n{\nif\n(\n!\ndmfArguments\n||\n!\ndmfArguments\n.\nlength\n)\nreturn\n\"\"\n;\nconst\nbaseEntityColumns\n=\nentityColumnsMap\n[\nbaseEntityName\n]\n;\nif\n(\n!\nbaseEntityColumns\n)\n{\nthrow\nnew\nError\n(\n`\nEntity\n${\nbaseEntityName\n}\nnot found in the cache\n`\n)\n;\n}\nconst\nbaseEntityColumnArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"COLUMN\"\n)\n.\nmap\n(\nparam\n=>\n{\nconst\ncolumn\n=\nbaseEntityColumns\n.\nfind\n(\ncol\n=>\ncol\n.\nname\n===\nparam\n.\nname\n)\n;\nreturn\ncolumn\n?\ncolumn\n.\ndataType\n:\nnull\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\nconst\nbaseEntityArguments\n=\n`\nTABLE(\n${\nbaseEntityColumnArguments\n}\n)\n`\n;\nconst\nreferencedEntityArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n.\nmap\n(\nentityParam\n=>\n{\nconst\nentityName\n=\nentityParam\n.\nname\n;\nconst\nentityColumns\n=\nentityColumnsMap\n[\nentityName\n]\n;\nif\n(\n!\nentityColumns\n)\n{\nthrow\nnew\nError\n(\n`\nEntity\n${\nentityName\n}\nnot found in the cache\n`\n)\n;\n}\nconst\ncolumnTypes\n=\nentityParam\n.\nnested\n.\nmap\n(\nnestedParam\n=>\n{\nconst\ncolumn\n=\nentityColumns\n.\nfind\n(\ncol\n=>\ncol\n.\nname\n===\nnestedParam\n.\nname\n)\n;\nreturn\ncolumn\n?\ncolumn\n.\ndataType\n:\nnull\n;\n}\n)\n.\nfilter\n(\nBoolean\n)\n.\njoin\n(\n\", \"\n)\n;\nreturn\n`\nTABLE(\n${\ncolumnTypes\n}\n)\n`\n;\n}\n)\n;\nconst\narguments\n=\n[\nbaseEntityArguments\n,\n...\nreferencedEntityArguments\n]\n.\njoin\n(\n\", \"\n)\n;\nreturn\narguments\n;\n}\n/**\n* Generates DMF arguments for SQL statements\n*\n@param\n{\nstring\n}\ndmfArguments\n- Array of DMF arguments\n*\n@returns\n{\nstring\n}\nFormatted DMF arguments for SQL statements\n*/\nfunction\ngenerateDMFColumnArguments\n(\ndmfArguments\n)\n{\nreturn\ndmfArguments\n.\nmap\n(\nparam\n=>\n{\nif\n(\nparam\n.\ntype\n===\n\"COLUMN\"\n)\n{\nreturn\nnormalizeIdentifier\n(\nparam\n.\nname\n)\n;\n}\n// Handle TABLE type with nested columns\nreturn\n`\nTABLE(\n${\nnormalizeIdentifier\n(\nparam\n.\nname\n)\n}\n(\n${\nparam\n.\nnested\n.\nmap\n(\nnested\n=>\nnormalizeIdentifier\n(\nnested\n.\nname\n)\n)\n.\njoin\n(\n\", \"\n)\n}\n))\n`\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\n}\n/**\n* Generates function parameters from DMF arguments\n*\n@param\n{\nArray\n}\ndmfArguments\n- Array of DMF arguments\n*\n@returns\n{\nstring\n}\nFormatted function parameters\n*/\nfunction\ngenerateFunctionParameters\n(\ndmfArguments\n)\n{\nreturn\ndmfArguments\n.\nmap\n(\nparam\n=>\n{\nif\n(\nparam\n.\ntype\n===\n\"TABLE\"\n)\n{\nconst\nnestedParams\n=\nparam\n.\nnested\n.\nmap\n(\nnested\n=>\n`\n${\nnested\n.\nname\n}\n${\nnested\n.\ndataType\n}\n`\n)\n.\njoin\n(\n\", \"\n)\n;\nreturn\n`\n${\nparam\n.\nname\n}\nTABLE(\n${\nnestedParams\n}\n)\n`\n;\n}\nreturn\n`\n${\nparam\n.\nname\n}\n${\nparam\n.\ndataType\n}\n`\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\n}\n// -----------------------------------------------------VALIDATION FUNCTIONS-----------------------------------------------------\n/**\n* Validates that mandatory arguments are provided and valid\n*\n@throws\n{\nError\n}\nIf any mandatory argument is missing or invalid\n*/\nfunction\nvalidateMandatoryArguments\n(\n)\n{\nconst\nVALID_ACTIONS\n=\nnew\nSet\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n,\n\"UPDATE_SCHEDULE\"\n,\n\"CREATE_DMF\"\n,\n\"EXECUTE_SQL\"\n,\n\"VALIDATE_SQL_PERMISSIONS\"\n]\n)\n;\nconst\nVALID_ENTITY_TYPES\n=\nnew\nSet\n(\n[\n\"TABLE\"\n,\n\"VIEW\"\n,\n\"MATERIALIZED VIEW\"\n,\n\"EXTERNAL TABLE\"\n,\n\"ICEBERG TABLE\"\n]\n)\n;\nconst\nDMF_OPS\n=\nnew\nSet\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n]\n)\n;\nconst\nVALID_SCHEDULE_TYPES\n=\nnew\nSet\n(\n[\n\"MINUTES\"\n,\n\"CRON\"\n,\n\"ON_DATA_CHANGE\"\n,\n\"NOT_SCHEDULED\"\n]\n)\n;\nconst\nSCHEDULE_TYPES_THAT_REQUIRE_VALUE\n=\nnew\nSet\n(\n[\n\"MINUTES\"\n,\n\"CRON\"\n]\n)\n;\nif\n(\n!\nVALID_ACTIONS\n.\nhas\n(\nACTION\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid ACTION: \"\n${\nACTION\n}\n\". Valid options are\n${\nArray\n.\nfrom\n(\nVALID_ACTIONS\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nENTITY_TYPE\n&&\n!\nVALID_ENTITY_TYPES\n.\nhas\n(\nENTITY_TYPE\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid ENTITY_TYPE: \"\n${\nENTITY_TYPE\n}\n\". Valid options are\n${\nArray\n.\nfrom\n(\nVALID_ENTITY_TYPES\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nDMF_OPS\n.\nhas\n(\nACTION\n)\n&&\n!\nDMF_NAME\n)\nthrow\nnew\nError\n(\n\"DMF_NAME is required for DMF related actions\"\n)\n;\nif\n(\nACTION\n===\n\"UPDATE_SCHEDULE\"\n)\n{\nif\n(\n!\nSCHEDULE_TYPE\n)\nthrow\nnew\nError\n(\n\"SCHEDULE_TYPE is required for SCHEDULE action\"\n)\n;\nif\n(\n!\nVALID_SCHEDULE_TYPES\n.\nhas\n(\nSCHEDULE_TYPE\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid schedule type: \"\n${\nSCHEDULE_TYPE\n}\n\". Valid options are\n${\nArray\n.\nfrom\n(\nVALID_SCHEDULE_TYPES\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nSCHEDULE_TYPES_THAT_REQUIRE_VALUE\n.\nhas\n(\nSCHEDULE_TYPE\n)\n&&\n!\nSCHEDULE_VALUE\n)\nthrow\nnew\nError\n(\n\"SCHEDULE_VALUE is required for SCHEDULE action\"\n)\n;\n}\nif\n(\nACTION\n===\n\"EXECUTE_SQL\"\n&&\n!\nDMF_DEFINITION\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a SQL query to execute.\"\n)\n;\n}\nif\n(\nACTION\n===\n\"VALIDATE_SQL_PERMISSIONS\"\n)\n{\nif\n(\n!\nDMF_DEFINITION\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a SQL query to validate permissions.\"\n)\n;\n}\nif\n(\n!\nROLE_TO_CHECK\n)\n{\nthrow\nnew\nError\n(\n\"Failed to fetch role to check permissions. Please ensure the role exists and is accessible.\"\n)\n;\n}\nif\n(\n!\nDATABASES_TO_CHECK\n&&\n!\nSCHEMAS_TO_CHECK\n&&\n!\nTABLES_TO_CHECK\n)\n{\nthrow\nnew\nError\n(\n\"No databases, schemas, or tables provided. Please provide at least one database, schema, or table to validate permissions for.\"\n)\n;\n}\n}\n}\n/**\n* Parses a fully qualified name into its components\n*\n@param\n{\nstring\n}\nfullyQualifiedName\n- Fully qualified name to parse\n*\n@returns\n{\nObject\n}\nObject with database, schema, and name properties\n*\n@throws\n{\nError\n}\nIf invalid fully qualified name\n*/\nfunction\nvalidateFullyQualifiedName\n(\nfullyQualifiedName\n)\n{\nconst\nparts\n=\nfullyQualifiedName\n.\nsplit\n(\n\".\"\n)\n.\nmap\n(\npart\n=>\npart\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n;\nif\n(\nparts\n.\nlength\n!==\n3\n)\nthrow\nnew\nError\n(\n`\nInvalid fully qualified name:\n${\nfullyQualifiedName\n}\n. Expected format: database.schema.name\n`\n)\n;\n}\n/**\n* Validates the structure of DMF arguments JSON\n*\n@param\n{\nstring\n}\nrawDMFArguments\n- Raw JSON string of DMF arguments\n*\n@throws\n{\nError\n}\nIf DMF arguments structure is invalid\n*/\nfunction\nvalidateDMFArgumentsStructure\n(\nrawDMFArguments\n)\n{\nconst\nparsedStructure\n=\nsafelyParseJSON\n(\nrawDMFArguments\n)\n;\nif\n(\n!\nparsedStructure\n)\nthrow\nnew\nError\n(\n\"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\"\n)\n;\nif\n(\n!\nArray\n.\nisArray\n(\nparsedStructure\n)\n)\nthrow\nnew\nError\n(\n\"DMF_ARGUMENTS_JSON must be an array\"\n)\n;\nconst\nreferencedEntities\n=\nparsedStructure\n.\nfilter\n(\n(\nparam\n)\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n;\nif\n(\nreferencedEntities\n.\nlength\n>\n1\n)\nthrow\nnew\nError\n(\n\"Only one referenced entity is allowed\"\n)\n;\nconst\nvalidationFunctions\n=\n{\narrayItem\n:\n(\nparam\n)\n=>\n[\n\"COLUMN\"\n,\n\"TABLE\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n&&\nparam\n.\nname\n,\nnestedItem\n:\n(\nparam\n)\n=>\n[\n\"COLUMN\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n&&\nparam\n.\nname\n,\n}\n;\nif\n(\n!\nparsedStructure\n.\nevery\n(\nvalidationFunctions\n.\narrayItem\n)\n)\nthrow\nnew\nError\n(\n\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"\n)\n;\nif\n(\nreferencedEntities\n.\nlength\n>\n0\n)\n{\nfor\n(\nconst\nreferencedEntity\nof\nreferencedEntities\n)\n{\nif\n(\n!\nArray\n.\nisArray\n(\nreferencedEntity\n.\nnested\n)\n||\n!\nreferencedEntity\n.\nnested\n.\nevery\n(\nvalidationFunctions\n.\nnestedItem\n)\n)\nthrow\nnew\nError\n(\n\"Invalid nested parameters\"\n)\n;\n}\n}\n}\n/**\n* Validates that all specified columns exist in an entity\n*\n@param\n{\nArray\n}\ncolumnsToCheck\n- Array of column names to validate\n*\n@param\n{\nArray\n}\nentityColumns\n- Array of column metadata from the entity\n*\n@param\n{\nstring\n}\nentityName\n- Name of the entity for error message\n*\n@throws\n{\nError\n}\nIf any column doesn't exist in the entity\n*/\nfunction\nvalidateColumnsExistInEntity\n(\nentityName\n,\nallColumnsInEntity\n,\ncolumnsToCheck\n)\n{\nfor\n(\nconst\ncolumn\nof\ncolumnsToCheck\n)\n{\nif\n(\n!\nallColumnsInEntity\n.\nsome\n(\ncol\n=>\ncol\n.\nname\n===\ncolumn\n)\n)\nthrow\nnew\nError\n(\n`\nColumn\n${\ncolumn\n}\nnot found in entity\n${\nentityName\n}\n`\n)\n;\n}\n}\n/**\n* Validates that all provided identifiers exist and are accessible\n* Checks entity names, column names, and DMF compatibility\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified name of the entity\n*\n@param\n{\nstring\n}\ndmfName\n- Fully qualified name of the DMF\n*\n@param\n{\nArray\n}\ndmfArguments\n- Array of DMF arguments\n*\n@throws\n{\nError\n}\nIf any identifier doesn't exist or is inaccessible\n*/\nfunction\nvalidateProvidedIdentifiers\n(\nentityName\n,\ndmfName\n=\n\"\"\n,\ndmfArguments\n=\n[\n]\n)\n{\nif\n(\n!\nentityName\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a valid entity name. The entity name is required to validate identifiers.\"\n)\n;\n}\nvalidateFullyQualifiedName\n(\nentityName\n)\n;\n// Validate the provided entity names and store all the columns for each entity in a map\nconst\nbaseEntityName\n=\nentityName\n;\nconst\nbaseEntityAllColumns\n=\ngetAllColumnsForEntity\n(\nentityName\n)\n;\nconst\nentityColumnsMap\n=\n{\n[\nbaseEntityName\n]\n:\nbaseEntityAllColumns\n}\n;\nconst\nallReferencedEntities\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n;\nfor\n(\nconst\nreferencedEntity\nof\nallReferencedEntities\n)\n{\nconst\ncolumns\n=\ngetAllColumnsForEntity\n(\nreferencedEntity\n.\nname\n)\n;\nentityColumnsMap\n[\nreferencedEntity\n.\nname\n]\n=\ncolumns\n;\n}\n// Valite all of the provided columns are valid and exist in their respective entities\nconst\nallBaseEntityColumnsInArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"COLUMN\"\n)\n.\nmap\n(\nparam\n=>\nparam\n.\nname\n)\n;\nvalidateColumnsExistInEntity\n(\nbaseEntityName\n,\nbaseEntityAllColumns\n,\nallBaseEntityColumnsInArguments\n)\n;\nfor\n(\nconst\nreferencedEntity\nof\nallReferencedEntities\n)\n{\nconst\ncolumnsInArguments\n=\nreferencedEntity\n.\nnested\n.\nmap\n(\nnestedParam\n=>\nnestedParam\n.\nname\n)\n;\nvalidateColumnsExistInEntity\n(\nreferencedEntity\n.\nname\n,\nentityColumnsMap\n[\nreferencedEntity\n.\nname\n]\n,\ncolumnsInArguments\n)\n;\n}\nif\n(\ndmfName\n)\n{\n// Validate that the DMF is valid and exists\nconst\ngeneratedTypeSignature\n=\ngenerateDMFTypeSignature\n(\ndmfArguments\n,\nentityColumnsMap\n,\nbaseEntityName\n)\n;\nisDMFValid\n(\ndmfName\n,\ngeneratedTypeSignature\n)\n;\n}\n// All provided identifiers are valid, actually exist and are accessible to the procedure owner\n}\n/**\n* Validates CRON expression syntax\n* Performs detailed validation of all CRON components and timezones to protect against SQL injection\n*\n@param\n{\nstring\n}\ncronExpression\n- CRON expression to validate\n*\n@throws\n{\nError\n}\nIf CRON expression is invalid\n*/\nfunction\nvalidateCronExpression\n(\ncronExpression\n)\n{\nif\n(\ncronExpression\n.\nlength\n>\n100\n)\nthrow\nnew\nError\n(\n\"Cron expression is too long\"\n)\n;\nconst\ncronFields\n=\ncronExpression\n.\ntrim\n(\n)\n.\nsplit\n(\n/\n\\s\n+\n/\n)\n;\nif\n(\ncronFields\n.\nlength\n!==\n6\n)\nthrow\nnew\nError\n(\n\"Invalid cron expression. Expected 6 fields\"\n)\n;\nconst\n[\nminute\n,\nhour\n,\ndayOfMonth\n,\nmonth\n,\ndayOfWeek\n,\ntimezone\n]\n=\ncronFields\n;\nconst\nisTimezoneValidResult\n=\nisTimezoneValid\n(\ntimezone\n)\n;\nif\n(\n!\nisTimezoneValidResult\n)\nthrow\nnew\nError\n(\n\"Invalid timezone provided in the cron expression\"\n)\n;\nconst\nregexPatterns\n=\n{\nminute\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\nhour\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\ndayOfMonth\n:\n/\n^\n(\n\\*\n|\nL\n|\n\\d\n+\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\nmonth\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\nJAN\n|\nFEB\n|\nMAR\n|\nAPR\n|\nMAY\n|\nJUN\n|\nJUL\n|\nAUG\n|\nSEP\n|\nOCT\n|\nNOV\n|\nDEC\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n[\nA\n-\nZ\n]\n{3}\n\\-\n[\nA\n-\nZ\n]\n{3}\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n|\n(\n[\nA\n-\nZ\n]\n{3}\n(\n,\n[\nA\n-\nZ\n]\n{3}\n)\n*\n)\n)\n$\n/\ni\n,\ndayOfWeek\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\nSUN\n|\nMON\n|\nTUE\n|\nWED\n|\nTHU\n|\nFRI\n|\nSAT\n|\n\\d\n+\nL\n|\n[\nA\n-\nZ\n]\n{3}\nL\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n[\nA\n-\nZ\n]\n{3}\n\\-\n[\nA\n-\nZ\n]\n{3}\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n|\n(\n[\nA\n-\nZ\n]\n{3}\n(\n,\n[\nA\n-\nZ\n]\n{3}\n)\n*\n)\n)\n$\n/\ni\n,\n}\n;\nif\n(\nminute\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nminute\n,\n0\n,\n59\n)\n)\nthrow\nnew\nError\n(\n\"Invalid minute value\"\n)\n;\nif\n(\nhour\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nhour\n,\n0\n,\n23\n)\n)\nthrow\nnew\nError\n(\n\"Invalid hour value\"\n)\n;\nif\n(\ndayOfMonth\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\ndayOfMonth\n,\n1\n,\n31\n)\n)\nthrow\nnew\nError\n(\n\"Invalid day of month value\"\n)\n;\nif\n(\nmonth\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nmonth\n,\n1\n,\n12\n)\n)\nthrow\nnew\nError\n(\n\"Invalid month value\"\n)\n;\nif\n(\ndayOfWeek\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\ndayOfWeek\n,\n0\n,\n6\n)\n)\nthrow\nnew\nError\n(\n\"Invalid day of week value\"\n)\n;\nif\n(\n!\nregexPatterns\n.\nminute\n.\ntest\n(\nminute\n)\n||\n!\nregexPatterns\n.\nhour\n.\ntest\n(\nhour\n)\n||\n!\nregexPatterns\n.\ndayOfMonth\n.\ntest\n(\ndayOfMonth\n)\n||\n!\nregexPatterns\n.\nmonth\n.\ntest\n(\nmonth\n)\n||\n!\nregexPatterns\n.\ndayOfWeek\n.\ntest\n(\ndayOfWeek\n)\n)\nthrow\nnew\nError\n(\n\"Invalid cron expression\"\n)\n;\n}\n/**\n* Validates schedule-specific arguments\n* Ensures schedule type and value are compatible and valid\n*\n@throws\n{\nError\n}\nIf schedule configuration is invalid\n*/\nfunction\nvalidateProvidedArgumentsForSchedule\n(\n)\n{\nconst\nVALID_MINUTES\n=\nnew\nSet\n(\n[\n\"5\"\n,\n\"15\"\n,\n\"30\"\n,\n\"60\"\n,\n\"720\"\n,\n\"1440\"\n]\n)\n;\nif\n(\nSCHEDULE_TYPE\n===\n\"MINUTES\"\n&&\n!\nVALID_MINUTES\n.\nhas\n(\nSCHEDULE_VALUE\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid SCHEDULE_VALUE for MINUTES. Valid options are\n${\nArray\n.\nfrom\n(\nVALID_MINUTES\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nSCHEDULE_TYPE\n===\n\"CRON\"\n)\nvalidateCronExpression\n(\nSCHEDULE_VALUE\n)\n;\n// SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE\n}\n/**\n* Validates DMF arguments with dataType checks\n*\n@param\n{\nstring\n}\nrawDMFArguments\n- Raw JSON string of DMF arguments\n*\n@throws\n{\nError\n}\nIf DMF arguments structure is invalid or dataType is missing\n*/\nfunction\nvalidateDMFArgumentsWithDataType\n(\nrawDMFArguments\n)\n{\nconst\nparsedStructure\n=\nsafelyParseJSON\n(\nrawDMFArguments\n)\n;\nif\n(\n!\nparsedStructure\n)\nthrow\nnew\nError\n(\n\"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\"\n)\n;\nif\n(\n!\nArray\n.\nisArray\n(\nparsedStructure\n)\n)\nthrow\nnew\nError\n(\n\"DMF_ARGUMENTS_JSON must be an array\"\n)\n;\nconst\nvalidationFunctions\n=\n{\narrayItem\n:\n(\nparam\n)\n=>\n{\nif\n(\n!\n[\n\"COLUMN\"\n,\n\"TABLE\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n||\n!\nparam\n.\nname\n)\n{\nreturn\nfalse\n;\n}\nif\n(\nparam\n.\ntype\n===\n\"COLUMN\"\n&&\n!\nparam\n.\ndataType\n)\n{\nthrow\nnew\nError\n(\n`\nMissing dataType for COLUMN parameter:\n${\nparam\n.\nname\n}\n`\n)\n;\n}\nreturn\ntrue\n;\n}\n,\nnestedItem\n:\n(\nparam\n)\n=>\n{\nif\n(\n!\n[\n\"COLUMN\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n||\n!\nparam\n.\nname\n)\n{\nreturn\nfalse\n;\n}\nif\n(\n!\nparam\n.\ndataType\n)\n{\nthrow\nnew\nError\n(\n`\nMissing dataType for nested COLUMN parameter:\n${\nparam\n.\nname\n}\n`\n)\n;\n}\nreturn\ntrue\n;\n}\n}\n;\nif\n(\n!\nparsedStructure\n.\nevery\n(\nvalidationFunctions\n.\narrayItem\n)\n)\nthrow\nnew\nError\n(\n\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"\n)\n;\nconst\nreferencedEntities\n=\nparsedStructure\n.\nfilter\n(\n(\nparam\n)\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n;\nfor\n(\nconst\nreferencedEntity\nof\nreferencedEntities\n)\n{\nif\n(\n!\nArray\n.\nisArray\n(\nreferencedEntity\n.\nnested\n)\n||\n!\nreferencedEntity\n.\nnested\n.\nevery\n(\nvalidationFunctions\n.\nnestedItem\n)\n)\nthrow\nnew\nError\n(\n\"Invalid nested parameters\"\n)\n;\n}\n}\n/**\n* Validates DMF name format\n*\n@param\n{\nstring\n}\ndmfName\n- Fully qualified name of the DMF\n*\n@throws\n{\nError\n}\nIf DMF name format is invalid\n*/\nfunction\nvalidateDmfName\n(\ndmfName\n)\n{\nconst\nparts\n=\ndmfName\n.\nsplit\n(\n\".\"\n)\n.\nmap\n(\npart\n=>\npart\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n;\nif\n(\nparts\n.\nlength\n!==\n3\n)\n{\nthrow\nnew\nError\n(\n`\nInvalid DMF_NAME:\n${\ndmfName\n}\n. Expected format: database.schema.name\n`\n)\n;\n}\n}\n/**\n* Validates that the provided SQL is read-only and doesn't contain dangerous operations\n*\n@param\n{\nstring\n}\nsqlExpression\n- SQL to validate\n*\n@returns\n{\nboolean\n}\nWhether the SQL is safe\n*\n@throws\n{\nError\n}\nIf SQL contains potentially dangerous operations\n*/\nfunction\nvalidateSqlExpression\n(\nsqlExpression\n)\n{\nif\n(\n!\nsqlExpression\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a SQL query. The SQL expression cannot be empty.\"\n)\n;\n}\n// Step 1: Normalize Unicode characters to prevent encoding-based attacks\nconst\nnormalizedSql\n=\nsqlExpression\n.\nnormalize\n(\n'NFKC'\n)\n;\n// Step 2: Check for multiple statements (handled by splitIntoSqlStatements)\nsplitIntoSqlStatements\n(\nnormalizedSql\n)\n;\n// Step 3: Check whether it is a read-query or not\nif\n(\n!\nisReadQuery\n(\nnormalizedSql\n)\n)\n{\nthrow\nnew\nError\n(\n\"Your query must start with SELECT or WITH. Only read operations are allowed.\"\n)\n;\n}\n// Step 4: Check for suspicious patterns that might bypass filters\ncheckForSuspiciousPatterns\n(\nnormalizedSql\n)\n;\n// Step 5: Check for dangerous operations\nconst\ndangerousOperation\n=\ncontainsDangerousOperation\n(\nnormalizedSql\n)\n;\nif\n(\ndangerousOperation\n)\n{\nthrow\nnew\nError\n(\n\"For security reasons, this operation is not permitted. Please use only read operations in your query.\"\n)\n;\n}\nreturn\ntrue\n;\n}\n/**\n* Enhanced detection of suspicious SQL patterns\n*\n@param\n{\nstring\n}\nsql\n- SQL query to check\n*\n@throws\n{\nError\n}\nIf suspicious patterns are detected\n*/\nfunction\ncheckForSuspiciousPatterns\n(\nsql\n)\n{\n// Create a copy where string literals are masked to prevent false positives\nconst\nsqlWithoutStrings\n=\nsql\n.\nreplace\n(\n/\n'\n[\n^\n'\n]\n*\n'\n/\ng\n,\n\"'STRING_LITERAL'\"\n)\n.\nreplace\n(\n/\n\"\n[\n^\n\"\n]\n*\n\"\n/\ng\n,\n'\"STRING_LITERAL\"'\n)\n;\nconst\nsuspiciousPatterns\n=\n[\n// Common SQL injection techniques\n{\npattern\n:\n/\n\\b\nOR\n\\s\n+\n[\n0\n-\n9\n]\n+\n\\s\n*\n=\n\\s\n*\n[\n0\n-\n9\n]\n+\n\\b\n/\ni\n,\nmessage\n:\n\"Suspicious always-true condition detected\"\n}\n,\n// Alias abuse detection\n{\npattern\n:\n/\n\\b\nAS\n\\s\n+\n[\n'\"`\n]\n?\n.\n*?\n(\nDELETE\n|\nINSERT\n|\nUPDATE\n|\nDROP\n|\nALTER\n|\nEXEC\n)\n\\b\n/\ni\n,\nmessage\n:\n\"Suspicious alias detected\"\n}\n,\n// Hex encoding and other obfuscation techniques\n{\npattern\n:\n/\n0x\n[\n0\n-\n9\na\n-\nf\n]\n{10,}\n/\ni\n,\nmessage\n:\n\"Suspicious hex encoding detected\"\n}\n,\n{\npattern\n:\n/\nCHAR\n\\s\n*\n\\(\n\\s\n*\n\\d\n+\n(\n\\s\n*\n,\n\\s\n*\n\\d\n+\n)\n+\n\\s\n*\n\\)\n/\ni\n,\nmessage\n:\n\"Character code conversion functions are not allowed\"\n}\n,\n]\n;\n// Check for suspicious patterns outside of string literals\nfor\n(\nconst\n{\npattern\n,\nmessage\n}\nof\nsuspiciousPatterns\n)\n{\nif\n(\npattern\n.\ntest\n(\nsqlWithoutStrings\n)\n)\n{\nthrow\nnew\nError\n(\nmessage\n)\n;\n}\n}\n}\n/**\n* Splits SQL into separate statements based on semicolons not in quotes\n*\n@param\n{\nstring\n}\nsql\n- SQL query\n*\n@returns\n{\nstring\n}\n- SQL query without semicolons\n*/\nfunction\nsplitIntoSqlStatements\n(\nsql\n)\n{\nlet\ninSingleQuote\n=\nfalse\n;\nlet\ninDoubleQuote\n=\nfalse\n;\nfor\n(\nlet\ni\n=\n0\n;\ni\n<\nsql\n.\nlength\n;\ni\n++\n)\n{\nconst\nchar\n=\nsql\n[\ni\n]\n;\n// Handle quotes\nif\n(\nchar\n===\n\"'\"\n&&\nsql\n[\ni\n-\n1\n]\n!==\n'\\\\'\n)\n{\ninSingleQuote\n=\n!\ninSingleQuote\n;\n}\nelse\nif\n(\nchar\n===\n'\"'\n&&\nsql\n[\ni\n-\n1\n]\n!==\n'\\\\'\n)\n{\ninDoubleQuote\n=\n!\ninDoubleQuote\n;\n}\n// If semicolon outside of quotes, throw error\nif\n(\nchar\n===\n';'\n&&\n!\ninSingleQuote\n&&\n!\ninDoubleQuote\n)\n{\nthrow\nnew\nError\n(\n\"Do not use semicolons to break or end your SQL statement. Submit your query without any semicolons.\"\n)\n;\n}\n}\n// If we get here, there were no semicolons outside quotes\nreturn\nsql\n.\ntrim\n(\n)\n;\n}\n/**\n* Checks if the SQL is a read-only query\n*\n@param\n{\nstring\n}\nsql\n- SQL query without comments\n*\n@returns\n{\nboolean\n}\n- True if it's a read-only query\n*/\nfunction\nisReadQuery\n(\nsql\n)\n{\nconst\nnormalizedSql\n=\nsql\n.\nreplace\n(\n/\n\\s\n+\n/\ng\n,\n' '\n)\n.\ntoUpperCase\n(\n)\n.\ntrim\n(\n)\n;\nif\n(\nnormalizedSql\n.\nstartsWith\n(\n'SELECT '\n)\n)\n{\nreturn\ntrue\n;\n}\nif\n(\nnormalizedSql\n.\nstartsWith\n(\n'WITH '\n)\n)\n{\nreturn\ntrue\n;\n}\nreturn\nfalse\n;\n}\n/**\n* Checks if SQL contains any dangerous operations - using single keywords with word boundaries\n*\n@param\n{\nstring\n}\nsql\n- SQL query without comments\n*\n@returns\n{\nstring\n|\nnull\n}\n- The dangerous operation found or null if safe\n*/\nfunction\ncontainsDangerousOperation\n(\nsql\n)\n{\n// Normalize whitespace and convert to uppercase for comparison\nconst\nnormalizedSql\n=\nsql\n.\nreplace\n(\n/\n\\s\n+\n/\ng\n,\n' '\n)\n.\ntoUpperCase\n(\n)\n;\n// Snowflake-specific dangerous commands - using single keywords with high precision\nconst\ndangerousCommands\n=\n[\n// Data Modification\n'INSERT'\n,\n'UPDATE'\n,\n'DELETE'\n,\n'MERGE'\n,\n'TRUNCATE'\n,\n'COPY'\n,\n// DDL statements\n'CREATE'\n,\n'DROP'\n,\n'ALTER'\n,\n'COMMENT'\n,\n'GRANT'\n,\n'REVOKE'\n,\n'UNDROP'\n,\n// Transaction control\n'BEGIN'\n,\n'COMMIT'\n,\n'ROLLBACK'\n,\n// System & session commands\n'SET'\n,\n'UNSET'\n,\n'USE'\n,\n'PUT'\n,\n'GET'\n,\n'REMOVE'\n,\n'LIST'\n,\n// Information Schema & Metadata\n'SHOW'\n,\n'DESCRIBE'\n,\n// Procedures and functions\n'CALL'\n,\n'EXECUTE'\n,\n'EXEC'\n,\n// Additional Snowflake operations\n'EXPLAIN'\n]\n;\n// Dangerous functions specific to Snowflake\nconst\ndangerousFunctions\n=\n[\n'SYSTEM'\n,\n'CURRENT_USER'\n,\n'CURRENT_ROLE'\n,\n'CURRENT_ACCOUNT'\n,\n'DATABASE'\n,\n'VERSION'\n,\n'SLEEP'\n,\n'CALL_INTEGRATION'\n,\n'PARSE_JSON'\n,\n'RUN_JAVASCRIPT'\n,\n'CALL_JAVASCRIPT'\n,\n'TO_JAVASCRIPT'\n]\n;\n// Create a regex pattern with word boundaries for all dangerous commands\nconst\ncommandPattern\n=\nnew\nRegExp\n(\n`\n\\\\b(\n${\ndangerousCommands\n.\njoin\n(\n'|'\n)\n}\n)\\\\b\n`\n,\n'i'\n)\n;\nconst\nfunctionPattern\n=\nnew\nRegExp\n(\n`\n\\\\b(\n${\ndangerousFunctions\n.\njoin\n(\n'|'\n)\n}\n)\\\\s*\\\\(\n`\n,\n'i'\n)\n;\n// Check for dangerous commands\nconst\ncommandMatch\n=\nnormalizedSql\n.\nmatch\n(\ncommandPattern\n)\n;\nif\n(\ncommandMatch\n)\n{\nreturn\n`\nDangerous operation detected:\n${\ncommandMatch\n[\n0\n]\n}\n`\n;\n}\n// Check for dangerous functions\nconst\nfunctionMatch\n=\nnormalizedSql\n.\nmatch\n(\nfunctionPattern\n)\n;\nif\n(\nfunctionMatch\n)\n{\nreturn\n`\nDangerous function call detected:\n${\nfunctionMatch\n[\n1\n]\n}\n`\n;\n}\n// Check for access to sensitive metadata\nif\n(\n/\n\\b\nINFORMATION_SCHEMA\n\\b\n|\n\\b\nACCOUNT_USAGE\n\\b\n/\ni\n.\ntest\n(\nnormalizedSql\n)\n)\n{\nreturn\n'Access to sensitive system metadata detected'\n;\n}\nreturn\nnull\n;\n}\n/**\n* Executes SQL and returns a numeric result\n*\n@param\n{\nstring\n}\nsql\n- SQL to execute\n*\n@returns\n{\nnumber\n}\nNumeric result\n*\n@throws\n{\nError\n}\nIf execution fails or result is not numeric\n*/\nfunction\nexecuteSqlAndReturnNumber\n(\nsql\n)\n{\ntry\n{\n// Execute without returnFirstRow to get full result set\nconst\nresult\n=\nexecuteQuery\n(\nsql\n,\n[\n]\n,\nfalse\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\n// Check if the result set exists\nif\n(\n!\nresult\n.\nresult\n)\n{\nthrow\nnew\nError\n(\n\"Your query didn't return any results. Please check your SQL and try again.\"\n)\n;\n}\n// Check number of columns\nconst\ncolumnCount\n=\nresult\n.\nresult\n.\ngetColumnCount\n(\n)\n;\nif\n(\ncolumnCount\n!==\n1\n)\n{\nthrow\nnew\nError\n(\n\"Your query should return exactly one column. Please modify your query to return a single numeric value.\"\n)\n;\n}\n// Check if we have exactly one row\nif\n(\n!\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nthrow\nnew\nError\n(\n\"Your query didn't return any rows. Please check your query and try again.\"\n)\n;\n}\n// Get the value\nconst\nvalue\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n1\n)\n;\n// Check if it's a number\nif\n(\ntypeof\nvalue\n!==\n'number'\n)\n{\nthrow\nnew\nError\n(\n\"Your query must return a number. Please modify your query to calculate a numeric result.\"\n)\n;\n}\n// Check if there are more rows\nif\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nthrow\nnew\nError\n(\n\"Your query returned multiple rows. Please modify your query to return a single result.\"\n)\n;\n}\nreturn\nvalue\n;\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\n${\nerr\n.\nmessage\n}\n`\n)\n;\n}\n}\n/**\n* Validates all parameters for DMF creation\n*\n@throws\n{\nError\n}\nIf any validation fails\n*/\nfunction\nvalidateCreateDmf\n(\n)\n{\nvalidateDmfName\n(\nDMF_NAME\n)\n;\nvalidateSqlExpression\n(\nDMF_DEFINITION\n)\n;\nvalidateDMFArgumentsWithDataType\n(\nDMF_ARGUMENTS_JSON\n)\n;\n}\n/**\n* Validates all provided arguments\n* Performs comprehensive validation on input parameters\n*\n@throws\n{\nError\n}\nIf any validation fails\n*/\nfunction\nvalidateAllArguments\n(\n)\n{\nvalidateMandatoryArguments\n(\n)\n;\n// Validates all mandatory arguments are provided in the correct format\nif\n(\nACTION\n===\n\"CREATE_DMF\"\n)\n{\nvalidateCreateDmf\n(\n)\n;\nreturn\n;\n}\nelse\nif\n(\nACTION\n===\n\"EXECUTE_SQL\"\n)\n{\nvalidateSqlExpression\n(\nDMF_DEFINITION\n)\n;\nreturn\n;\n}\nelse\nif\n(\nACTION\n===\n\"VALIDATE_SQL_PERMISSIONS\"\n)\n{\nvalidateSqlExpression\n(\nDMF_DEFINITION\n)\n;\nreturn\n;\n}\nelse\nif\n(\nACTION\n===\n\"UPDATE_SCHEDULE\"\n)\n{\nvalidateProvidedArgumentsForSchedule\n(\n)\n;\n// Validates the provided schedule type and value\n}\nelse\n{\nvalidateDMFArgumentsStructure\n(\nDMF_ARGUMENTS_JSON\n)\n;\n}\nvalidateProvidedIdentifiers\n(\nENTITY_NAME\n,\nDMF_NAME\n,\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n)\n;\n// All provided arguments are valid and legal\n}\n// -----------------------------------------------------MAIN FUNCTION-----------------------------------------------------\n/**\n* Extracts database, schema and table name from fully qualified entity name\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified entity name\n*\n@returns\n{\nObject\n}\nObject containing database, schema and table name\n*/\nfunction\nparseEntityName\n(\nentityName\n)\n{\nconst\n[\ndb\n,\nschema\n,\ntable\n]\n=\nentityName\n.\nsplit\n(\n\".\"\n)\n;\nreturn\n{\ndb\n,\nschema\n,\ntable\n}\n;\n}\n/**\n* Gets the owner of a table from information schema\n*\n@param\n{\nstring\n}\ndb\n- Database name\n*\n@param\n{\nstring\n}\nschema\n- Schema name\n*\n@param\n{\nstring\n}\ntable\n- Table name\n*\n@returns\n{\nObject\n}\nObject containing success status and table owner\n*/\nfunction\ngetTableOwner\n(\ndb\n,\nschema\n,\ntable\n)\n{\nconst\nquery\n=\n`\nSELECT TABLE_OWNER\nFROM\n${\ndb\n}\n.INFORMATION_SCHEMA.TABLES\nWHERE TABLE_CATALOG = ?\nAND TABLE_SCHEMA = ?\nAND TABLE_NAME = ?\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\ndb\n,\nschema\n,\ntable\n]\n,\ntrue\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nFailed to get table owner:\n${\nresult\n.\nmessage\n}\n`\n,\nowner\n:\nnull\n}\n;\n}\nconst\nowner\n=\nresult\n.\nresult\n?.\ngetColumnValue\n(\n\"TABLE_OWNER\"\n)\n;\nif\n(\n!\nowner\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nCould not find owner for table\n${\ndb\n}\n.\n${\nschema\n}\n.\n${\ntable\n}\n`\n,\nowner\n:\nnull\n}\n;\n}\nreturn\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n\"Successfully retrieved table owner\"\n,\nowner\n}\n;\n}\n/**\n* Grants required permissions to a role\n*\n@param\n{\nstring\n}\nrole\n- Role to grant permissions to\n*\n@returns\n{\nObject\n}\nObject containing success status and message\n*/\nfunction\ngrantPermissions\n(\nrole\n)\n{\nconst\nquery\n=\n`\nBEGIN\nGRANT USAGE ON SCHEMA ATLAN_DQ.DMFS TO ROLE \"\n${\nrole\n}\n\";\nGRANT USAGE ON DATABASE ATLAN_DQ TO ROLE \"\n${\nrole\n}\n\";\nGRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ.DMFS TO ROLE \"\n${\nrole\n}\n\";\nEND;\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nFailed to grant permissions:\n${\nresult\n.\nmessage\n}\n`\n}\n;\n}\nreturn\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n`\nSuccessfully granted permissions to role\n${\nrole\n}\n`\n}\n;\n}\n/**\n* Handles permissions for DMF operations\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified entity name\n*\n@returns\n{\nObject\n}\nObject containing success status and message\n*/\nfunction\nhandleDMFPermissions\n(\nentityName\n)\n{\ntry\n{\n// Parse entity name\nconst\n{\ndb\n,\nschema\n,\ntable\n}\n=\nparseEntityName\n(\nentityName\n)\n;\n// Get table owner\nconst\nownerResult\n=\ngetTableOwner\n(\ndb\n,\nschema\n,\ntable\n)\n;\nif\n(\n!\nownerResult\n.\nisSuccessful\n)\n{\nreturn\nownerResult\n;\n}\n// Grant permissions\nreturn\ngrantPermissions\n(\nownerResult\n.\nowner\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nError handling permissions:\n${\nerr\n.\nmessage\n}\n`\n}\n;\n}\n}\n/**\n* Parses comma-separated object lists into arrays\n*\n@param\n{\nstring\n}\ndatabasesToCheck\n- Comma-separated list of databases\n*\n@param\n{\nstring\n}\nschemasToCheck\n- Comma-separated list of schemas\n*\n@param\n{\nstring\n}\ntablesToCheck\n- Comma-separated list of tables\n*\n@returns\n{\nObject\n}\nObject with parsed arrays\n*/\nfunction\nparseCommaSeparatedLists\n(\ndatabasesToCheck\n,\nschemasToCheck\n,\ntablesToCheck\n)\n{\nreturn\n{\ndatabases\n:\ndatabasesToCheck\n?\ndatabasesToCheck\n.\nsplit\n(\n','\n)\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n:\n[\n]\n,\nschemas\n:\nschemasToCheck\n?\nschemasToCheck\n.\nsplit\n(\n','\n)\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n:\n[\n]\n,\ntables\n:\ntablesToCheck\n?\ntablesToCheck\n.\nsplit\n(\n','\n)\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n:\n[\n]\n}\n;\n}\n/**\n* Checks database access for a role using information schema\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nArray\n}\ndatabases\n- Array of databases to check\n*\n@returns\n{\nArray\n}\nArray of accessible databases\n*/\nfunction\ncheckDatabaseAccess\n(\nroleToCheck\n,\ndatabases\n)\n{\nconst\naccessibleDatabases\n=\n[\n]\n;\nfor\n(\nconst\ndatabase\nof\ndatabases\n)\n{\ntry\n{\nconst\nquery\n=\n`\nSELECT PRIVILEGE_TYPE\nFROM\n${\ndatabase\n}\n.INFORMATION_SCHEMA.OBJECT_PRIVILEGES\nWHERE GRANTEE = '\n${\nroleToCheck\n}\n'\nAND OBJECT_TYPE = 'DATABASE'\nAND OBJECT_NAME = '\n${\ndatabase\n}\n'\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\nwhile\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nconst\nprivilege\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n\"PRIVILEGE_TYPE\"\n)\n;\nif\n(\nprivilege\n===\n\"USAGE\"\n||\nprivilege\n===\n\"OWNERSHIP\"\n)\n{\naccessibleDatabases\n.\npush\n(\ndatabase\n)\n;\nbreak\n;\n}\n}\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\nFailed to check permissions for database '\n${\ndatabase\n}\n'. Role '\n${\nroleToCheck\n}\n' may not have access or the database may not exist.\n`\n)\n;\n}\n}\nreturn\naccessibleDatabases\n;\n}\n/**\n* Checks schema access for a role using information schema\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nArray\n}\nschemas\n- Array of schemas to check (format: database.schema)\n*\n@returns\n{\nArray\n}\nArray of accessible schemas\n*/\nfunction\ncheckSchemaAccess\n(\nroleToCheck\n,\nschemas\n)\n{\nconst\naccessibleSchemas\n=\n[\n]\n;\nfor\n(\nconst\nschema\nof\nschemas\n)\n{\ntry\n{\nconst\nparts\n=\nschema\n.\nsplit\n(\n'.'\n)\n;\nif\n(\nparts\n.\nlength\n!==\n2\n)\n{\nthrow\nnew\nError\n(\n`\nInvalid schema format: '\n${\nschema\n}\n'. Expected format: database.schema\n`\n)\n;\n}\nconst\n[\ndatabase\n,\nschemaName\n]\n=\nparts\n;\nconst\nquery\n=\n`\nSELECT PRIVILEGE_TYPE\nFROM\n${\ndatabase\n}\n.INFORMATION_SCHEMA.OBJECT_PRIVILEGES\nWHERE GRANTEE = '\n${\nroleToCheck\n}\n'\nAND OBJECT_TYPE = 'SCHEMA'\nAND OBJECT_CATALOG = '\n${\ndatabase\n}\n'\nAND OBJECT_NAME = '\n${\nschemaName\n}\n'\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\nwhile\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nconst\nprivilege\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n\"PRIVILEGE_TYPE\"\n)\n;\nif\n(\nprivilege\n===\n\"USAGE\"\n||\nprivilege\n===\n\"OWNERSHIP\"\n)\n{\naccessibleSchemas\n.\npush\n(\nschema\n)\n;\nbreak\n;\n}\n}\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\nFailed to check permissions for schema '\n${\nschema\n}\n'. Role '\n${\nroleToCheck\n}\n' may not have access or the schema may not exist.\n`\n)\n;\n}\n}\nreturn\naccessibleSchemas\n;\n}\n/**\n* Checks table access for a role using information schema\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nArray\n}\ntables\n- Array of tables to check (format: database.schema.table)\n*\n@returns\n{\nArray\n}\nArray of accessible tables\n*/\nfunction\ncheckTableAccess\n(\nroleToCheck\n,\ntables\n)\n{\nconst\naccessibleTables\n=\n[\n]\n;\nfor\n(\nconst\ntable\nof\ntables\n)\n{\ntry\n{\nconst\nparts\n=\ntable\n.\nsplit\n(\n'.'\n)\n;\nif\n(\nparts\n.\nlength\n!==\n3\n)\n{\nthrow\nnew\nError\n(\n`\nInvalid table format: '\n${\ntable\n}\n'. Expected format: database.schema.table\n`\n)\n;\n}\nconst\n[\ndatabase\n,\nschema\n,\ntableName\n]\n=\nparts\n;\nconst\nquery\n=\n`\nSELECT PRIVILEGE_TYPE\nFROM\n${\ndatabase\n}\n.INFORMATION_SCHEMA.OBJECT_PRIVILEGES\nWHERE GRANTEE = '\n${\nroleToCheck\n}\n'\nAND OBJECT_TYPE IN ('TABLE', 'VIEW')\nAND OBJECT_CATALOG = '\n${\ndatabase\n}\n'\nAND OBJECT_SCHEMA = '\n${\nschema\n}\n'\nAND OBJECT_NAME = '\n${\ntableName\n}\n'\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\nwhile\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nconst\nprivilege\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n\"PRIVILEGE_TYPE\"\n)\n;\nif\n(\nprivilege\n===\n\"SELECT\"\n||\nprivilege\n===\n\"OWNERSHIP\"\n)\n{\naccessibleTables\n.\npush\n(\ntable\n)\n;\nbreak\n;\n}\n}\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\nFailed to check permissions for table '\n${\ntable\n}\n'. Role '\n${\nroleToCheck\n}\n' may not have access or the table may not exist.\n`\n)\n;\n}\n}\nreturn\naccessibleTables\n;\n}\n/**\n* Validates SQL permissions for a given role and returns accessible objects\n*\n@param\n{\nstring\n}\nsql\n- SQL to validate\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nstring\n}\ndatabasesToCheck\n- Comma-separated list of databases to check access for\n*\n@param\n{\nstring\n}\nschemasToCheck\n- Comma-separated list of schemas to check access for\n*\n@param\n{\nstring\n}\ntablesToCheck\n- Comma-separated list of tables to check access for\n*\n@returns\n{\nObject\n}\nObject with validation result and accessible objects\n*\n@throws\n{\nError\n}\nIf SQL validation fails\n*/\nfunction\nvalidateSqlPermissions\n(\nsql\n,\nroleToCheck\n,\ndatabasesToCheck\n,\nschemasToCheck\n,\ntablesToCheck\n)\n{\ntry\n{\n// Step 1: Run EXPLAIN command to validate SQL syntax and plan\nconst\nexplainSql\n=\n`\nEXPLAIN\n${\nsql\n}\n`\n;\nconst\nexplainResult\n=\nexecuteQuery\n(\nexplainSql\n,\n[\n]\n)\n;\nif\n(\nexplainResult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\n\"We couldn't validate your query. Please check your SQL syntax and permissions and try again.\"\n)\n;\n}\n// Step 2: Parse objects to check\nconst\nobjectsToCheck\n=\nparseCommaSeparatedLists\n(\ndatabasesToCheck\n,\nschemasToCheck\n,\ntablesToCheck\n)\n;\n// Step 3: Check access for each object type\nconst\naccessibleDatabases\n=\ncheckDatabaseAccess\n(\nroleToCheck\n,\nobjectsToCheck\n.\ndatabases\n)\n;\nconst\naccessibleSchemas\n=\ncheckSchemaAccess\n(\nroleToCheck\n,\nobjectsToCheck\n.\nschemas\n)\n;\nconst\naccessibleTables\n=\ncheckTableAccess\n(\nroleToCheck\n,\nobjectsToCheck\n.\ntables\n)\n;\nreturn\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n\"SQL permissions validation successful\"\n,\naccessibleObjects\n:\n{\ndatabases\n:\naccessibleDatabases\n,\nschemas\n:\naccessibleSchemas\n,\ntables\n:\naccessibleTables\n}\n}\n;\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\n${\nerr\n.\nmessage\n}\n`\n)\n;\n}\n}\n/**\n* Main function to manage DMF operations\n* Validates all arguments and executes the main logic\n*\n@returns\n{\nstring\n}\nJSON string with operation status and result message\n*\n@throws\n{\nError\n}\nIf any operation step fails\n*/\nfunction\nmain\n(\n)\n{\nvalidateAllArguments\n(\n)\n;\n// Handle permissions for DMF attachment/detachment operations\nif\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n]\n.\nincludes\n(\nACTION\n)\n)\n{\nconst\npermissionResult\n=\nhandleDMFPermissions\n(\nENTITY_NAME\n)\n;\nif\n(\n!\npermissionResult\n.\nisSuccessful\n)\n{\nreturn\nJSON\n.\nstringify\n(\npermissionResult\n)\n;\n}\n}\n// If the provided arguments are valid, proceed with the main logic\nconst\ndmfArguments\n=\ngenerateDMFColumnArguments\n(\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n)\n;\nconst\nSQL_TEMPLATES\n=\n{\nATTACH_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nADD DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n)\n`\n,\nDETACH_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nDROP DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n)\n`\n,\nSUSPEND_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nMODIFY DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n) SUSPEND\n`\n,\nRESUME_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nMODIFY DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n) RESUME\n`\n,\nUPDATE_SCHEDULE\n:\n{\nMINUTES\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nSET DATA_METRIC_SCHEDULE = '\n${\nSCHEDULE_VALUE\n}\nMINUTE'\n`\n,\nCRON\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nSET DATA_METRIC_SCHEDULE = 'USING CRON\n${\nSCHEDULE_VALUE\n}\n'\n`\n,\nON_DATA_CHANGE\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nSET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES'\n`\n,\nNOT_SCHEDULED\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nUNSET DATA_METRIC_SCHEDULE\n`\n,\n}\n,\n}\n;\nlet\nsqlText\n=\n\"\"\n;\nlet\nreturnMessage\n=\n\"\"\n;\nlet\nbinds\n=\n[\n]\n;\nif\n(\nACTION\n===\n\"UPDATE_SCHEDULE\"\n)\n{\nsqlText\n=\nSQL_TEMPLATES\n[\nACTION\n]\n[\nSCHEDULE_TYPE\n]\n;\nreturnMessage\n=\n`\nSuccessfully updated schedule for\n${\nENTITY_NAME\n}\nto\n${\nSCHEDULE_TYPE\n}\n${\nSCHEDULE_VALUE\n}\n`\n;\n}\nelse\nif\n(\nACTION\n===\n\"CREATE_DMF\"\n)\n{\nconst\nDOLLAR\n=\nString\n.\nfromCharCode\n(\n36\n)\n;\n// ASCII code for $\nconst\ndmfArguments\n=\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n;\nconst\nfunctionParams\n=\ngenerateFunctionParameters\n(\ndmfArguments\n)\n;\nsqlText\n=\n\"CREATE OR REPLACE DATA METRIC FUNCTION \"\n+\nDMF_NAME\n+\n\" (\"\n+\nfunctionParams\n+\n\" )\"\n+\n\"RETURNS NUMBER AS \"\n+\nDOLLAR\n+\nDOLLAR\n+\n\" \"\n+\nDMF_DEFINITION\n+\n\" \"\n+\nDOLLAR\n+\nDOLLAR\n;\nreturnMessage\n=\n`\nDMF\n${\nDMF_NAME\n}\nregistered successfully\n`\n;\n}\nelse\nif\n(\nACTION\n===\n\"EXECUTE_SQL\"\n)\n{\n// Execute SQL and get numeric result\nconst\nresult\n=\nexecuteSqlAndReturnNumber\n(\nDMF_DEFINITION\n)\n;\nconst\nresponse\n=\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n\"SQL executed successfully\"\n,\nresult\n:\nresult\n}\n;\nreturn\nJSON\n.\nstringify\n(\nresponse\n)\n;\n}\nelse\nif\n(\nACTION\n===\n\"VALIDATE_SQL_PERMISSIONS\"\n)\n{\nconst\nvalidationResult\n=\nvalidateSqlPermissions\n(\nDMF_DEFINITION\n,\nROLE_TO_CHECK\n,\nDATABASES_TO_CHECK\n,\nSCHEMAS_TO_CHECK\n,\nTABLES_TO_CHECK\n)\n;\nreturn\nJSON\n.\nstringify\n(\nvalidationResult\n)\n;\n}\nelse\n{\nsqlText\n=\nSQL_TEMPLATES\n[\nACTION\n]\n;\nreturnMessage\n=\n`\nACTION:\n${\nACTION\n}\nperformed successfully on\n${\nENTITY_NAME\n}\nwith DMF:\n${\nDMF_NAME\n}\nand DMF Arguments:\n${\ndmfArguments\n}\n`\n;\n}\nconst\nresult\n=\nexecuteQuery\n(\nsqlText\n,\nbinds\n)\n;\nreturn\nJSON\n.\nstringify\n(\n{\nisSuccessful\n:\n!\nresult\n.\nisErrored\n,\nmessage\n:\nresult\n.\nisErrored\n?\nresult\n.\nmessage\n:\nreturnMessage\n,\n}\n)\n;\n}\n// Execute the main function and return the result\ntry\n{\nreturn\nmain\n(\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\nJSON\n.\nstringify\n(\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\nerr\n.\nmessage\n,\n}\n)\n;\n}\n$$\n;\nTransfer ownership to\ndq_admin\nrole:\nGRANT\nOWNERSHIP\nON\nDATABASE\nATLAN_DQ\nTO\nROLE dq_admin\n;\nGRANT\nOWNERSHIP\nON\nSCHEMA\nATLAN_DQ\n.\nSHARED\nTO\nROLE dq_admin\n;\nGRANT\nOWNERSHIP\nON\nSCHEMA\nATLAN_DQ\n.\nDMFS\nTO\nROLE dq_admin\n;\nGRANT\nOWNERSHIP\nON\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n)\nTO\nROLE dq_admin\n;\nGrant privileges\nâ\nGrant the necessary permissions to enable data quality operations and maintain proper access control.\nSystem privileges: Grant Snowflake system-level permissions to enable data metric functions and monitoring capabilities.\n-- For DQ Admin Role\nGRANT\nDATABASE\nROLE SNOWFLAKE\n.\nDATA_METRIC_USER\nTO\nROLE dq_admin\n;\n-- For Atlan Service Role\nGRANT\nAPPLICATION ROLE SNOWFLAKE\n.\nDATA_QUALITY_MONITORING_VIEWER\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nDATABASE\nROLE SNOWFLAKE\n.\nDATA_METRIC_USER\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nEXECUTE\nTASK\nON\nACCOUNT\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nEXECUTE\nMANAGED TASK\nON\nACCOUNT\nTO\nROLE atlan_dq_service_role\n;\nTable owner privileges: For every role that owns tables in your environment (denoted by\n<table_owner>\n), grant the following privileges:\nGRANT\nROLE\n<\ntable_owner\n>\nTO\nROLE dq_admin\n;\nGRANT\nDATABASE\nROLE SNOWFLAKE\n.\nDATA_METRIC_USER\nTO\nROLE\n<\ntable_owner\n>\n;\nGRANT\nEXECUTE\nDATA\nMETRIC\nFUNCTION\nON\nACCOUNT\nTO\nROLE\n<\ntable_owner\n>\n;\nTo identify table owner roles in your environment:\n-- Find table owners\nSELECT\nTABLE_CATALOG\n,\nTABLE_OWNER\nFROM\nSNOWFLAKE\n.\nACCOUNT_USAGE\n.\nTABLES\nWHERE\nDELETED\nIS\nNULL\nAND\nTABLE_OWNER\nIS\nNOT\nNULL\nGROUP\nBY\nTABLE_CATALOG\n,\nTABLE_OWNER\n;\nDatabase access: Grant Atlan's service role access to the created objects:\nGRANT\nUSAGE\nON\nDATABASE\nATLAN_DQ\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\nATLAN_DQ\n.\nSHARED\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\nATLAN_DQ\n.\nDMFS\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n)\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nCREATE\nSCHEMA\nON\nDATABASE\nATLAN_DQ\nTO\nROLE atlan_dq_service_role\n;\nNext steps\nâ\nEnable data quality on connection\n- Configure your Snowflake connection for data quality monitoring\nNeed help\nâ\nIf you have questions or need assistance with setting up Snowflake for data quality, reach out to Atlan Support by\nsubmitting a support request\n.\nSee also\nâ\nData quality permissions\n- Understand the required permissions and roles for data quality operations\nConfigure alerts for data quality rules\n- Set up real-time notifications for rule failures\nTags:\nsnowflake\ndata-quality\nsetup\ngovernance\nPrevious\nSnowflake Data Quality Studio\nNext\nEnable data quality on connection\nSystem requirements\nPrerequisites\nCreate roles\nCreate user\nSet up database objects\nGrant privileges\nNext steps\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/faq/setup-and-configuration",
    "content": "Build governance\nData Quality Studio\nDatabricks Data Quality\nFAQ\nSetup and configuration\nOn this page\nSetup and configuration\nThis document answers common questions about prerequisites, permissions, and environment settings required to run Atlanâs Data Quality Studio on Databricks.\nWhat Databricks edition is required for data quality?\nâ\nAtlan DQ support for Databricks is supported only on Premium and Enterprise tiers of Databricks.\nWhat administrative access is required?\nâ\nThe user performing the setup must be:\nA Workspace admin; and\nA Metastore Admin or have\nCREATE CATALOG\nprivilege on the metastore linked to the workspace\nIs serverless compute required?\nâ\nYes, your workspace must have the following feature enabled:\nServerless Compute for Jobs & Notebooks\nThis is required to permit execution of Atlan's DQ jobs in your Databricks Workspace using Serverless compute.\nWhat SQL warehouse is recommended?\nâ\nA dedicated SQL warehouse must be identified for running DQ-related queries. While Atlan supports any SQL Warehouse, Atlan recommends using a Serverless SQL Warehouse for faster startup times.\nIs network access configuration required?\nâ\nOutbound Network Access Must Be Allowed from Serverless Compute: Databricks Serverless Compute uses network policies to control outbound traffic [only for Enterprise tier]. Verify that outbound connectivity to Atlan is permitted from the Serverless environment.\nWhat Atlan prerequisites are needed?\nâ\nBefore integrating with Databricks, you need to generate an API token in Atlan. This token is securely stored in Databricks in a secret and used to authenticate API requests from within Databricks.\nCan I enable data quality on multiple connections?\nâ\nCurrently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection,\nraise a support request\n.\nHow long does the setup take?\nâ\nAfter completing the setup steps, Atlan takes approximately 10 minutes to complete the setup in the background. Once finished, you'll see data quality options available on your Databricks assets.\nCan I use private channels for alerts?\nâ\nOnly public channels are supported for data quality alerts. Alerts can't be routed to private channels or Direct Messages at this time.\nTags:\ndatabricks\ndata-quality\nfaq\ntroubleshooting\nPrevious\nEnable data quality on connection\nWhat Databricks edition is required for data quality?\nWhat administrative access is required?\nIs serverless compute required?\nWhat SQL warehouse is recommended?\nIs network access configuration required?\nWhat Atlan prerequisites are needed?\nCan I enable data quality on multiple connections?\nHow long does the setup take?\nCan I use private channels for alerts?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda",
    "content": "Connect data\nData Quality & Observability\nSoda\nOn this page\nSoda\nOverview:\nCatalog Soda checks and test results in Atlan. Gain visibility into data quality metrics, alerts, and governance for your data assets.\nGet started\nâ\nFollow these steps to connect and catalog Soda assets in Atlan:\nSet up the connector\nCrawl Soda assets\nReferences\nâ\nWhat does Atlan crawl from Soda\n: Learn about the Soda assets and metadata that Atlan discovers and catalogs.\nTags:\nsoda\nconnector\nobservability\ndata quality\nconnectivity\nNext\nSet up Soda\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/migrate-snowflake",
    "content": "Build governance\nData Quality Studio\nSnowflake Data Quality\nUpgrade setup\nUpgrade to Snowflake data quality studio\nOn this page\nUpgrade to Snowflake data quality studio\nPrivate Preview\nUpgrade your existing Snowflake data quality setup to the latest version to access new features and improvements. This guide helps you migrate from an older version of the Snowflake data quality integration to the latest version.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nExisting Snowflake data quality setup configured\nPermissions required\nâ\nYou need the following Snowflake roles:\ndq_admin\nrole access\natlan_dq_service_role\nrole access\nUpgrade data quality setup\nâ\nFollow these steps to upgrade your existing Snowflake data quality setup to the latest version.\nSwitch to\ndq_admin\nrole:\nUSE\nROLE dq_admin\n;\nDrop the existing procedure:\nIf you have\nalready set up custom SQL\npreviously, use the following command:\nDROP\nPROCEDURE\nIF\nEXISTS\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n)\n;\nIf you\nhaven't set up custom SQL\n, use this command:\nDROP\nPROCEDURE\nIF\nEXISTS\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n)\n;\ninfo\nIf youâre not sure which command to use, go to the\nATLAN_DQ.SHARED\nschema in your Snowflake environment and check the procedure signature for\nMANAGE_DMF\nto confirm which version youâre using.\n:::\nCreate the required schema:\nCREATE\nSCHEMA\nIF\nNOT\nEXISTS\nATLAN_DQ\n.\nDMFS\n;\nCreate the updated procedure:\nView procedure code\n/**\n* Manages Data Metric Functions (DMF) operations for Snowflake tabular entities.\n* This procedure handles various DMF operations including:\n* - Creating and managing DMFs (CREATE_DMF)\n* - Attaching/detaching DMFs to entities (ATTACH_DMF, DETACH_DMF)\n* - Managing DMF schedules (UPDATE_SCHEDULE)\n* - Executing SQL expressions (EXECUTE_SQL)\n* - Validating SQL permissions (VALIDATE_SQL_PERMISSIONS)\n*\n* The procedure runs with the privileges of the procedure owner and includes comprehensive\n* validation of all inputs and permissions before executing any operations.\n*\n*\n@param\n{\nstring\n}\nACTION\n- Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE, CREATE_DMF, EXECUTE_SQL, VALIDATE_SQL_PERMISSIONS)\n*\n@param\n{\nstring\n}\nENTITY_TYPE\n- Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE)\n*\n@param\n{\nstring\n}\nENTITY_NAME\n- Fully qualified name of the entity (database.schema.name)\n*\n@param\n{\nstring\n}\n[\nDMF_NAME\n=\nnull\n]\n- Fully qualified name of the DMF (database.schema.name)\n*\n@param\n{\nstring\n}\n[DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations\n*\n@param\n{\nstring\n}\n[\nSCHEDULE_TYPE\n=\nnull\n]\n- Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED)\n*\n@param\n{\nstring\n}\n[\nSCHEDULE_VALUE\n=\nnull\n]\n- Schedule value based on type\n*\n@param\n{\nstring\n}\n[\nDMF_DEFINITION\n=\nnull\n]\n- SQL expression defining the DMF\n*\n@param\n{\nstring\n}\n[\nROLE_TO_CHECK\n=\nnull\n]\n- Role to check permissions for\n*\n@param\n{\nstring\n}\n[\nDATABASES_TO_CHECK\n=\nnull\n]\n- Comma-separated list of databases to validate permissions for\n*\n@param\n{\nstring\n}\n[\nSCHEMAS_TO_CHECK\n=\nnull\n]\n- Comma-separated list of schemas to validate permissions for\n*\n@param\n{\nstring\n}\n[\nTABLES_TO_CHECK\n=\nnull\n]\n- Comma-separated list of tables to validate permissions for\n*\n@returns\n{\nstring\n}\n- JSON string with operation status and result message\n*/\nCREATE\nOR\nREPLACE\nSECURE\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nACTION\nSTRING\n,\nENTITY_TYPE\nSTRING\nDEFAULT\nNULL\n,\nENTITY_NAME\nSTRING\nDEFAULT\nNULL\n,\nDMF_NAME\nSTRING\nDEFAULT\nNULL\n,\nDMF_ARGUMENTS_JSON\nSTRING\nDEFAULT\n'[]'\n,\nSCHEDULE_TYPE\nSTRING\nDEFAULT\nNULL\n,\nSCHEDULE_VALUE\nSTRING\nDEFAULT\nNULL\n,\nDMF_DEFINITION\nSTRING\nDEFAULT\nNULL\n,\nROLE_TO_CHECK\nSTRING\nDEFAULT\nNULL\n,\nDATABASES_TO_CHECK\nSTRING\nDEFAULT\nNULL\n,\nSCHEMAS_TO_CHECK\nSTRING\nDEFAULT\nNULL\n,\nTABLES_TO_CHECK\nSTRING\nDEFAULT\nNULL\n)\nRETURNS\nSTRING\nLANGUAGE\nJAVASCRIPT\nEXECUTE\nAS\nOWNER\nAS\n$$\n// -----------------------------------------------------UTILITY FUNCTIONS-----------------------------------------------------\n/**\n* Executes a SQL query with parameters\n*\n@param\n{\nstring\n}\nsqlText\n- SQL statement to execute\n*\n@param\n{\nArray\n}\n[binds=[]] - Array of bind parameters for the query\n*\n@param\n{\nboolean\n}\n[\nreturnFirstRow\n=\nfalse\n]\n- Whether to return only the first row\n*\n@returns\n{\nObject\n}\nObject containing execution result or error information\n*/\nfunction\nexecuteQuery\n(\nsqlText\n,\nbinds\n=\n[\n]\n,\nreturnFirstRow\n=\nfalse\n)\n{\ntry\n{\nif\n(\n!\nsqlText\n)\nreturn\n{\nisErrored\n:\ntrue\n,\nmessage\n:\n\"SQL Text is required\"\n,\nresult\n:\nnull\n,\n}\n;\nconst\nstatement\n=\nsnowflake\n.\ncreateStatement\n(\n{\nsqlText\n,\nbinds\n}\n)\n;\nconst\nresult\n=\nstatement\n.\nexecute\n(\n)\n;\nconst\nresponse\n=\n{\nisErrored\n:\nfalse\n,\nmessage\n:\n\"\"\n,\nresult\n:\nnull\n,\n}\n;\nif\n(\nreturnFirstRow\n)\n{\nresponse\n.\nresult\n=\nresult\n.\nnext\n(\n)\n?\nresult\n:\nnull\n;\nreturn\nresponse\n;\n}\nresponse\n.\nresult\n=\nresult\n;\nreturn\nresponse\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\n{\nisErrored\n:\ntrue\n,\nmessage\n:\n`\n${\nerr\n.\ncode\n}\n-\n${\nerr\n.\nmessage\n}\n-\n${\nsqlText\n}\nwith binds:\n${\nbinds\n.\njoin\n(\n\", \"\n)\n}\n`\n,\nresult\n:\nnull\n,\n}\n;\n}\n}\n/**\n* Safely parses a JSON string\n*\n@param\n{\nstring\n}\njsonString\n- JSON string to parse\n*\n@returns\n{\nObject\n}\nParsed JSON object or null if invalid\n*/\nfunction\nsafelyParseJSON\n(\njsonString\n)\n{\ntry\n{\nreturn\nJSON\n.\nparse\n(\njsonString\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\nnull\n;\n}\n}\n/**\n* Validates a number within a range\n*\n@param\n{\nstring\n}\nvalue\n- Number to validate\n*\n@param\n{\nnumber\n}\nmin\n- Minimum value\n*\n@param\n{\nnumber\n}\nmax\n- Maximum value\n*\n@returns\n{\nboolean\n}\nTrue if number is valid\n*\n@returns\n{\nboolean\n}\nFalse if number is invalid\n*/\nfunction\nisNumberValid\n(\nvalue\n,\nmin\n,\nmax\n)\n{\nconst\nnum\n=\nparseInt\n(\nvalue\n,\n10\n)\n;\nreturn\n!\nisNaN\n(\nnum\n)\n&&\nnum\n>=\nmin\n&&\nnum\n<=\nmax\n;\n}\n/**\n* Escapes and quotes a Snowflake identifier\n*\n@param\n{\nstring\n}\nidentifier\n- Raw identifier to normalize\n*\n@returns\n{\nstring\n}\nProperly quoted identifier safe for SQL\n*/\nfunction\nnormalizeIdentifier\n(\nidentifier\n)\n{\nreturn\n`\n\"\n${\nidentifier\n.\nreplace\n(\n/\n\"\n/\ng\n,\n'\"\"'\n)\n}\n\"\n`\n;\n}\n/**\n* Retrieves all columns for a given entity. Validates that the entityexists and procedure owner has access to it.\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified entity name\n*\n@returns\n{\nArray\n}\nArray of column objects with name and dataType properties\n*\n@throws\n{\nError\n}\nIf entity doesn't exist or is inaccessible\n*/\nfunction\ngetAllColumnsForEntity\n(\nentityName\n)\n{\nconst\nsqlText\n=\n\"SHOW COLUMNS IN IDENTIFIER(?)\"\n;\nconst\nbinds\n=\n[\nentityName\n]\n;\nconst\n{\nresult\n,\nisErrored\n,\nmessage\n}\n=\nexecuteQuery\n(\nsqlText\n,\nbinds\n)\n;\nif\n(\nisErrored\n)\n{\n// Validates that the entity exists and procedure owner has access to it\nthrow\nnew\nError\n(\nmessage\n)\n;\n}\nconst\ncolumns\n=\n[\n]\n;\nwhile\n(\nresult\n.\nnext\n(\n)\n)\n{\nconst\ncolumn\n=\n{\nname\n:\nresult\n.\ngetColumnValue\n(\n\"column_name\"\n)\n,\ndataType\n:\nJSON\n.\nparse\n(\nresult\n.\ngetColumnValue\n(\n\"data_type\"\n)\n)\n.\ntype\n,\n}\n;\nif\n(\ncolumn\n.\ndataType\n===\n\"FIXED\"\n)\ncolumn\n.\ndataType\n=\n\"NUMBER\"\n;\ncolumns\n.\npush\n(\ncolumn\n)\n;\n}\nreturn\ncolumns\n;\n}\n/**\n* Validates that the DMF is valid and exists\n*\n@param\n{\nstring\n}\ndmfName\n- Fully qualified name of the DMF\n*\n@param\n{\nstring\n}\ndmfArguments\n- Arguments for the DMF\n*\n@returns\n{\nboolean\n}\nWhether the DMF is valid\n*\n@throws\n{\nError\n}\nIf DMF is invalid\n*/\nfunction\nisDMFValid\n(\ndmfName\n,\ndmfArguments\n)\n{\nconst\n{\nisErrored\n,\nmessage\n}\n=\nexecuteQuery\n(\n`\nDESCRIBE FUNCTION IDENTIFIER(?)(\n${\ndmfArguments\n}\n)\n`\n,\n[\ndmfName\n]\n,\ntrue\n)\n;\nif\n(\nisErrored\n)\nthrow\nnew\nError\n(\nmessage\n)\n;\nreturn\ntrue\n;\n}\n/**\n* Checks if a timezone is valid\n*\n@param\n{\nstring\n}\ntimezone\n- Timezone to validate\n*\n@returns\n{\nboolean\n}\nTrue if timezone is valid\n*\n@returns\n{\nboolean\n}\nFalse if timezone is invalid\n*/\nfunction\nisTimezoneValid\n(\ntimezone\n)\n{\nconst\nresult\n=\nexecuteQuery\n(\n`\nSELECT CONVERT_TIMEZONE(?, CURRENT_TIMESTAMP())\n`\n,\n[\ntimezone\n]\n,\ntrue\n)\n;\nreturn\n!\nresult\n.\nisErrored\n;\n}\n/**\n* Generates a DMF type signature based on the arguments and entity columns\n*\n@param\n{\nArray\n}\ndmfArguments\n- Array of DMF arguments\n*\n@param\n{\nObject\n}\nentityColumnsMap\n- Map of entity names to column objects in the format\n{\n<ENTITY_NAME>: [\n{\nname: <COLUMN_NAME> , dataType: <DATA_TYPE>\n}\n]\n}\n*\n@param\n{\nstring\n}\nbaseEntityName\n- Name of the base entity\n*\n@returns\n{\nstring\n}\nDMF type signature\n*\n@throws\n{\nError\n}\nIf entity not found in the cache\n*/\nfunction\ngenerateDMFTypeSignature\n(\ndmfArguments\n,\nentityColumnsMap\n,\nbaseEntityName\n)\n{\nif\n(\n!\ndmfArguments\n||\n!\ndmfArguments\n.\nlength\n)\nreturn\n\"\"\n;\nconst\nbaseEntityColumns\n=\nentityColumnsMap\n[\nbaseEntityName\n]\n;\nif\n(\n!\nbaseEntityColumns\n)\n{\nthrow\nnew\nError\n(\n`\nEntity\n${\nbaseEntityName\n}\nnot found in the cache\n`\n)\n;\n}\nconst\nbaseEntityColumnArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"COLUMN\"\n)\n.\nmap\n(\nparam\n=>\n{\nconst\ncolumn\n=\nbaseEntityColumns\n.\nfind\n(\ncol\n=>\ncol\n.\nname\n===\nparam\n.\nname\n)\n;\nreturn\ncolumn\n?\ncolumn\n.\ndataType\n:\nnull\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\nconst\nbaseEntityArguments\n=\n`\nTABLE(\n${\nbaseEntityColumnArguments\n}\n)\n`\n;\nconst\nreferencedEntityArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n.\nmap\n(\nentityParam\n=>\n{\nconst\nentityName\n=\nentityParam\n.\nname\n;\nconst\nentityColumns\n=\nentityColumnsMap\n[\nentityName\n]\n;\nif\n(\n!\nentityColumns\n)\n{\nthrow\nnew\nError\n(\n`\nEntity\n${\nentityName\n}\nnot found in the cache\n`\n)\n;\n}\nconst\ncolumnTypes\n=\nentityParam\n.\nnested\n.\nmap\n(\nnestedParam\n=>\n{\nconst\ncolumn\n=\nentityColumns\n.\nfind\n(\ncol\n=>\ncol\n.\nname\n===\nnestedParam\n.\nname\n)\n;\nreturn\ncolumn\n?\ncolumn\n.\ndataType\n:\nnull\n;\n}\n)\n.\nfilter\n(\nBoolean\n)\n.\njoin\n(\n\", \"\n)\n;\nreturn\n`\nTABLE(\n${\ncolumnTypes\n}\n)\n`\n;\n}\n)\n;\nconst\narguments\n=\n[\nbaseEntityArguments\n,\n...\nreferencedEntityArguments\n]\n.\njoin\n(\n\", \"\n)\n;\nreturn\narguments\n;\n}\n/**\n* Generates DMF arguments for SQL statements\n*\n@param\n{\nstring\n}\ndmfArguments\n- Array of DMF arguments\n*\n@returns\n{\nstring\n}\nFormatted DMF arguments for SQL statements\n*/\nfunction\ngenerateDMFColumnArguments\n(\ndmfArguments\n)\n{\nreturn\ndmfArguments\n.\nmap\n(\nparam\n=>\n{\nif\n(\nparam\n.\ntype\n===\n\"COLUMN\"\n)\n{\nreturn\nnormalizeIdentifier\n(\nparam\n.\nname\n)\n;\n}\n// Handle TABLE type with nested columns\nreturn\n`\nTABLE(\n${\nnormalizeIdentifier\n(\nparam\n.\nname\n)\n}\n(\n${\nparam\n.\nnested\n.\nmap\n(\nnested\n=>\nnormalizeIdentifier\n(\nnested\n.\nname\n)\n)\n.\njoin\n(\n\", \"\n)\n}\n))\n`\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\n}\n/**\n* Generates function parameters from DMF arguments\n*\n@param\n{\nArray\n}\ndmfArguments\n- Array of DMF arguments\n*\n@returns\n{\nstring\n}\nFormatted function parameters\n*/\nfunction\ngenerateFunctionParameters\n(\ndmfArguments\n)\n{\nreturn\ndmfArguments\n.\nmap\n(\nparam\n=>\n{\nif\n(\nparam\n.\ntype\n===\n\"TABLE\"\n)\n{\nconst\nnestedParams\n=\nparam\n.\nnested\n.\nmap\n(\nnested\n=>\n`\n${\nnested\n.\nname\n}\n${\nnested\n.\ndataType\n}\n`\n)\n.\njoin\n(\n\", \"\n)\n;\nreturn\n`\n${\nparam\n.\nname\n}\nTABLE(\n${\nnestedParams\n}\n)\n`\n;\n}\nreturn\n`\n${\nparam\n.\nname\n}\n${\nparam\n.\ndataType\n}\n`\n;\n}\n)\n.\njoin\n(\n\", \"\n)\n;\n}\n// -----------------------------------------------------VALIDATION FUNCTIONS-----------------------------------------------------\n/**\n* Validates that mandatory arguments are provided and valid\n*\n@throws\n{\nError\n}\nIf any mandatory argument is missing or invalid\n*/\nfunction\nvalidateMandatoryArguments\n(\n)\n{\nconst\nVALID_ACTIONS\n=\nnew\nSet\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n,\n\"UPDATE_SCHEDULE\"\n,\n\"CREATE_DMF\"\n,\n\"EXECUTE_SQL\"\n,\n\"VALIDATE_SQL_PERMISSIONS\"\n]\n)\n;\nconst\nVALID_ENTITY_TYPES\n=\nnew\nSet\n(\n[\n\"TABLE\"\n,\n\"VIEW\"\n,\n\"MATERIALIZED VIEW\"\n,\n\"EXTERNAL TABLE\"\n,\n\"ICEBERG TABLE\"\n]\n)\n;\nconst\nDMF_OPS\n=\nnew\nSet\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n]\n)\n;\nconst\nVALID_SCHEDULE_TYPES\n=\nnew\nSet\n(\n[\n\"MINUTES\"\n,\n\"CRON\"\n,\n\"ON_DATA_CHANGE\"\n,\n\"NOT_SCHEDULED\"\n]\n)\n;\nconst\nSCHEDULE_TYPES_THAT_REQUIRE_VALUE\n=\nnew\nSet\n(\n[\n\"MINUTES\"\n,\n\"CRON\"\n]\n)\n;\nif\n(\n!\nVALID_ACTIONS\n.\nhas\n(\nACTION\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid ACTION: \"\n${\nACTION\n}\n\". Valid options are\n${\nArray\n.\nfrom\n(\nVALID_ACTIONS\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nENTITY_TYPE\n&&\n!\nVALID_ENTITY_TYPES\n.\nhas\n(\nENTITY_TYPE\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid ENTITY_TYPE: \"\n${\nENTITY_TYPE\n}\n\". Valid options are\n${\nArray\n.\nfrom\n(\nVALID_ENTITY_TYPES\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nDMF_OPS\n.\nhas\n(\nACTION\n)\n&&\n!\nDMF_NAME\n)\nthrow\nnew\nError\n(\n\"DMF_NAME is required for DMF related actions\"\n)\n;\nif\n(\nACTION\n===\n\"UPDATE_SCHEDULE\"\n)\n{\nif\n(\n!\nSCHEDULE_TYPE\n)\nthrow\nnew\nError\n(\n\"SCHEDULE_TYPE is required for SCHEDULE action\"\n)\n;\nif\n(\n!\nVALID_SCHEDULE_TYPES\n.\nhas\n(\nSCHEDULE_TYPE\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid schedule type: \"\n${\nSCHEDULE_TYPE\n}\n\". Valid options are\n${\nArray\n.\nfrom\n(\nVALID_SCHEDULE_TYPES\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nSCHEDULE_TYPES_THAT_REQUIRE_VALUE\n.\nhas\n(\nSCHEDULE_TYPE\n)\n&&\n!\nSCHEDULE_VALUE\n)\nthrow\nnew\nError\n(\n\"SCHEDULE_VALUE is required for SCHEDULE action\"\n)\n;\n}\nif\n(\nACTION\n===\n\"EXECUTE_SQL\"\n&&\n!\nDMF_DEFINITION\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a SQL query to execute.\"\n)\n;\n}\nif\n(\nACTION\n===\n\"VALIDATE_SQL_PERMISSIONS\"\n)\n{\nif\n(\n!\nDMF_DEFINITION\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a SQL query to validate permissions.\"\n)\n;\n}\nif\n(\n!\nROLE_TO_CHECK\n)\n{\nthrow\nnew\nError\n(\n\"Failed to fetch role to check permissions. Please ensure the role exists and is accessible.\"\n)\n;\n}\nif\n(\n!\nDATABASES_TO_CHECK\n&&\n!\nSCHEMAS_TO_CHECK\n&&\n!\nTABLES_TO_CHECK\n)\n{\nthrow\nnew\nError\n(\n\"No databases, schemas, or tables provided. Please provide at least one database, schema, or table to validate permissions for.\"\n)\n;\n}\n}\n}\n/**\n* Parses a fully qualified name into its components\n*\n@param\n{\nstring\n}\nfullyQualifiedName\n- Fully qualified name to parse\n*\n@returns\n{\nObject\n}\nObject with database, schema, and name properties\n*\n@throws\n{\nError\n}\nIf invalid fully qualified name\n*/\nfunction\nvalidateFullyQualifiedName\n(\nfullyQualifiedName\n)\n{\nconst\nparts\n=\nfullyQualifiedName\n.\nsplit\n(\n\".\"\n)\n.\nmap\n(\npart\n=>\npart\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n;\nif\n(\nparts\n.\nlength\n!==\n3\n)\nthrow\nnew\nError\n(\n`\nInvalid fully qualified name:\n${\nfullyQualifiedName\n}\n. Expected format: database.schema.name\n`\n)\n;\n}\n/**\n* Validates the structure of DMF arguments JSON\n*\n@param\n{\nstring\n}\nrawDMFArguments\n- Raw JSON string of DMF arguments\n*\n@throws\n{\nError\n}\nIf DMF arguments structure is invalid\n*/\nfunction\nvalidateDMFArgumentsStructure\n(\nrawDMFArguments\n)\n{\nconst\nparsedStructure\n=\nsafelyParseJSON\n(\nrawDMFArguments\n)\n;\nif\n(\n!\nparsedStructure\n)\nthrow\nnew\nError\n(\n\"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\"\n)\n;\nif\n(\n!\nArray\n.\nisArray\n(\nparsedStructure\n)\n)\nthrow\nnew\nError\n(\n\"DMF_ARGUMENTS_JSON must be an array\"\n)\n;\nconst\nreferencedEntities\n=\nparsedStructure\n.\nfilter\n(\n(\nparam\n)\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n;\nif\n(\nreferencedEntities\n.\nlength\n>\n1\n)\nthrow\nnew\nError\n(\n\"Only one referenced entity is allowed\"\n)\n;\nconst\nvalidationFunctions\n=\n{\narrayItem\n:\n(\nparam\n)\n=>\n[\n\"COLUMN\"\n,\n\"TABLE\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n&&\nparam\n.\nname\n,\nnestedItem\n:\n(\nparam\n)\n=>\n[\n\"COLUMN\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n&&\nparam\n.\nname\n,\n}\n;\nif\n(\n!\nparsedStructure\n.\nevery\n(\nvalidationFunctions\n.\narrayItem\n)\n)\nthrow\nnew\nError\n(\n\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"\n)\n;\nif\n(\nreferencedEntities\n.\nlength\n>\n0\n)\n{\nfor\n(\nconst\nreferencedEntity\nof\nreferencedEntities\n)\n{\nif\n(\n!\nArray\n.\nisArray\n(\nreferencedEntity\n.\nnested\n)\n||\n!\nreferencedEntity\n.\nnested\n.\nevery\n(\nvalidationFunctions\n.\nnestedItem\n)\n)\nthrow\nnew\nError\n(\n\"Invalid nested parameters\"\n)\n;\n}\n}\n}\n/**\n* Validates that all specified columns exist in an entity\n*\n@param\n{\nArray\n}\ncolumnsToCheck\n- Array of column names to validate\n*\n@param\n{\nArray\n}\nentityColumns\n- Array of column metadata from the entity\n*\n@param\n{\nstring\n}\nentityName\n- Name of the entity for error message\n*\n@throws\n{\nError\n}\nIf any column doesn't exist in the entity\n*/\nfunction\nvalidateColumnsExistInEntity\n(\nentityName\n,\nallColumnsInEntity\n,\ncolumnsToCheck\n)\n{\nfor\n(\nconst\ncolumn\nof\ncolumnsToCheck\n)\n{\nif\n(\n!\nallColumnsInEntity\n.\nsome\n(\ncol\n=>\ncol\n.\nname\n===\ncolumn\n)\n)\nthrow\nnew\nError\n(\n`\nColumn\n${\ncolumn\n}\nnot found in entity\n${\nentityName\n}\n`\n)\n;\n}\n}\n/**\n* Validates that all provided identifiers exist and are accessible\n* Checks entity names, column names, and DMF compatibility\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified name of the entity\n*\n@param\n{\nstring\n}\ndmfName\n- Fully qualified name of the DMF\n*\n@param\n{\nArray\n}\ndmfArguments\n- Array of DMF arguments\n*\n@throws\n{\nError\n}\nIf any identifier doesn't exist or is inaccessible\n*/\nfunction\nvalidateProvidedIdentifiers\n(\nentityName\n,\ndmfName\n=\n\"\"\n,\ndmfArguments\n=\n[\n]\n)\n{\nif\n(\n!\nentityName\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a valid entity name. The entity name is required to validate identifiers.\"\n)\n;\n}\nvalidateFullyQualifiedName\n(\nentityName\n)\n;\n// Validate the provided entity names and store all the columns for each entity in a map\nconst\nbaseEntityName\n=\nentityName\n;\nconst\nbaseEntityAllColumns\n=\ngetAllColumnsForEntity\n(\nentityName\n)\n;\nconst\nentityColumnsMap\n=\n{\n[\nbaseEntityName\n]\n:\nbaseEntityAllColumns\n}\n;\nconst\nallReferencedEntities\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n;\nfor\n(\nconst\nreferencedEntity\nof\nallReferencedEntities\n)\n{\nconst\ncolumns\n=\ngetAllColumnsForEntity\n(\nreferencedEntity\n.\nname\n)\n;\nentityColumnsMap\n[\nreferencedEntity\n.\nname\n]\n=\ncolumns\n;\n}\n// Valite all of the provided columns are valid and exist in their respective entities\nconst\nallBaseEntityColumnsInArguments\n=\ndmfArguments\n.\nfilter\n(\nparam\n=>\nparam\n.\ntype\n===\n\"COLUMN\"\n)\n.\nmap\n(\nparam\n=>\nparam\n.\nname\n)\n;\nvalidateColumnsExistInEntity\n(\nbaseEntityName\n,\nbaseEntityAllColumns\n,\nallBaseEntityColumnsInArguments\n)\n;\nfor\n(\nconst\nreferencedEntity\nof\nallReferencedEntities\n)\n{\nconst\ncolumnsInArguments\n=\nreferencedEntity\n.\nnested\n.\nmap\n(\nnestedParam\n=>\nnestedParam\n.\nname\n)\n;\nvalidateColumnsExistInEntity\n(\nreferencedEntity\n.\nname\n,\nentityColumnsMap\n[\nreferencedEntity\n.\nname\n]\n,\ncolumnsInArguments\n)\n;\n}\nif\n(\ndmfName\n)\n{\n// Validate that the DMF is valid and exists\nconst\ngeneratedTypeSignature\n=\ngenerateDMFTypeSignature\n(\ndmfArguments\n,\nentityColumnsMap\n,\nbaseEntityName\n)\n;\nisDMFValid\n(\ndmfName\n,\ngeneratedTypeSignature\n)\n;\n}\n// All provided identifiers are valid, actually exist and are accessible to the procedure owner\n}\n/**\n* Validates CRON expression syntax\n* Performs detailed validation of all CRON components and timezones to protect against SQL injection\n*\n@param\n{\nstring\n}\ncronExpression\n- CRON expression to validate\n*\n@throws\n{\nError\n}\nIf CRON expression is invalid\n*/\nfunction\nvalidateCronExpression\n(\ncronExpression\n)\n{\nif\n(\ncronExpression\n.\nlength\n>\n100\n)\nthrow\nnew\nError\n(\n\"Cron expression is too long\"\n)\n;\nconst\ncronFields\n=\ncronExpression\n.\ntrim\n(\n)\n.\nsplit\n(\n/\n\\s\n+\n/\n)\n;\nif\n(\ncronFields\n.\nlength\n!==\n6\n)\nthrow\nnew\nError\n(\n\"Invalid cron expression. Expected 6 fields\"\n)\n;\nconst\n[\nminute\n,\nhour\n,\ndayOfMonth\n,\nmonth\n,\ndayOfWeek\n,\ntimezone\n]\n=\ncronFields\n;\nconst\nisTimezoneValidResult\n=\nisTimezoneValid\n(\ntimezone\n)\n;\nif\n(\n!\nisTimezoneValidResult\n)\nthrow\nnew\nError\n(\n\"Invalid timezone provided in the cron expression\"\n)\n;\nconst\nregexPatterns\n=\n{\nminute\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\nhour\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\ndayOfMonth\n:\n/\n^\n(\n\\*\n|\nL\n|\n\\d\n+\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n)\n$\n/\n,\nmonth\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\nJAN\n|\nFEB\n|\nMAR\n|\nAPR\n|\nMAY\n|\nJUN\n|\nJUL\n|\nAUG\n|\nSEP\n|\nOCT\n|\nNOV\n|\nDEC\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n[\nA\n-\nZ\n]\n{3}\n\\-\n[\nA\n-\nZ\n]\n{3}\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n|\n(\n[\nA\n-\nZ\n]\n{3}\n(\n,\n[\nA\n-\nZ\n]\n{3}\n)\n*\n)\n)\n$\n/\ni\n,\ndayOfWeek\n:\n/\n^\n(\n\\*\n|\n\\d\n+\n|\nSUN\n|\nMON\n|\nTUE\n|\nWED\n|\nTHU\n|\nFRI\n|\nSAT\n|\n\\d\n+\nL\n|\n[\nA\n-\nZ\n]\n{3}\nL\n|\n\\*\n\\/\n\\d\n+\n|\n\\d\n+\n\\-\n\\d\n+\n|\n[\nA\n-\nZ\n]\n{3}\n\\-\n[\nA\n-\nZ\n]\n{3}\n|\n\\d\n+\n(\n,\n\\d\n+\n)\n*\n|\n(\n[\nA\n-\nZ\n]\n{3}\n(\n,\n[\nA\n-\nZ\n]\n{3}\n)\n*\n)\n)\n$\n/\ni\n,\n}\n;\nif\n(\nminute\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nminute\n,\n0\n,\n59\n)\n)\nthrow\nnew\nError\n(\n\"Invalid minute value\"\n)\n;\nif\n(\nhour\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nhour\n,\n0\n,\n23\n)\n)\nthrow\nnew\nError\n(\n\"Invalid hour value\"\n)\n;\nif\n(\ndayOfMonth\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\ndayOfMonth\n,\n1\n,\n31\n)\n)\nthrow\nnew\nError\n(\n\"Invalid day of month value\"\n)\n;\nif\n(\nmonth\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\nmonth\n,\n1\n,\n12\n)\n)\nthrow\nnew\nError\n(\n\"Invalid month value\"\n)\n;\nif\n(\ndayOfWeek\n.\nmatch\n(\n/\n^\n\\d\n+\n$\n/\n)\n)\nif\n(\n!\nisNumberValid\n(\ndayOfWeek\n,\n0\n,\n6\n)\n)\nthrow\nnew\nError\n(\n\"Invalid day of week value\"\n)\n;\nif\n(\n!\nregexPatterns\n.\nminute\n.\ntest\n(\nminute\n)\n||\n!\nregexPatterns\n.\nhour\n.\ntest\n(\nhour\n)\n||\n!\nregexPatterns\n.\ndayOfMonth\n.\ntest\n(\ndayOfMonth\n)\n||\n!\nregexPatterns\n.\nmonth\n.\ntest\n(\nmonth\n)\n||\n!\nregexPatterns\n.\ndayOfWeek\n.\ntest\n(\ndayOfWeek\n)\n)\nthrow\nnew\nError\n(\n\"Invalid cron expression\"\n)\n;\n}\n/**\n* Validates schedule-specific arguments\n* Ensures schedule type and value are compatible and valid\n*\n@throws\n{\nError\n}\nIf schedule configuration is invalid\n*/\nfunction\nvalidateProvidedArgumentsForSchedule\n(\n)\n{\nconst\nVALID_MINUTES\n=\nnew\nSet\n(\n[\n\"5\"\n,\n\"15\"\n,\n\"30\"\n,\n\"60\"\n,\n\"720\"\n,\n\"1440\"\n]\n)\n;\nif\n(\nSCHEDULE_TYPE\n===\n\"MINUTES\"\n&&\n!\nVALID_MINUTES\n.\nhas\n(\nSCHEDULE_VALUE\n)\n)\nthrow\nnew\nError\n(\n`\nInvalid SCHEDULE_VALUE for MINUTES. Valid options are\n${\nArray\n.\nfrom\n(\nVALID_MINUTES\n)\n.\njoin\n(\n\", \"\n)\n}\n`\n)\n;\nif\n(\nSCHEDULE_TYPE\n===\n\"CRON\"\n)\nvalidateCronExpression\n(\nSCHEDULE_VALUE\n)\n;\n// SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE\n}\n/**\n* Validates DMF arguments with dataType checks\n*\n@param\n{\nstring\n}\nrawDMFArguments\n- Raw JSON string of DMF arguments\n*\n@throws\n{\nError\n}\nIf DMF arguments structure is invalid or dataType is missing\n*/\nfunction\nvalidateDMFArgumentsWithDataType\n(\nrawDMFArguments\n)\n{\nconst\nparsedStructure\n=\nsafelyParseJSON\n(\nrawDMFArguments\n)\n;\nif\n(\n!\nparsedStructure\n)\nthrow\nnew\nError\n(\n\"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\"\n)\n;\nif\n(\n!\nArray\n.\nisArray\n(\nparsedStructure\n)\n)\nthrow\nnew\nError\n(\n\"DMF_ARGUMENTS_JSON must be an array\"\n)\n;\nconst\nvalidationFunctions\n=\n{\narrayItem\n:\n(\nparam\n)\n=>\n{\nif\n(\n!\n[\n\"COLUMN\"\n,\n\"TABLE\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n||\n!\nparam\n.\nname\n)\n{\nreturn\nfalse\n;\n}\nif\n(\nparam\n.\ntype\n===\n\"COLUMN\"\n&&\n!\nparam\n.\ndataType\n)\n{\nthrow\nnew\nError\n(\n`\nMissing dataType for COLUMN parameter:\n${\nparam\n.\nname\n}\n`\n)\n;\n}\nreturn\ntrue\n;\n}\n,\nnestedItem\n:\n(\nparam\n)\n=>\n{\nif\n(\n!\n[\n\"COLUMN\"\n]\n.\nincludes\n(\nparam\n.\ntype\n)\n||\n!\nparam\n.\nname\n)\n{\nreturn\nfalse\n;\n}\nif\n(\n!\nparam\n.\ndataType\n)\n{\nthrow\nnew\nError\n(\n`\nMissing dataType for nested COLUMN parameter:\n${\nparam\n.\nname\n}\n`\n)\n;\n}\nreturn\ntrue\n;\n}\n}\n;\nif\n(\n!\nparsedStructure\n.\nevery\n(\nvalidationFunctions\n.\narrayItem\n)\n)\nthrow\nnew\nError\n(\n\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"\n)\n;\nconst\nreferencedEntities\n=\nparsedStructure\n.\nfilter\n(\n(\nparam\n)\n=>\nparam\n.\ntype\n===\n\"TABLE\"\n)\n;\nfor\n(\nconst\nreferencedEntity\nof\nreferencedEntities\n)\n{\nif\n(\n!\nArray\n.\nisArray\n(\nreferencedEntity\n.\nnested\n)\n||\n!\nreferencedEntity\n.\nnested\n.\nevery\n(\nvalidationFunctions\n.\nnestedItem\n)\n)\nthrow\nnew\nError\n(\n\"Invalid nested parameters\"\n)\n;\n}\n}\n/**\n* Validates DMF name format\n*\n@param\n{\nstring\n}\ndmfName\n- Fully qualified name of the DMF\n*\n@throws\n{\nError\n}\nIf DMF name format is invalid\n*/\nfunction\nvalidateDmfName\n(\ndmfName\n)\n{\nconst\nparts\n=\ndmfName\n.\nsplit\n(\n\".\"\n)\n.\nmap\n(\npart\n=>\npart\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n;\nif\n(\nparts\n.\nlength\n!==\n3\n)\n{\nthrow\nnew\nError\n(\n`\nInvalid DMF_NAME:\n${\ndmfName\n}\n. Expected format: database.schema.name\n`\n)\n;\n}\n}\n/**\n* Validates that the provided SQL is read-only and doesn't contain dangerous operations\n*\n@param\n{\nstring\n}\nsqlExpression\n- SQL to validate\n*\n@returns\n{\nboolean\n}\nWhether the SQL is safe\n*\n@throws\n{\nError\n}\nIf SQL contains potentially dangerous operations\n*/\nfunction\nvalidateSqlExpression\n(\nsqlExpression\n)\n{\nif\n(\n!\nsqlExpression\n)\n{\nthrow\nnew\nError\n(\n\"Please provide a SQL query. The SQL expression cannot be empty.\"\n)\n;\n}\n// Step 1: Normalize Unicode characters to prevent encoding-based attacks\nconst\nnormalizedSql\n=\nsqlExpression\n.\nnormalize\n(\n'NFKC'\n)\n;\n// Step 2: Check for multiple statements (handled by splitIntoSqlStatements)\nsplitIntoSqlStatements\n(\nnormalizedSql\n)\n;\n// Step 3: Check whether it is a read-query or not\nif\n(\n!\nisReadQuery\n(\nnormalizedSql\n)\n)\n{\nthrow\nnew\nError\n(\n\"Your query must start with SELECT or WITH. Only read operations are allowed.\"\n)\n;\n}\n// Step 4: Check for suspicious patterns that might bypass filters\ncheckForSuspiciousPatterns\n(\nnormalizedSql\n)\n;\n// Step 5: Check for dangerous operations\nconst\ndangerousOperation\n=\ncontainsDangerousOperation\n(\nnormalizedSql\n)\n;\nif\n(\ndangerousOperation\n)\n{\nthrow\nnew\nError\n(\n\"For security reasons, this operation is not permitted. Please use only read operations in your query.\"\n)\n;\n}\nreturn\ntrue\n;\n}\n/**\n* Enhanced detection of suspicious SQL patterns\n*\n@param\n{\nstring\n}\nsql\n- SQL query to check\n*\n@throws\n{\nError\n}\nIf suspicious patterns are detected\n*/\nfunction\ncheckForSuspiciousPatterns\n(\nsql\n)\n{\n// Create a copy where string literals are masked to prevent false positives\nconst\nsqlWithoutStrings\n=\nsql\n.\nreplace\n(\n/\n'\n[\n^\n'\n]\n*\n'\n/\ng\n,\n\"'STRING_LITERAL'\"\n)\n.\nreplace\n(\n/\n\"\n[\n^\n\"\n]\n*\n\"\n/\ng\n,\n'\"STRING_LITERAL\"'\n)\n;\nconst\nsuspiciousPatterns\n=\n[\n// Common SQL injection techniques\n{\npattern\n:\n/\n\\b\nOR\n\\s\n+\n[\n0\n-\n9\n]\n+\n\\s\n*\n=\n\\s\n*\n[\n0\n-\n9\n]\n+\n\\b\n/\ni\n,\nmessage\n:\n\"Suspicious always-true condition detected\"\n}\n,\n// Alias abuse detection\n{\npattern\n:\n/\n\\b\nAS\n\\s\n+\n[\n'\"`\n]\n?\n.\n*?\n(\nDELETE\n|\nINSERT\n|\nUPDATE\n|\nDROP\n|\nALTER\n|\nEXEC\n)\n\\b\n/\ni\n,\nmessage\n:\n\"Suspicious alias detected\"\n}\n,\n// Hex encoding and other obfuscation techniques\n{\npattern\n:\n/\n0x\n[\n0\n-\n9\na\n-\nf\n]\n{10,}\n/\ni\n,\nmessage\n:\n\"Suspicious hex encoding detected\"\n}\n,\n{\npattern\n:\n/\nCHAR\n\\s\n*\n\\(\n\\s\n*\n\\d\n+\n(\n\\s\n*\n,\n\\s\n*\n\\d\n+\n)\n+\n\\s\n*\n\\)\n/\ni\n,\nmessage\n:\n\"Character code conversion functions are not allowed\"\n}\n,\n]\n;\n// Check for suspicious patterns outside of string literals\nfor\n(\nconst\n{\npattern\n,\nmessage\n}\nof\nsuspiciousPatterns\n)\n{\nif\n(\npattern\n.\ntest\n(\nsqlWithoutStrings\n)\n)\n{\nthrow\nnew\nError\n(\nmessage\n)\n;\n}\n}\n}\n/**\n* Splits SQL into separate statements based on semicolons not in quotes\n*\n@param\n{\nstring\n}\nsql\n- SQL query\n*\n@returns\n{\nstring\n}\n- SQL query without semicolons\n*/\nfunction\nsplitIntoSqlStatements\n(\nsql\n)\n{\nlet\ninSingleQuote\n=\nfalse\n;\nlet\ninDoubleQuote\n=\nfalse\n;\nfor\n(\nlet\ni\n=\n0\n;\ni\n<\nsql\n.\nlength\n;\ni\n++\n)\n{\nconst\nchar\n=\nsql\n[\ni\n]\n;\n// Handle quotes\nif\n(\nchar\n===\n\"'\"\n&&\nsql\n[\ni\n-\n1\n]\n!==\n'\\\\'\n)\n{\ninSingleQuote\n=\n!\ninSingleQuote\n;\n}\nelse\nif\n(\nchar\n===\n'\"'\n&&\nsql\n[\ni\n-\n1\n]\n!==\n'\\\\'\n)\n{\ninDoubleQuote\n=\n!\ninDoubleQuote\n;\n}\n// If semicolon outside of quotes, throw error\nif\n(\nchar\n===\n';'\n&&\n!\ninSingleQuote\n&&\n!\ninDoubleQuote\n)\n{\nthrow\nnew\nError\n(\n\"Do not use semicolons to break or end your SQL statement. Submit your query without any semicolons.\"\n)\n;\n}\n}\n// If we get here, there were no semicolons outside quotes\nreturn\nsql\n.\ntrim\n(\n)\n;\n}\n/**\n* Checks if the SQL is a read-only query\n*\n@param\n{\nstring\n}\nsql\n- SQL query without comments\n*\n@returns\n{\nboolean\n}\n- True if it's a read-only query\n*/\nfunction\nisReadQuery\n(\nsql\n)\n{\nconst\nnormalizedSql\n=\nsql\n.\nreplace\n(\n/\n\\s\n+\n/\ng\n,\n' '\n)\n.\ntoUpperCase\n(\n)\n.\ntrim\n(\n)\n;\nif\n(\nnormalizedSql\n.\nstartsWith\n(\n'SELECT '\n)\n)\n{\nreturn\ntrue\n;\n}\nif\n(\nnormalizedSql\n.\nstartsWith\n(\n'WITH '\n)\n)\n{\nreturn\ntrue\n;\n}\nreturn\nfalse\n;\n}\n/**\n* Checks if SQL contains any dangerous operations - using single keywords with word boundaries\n*\n@param\n{\nstring\n}\nsql\n- SQL query without comments\n*\n@returns\n{\nstring\n|\nnull\n}\n- The dangerous operation found or null if safe\n*/\nfunction\ncontainsDangerousOperation\n(\nsql\n)\n{\n// Normalize whitespace and convert to uppercase for comparison\nconst\nnormalizedSql\n=\nsql\n.\nreplace\n(\n/\n\\s\n+\n/\ng\n,\n' '\n)\n.\ntoUpperCase\n(\n)\n;\n// Snowflake-specific dangerous commands - using single keywords with high precision\nconst\ndangerousCommands\n=\n[\n// Data Modification\n'INSERT'\n,\n'UPDATE'\n,\n'DELETE'\n,\n'MERGE'\n,\n'TRUNCATE'\n,\n'COPY'\n,\n// DDL statements\n'CREATE'\n,\n'DROP'\n,\n'ALTER'\n,\n'COMMENT'\n,\n'GRANT'\n,\n'REVOKE'\n,\n'UNDROP'\n,\n// Transaction control\n'BEGIN'\n,\n'COMMIT'\n,\n'ROLLBACK'\n,\n// System & session commands\n'SET'\n,\n'UNSET'\n,\n'USE'\n,\n'PUT'\n,\n'GET'\n,\n'REMOVE'\n,\n'LIST'\n,\n// Information Schema & Metadata\n'SHOW'\n,\n'DESCRIBE'\n,\n// Procedures and functions\n'CALL'\n,\n'EXECUTE'\n,\n'EXEC'\n,\n// Additional Snowflake operations\n'EXPLAIN'\n]\n;\n// Dangerous functions specific to Snowflake\nconst\ndangerousFunctions\n=\n[\n'SYSTEM'\n,\n'CURRENT_USER'\n,\n'CURRENT_ROLE'\n,\n'CURRENT_ACCOUNT'\n,\n'DATABASE'\n,\n'VERSION'\n,\n'SLEEP'\n,\n'CALL_INTEGRATION'\n,\n'PARSE_JSON'\n,\n'RUN_JAVASCRIPT'\n,\n'CALL_JAVASCRIPT'\n,\n'TO_JAVASCRIPT'\n]\n;\n// Create a regex pattern with word boundaries for all dangerous commands\nconst\ncommandPattern\n=\nnew\nRegExp\n(\n`\n\\\\b(\n${\ndangerousCommands\n.\njoin\n(\n'|'\n)\n}\n)\\\\b\n`\n,\n'i'\n)\n;\nconst\nfunctionPattern\n=\nnew\nRegExp\n(\n`\n\\\\b(\n${\ndangerousFunctions\n.\njoin\n(\n'|'\n)\n}\n)\\\\s*\\\\(\n`\n,\n'i'\n)\n;\n// Check for dangerous commands\nconst\ncommandMatch\n=\nnormalizedSql\n.\nmatch\n(\ncommandPattern\n)\n;\nif\n(\ncommandMatch\n)\n{\nreturn\n`\nDangerous operation detected:\n${\ncommandMatch\n[\n0\n]\n}\n`\n;\n}\n// Check for dangerous functions\nconst\nfunctionMatch\n=\nnormalizedSql\n.\nmatch\n(\nfunctionPattern\n)\n;\nif\n(\nfunctionMatch\n)\n{\nreturn\n`\nDangerous function call detected:\n${\nfunctionMatch\n[\n1\n]\n}\n`\n;\n}\n// Check for access to sensitive metadata\nif\n(\n/\n\\b\nINFORMATION_SCHEMA\n\\b\n|\n\\b\nACCOUNT_USAGE\n\\b\n/\ni\n.\ntest\n(\nnormalizedSql\n)\n)\n{\nreturn\n'Access to sensitive system metadata detected'\n;\n}\nreturn\nnull\n;\n}\n/**\n* Executes SQL and returns a numeric result\n*\n@param\n{\nstring\n}\nsql\n- SQL to execute\n*\n@returns\n{\nnumber\n}\nNumeric result\n*\n@throws\n{\nError\n}\nIf execution fails or result is not numeric\n*/\nfunction\nexecuteSqlAndReturnNumber\n(\nsql\n)\n{\ntry\n{\n// Execute without returnFirstRow to get full result set\nconst\nresult\n=\nexecuteQuery\n(\nsql\n,\n[\n]\n,\nfalse\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\n// Check if the result set exists\nif\n(\n!\nresult\n.\nresult\n)\n{\nthrow\nnew\nError\n(\n\"Your query didn't return any results. Please check your SQL and try again.\"\n)\n;\n}\n// Check number of columns\nconst\ncolumnCount\n=\nresult\n.\nresult\n.\ngetColumnCount\n(\n)\n;\nif\n(\ncolumnCount\n!==\n1\n)\n{\nthrow\nnew\nError\n(\n\"Your query should return exactly one column. Please modify your query to return a single numeric value.\"\n)\n;\n}\n// Check if we have exactly one row\nif\n(\n!\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nthrow\nnew\nError\n(\n\"Your query didn't return any rows. Please check your query and try again.\"\n)\n;\n}\n// Get the value\nconst\nvalue\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n1\n)\n;\n// Check if it's a number\nif\n(\ntypeof\nvalue\n!==\n'number'\n)\n{\nthrow\nnew\nError\n(\n\"Your query must return a number. Please modify your query to calculate a numeric result.\"\n)\n;\n}\n// Check if there are more rows\nif\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nthrow\nnew\nError\n(\n\"Your query returned multiple rows. Please modify your query to return a single result.\"\n)\n;\n}\nreturn\nvalue\n;\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\n${\nerr\n.\nmessage\n}\n`\n)\n;\n}\n}\n/**\n* Validates all parameters for DMF creation\n*\n@throws\n{\nError\n}\nIf any validation fails\n*/\nfunction\nvalidateCreateDmf\n(\n)\n{\nvalidateDmfName\n(\nDMF_NAME\n)\n;\nvalidateSqlExpression\n(\nDMF_DEFINITION\n)\n;\nvalidateDMFArgumentsWithDataType\n(\nDMF_ARGUMENTS_JSON\n)\n;\n}\n/**\n* Validates all provided arguments\n* Performs comprehensive validation on input parameters\n*\n@throws\n{\nError\n}\nIf any validation fails\n*/\nfunction\nvalidateAllArguments\n(\n)\n{\nvalidateMandatoryArguments\n(\n)\n;\n// Validates all mandatory arguments are provided in the correct format\nif\n(\nACTION\n===\n\"CREATE_DMF\"\n)\n{\nvalidateCreateDmf\n(\n)\n;\nreturn\n;\n}\nelse\nif\n(\nACTION\n===\n\"EXECUTE_SQL\"\n)\n{\nvalidateSqlExpression\n(\nDMF_DEFINITION\n)\n;\nreturn\n;\n}\nelse\nif\n(\nACTION\n===\n\"VALIDATE_SQL_PERMISSIONS\"\n)\n{\nvalidateSqlExpression\n(\nDMF_DEFINITION\n)\n;\nreturn\n;\n}\nelse\nif\n(\nACTION\n===\n\"UPDATE_SCHEDULE\"\n)\n{\nvalidateProvidedArgumentsForSchedule\n(\n)\n;\n// Validates the provided schedule type and value\n}\nelse\n{\nvalidateDMFArgumentsStructure\n(\nDMF_ARGUMENTS_JSON\n)\n;\n}\nvalidateProvidedIdentifiers\n(\nENTITY_NAME\n,\nDMF_NAME\n,\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n)\n;\n// All provided arguments are valid and legal\n}\n// -----------------------------------------------------MAIN FUNCTION-----------------------------------------------------\n/**\n* Extracts database, schema and table name from fully qualified entity name\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified entity name\n*\n@returns\n{\nObject\n}\nObject containing database, schema and table name\n*/\nfunction\nparseEntityName\n(\nentityName\n)\n{\nconst\n[\ndb\n,\nschema\n,\ntable\n]\n=\nentityName\n.\nsplit\n(\n\".\"\n)\n;\nreturn\n{\ndb\n,\nschema\n,\ntable\n}\n;\n}\n/**\n* Gets the owner of a table from information schema\n*\n@param\n{\nstring\n}\ndb\n- Database name\n*\n@param\n{\nstring\n}\nschema\n- Schema name\n*\n@param\n{\nstring\n}\ntable\n- Table name\n*\n@returns\n{\nObject\n}\nObject containing success status and table owner\n*/\nfunction\ngetTableOwner\n(\ndb\n,\nschema\n,\ntable\n)\n{\nconst\nquery\n=\n`\nSELECT TABLE_OWNER\nFROM\n${\ndb\n}\n.INFORMATION_SCHEMA.TABLES\nWHERE TABLE_CATALOG = ?\nAND TABLE_SCHEMA = ?\nAND TABLE_NAME = ?\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\ndb\n,\nschema\n,\ntable\n]\n,\ntrue\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nFailed to get table owner:\n${\nresult\n.\nmessage\n}\n`\n,\nowner\n:\nnull\n}\n;\n}\nconst\nowner\n=\nresult\n.\nresult\n?.\ngetColumnValue\n(\n\"TABLE_OWNER\"\n)\n;\nif\n(\n!\nowner\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nCould not find owner for table\n${\ndb\n}\n.\n${\nschema\n}\n.\n${\ntable\n}\n`\n,\nowner\n:\nnull\n}\n;\n}\nreturn\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n\"Successfully retrieved table owner\"\n,\nowner\n}\n;\n}\n/**\n* Grants required permissions to a role\n*\n@param\n{\nstring\n}\nrole\n- Role to grant permissions to\n*\n@returns\n{\nObject\n}\nObject containing success status and message\n*/\nfunction\ngrantPermissions\n(\nrole\n)\n{\nconst\nquery\n=\n`\nBEGIN\nGRANT USAGE ON SCHEMA ATLAN_DQ.DMFS TO ROLE \"\n${\nrole\n}\n\";\nGRANT USAGE ON DATABASE ATLAN_DQ TO ROLE \"\n${\nrole\n}\n\";\nGRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ.DMFS TO ROLE \"\n${\nrole\n}\n\";\nEND;\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nFailed to grant permissions:\n${\nresult\n.\nmessage\n}\n`\n}\n;\n}\nreturn\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n`\nSuccessfully granted permissions to role\n${\nrole\n}\n`\n}\n;\n}\n/**\n* Handles permissions for DMF operations\n*\n@param\n{\nstring\n}\nentityName\n- Fully qualified entity name\n*\n@returns\n{\nObject\n}\nObject containing success status and message\n*/\nfunction\nhandleDMFPermissions\n(\nentityName\n)\n{\ntry\n{\n// Parse entity name\nconst\n{\ndb\n,\nschema\n,\ntable\n}\n=\nparseEntityName\n(\nentityName\n)\n;\n// Get table owner\nconst\nownerResult\n=\ngetTableOwner\n(\ndb\n,\nschema\n,\ntable\n)\n;\nif\n(\n!\nownerResult\n.\nisSuccessful\n)\n{\nreturn\nownerResult\n;\n}\n// Grant permissions\nreturn\ngrantPermissions\n(\nownerResult\n.\nowner\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\n`\nError handling permissions:\n${\nerr\n.\nmessage\n}\n`\n}\n;\n}\n}\n/**\n* Parses comma-separated object lists into arrays\n*\n@param\n{\nstring\n}\ndatabasesToCheck\n- Comma-separated list of databases\n*\n@param\n{\nstring\n}\nschemasToCheck\n- Comma-separated list of schemas\n*\n@param\n{\nstring\n}\ntablesToCheck\n- Comma-separated list of tables\n*\n@returns\n{\nObject\n}\nObject with parsed arrays\n*/\nfunction\nparseCommaSeparatedLists\n(\ndatabasesToCheck\n,\nschemasToCheck\n,\ntablesToCheck\n)\n{\nreturn\n{\ndatabases\n:\ndatabasesToCheck\n?\ndatabasesToCheck\n.\nsplit\n(\n','\n)\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n:\n[\n]\n,\nschemas\n:\nschemasToCheck\n?\nschemasToCheck\n.\nsplit\n(\n','\n)\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n:\n[\n]\n,\ntables\n:\ntablesToCheck\n?\ntablesToCheck\n.\nsplit\n(\n','\n)\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n(\n)\n)\n.\nfilter\n(\nBoolean\n)\n:\n[\n]\n}\n;\n}\n/**\n* Checks database access for a role using information schema\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nArray\n}\ndatabases\n- Array of databases to check\n*\n@returns\n{\nArray\n}\nArray of accessible databases\n*/\nfunction\ncheckDatabaseAccess\n(\nroleToCheck\n,\ndatabases\n)\n{\nconst\naccessibleDatabases\n=\n[\n]\n;\nfor\n(\nconst\ndatabase\nof\ndatabases\n)\n{\ntry\n{\nconst\nquery\n=\n`\nSELECT PRIVILEGE_TYPE\nFROM\n${\ndatabase\n}\n.INFORMATION_SCHEMA.OBJECT_PRIVILEGES\nWHERE GRANTEE = '\n${\nroleToCheck\n}\n'\nAND OBJECT_TYPE = 'DATABASE'\nAND OBJECT_NAME = '\n${\ndatabase\n}\n'\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\nwhile\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nconst\nprivilege\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n\"PRIVILEGE_TYPE\"\n)\n;\nif\n(\nprivilege\n===\n\"USAGE\"\n||\nprivilege\n===\n\"OWNERSHIP\"\n)\n{\naccessibleDatabases\n.\npush\n(\ndatabase\n)\n;\nbreak\n;\n}\n}\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\nFailed to check permissions for database '\n${\ndatabase\n}\n'. Role '\n${\nroleToCheck\n}\n' may not have access or the database may not exist.\n`\n)\n;\n}\n}\nreturn\naccessibleDatabases\n;\n}\n/**\n* Checks schema access for a role using information schema\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nArray\n}\nschemas\n- Array of schemas to check (format: database.schema)\n*\n@returns\n{\nArray\n}\nArray of accessible schemas\n*/\nfunction\ncheckSchemaAccess\n(\nroleToCheck\n,\nschemas\n)\n{\nconst\naccessibleSchemas\n=\n[\n]\n;\nfor\n(\nconst\nschema\nof\nschemas\n)\n{\ntry\n{\nconst\nparts\n=\nschema\n.\nsplit\n(\n'.'\n)\n;\nif\n(\nparts\n.\nlength\n!==\n2\n)\n{\nthrow\nnew\nError\n(\n`\nInvalid schema format: '\n${\nschema\n}\n'. Expected format: database.schema\n`\n)\n;\n}\nconst\n[\ndatabase\n,\nschemaName\n]\n=\nparts\n;\nconst\nquery\n=\n`\nSELECT PRIVILEGE_TYPE\nFROM\n${\ndatabase\n}\n.INFORMATION_SCHEMA.OBJECT_PRIVILEGES\nWHERE GRANTEE = '\n${\nroleToCheck\n}\n'\nAND OBJECT_TYPE = 'SCHEMA'\nAND OBJECT_CATALOG = '\n${\ndatabase\n}\n'\nAND OBJECT_NAME = '\n${\nschemaName\n}\n'\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\nwhile\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nconst\nprivilege\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n\"PRIVILEGE_TYPE\"\n)\n;\nif\n(\nprivilege\n===\n\"USAGE\"\n||\nprivilege\n===\n\"OWNERSHIP\"\n)\n{\naccessibleSchemas\n.\npush\n(\nschema\n)\n;\nbreak\n;\n}\n}\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\nFailed to check permissions for schema '\n${\nschema\n}\n'. Role '\n${\nroleToCheck\n}\n' may not have access or the schema may not exist.\n`\n)\n;\n}\n}\nreturn\naccessibleSchemas\n;\n}\n/**\n* Checks table access for a role using information schema\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nArray\n}\ntables\n- Array of tables to check (format: database.schema.table)\n*\n@returns\n{\nArray\n}\nArray of accessible tables\n*/\nfunction\ncheckTableAccess\n(\nroleToCheck\n,\ntables\n)\n{\nconst\naccessibleTables\n=\n[\n]\n;\nfor\n(\nconst\ntable\nof\ntables\n)\n{\ntry\n{\nconst\nparts\n=\ntable\n.\nsplit\n(\n'.'\n)\n;\nif\n(\nparts\n.\nlength\n!==\n3\n)\n{\nthrow\nnew\nError\n(\n`\nInvalid table format: '\n${\ntable\n}\n'. Expected format: database.schema.table\n`\n)\n;\n}\nconst\n[\ndatabase\n,\nschema\n,\ntableName\n]\n=\nparts\n;\nconst\nquery\n=\n`\nSELECT PRIVILEGE_TYPE\nFROM\n${\ndatabase\n}\n.INFORMATION_SCHEMA.OBJECT_PRIVILEGES\nWHERE GRANTEE = '\n${\nroleToCheck\n}\n'\nAND OBJECT_TYPE IN ('TABLE', 'VIEW')\nAND OBJECT_CATALOG = '\n${\ndatabase\n}\n'\nAND OBJECT_SCHEMA = '\n${\nschema\n}\n'\nAND OBJECT_NAME = '\n${\ntableName\n}\n'\n`\n;\nconst\nresult\n=\nexecuteQuery\n(\nquery\n,\n[\n]\n)\n;\nif\n(\nresult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\nresult\n.\nmessage\n)\n;\n}\nwhile\n(\nresult\n.\nresult\n.\nnext\n(\n)\n)\n{\nconst\nprivilege\n=\nresult\n.\nresult\n.\ngetColumnValue\n(\n\"PRIVILEGE_TYPE\"\n)\n;\nif\n(\nprivilege\n===\n\"SELECT\"\n||\nprivilege\n===\n\"OWNERSHIP\"\n)\n{\naccessibleTables\n.\npush\n(\ntable\n)\n;\nbreak\n;\n}\n}\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\nFailed to check permissions for table '\n${\ntable\n}\n'. Role '\n${\nroleToCheck\n}\n' may not have access or the table may not exist.\n`\n)\n;\n}\n}\nreturn\naccessibleTables\n;\n}\n/**\n* Validates SQL permissions for a given role and returns accessible objects\n*\n@param\n{\nstring\n}\nsql\n- SQL to validate\n*\n@param\n{\nstring\n}\nroleToCheck\n- Role to check permissions for\n*\n@param\n{\nstring\n}\ndatabasesToCheck\n- Comma-separated list of databases to check access for\n*\n@param\n{\nstring\n}\nschemasToCheck\n- Comma-separated list of schemas to check access for\n*\n@param\n{\nstring\n}\ntablesToCheck\n- Comma-separated list of tables to check access for\n*\n@returns\n{\nObject\n}\nObject with validation result and accessible objects\n*\n@throws\n{\nError\n}\nIf SQL validation fails\n*/\nfunction\nvalidateSqlPermissions\n(\nsql\n,\nroleToCheck\n,\ndatabasesToCheck\n,\nschemasToCheck\n,\ntablesToCheck\n)\n{\ntry\n{\n// Step 1: Run EXPLAIN command to validate SQL syntax and plan\nconst\nexplainSql\n=\n`\nEXPLAIN\n${\nsql\n}\n`\n;\nconst\nexplainResult\n=\nexecuteQuery\n(\nexplainSql\n,\n[\n]\n)\n;\nif\n(\nexplainResult\n.\nisErrored\n)\n{\nthrow\nnew\nError\n(\n\"We couldn't validate your query. Please check your SQL syntax and permissions and try again.\"\n)\n;\n}\n// Step 2: Parse objects to check\nconst\nobjectsToCheck\n=\nparseCommaSeparatedLists\n(\ndatabasesToCheck\n,\nschemasToCheck\n,\ntablesToCheck\n)\n;\n// Step 3: Check access for each object type\nconst\naccessibleDatabases\n=\ncheckDatabaseAccess\n(\nroleToCheck\n,\nobjectsToCheck\n.\ndatabases\n)\n;\nconst\naccessibleSchemas\n=\ncheckSchemaAccess\n(\nroleToCheck\n,\nobjectsToCheck\n.\nschemas\n)\n;\nconst\naccessibleTables\n=\ncheckTableAccess\n(\nroleToCheck\n,\nobjectsToCheck\n.\ntables\n)\n;\nreturn\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n\"SQL permissions validation successful\"\n,\naccessibleObjects\n:\n{\ndatabases\n:\naccessibleDatabases\n,\nschemas\n:\naccessibleSchemas\n,\ntables\n:\naccessibleTables\n}\n}\n;\n}\ncatch\n(\nerr\n)\n{\nthrow\nnew\nError\n(\n`\n${\nerr\n.\nmessage\n}\n`\n)\n;\n}\n}\n/**\n* Main function to manage DMF operations\n* Validates all arguments and executes the main logic\n*\n@returns\n{\nstring\n}\nJSON string with operation status and result message\n*\n@throws\n{\nError\n}\nIf any operation step fails\n*/\nfunction\nmain\n(\n)\n{\nvalidateAllArguments\n(\n)\n;\n// Handle permissions for DMF attachment/detachment operations\nif\n(\n[\n\"ATTACH_DMF\"\n,\n\"DETACH_DMF\"\n,\n\"SUSPEND_DMF\"\n,\n\"RESUME_DMF\"\n]\n.\nincludes\n(\nACTION\n)\n)\n{\nconst\npermissionResult\n=\nhandleDMFPermissions\n(\nENTITY_NAME\n)\n;\nif\n(\n!\npermissionResult\n.\nisSuccessful\n)\n{\nreturn\nJSON\n.\nstringify\n(\npermissionResult\n)\n;\n}\n}\n// If the provided arguments are valid, proceed with the main logic\nconst\ndmfArguments\n=\ngenerateDMFColumnArguments\n(\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n)\n;\nconst\nSQL_TEMPLATES\n=\n{\nATTACH_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nADD DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n)\n`\n,\nDETACH_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nDROP DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n)\n`\n,\nSUSPEND_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nMODIFY DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n) SUSPEND\n`\n,\nRESUME_DMF\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nMODIFY DATA METRIC FUNCTION\n${\nDMF_NAME\n}\nON (\n${\ndmfArguments\n}\n) RESUME\n`\n,\nUPDATE_SCHEDULE\n:\n{\nMINUTES\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nSET DATA_METRIC_SCHEDULE = '\n${\nSCHEDULE_VALUE\n}\nMINUTE'\n`\n,\nCRON\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nSET DATA_METRIC_SCHEDULE = 'USING CRON\n${\nSCHEDULE_VALUE\n}\n'\n`\n,\nON_DATA_CHANGE\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nSET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES'\n`\n,\nNOT_SCHEDULED\n:\n`\nALTER\n${\nENTITY_TYPE\n}\n${\nENTITY_NAME\n}\nUNSET DATA_METRIC_SCHEDULE\n`\n,\n}\n,\n}\n;\nlet\nsqlText\n=\n\"\"\n;\nlet\nreturnMessage\n=\n\"\"\n;\nlet\nbinds\n=\n[\n]\n;\nif\n(\nACTION\n===\n\"UPDATE_SCHEDULE\"\n)\n{\nsqlText\n=\nSQL_TEMPLATES\n[\nACTION\n]\n[\nSCHEDULE_TYPE\n]\n;\nreturnMessage\n=\n`\nSuccessfully updated schedule for\n${\nENTITY_NAME\n}\nto\n${\nSCHEDULE_TYPE\n}\n${\nSCHEDULE_VALUE\n}\n`\n;\n}\nelse\nif\n(\nACTION\n===\n\"CREATE_DMF\"\n)\n{\nconst\nDOLLAR\n=\nString\n.\nfromCharCode\n(\n36\n)\n;\n// ASCII code for $\nconst\ndmfArguments\n=\nsafelyParseJSON\n(\nDMF_ARGUMENTS_JSON\n)\n;\nconst\nfunctionParams\n=\ngenerateFunctionParameters\n(\ndmfArguments\n)\n;\nsqlText\n=\n\"CREATE OR REPLACE DATA METRIC FUNCTION \"\n+\nDMF_NAME\n+\n\" (\"\n+\nfunctionParams\n+\n\" )\"\n+\n\"RETURNS NUMBER AS \"\n+\nDOLLAR\n+\nDOLLAR\n+\n\" \"\n+\nDMF_DEFINITION\n+\n\" \"\n+\nDOLLAR\n+\nDOLLAR\n;\nreturnMessage\n=\n`\nDMF\n${\nDMF_NAME\n}\nregistered successfully\n`\n;\n}\nelse\nif\n(\nACTION\n===\n\"EXECUTE_SQL\"\n)\n{\n// Execute SQL and get numeric result\nconst\nresult\n=\nexecuteSqlAndReturnNumber\n(\nDMF_DEFINITION\n)\n;\nconst\nresponse\n=\n{\nisSuccessful\n:\ntrue\n,\nmessage\n:\n\"SQL executed successfully\"\n,\nresult\n:\nresult\n}\n;\nreturn\nJSON\n.\nstringify\n(\nresponse\n)\n;\n}\nelse\nif\n(\nACTION\n===\n\"VALIDATE_SQL_PERMISSIONS\"\n)\n{\nconst\nvalidationResult\n=\nvalidateSqlPermissions\n(\nDMF_DEFINITION\n,\nROLE_TO_CHECK\n,\nDATABASES_TO_CHECK\n,\nSCHEMAS_TO_CHECK\n,\nTABLES_TO_CHECK\n)\n;\nreturn\nJSON\n.\nstringify\n(\nvalidationResult\n)\n;\n}\nelse\n{\nsqlText\n=\nSQL_TEMPLATES\n[\nACTION\n]\n;\nreturnMessage\n=\n`\nACTION:\n${\nACTION\n}\nperformed successfully on\n${\nENTITY_NAME\n}\nwith DMF:\n${\nDMF_NAME\n}\nand DMF Arguments:\n${\ndmfArguments\n}\n`\n;\n}\nconst\nresult\n=\nexecuteQuery\n(\nsqlText\n,\nbinds\n)\n;\nreturn\nJSON\n.\nstringify\n(\n{\nisSuccessful\n:\n!\nresult\n.\nisErrored\n,\nmessage\n:\nresult\n.\nisErrored\n?\nresult\n.\nmessage\n:\nreturnMessage\n,\n}\n)\n;\n}\n// Execute the main function and return the result\ntry\n{\nreturn\nmain\n(\n)\n;\n}\ncatch\n(\nerr\n)\n{\nreturn\nJSON\n.\nstringify\n(\n{\nisSuccessful\n:\nfalse\n,\nmessage\n:\nerr\n.\nmessage\n,\n}\n)\n;\n}\n$$\n;\nGrant usage to procedure and schema to the service role:\nGRANT\nUSAGE\nON\nPROCEDURE\nATLAN_DQ\n.\nSHARED\n.\nMANAGE_DMF\n(\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n,\nSTRING\n)\nTO\nROLE atlan_dq_service_role\n;\nGRANT\nUSAGE\nON\nSCHEMA\nATLAN_DQ\n.\nDMFS\nTO\nROLE atlan_dq_service_role\n;\nNeed help\nâ\nIf you have questions or need assistance with migrating your Snowflake data quality setup, reach out to Atlan Support by\nsubmitting a support request\n.\nSee also\nâ\nData quality permissions\n- Review the permissions required for the upgraded setup\nTags:\nsnowflake\ndata-quality\nmigration\ngovernance\nPrevious\nEnable auto re-attachment of rules\nNext\nOperations\nPrerequisites\nPermissions required\nUpgrade data quality setup\nNeed help\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/concepts/auto-re-attachment-rules",
    "content": "Build governance\nData Quality Studio\nConcepts\nWhat's auto re-attachment\nOn this page\nWhat's auto re-attachment\nIn modern data environments, pipelines (such as dbt) often drop and recreate tables or views as part of routine execution. This behavior can unintentionally detach existing data quality rules. To address this, Atlan supports automatic re-attachment of rules to assets, ensuring continuous enforcement of data quality.\nAuto re-attachment is a capability that enables Atlan to automatically re-link existing data quality rules to assets that have been dropped and recreated with the same name and structure. This prevents rules from being silently lost when underlying assets are refreshed by ELT pipelines.\nImportant!\nAuto re-attachment is currently available for Snowflake only.\nWhy it matters\nâ\nWhen an asset is dropped and recreated, any associated data quality rules may be lost unless they're manually reattached. This can lead to gaps in data quality enforcement and loss of historical context.\nWithout auto re-attachment:\nRules become inactive after the asset is recreated.\nUsers must manually reattach rules.\nHistorical rule execution and incident data become disconnected.\nWith auto re-attachment:\nRules are automatically linked to the new version of the asset.\nNo manual intervention is required.\nHistorical context is preserved.\nHow it works\nâ\nOnce enabled, Atlan continuously monitors for asset recreations. When a table or view is recreated:\nAtlan checks whether the asset matches a previously existing one based on name and structure.\nIf a match is found, any undeleted rules associated with the old asset are automatically reattached.\nRules are reapplied to the asset to resume enforcement.\nExecution history and incident tracking continue seamlessly.\nTip:\nReattachment typically occurs within a few minutes of asset recreation, provided the asset name and structure are unchanged and the original rule still exists.\nNext steps\nâ\nTo get started, explore:\nEnable auto re-attachment rules in Snowflake\n: Learn how to configure permissions and trigger automatic rule reattachment.\nTags:\ndata-quality\nauto-re-attachment\nPrevious\nConfigure alerts\nWhy it matters\nHow it works\nNext steps"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/concepts/data-quality-studio",
    "content": "Build governance\nData Quality Studio\nWhat is Data Quality Studio\nOn this page\nWhat's Data Quality Studio\nâ\nAvailable via the Data Quality Studio package\nData Quality Studio is Atlan's native data quality module that enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations directly within the Atlan platform.\nWhy it exists\nâ\nData teams often rely on disconnected scripts or tools to define and run quality checks. These are typically siloed, difficult to maintain, and fail to deliver visibility to business users. This leads to:\nBlind spots in data pipelines\nDelayed issue detection\nLack of trust across the organization\nData Quality Studio bridges these gaps by embedding data quality into your warehouse and surfacing trust signals across Atlan, where your teams already work.\nWhat Data Quality Studio enables\nâ\nWith Data Quality Studio, you can:\nDefine expectations\nabout your data using familiar SQL logic\nExecute checks\nwhere your data lives, directly in the warehouse\nSurface trust\nacross Atlan through warnings, trust scores, and notifications\nThis helps build a proactive, transparent culture of data trust across your organization.\nWho is it for\nâ\nData Quality Studio is designed for:\nAnalytics engineers\nwho own data transformation pipelines\nData stewards\nresponsible for data quality and governance\nBusiness users\nwho need visibility into data they can trust\nEach persona benefits from embedded checks, alerts, and transparency across the data lifecycle.\nCore mental model\nâ\nTo understand how Data Quality Studio works, here are some key terms:\nRule\n: A SQL-based expectation about your data\nRule set\n: A group of related rules, typically applied to a table or dataset\nCheck run\n: Execution of rules in your warehouse\nStatus\n: The result of a checkâpassed, failed, or warning\nTrust signals\n: Visual indicators and alerts shown in Atlan\nThese concepts form the foundation of how data quality is evaluated and shared.\nHow it works\nâ\nData Quality Studio uses a push-down execution model. Rules are defined through Atlanâs interface and executed natively in your data warehouse without needing additional infrastructure.\nThe flow looks like this:\nDefine rule sets\nusing SQL logic that reflects your data expectations\nPush execution to your warehouse\ntriggers native compute in your environment:\nSnowflake: Executes rules via Data Metric Functions (DMFs)\nDatabricks: Leverages Delta Live Tables for rule execution\nCapture results\nas check runs with pass, fail, or warning statuses\nSurface signals\nthrough trust scores, visual indicators, and metadata in Atlan\nNotify users\nusing alerts and integrations when checks fail or thresholds are breached\nThis system ensures quality checks are versioned, repeatable, and integrated into your data stack.\nKey capabilities\nâ\nThese are the core capabilities that power the system:\nSQL-based rule authoring\nVersioned execution and history tracking\nMulti-rule validation per dataset\nAlerting and integrations with downstream tools\nTrust scoring and visual feedback in Atlan\nQuery-based diagnostics for failed rules\nCentralized rule management\nThese features combine to help teams operationalize trust across every dataset.\nNext steps\nâ\nTo get started, explore:\nSnowflake data quality setup guide\n: Learn how to define, execute, and manage rule sets natively in Snowflake\nDatabricks data quality setup guide\n: Set up and run rule sets using Delta Live Tables in Databricks\nSee also\nâ\nRules and dimensions reference\n: Explore all supported rule types, dimensions, and examples\nAdvanced configuration\n: Set up notifications for failed rules, thresholds, and more\nTags:\ndata-quality\ngovernance\nmonitoring\nrules\nNext\nData quality permissions\nWhy it exists\nWhat Data Quality Studio enables\nWho is it for\nCore mental model\nHow it works\nKey capabilities\nNext steps\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/claude-remote-mcp",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nRemote MCP\nClaude with Remote MCP\nOn this page\nSet up Claude with Remote MCP\nPrivate Preview\nYou can connect Claude with the Atlan Remote MCP server to access Atlan metadata in your conversations or desktop environment. This enables you to search for assets, explore lineage, update metadata, create glossary terms, and more directly from Claude.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAn Atlan tenant with Remote MCP enabled. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nClaude Connector\nor\nClaude Desktop\ninstalled and updated to the latest version.\nSet up Claude connector\nâ\nYou can connect Claude Connector to Atlan Remote MCP using OAuth, which enables users to log in with their Atlan account and use their existing roles and permissions. API Key authentication isn't supported for Claude Connector.\nIn Claude, navigate to\nSettings\nâ\nConnectors\nâ\nOrganisation Connector\nâ\nAdd custom connector\n, and provide the\nMCP server URL\n. This step must be completed by an administrator.\nhttps://<your-tenant>.atlan.com/mcp\nSave the connector.\nIn the Chat UI, go to\nSearch and Tools â Add Custom Connector â Your Connector\n.\nSelect the Atlan connector and complete the OAuth login flow. Once you complete authentication, the connector is ready to use.\nYou can now use Atlan MCP tools directly in your Claude conversations.\nSet up Claude Desktop\nâ\nClaude Desktop integrates with Remote MCP through the\nmcp-remote\nnpm package. This provides a way to connect Claude Desktop with Atlan using either OAuth or API Key authentication.\nYou can set up Claude Desktop with Remote MCP if you want to access Atlan metadata directly from your desktop environment, without switching to the web interface. This is useful for developers and analysts who prefer working locally while still leveraging Atlanâs metadata context.\nOAuth\nAPI Key\nOpen Claude Desktop and go to\nSettings â Developer â Edit Config\n.\nAdd the following configuration:\n{\n\"mcpServers\"\n:\n{\n\"atlan\"\n:\n{\n\"command\"\n:\n\"npx\"\n,\n\"args\"\n:\n[\n\"mcp-remote\"\n,\n\"https://<your-tenant>.atlan.com/mcp\"\n]\n}\n}\n}\nRestart Claude Desktop.\nA login prompt appears to complete authentication. After login, Claude Desktop connects to the Remote MCP server using your Atlan roles and permissions.\nYou can now use Atlan MCP tools inside Claude Desktop.\nIn Atlan, generate an API key from\nAdmin Settings â API Keys / Tokens\nand copy it securely.\nOnce the API key is generated, open Claude Desktop and go to\nSettings â Developer â Edit Config\n.\nAdd the following configuration:\n{\n\"mcpServers\"\n:\n{\n\"atlan\"\n:\n{\n\"command\"\n:\n\"npx\"\n,\n\"args\"\n:\n[\n\"mcp-remote\"\n,\n\"https://<your-tenant>.atlan.com/mcp/api-key\"\n,\n\"--header\"\n,\n\"Authorization: <your-api-key>\"\n]\n}\n}\n}\nRestart Claude Desktop. The server is available with API Key authentication.\nYou can now use Atlan MCP tools inside Claude Desktop.\nTroubleshooting\nâ\nIf you encounter\nInvalidGrantError: Code not valid\n, this typically indicates that OAuth was retried too quickly. Quit Claude Desktop, wait 5 minutes, and then reopen the application.\nIf tools stop functioning after Claude Desktop has been open for an extended period, restart the application, the authentication token may have expired and not refreshed correctly.\nIf you have any issues while configuring Claude Connector or Claude Desktop, contact\nAtlan Support\nfor assistance.\nSee also\nâ\nSet up the Atlan MCP server locally\nAtlan MCP tools\nTags:\nAtlan MCP\nremote\nClaude\nsetup\nPrevious\nCursor with Remote MCP\nNext\nn8n with Remote MCP\nPrerequisites\nSet up Claude connector\nSet up Claude Desktop\nTroubleshooting\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/cursor-remote-mcp",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nRemote MCP\nCursor with Remote MCP\nOn this page\nSet up Cursor with Remote MCP\nPrivate Preview\nYou can connect Cursor, an AI-powered editor, with the Atlan Remote MCP server. This lets you search for assets, explore lineage, update metadata, create glossary terms, and more directly from your development environment.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAn Atlan tenant with Remote MCP enabled. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nCursor\ninstalled and updated to the latest version.\nSet up Remote MCP\nâ\nYou can connect Cursor to the Atlan Remote MCP server using either OAuth or an API key. Choose the method that best fits your use case.\nOAuth\nAPI Key\nYou can set up OAuth if you want Cursor to connect using your Atlan login and permissions.\nIn Cursor, go to\nSettings â Cursor Settings â Tools and Integration\nfrom the left-hand panel.\nClick\nNew MCP Server\nand add the configuration:\n{\n\"mcpServers\"\n:\n{\n\"Atlan\"\n:\n{\n\"url\"\n:\n\"https://<your-tenant>.atlan.com/mcp\"\n}\n}\n}\nSave and return. Cursor prompts you to log in with your Atlan account.\nAfter selecting log in, you are redirected to the Atlan login page (SSO is supported). Once you complete the login, Cursor connects to the Remote MCP server using your roles and permissions.\nAfter setup, you can use MCP tools directly inside Cursor. For more information, see the list of\nAtlan MCP tools\n.\nYou can set up API Key authentication if you want Cursor to connect using a static token for automation or service-to-service use cases.\nIn Atlan, generate an API key by going to\nAdmin Settings â API Keys / Tokens\n.\nOnce the API key is generated, open Cursor and navigate to\nSettings â Cursor Settings â Tools and Integration\nfrom the left-hand panel.\nClick\nNew MCP Server\nand add the configuration:\n{\n\"mcpServers\"\n:\n{\n\"Atlan MCP Remote API Key\"\n:\n{\n\"url\"\n:\n\"https://<your-tenant>.atlan.com/mcp/api-key\"\n,\n\"headers\"\n:\n{\n\"Authorization\"\n:\n\"<your-api-key>\"\n}\n}\n}\n}\nSave the configuration. The server is ready with API Key authentication, and Cursor connects to Atlan metadata through Remote MCP.\nAfter setup, you can use MCP tools directly inside Cursor. For more information, see the list of\nAtlan MCP tools\n.\nNeed help?\nâ\nIf you have any issues while configuring the integration, contact\nAtlan Support\nfor assistance.\nSee also\nâ\nSet up the Atlan MCP server locally\nAtlan MCP tools\nTags:\nAtlan MCP\nremote\nCursor\nsetup\nPrevious\nRemote MCP\nNext\nClaude with Remote MCP\nPrerequisites\nSet up Remote MCP\nNeed help?\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/implement-the-atlan-mcp-server",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nLocal MCP\nSet up Local MCP Server\nOn this page\nSet up Local MCP Server\nThe\nModel Context Protocol\n(MCP) is an open standard that enables AI agents to access contextual metadata from external systems.\nAtlan provides a reference implementation of MCP through the\nAtlan MCP server\n. This server acts as a bridge between Atlanâs metadata platform and AI tools such as Claude and Cursor.\nYou can use the Atlan MCP server to support AI-driven use cases like searching for assets, understanding lineage, or updating metadata, all using real-time context from Atlan.\nGet started\nâ\nThe Atlan MCP server can be configured in multiple environments depending on your preferred development setup and integration target. Follow the instructions below to set up the server with your desired tool:\nCursor\nClaude\nLocal development\nSet up the Atlan MCP server in Cursor:\nUsing uv\n:\nuv\nis a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker.\nUsing Docker\n: Use Docker to run the MCP server in an isolated, containerized environment.\nSet up the Atlan MCP server in Claude Desktop:\nUsing uv\n:\nuv\nis a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker.\nUsing Docker\n: Use Docker to run the MCP server in an isolated, containerized environment.\nBuild and run the Atlan MCP server locally:\nLocal build guide\n: Walkthrough for cloning, building, and running the server locally.\nNeed help?\nâ\nFor troubleshooting and feature requests, see the\nGitHub repo\n.\nContact\nAtlan support\nfor help with setup or integration.\nSee also\nâ\nAtlan MCP Server README on GitHub\nTags:\nAtlan MCP\nsetup\nmodel\nPrevious\nMicrosoft Copilot Studio with Remote MCP\nNext\nHow to use Atlan AI for documentation\nGet started\nNeed help?\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/copilot-remote-mcp",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nRemote MCP\nMicrosoft Copilot Studio with Remote MCP\nOn this page\nSet up Microsoft Copilot Studio with Remote MCP\nPrivate Preview\nYou can connect Microsoft Copilot Studio with the Atlan Remote MCP server to enable metadata-powered experiences.\nCopilot Studio supports API Key authentication only. OAuth isn't available.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAn Atlan tenant with Remote MCP enabled. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nMicrosoft Copilot Studio access with permissions to create connectors and agents.\nCreate a custom connector\nâ\nIn Atlan, generate an API key by going to\nAdmin Settings â API Keys / Tokens\n.\nIn Microsoft Copilot Studio, go to\nTools\n, and select\nCustom Connector\n.\nChoose\nCreate From Blank\nand add a name for the connector.\nIn the connector, go to the\nGeneral Information\ntab and then open the\nSwagger Editor\ntab.\nAdd the following Swagger YAML in the editor:\nswagger\n:\n'2.0'\ninfo\n:\ntitle\n:\nAtlan\ndescription\n:\nAtlan ModelContextProtocol server for copilot testing\nversion\n:\n1.0.0\nhost\n:\n<your\n-\ntenant\n>\n.atlan.com\nbasePath\n:\n/\nschemes\n:\n-\nhttps\npaths\n:\n/mcp/api-key\n:\npost\n:\nsummary\n:\nAtlan copilot mcp server\nx-ms-agentic-protocol\n:\nmcp\n-\nstreamable\n-\n1.0\nconsumes\n:\n-\napplication/json\noperationId\n:\nInvokeMCP\nresponses\n:\n'200'\n:\ndescription\n:\nSuccess\nparameters\n:\n-\nname\n:\nAccept\nin\n:\nheader\nrequired\n:\ntrue\ntype\n:\nstring\nenum\n:\n-\ntext/event\n-\nstream\n,\napplication/json\ndefault\n:\ntext/event\n-\nstream\n,\napplication/json\nsecurityDefinitions\n:\napi_key\n:\ntype\n:\napiKey\nin\n:\nheader\nname\n:\nAuthorization\nsecurity\n:\n-\napi_key\n:\n[\n]\nAfter adding the Swagger YAML, click\nCreate Connector\nand wait for it to update.\nCreate and configure an agent\nâ\nAfter setting up the custom connector, you now need to create an agent in Copilot Studio and link it to the connector so it can access Atlanâs MCP tools.\nIn Copilot Studio, go to\nAgents\n.\nCreate a new agent by adding a name and description, then click\nCreate\n.\nIn the new agent, go to the\nTools\ntab and click\nAdd a Tool\n.\nSearch for the custom connector you created earlier and add it.\nIn\nConnection\n, choose\nCreate New Connection\nand add the\nAPI key\nfor your tenant.\nClick\nAdd to Agent\n. Then click on the newly added tool name.\nReload inputs and from the options dropdown choose:\ntext/event-stream, application/json\nGo to\nTools\nand reload. You'll now see all the tools available.\nOpen the chatbot panel and run a query. If prompted to connect, use\nConnection Manager\nto create or select a connection and submit. Once connected, queries run successfully through Atlan MCP.\nFor more information, see the list of\nAtlan MCP tools\n.\nNeed help?\nâ\nIf you have any issues while configuring Microsoft Copilot Studio with Remote MCP, contact\nAtlan Support\nfor assistance.\nTags:\nAtlan MCP\nremote\nMicrosoft Copilot Studio\nsetup\nPrevious\nWindsurf with Remote MCP\nNext\nSet up Local MCP Server\nPrerequisites\nCreate a custom connector\nCreate and configure an agent\nNeed help?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/n8n-remote-mcp",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nRemote MCP\nn8n with Remote MCP\nOn this page\nSet up n8n with Remote MCP\nPrivate Preview\nYou can integrate\nn8n\nwith the Atlan Remote MCP server to use Atlan metadata within your automated workflows. Authentication in n8n is supported only through API keys, OAuth isn't available.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAn Atlan tenant with Remote MCP enabled. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nn8n installed and updated to the latest version.\nn8n AI Agent node and LiteLLM\nn8n doesn't include a native LiteLLM node for the Agent chat model. Community LiteLLM nodes may fail or return unexpected responses. If you need LiteLLM models, use the\nOpenAI Chat Model node\nwith the Atlan gateway as the base URL and your Atlan API key.\nSet up Remote MCP in n8n\nâ\nYou can configure n8n with the Atlan Remote MCP server using API Key authentication.\nIn Atlan, generate an API key by going to\nAdmin Settings â API Keys / Tokens\n.\nIn n8n, create a new\nWorkflow\n.\nAdd an\nMCP Client\nnode to the workflow.\nIn the MCP Client node, choose the action\nList Tools\n. This enables you to test the connection and see available tools.\nUnder\nCredentials\n, select\nCreate New Credentials\nand fill the details:\nHTTP Stream URL\n: The endpoint for Remote MCP with API Key.\nhttps://<your-tenant>.atlan.com/mcp/api-key\nHTTP Connection Timeout\n: Time in milliseconds to wait for a response.\n60000\nAdditional Headers\n: Add the API key for authentication.\nAuthorization = <your-api-key>\nSave the credentials. When you execute the node with\nList Tools\n, you see all the MCP tools available in your tenant.\nTo run a specific tool:\nIn the MCP Client node, change\nOperation\nto\nExecute Tool\n.\nSelect the tool you want to use (for example,\nsearch_assets\n).\nFill in the required parameters in the node configuration.\nFor example, you can configure the node to search for ten tables in Snowflake by setting the query to\ntables\n, the limit to\n10\n, and selecting the Snowflake connection.\nNeed help?\nâ\nIf you have any issues while configuring n8n with Remote MCP, contact\nAtlan Support\nfor assistance.\nTags:\nAtlan MCP\nremote\nn8n\nsetup\nPrevious\nClaude with Remote MCP\nNext\nWindsurf with Remote MCP\nPrerequisites\nSet up Remote MCP in n8n\nNeed help?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/windsurf-remote-mcp",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nRemote MCP\nWindsurf with Remote MCP\nOn this page\nSet up Windsurf with Remote MCP\nPrivate Preview\nYou can connect Windsurf with the Atlan Remote MCP server to bring Atlan metadata into your AI-driven flows. Windsurf supports API Key authentication, OAuth isn't available.\nPrerequisites\nâ\nBefore you begin, make sure you have:\nAn Atlan tenant with Remote MCP enabled. If you don't have access, contact\nAtlan support\nor your Atlan customer team to request it.\nWindsurf\ninstalled and updated to the latest version.\nSet up Remote MCP in Windsurf\nâ\nYou can set up Windsurf with the Remote MCP server using API Key authentication.\nIn Atlan, generate an API key by going to\nAdmin Settings â API Keys / Tokens\n, creating a new key, and copying it securely.\nFor Remote MCP with API Key, use the following endpoint:\nhttps://<your-tenant>.atlan.com/mcp/api-key\nIn Windsurf, open the MCP configuration file and add the following configuration:\n{\n\"mcpServers\"\n:\n{\n\"Atlan MCP Remote\"\n:\n{\n\"serverUrl\"\n:\n\"https://<your-tenant>.atlan.com/mcp/api-key\"\n,\n\"headers\"\n:\n{\n\"Authorization\"\n:\n\"<your-api-key>\"\n}\n}\n}\n}\nSave the configuration. Windsurf is now connected to Atlan Remote MCP with API Key authentication.\nAfter setup, you can use MCP tools directly inside Windsurf. For more information, see the list of\nAtlan MCP tools\n.\nNeed help?\nâ\nIf you have any issues while configuring Windsurf with Remote MCP, contact\nAtlan Support\nfor assistance.\nTags:\nAtlan MCP\nremote\nWindsurf\nsetup\nPrevious\nn8n with Remote MCP\nNext\nMicrosoft Copilot Studio with Remote MCP\nPrerequisites\nSet up Remote MCP in Windsurf\nNeed help?"
  },
  {
    "url": "https://docs.atlan.com/faq",
    "content": "Configure Atlan\nFrequently Asked Questions\nOn this page\nFrequently Asked Questions\nOverview:\nGet quick answers to frequently asked questions about Atlan's features, functionality, and common use cases. Browse by category to quickly locate solutions and learn best practices for getting the most out of your Atlan workspace.\nCore platform\nâ\nFind answers efficiently by exploring these main FAQ categories:\nAdministration and user management\nâ\nAdministration and configuration\n: Comprehensive guide to Atlan administration, from workspace configuration to system management and troubleshooting\nUser management and access control\n: Managing users, groups, roles, permissions, and access control policies in Atlan\nCapabilities\nâ\nDiscovery\n: Filtering, search, and navigation tips in Atlan\nInsights\n: Query limits, scheduling, and Visual Query FAQs\nReporting\n: Materialized views, dashboard metadata, and more\nLineage\n: Supported lineage types, troubleshooting, and best practices\nCustom metadata\n: Managing custom metadata sets and attributes\nGlossary\n: Glossary setup, backups, permissions, and term management\nData management\nâ\nData connections and integration\n: Everything about connecting and integrating various data sources with Atlan, including configuration and troubleshooting\nTags and metadata management\n: Complete guide to managing tags, classifications, and metadata in your data catalog\nWorkflows and data processing\n: Data processing workflows, automation, scheduling, and optimization techniques\nIntegrations and connectors\nâ\nBrowser extension integration\n: Installing, troubleshooting, and using Atlan's browser extension\nSingle sign-on (SSO) integration\n: Configuring and managing SSO providers\nJira integration\n: Creating issues, field mappings, and common troubleshooting\nSalesforce connector\n: Prerequisites, metadata extraction, and common issues\nMicrosoft Power BI connector\n: Data sources vs dataflows and other connector specifics\nConnector framework FAQs\n: General questions around connectivity capabilities and supported sources\nMicrosoft Teams integration\n: Linking channels, notifications, and common issues\nSlack integration\n: Sharing assets, permissions, and troubleshooting\nAdvanced features\nâ\nAI and automation features\n: Guide to Atlan's AI capabilities and automation features for enhanced data operations\nSecurity and compliance\n: Security protocols, compliance standards, and data protection measures in Atlan\nSupport\nâ\nNeed additional help beyond these FAQs?\nContact support\n: Reach out to the support team through the Atlan platform or\nraise a ticket\n.\nTags:\nfaq\nhelp\nsupport\ntroubleshooting\nguides\nNext\nAdministration and Configuration\nCore platform\nSupport"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/troubleshooting/troubleshooting-fivetran-connectivity",
    "content": "Connect data\nETL Tools\nFivetran\nTroubleshooting\nTroubleshooting Fivetran connectivity\nOn this page\nTroubleshooting Fivetran connectivity\nWhat are the known limitations of the Fivetran connector?\nâ\nAtlan currently doesn't support the following:\nThere are two types of\nalerts in Fivetran\n-  errors and warnings. While errors are supported with the Fivetran Platform Connector package, warnings are currently not supported.\nWhy is lineage missing?\nâ\nIf you notice any Fivetran lineage missing in Atlan after migrating to the Fivetran Platform Connector in both Fivetran and Atlan, following may be the possible causes:\nIf any Fivetran Platform Connector tables or schema are dropped or renamed in the destination warehouse, then Fivetran recreates them with the original names in the next sync. However, it only adds any new incremental data. To access the historical data or fix any data integrity issues, you may need to trigger a re-sync. Refer to\nFivetran documentation\nto learn more.\nWhy are partial assets created instead of connecting lineage to existing assets or Why does lineage show up wrong across environments?\nâ\nIf you notice incorrect lineage such as a partial asset being created when a catalogued asset already exists in Atlan, or incorrect lineage across different environments, please contact Fivetran team to enable \"connection_details\" private preview feature. This feature gives Atlan the host and database information needed to accurately stitch together the end-to-end lineage.\nWhy is click-through to Fivetran not working?\nâ\nIf you click\nView in Fivetran\nin Atlan and you are redirected to the Fivetran homepage, then you may not have the necessary permissions in Fivetran to view the connector. You must have at least a\nConnector Reviewer\nrole\nor any other role hierarchically to view Fivetran connectors.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nalerts\nmonitoring\nnotifications\nPrevious\nWhat does Atlan crawl from Fivetran?"
  },
  {
    "url": "https://docs.atlan.com/tags/dashboards",
    "content": "6 docs tagged with \"dashboards\"\nView all tags\nAllow members to view reports\nPermission to view the [governance](/product/capabilities/reporting/how-tos/report-on-governance) and [automations](/product/capabilities/reporting/how-tos/report-on-automations) dashboards in the reporting center is reserved for admin users only.\nSecurity monitoring\nLearn about security monitoring.\nTroubleshooting Looker connectivity\nLearn about troubleshooting looker connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity.\nTroubleshooting Salesforce connectivity\nLearn about troubleshooting salesforce connectivity.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase."
  },
  {
    "url": "https://docs.atlan.com/tags/visualization",
    "content": "6 docs tagged with \"visualization\"\nView all tags\nAllow members to view reports\nPermission to view the [governance](/product/capabilities/reporting/how-tos/report-on-governance) and [automations](/product/capabilities/reporting/how-tos/report-on-automations) dashboards in the reporting center is reserved for admin users only.\nSecurity monitoring\nLearn about security monitoring.\nTroubleshooting Looker connectivity\nLearn about troubleshooting looker connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity.\nTroubleshooting Salesforce connectivity\nLearn about troubleshooting salesforce connectivity.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase."
  },
  {
    "url": "https://docs.atlan.com/tags/analytics",
    "content": "8 docs tagged with \"analytics\"\nView all tags\nAllow members to view reports\nPermission to view the [governance](/product/capabilities/reporting/how-tos/report-on-governance) and [automations](/product/capabilities/reporting/how-tos/report-on-automations) dashboards in the reporting center is reserved for admin users only.\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan.\nReporting\nGenerate comprehensive reports on your data assets, usage, and governance.\nSecurity monitoring\nLearn about security monitoring.\nTroubleshooting Looker connectivity\nLearn about troubleshooting looker connectivity.\nTroubleshooting Redash connectivity\nLearn about troubleshooting redash connectivity.\nTroubleshooting Salesforce connectivity\nLearn about troubleshooting salesforce connectivity.\nWhat does Atlan crawl from Metabase?\nAtlan crawls and maps the following assets and properties from Metabase."
  },
  {
    "url": "https://docs.atlan.com/faq/administration-and-configuration",
    "content": "Configure Atlan\nFrequently Asked Questions\nAdministration and Configuration\nOn this page\nAdministration and Configuration\nComplete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization.\nHow do I change my Atlan URL?\nâ\nTo change the URL of your Atlan instance,\ncontact Atlan support\n. Requests for instances hosted on Microsoft Azure and Google Cloud Platform may take longer to complete.\nIf you use\nintegrated SSO\nin Atlan, ask your SSO administrator to reconfigure the Atlan URL in your SSO provider to maintain connectivity.\nHow can I change the workspace name?\nâ\nYou must be an admin user to be able to change the name of your Atlan workspace.\nTo update the workspace name:\nFrom the left menu in Atlan, click\nAdmin\n.\nIn the\nOverview\ntab, under\nWorkspace name\n, click the pencil icon to edit the workspace name.\nIn the Workspace name dialog, enter a name for your Atlan workspace and then click\nUpdate\n.\nFor example, you can use the name of your organization to customize your Atlan workspace. After you update the name, the change is visible to all your Atlan users.\nHow can I add my own branding?\nâ\nTo add your own logo to Atlan, any admin user can go to the\nAdmin Center\nand click the image in the\nOverview\nsection to upload a new logo. Atlan resizes the logo automatically, maintaining its aspect ratio.\nAtlan recommends that you use the following size and format for your logo:\nA logo with a 1:1 aspect ratio\nIn either PNG or JPEG format\nWith a size of 150x150 pixels\nI am having issues editing a persona, purpose, or policy\nâ\nIf you have the\nAdmin role\n(or another role that grants edit access) but still can't modify a persona, purpose, or policy, check the name of the item you are trying to change. Names can't include special characters such as\n\\\\\n,\n*\n, and similar symbols. Dashes (\n-\n) are allowed.\nIf the name already contains unsupported characters, or you continue to encounter errors after correcting the name, please\nraise a support ticket\n.\nHow do I access audit logs?\nâ\nOpen any asset's\nactivity log\nand audit all activity by using the\nauditSearch\nAPI. Configure the request with your preferred filtersâsuch as time range or action typeâto retrieve the audit records you need.\nCan restricting access to certain assets impact the lineage view?\nâ\nAccess control policies\ndon't change which assets appear in the\nlineage graph\n. If an asset is restricted, its properties and attributes are simply hidden in the asset sidebar. The metadata policy assigned to each user determines which asset metadata they can view.\nTags:\nintegration\napi\nconfiguration\nfaq-administration\nPrevious\nFrequently Asked Questions\nNext\nAI and Automation Features"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/faq/auto-assign-owners",
    "content": "Configure Atlan\nIntegrations\nProject Management\nJira\nFAQ\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nCan I configure additional fields or auto-assign owners to Jira tickets created from Atlan?\nRefer to our\ntroubleshooting Jira documentation\nto learn more.\nTags:\nintegration\nfaq\nfaq-integrations\nPrevious\nWhat is included in the Jira integration?\nNext\nCan site renaming affect the Jira integration?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/create-glossary-backups",
    "content": "Build governance\nGlossary\nFAQ\nCan I create backups of glossaries?\nCan I create backups of glossaries?\nCurrently, deleting a glossary and its components will result in their permanent deletion. Therefore, when you attempt to archive a glossary, you will get a popup message asking you to confirm your intended action:\nAre you sure you want to archive [insert glossary name] and all its contents?\nAtlan also allows you to\nexport your glossary assets\nto spreadsheets and keep a record of your contextual information.\nIf you inadvertently archived your glossary, Atlan creates a\ndaily backup\nand you could restore the backup snapshot from the previous day. This will override any other changes that were made in the instance since that time. Note that there is no way to selectively restore specific elements that have been deleted. Hence, Atlan advises caution before proceeding with any delete actions.\nTags:\nintegration\nfaq\nfaq-governance\nPrevious\nUse personas to update a term in a glossary\nNext\nFully delete glossary terms or archived items"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/send-messages-search-assets-slack",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nFAQ\nHow do I send messages or search assets from Slack?\nHow do I send messages or search assets from Slack?\nSending messages and searching assets from Slack are disabled. Refer to\nTroubleshooting Slack\nto learn more.\nTags:\nintegration\nfaq\nfaq-integrations\nPrevious\nTroubleshooting Slack\nNext\nSlack permissions explained"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-glossary-visibility",
    "content": "Configure Atlan\nAdministration\nFeature Management\nRestrict glossary visibility\nOn this page\nRestrict glossary visibility\nPrivate Preview\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to restrict\nglossary\nvisibility.\nNote that\nasset access works slightly differently\n.\nOnce you have restricted\nglossary\nvisibility:\nIf a glossary policy has been configured for a selected persona, only the user belonging to that persona be able to view the glossaries curated through glossary policies.\nIf a user isn't part of any persona, they're unable to view any glossaries in Atlan.\nIf a user is part of a persona that doesn't have glossary policies specifically or any\npolicies\nat all configured, the user is unable to view all glossaries.\nThe restriction is applicable to the\nAssets\npage, glossary tree on the\nGlossary\npage,\nterms filter\non the\nAssets\npage, terms in an asset sidebar,\nlinked assets\n, and\nrelated terms\n.\nThe\nAll glossaries\nÂ view on the\nGlossary\nÂ page is enabled for all users by default. To turn it off for your member and guest users, complete the following steps.\nTo disable all glossaries view:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAccess control\nheading on the\nLabs\npage, turn on\nPersona switcher in Glossary\n.\nYour member and guest users now only have access to the curated glossaries for their persona! ð\nIf you'd like to restore the default all glossaries view, repeat steps 1 to 3 and then turn off the toggle.\nSummary\nâ\nPersona switcher in Glossary in Labs is OFF: Users can see all glossaries.\nPersona switcher in Glossary in Labs is ON: Users can't see any glossaries until provided in a persona.\nDid you know?\nIf a user doesn't belong to any persona and the\nAll glossaries\nview is disabled, then the user is prompted to reach out to their Atlan administrator and request to be added to a persona.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nRestrict asset visibility\nNext\nHow to view event logs\nSummary"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-support",
    "content": "One doc tagged with \"faq-support\"\nView all tags\nSupport and Technical Help\nComplete guide to getting support, understanding API limits, and accessing technical assistance for Atlan."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/references/what-does-atlan-crawl-from-sap-hana",
    "content": "Connect data\nDatabases\nSQL Databases\nSAP HANA\nReferences\nWhat does Atlan crawl from SAP HANA?\nOn this page\nWhat does Atlan crawl from SAP HANA?\nAtlan crawls and maps the following assets and properties from SAP HANA.\nSchemas\nâ\nAtlan maps schemas from SAP HANA to its\nSchema\nasset type.\nSource property\nAtlan property\nSCHEMA_NAME\nname\nTABLE_COUNT\ntableCount\nVIEW_COUNT\nviewsCount\nTables\nâ\nAtlan maps tables from SAP HANA to its\nTable\nasset type.\nSource property\nAtlan property\nTABLE_NAME\nname\nCOMMENTS\ndescription\nCOLUMN_COUNT\ncolumnCount\nROW_COUNT\nrowCount\nTABLE_SIZE\nsizeBytes\nIS_PARTITIONED\nisPartitioned\nIS_TEMPORARY\nisTemporary\nViews\nâ\nAtlan maps views from SAP HANA to its\nView\nasset type.\nSource property\nAtlan property\nVIEW_NAME\nname\nCOMMENTS\ndescription\nCOLUMN_COUNT\ncolumnCount\nDEFINITION\ndefinition\nCalculation views\nâ\nAtlan maps calculation views from SAP HANA to its\nCalculationView\nasset type. Atlan supports upstream lineage to source tables and calculation views. Column-level lineage is currently not supported.\nSource property\nAtlan property\nOBJECT_NAME\nname\nCOLUMN_COUNT\ncolumnCount\nVERSION_ID\ncalculationViewVersionId\nACTIVATED_BY\ncalculationViewActivatedBy\nACTIVATED_AT\ncalculationViewActivatedAt\nColumns\nâ\nFor tables and views\nâ\nAtlan maps columns for tables and views from SAP HANA to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nCOMMENTS\ndescription\nPOSITION\norder\nDATA_TYPE_NAME\ndataType\nIS_NULLABLE\nisNullable\nLENGTH\nmaxLength\nCONSTRAINT (PRIMARY KEY)\nisPrimary\nREFERENCED_COLUMN_NAME\nisForeign\nDECIMAL_DIGITS\nprecision\nSCALE\nnumericScale\nDEFAULT_VALUE\ndefaultValue\nFor calculation views\nâ\nAtlan maps columns for calculation views from SAP HANA to its\nColumn\nasset type.\nSource property\nAtlan property\nCOLUMN_NAME\nname\nLABEL\ndescription\nORDER\norder\nCOLUMN_SQL_TYPE\ndataType\nÂ Â Â Â Â Â Â Â Â Â Â  -\ncolumnIsMeasure\nMEASURE_TYPE\ncolumnMeasureType\nThe\ncolumnIsMeasure\nproperty is derived as a Boolean value based on the\nMEASURE_TYPE\nfield.\nStored procedures\nâ\nAtlan maps stored procedures in SAP HANA to its\nProcedure\nasset type.\nSource property\nAtlan property\nPROCEDURE_NAME\nname\nPROCEDURE_TYPE\nsubType\nDEFINITION\ndefinition\nTags:\nschema\nschema-drift\nschema-monitoring\nPrevious\nCrawl SAP HANA\nNext\nPreflight checks for SAP S/4HANA\nSchemas\nTables\nViews\nCalculation views\nColumns\nStored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/references/what-does-atlan-crawl-from-alteryx",
    "content": "Connect data\nETL Tools\nAlteryx\nReferences\nWhat does Atlan crawl from Alteryx?\nOn this page\nWhat does Atlan crawl from Alteryx?\nPrivate preview\nThis document outlines the Alteryx metadata that Atlan crawls and maps as part of its cataloging process. Atlan supports two ingestion modes with varying property coverage.\nOnce Alteryx is integrated with Atlan, connector-specific filters can be used for efficient asset discovery.\nSupported filters\nâ\nFilter Type\nDescription\nStatus filter\nFilters workflows by their last run status.\nDuration filter\nFilters workflows by their last run duration.\nAsset mapping\nâ\nAlteryx workflows are represented in Atlan using the\nAlteryxWorkflow\nasset type. The following table shows how Alteryx properties are mapped to Atlan properties:\nAlteryx Source Property\nMapped Atlan Property\neventTime\nalteryxRunStartTime\neventTime\nalteryxRunEndTime\neventType\nalteryxRunOpenLineageState\nrunid\nalteryxRunID\njob.owners\nsourceOwner\nworkflowID\nalteryxWorkflowID\nrun.errorMessage\nalteryxWorkflowError\nMultiple\neventTime\nfields are mapped to both\nRun Start Time\nand\nRun End Time\n, depending on context.\nWorkflow run metadata includes operational details like error messages and lineage states, which are essential for observability and debugging.\nSee also\nâ\nSet up Alteryx\nPrevious\nSet up Alteryx\nNext\nConnection issues\nSupported filters\nAsset mapping\nSee also"
  },
  {
    "url": "https://docs.atlan.com/tags/insights",
    "content": "One doc tagged with \"insights\"\nView all tags\nInsights\nQuery and analyze your data using Atlan's powerful query builder and SQL capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/query",
    "content": "One doc tagged with \"query\"\nView all tags\nInsights\nQuery and analyze your data using Atlan's powerful query builder and SQL capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/analysis",
    "content": "One doc tagged with \"analysis\"\nView all tags\nInsights\nQuery and analyze your data using Atlan's powerful query builder and SQL capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-insights",
    "content": "9 docs tagged with \"faq-insights\"\nView all tags\nAre there any limits on concurrent queries?\nLearn about are there any limits on concurrent queries?.\nCan I query any DW/DL?\nYou can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources#data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature.\nCan I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.\nCan we restrict who can query our data warehouse?\nLearn about can we restrict who can query our data warehouse?.\nHow can I identify an Insights query in my database access log?\nAtlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs.\nHow to use parameterized queries?\nLearn about how to use parameterized queries?.\nMonitor for runaway queries?\nLearn about monitor for runaway queries?.\nWhat controls the frequency of queries?\nLearn about what controls the frequency of queries?.\nWhy do I only see tables from the same schema to join from in a visual query?\nWhen [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/slack-permissions",
    "content": "Configure Atlan\nIntegrations\nCollaboration\nSlack\nFAQ\nSlack permissions explained\nOn this page\nWhat does Atlan do with each Slack permission?\nAs you set up the\nSlack integration\n, you will need to provide an Atlan Slack app permissions in Slack. Note the following:\nAtlan does not store any data for Slack messages. Slack messages are fetched on demand.\nAtlan connects to Slack via Slack APIs, and stores the following metadata to send messages on behalf of users   -  username, channel name, tenant name, refresh token, and access token.\nAtlan does not store any personally identifiable information of users.\nHere is what Atlan does with each of those permissions:\nWhat will Atlan be able to view?\nâ\nContent and info about you\nâ\nView profile details about people in your workspace (\nusers.profile:read\n)\nView information about your identity\nAtlan requires these permissions to:\nShow rich Slack message previews within Atlan (for example, a user's profile picture)\nContent and info about channels & conversations\nâ\nView messages and other content in your public channels (\nchannels:history\n)\nView basic information about public channels in your workspace (\nchannels:read\n)\nView Atlan URLs in messages (\nlinks:read\n)\nView messages and other content in public channels that Atlan has been added to (\nchannels:history\n)\nAtlan requires these permissions to:\nShow rich Slack message previews within Atlan (the message itself, the channel where it was posted)\nContent and info about your workspace\nâ\nView people in your workspace (\nusers:read\n)\nView profile details about people in your workspace (\nusers.profile:read\n)\nView email addresses of people in your workspace (\nusers:read.email\n)\nAtlan requires these permissions to:\nShow rich Slack message previews within Atlan (for example, a user's profile picture)\nSend notifications about requests made in Atlan (\nusers:read.email\n)\nWhat will Atlan be able to do?\nâ\nPerform actions in channels and conversations\nâ\nShow previews of Atlan URLs in messages (\nlinks:write\n)\nSend messages as\n@atlan\n(\nchat:write\n)\nSend messages to channels\n@atlan\nisn't a member of (\nchat:write.public\n)\nAdd and edit emoji reactions\nView messages that directly mention\n@atlan\nin conversations that the app is in\nUpload, edit, and delete files as Atlan (\nfiles.write\n)\nAtlan requires these permissions to:\nUnfurl Atlan links (show previews for them) within Slack\nAllow the Slack share button on Atlan assets to work (sending messages to Slack as\n@atlan\n)\nSearch Atlan from any Slack channel and share search results to the channel, all from within Slack\nShare query output from within Atlan to Slack (\nfiles.write\n)\nPerform actions in your workspace\nâ\nAdd shortcuts and/or slash commands that people can use (\ncommands\n)\nAtlan requires these permissions to:\nSearch Atlan from within Slack (the\n/search-term\nand\n/search-query\ncommands)\nDocument a Slack conversation into Atlan (using the Slack context menu on any Slack conversation)\nTags:\nslack\nfaq\nfaq-integrations\nPrevious\nHow do I send messages or search assets from Slack?\nNext\nWhat is included in the Slack integration?\nWhat will Atlan be able to view?\nWhat will Atlan be able to do?"
  },
  {
    "url": "https://docs.atlan.com/tags/monte-carlo",
    "content": "One doc tagged with \"monte carlo\"\nView all tags\nMonte Carlo\nIntegrate, catalog, and govern Monte Carlo assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/observability",
    "content": "3 docs tagged with \"observability\"\nView all tags\nAnomalo\nIntegrate, catalog, and govern Anomalo assets in Atlan.\nMonte Carlo\nIntegrate, catalog, and govern Monte Carlo assets in Atlan.\nSoda\nIntegrate, catalog, and govern Soda assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/delete-glossary-terms",
    "content": "Build governance\nGlossary\nFAQ\nFully delete glossary terms or archived items\nHow do I fully delete glossary terms or archived items?\nTo fully delete glossary terms or archived items:\nArchived assets\nare\nsoft-deleted\n. This means that those assets will not be cluttering the UI but are still available to search for and even recover if needed.\nIf absolutely essential, assets can be\nhard-deletedÂ via API\n. At that point, they are gone forever.\nTags:\napi\nrest-api\ngraphql\nglossary\nbusiness-terms\ndefinitions\nfaq-governance\nPrevious\nCan I create backups of glossaries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/snowflake-lookml-table-matching",
    "content": "Use data\nLineage\nFAQ\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nEach\nproject\nin Looker can only be linked to a source that is individually obtained from the Looker APIs. In addition, the API provides the default database name while the LookML files contain the table name. Atlan links the two pieces of information to generate lineage.\nTags:\nlineage\ndata-lineage\nimpact-analysis\napi\nrest-api\ngraphql\nfaq-lineage\nPrevious\nHow do you enable data lineage for different data sources?\nNext\nHow does Atlan handle lineage from Spark jobs?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/tableau-upstream-lineage",
    "content": "Use data\nLineage\nFAQ\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nTo understand how Atlan generates upstream lineage for supported sources, see\nWhat is lineage?\n.\nFor Tableau specifically, Atlan also retrieves asset lineage data through the\nTableau APIs\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nupstream-dependencies\ndata-sources\napi\nrest-api\ngraphql\nfaq-lineage\nPrevious\nHow is the Atlan lineage graph depicted using Power BI measures?\nNext\nIs there a way to build lineage from NetSuite to Snowflake?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage/how-tos/integrate-google-cloud-composer-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nGoogle Cloud OpenLineage\nGet Started\nHow to integrate Google Cloud Composer/OpenLineage\nOn this page\nIntegrate Google Cloud Composer/OpenLineage\nTo integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to\nOpenLineage configuration and facets\n.\nDid you know?\nFor Apache Airflow operators supported for OpenLineage extraction, you can refer to\nAirflow's Supported operators\ndocumentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see\nHow to implement OpenLineage in Airflow operators\n. Also, check the\nrecommended provider package versions for OpenLineage\n.\nCreate an API token in Atlan\nâ\nBefore running the workflow, you will need to\ncreate an API token\nin Atlan.\nConfigure the integration in Atlan\nâ\nSelect the source in Atlan\nâ\nTo select Google Cloud Composer/OpenLineage as your source, from within Atlan:\nIn the top right of any screen, click\nNew\nand then click\nNew workflow\n.\nFrom the filters along the top, click\nOrchestrator\n.\nFrom the list of packages, select\nGoogle Cloud Composer Airflow Assets\nand then click\nSetup Workflow\n.\nCreate the connection\nâ\ndanger\nA single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior.\nYou will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets.\nTo configure the Google Cloud Composer/OpenLineage connection, from within Atlan:\nFor\nConnection Name\n, provide a connection name that represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, no one will be able to manage the connection   -  not even admins.\n(Optional) For\nHost\n, enter the URL of your Google Cloud Composer Airflow UI. This will allow Atlan to help you view your assets directly in Google Cloud Composer from the asset profile.\n(Optional) For\nPort\n, enter the port number for your Google Cloud Composer Airflow UI.\nFor\nEnable OpenLineage Events\n, click\nYes\nto enable the processing of OpenLineage events or click\nNo\nto disable it. If disabled, new events will not be processed in Atlan.\nTo create a connection, at the bottom of the screen, click the\nCreate connection\nbutton.\nConfigure the integration in Google Cloud Composer\nâ\nDid you know?\nYou will need the Atlan API token and connection name to configure the integration in Google Cloud Composer. This will allow Google Cloud Composer to connect with the OpenLineage API and send events to Atlan.\ndanger\nAtlan does not support integrating with Apache Airflow versions older than 2.5.0.\nTo configure Google Cloud Composer to send OpenLineage events to Atlan:\nYou will need to configure Google Cloud Composer for the integration:\nOpen your Google Cloud console and navigate to the\nEnvironments\npage.\nFrom the list of environments, click the name of your environment. Configure the following:\nFor Apache Airflow versions 2.7.0 onward,\nset override Airflow configuration options\n:\nIn the\nEnvironment details\npage, click the\nAirflow configuration overrides\ntab and then click\nEdit\n.\nIn the\nAirflow configuration overrides\nform, click the\nAdd Airflow configuration override\nbutton to specify the first set of values:\nFor\nSection 1\n, enter\nopenlineage\n.\nFor\nKey 1\n, enter\nnamespace\n.\nFor\nValue 1\n, enter the connection name as exactly configured in Atlan.\nClick the\nAdd Airflow configuration override\nbutton to specify the second set of values:\nFor\nSection 2\n, enter\nopenlineage\n.\nFor\nKey 2\n, enter\ntransport\n.\nFor\nValue 2\n, enter the following:\n{\n\"type\"\n:\n\"http\"\n,\n\"url\"\n:\n\"https://<instance>.atlan.com/events/openlineage/airflow-cloud-composer/\"\n,\n\"auth\"\n:\n{\n\"type\"\n:\n\"api_key\"\n,\n\"api_key\"\n:\n\"<API_KEY>\"\n}\n}\nFor\n<API_key>\n, set the API token generated in Atlan.\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0,\nset environment variables\n:\nIn the\nEnvironment details\npage, click the\nEnvironment variables\ntab and then click\nEdit\n.\nAdd the following environment variable names and corresponding values:\nOPENLINEAGE_URL\n: points to the service that will consume OpenLineage events   -  for example,\nhttps://<instance>.atlan.com/events/openlineage/airflow-cloud-composer/\n.\nOPENLINEAGE_API_KEY\n: set the API token generated in Atlan.\nOPENLINEAGE_NAMESPACE\n: set the connection name as exactly configured in Atlan.\nClick\nSave\nto save your changes.\nYou will also need to install the OpenLineage PyPI package in Google Cloud Composer. To install the OpenLineage PyPI package in your environment:\nIn the\nEnvironment details\npage, click the\nPyPI packages\ntab and then click\nEdit\n.\nClick\nAdd package\nto add a custom package.\nUnder\nPyPI packages\n, for\nPackage name\n, specify the package name.\nFor Apache Airflow versions 2.7.0 onward:\napache-airflow-providers-openlineage\nFor Apache Airflow versions 2.5.0 onward:\nopenlineage-airflow\nClick\nSave\nto save your configuration.\nVerify the Atlan connection in Google Cloud Composer\nâ\nTo verify connectivity to Google Cloud Composer:\nFor\nVerify connection with Cloud Composer\n, click the clipboard icon to copy and run the preflight check DAG on your Google Cloud Composer instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the\npreflight checks documentation\n.\nClick\nDone\nto complete setup.\nOnce your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð\nYou can also\nview event logs\nin Atlan to track and debug events received from OpenLineage.\nTags:\nlineage\ndata-lineage\nimpact-analysis\napi\nrest-api\ngraphql\nPrevious\nGoogle Cloud Composer OpenLineage\nNext\nWhat does Atlan crawl from Google Cloud Composer/OpenLineage?\nCreate an API token in Atlan\nConfigure the integration in Atlan\nConfigure the integration in Google Cloud Composer\nVerify the Atlan connection in Google Cloud Composer"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/set-up-qlik-sense-enterprise-on-windows",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nQlik Sense Enterprise on Windows\nGet Started\nSet up Qlik Sense Enterprise on Windows\nOn this page\nSet up Qlik Sense Enterprise on Windows\nWho can do this?\nYou will need your Qlik Sense Enterprise on Windows administrator to complete these steps   -  you may not have access yourself.\nCreate user in Qlik Sense Enterprise on Windows\nâ\nDid you know?\nBy default, your identity provider for your Qlik Sense Enterprise on Windows version will be Microsoft Windows. So, your Microsoft Windows users will be your Qlik users. To add a new user in this case, you only need to create a\nnew Windows local account\n.\nWe recommend that you create a new user Qlik Sense Enterprise on Windows for integration with Atlan.\nTo create a new user, follow the\nsteps in the Microsoft Windows documentation\nand then add the new user:\nLog in to your Qlik Sense Enterprise on Windows instance.\nNavigate to the active directory or identity provider of your Qlik Sense Enterprise on Windows version and add the new user.\nAllocate user access\nâ\nOnce you've created a new user, you will need to allocate user access for integration with Atlan.\nTo\nallocate user access\nto the new user:\nLog in to\nQlik Management Console\n(QMC):\nhttps://<QPS server name>/qmc\n.\nTo allocate a license to the new user, in the left menu,Â click\nLicense management\n.\nIn the right panel for\nLicense management\n, click\nProfessional access allocations\n.\nAt the bottom of the\nProfessional access allocations\nscreen, click\nAllocate\n.\nFrom the\nUsers\ndialog, select the\nnew user you created\nand click\nAllocate\nto complete user allocation.\nYou can also set up roles and groups for robust access management.\nSet permissions\nâ\nDid you know?\nAtlan does\nnot\nmake any API requests or queries that will update the objects in your Qlik Sense Enterprise on Windows instance.\nOnce you've added the new user, you will need to provide the new user with\nRead\npermission to your streams, apps, sheets, charts, and connections.\nTo set the\nminimum permissions\nrequired to\ncrawl Qlik Sense Enterprise on Windows\n:Â\nLog in to\nQlik Management Console\n(QMC):\nhttps://<QPS server name>/qmc\n.\nIn the left menuÂ under\nManage Resources\n, click\nSecurity rules\n.\nAt the bottom of the\nSecurity rules\nscreen, click\nCreate new\n.Â\nIn the\nEdit security\ndialog, enter the following details:\nFor\nName\n, enter a meaningful name for your security rule.Â\nFor\nBasic\n, under\nActions\n, click\nRead\nto provide\nRead\naccess.\nAt the bottom of the dialog, click\nApply\nto apply your security rule.Â\ndanger\nIf JWT authentication is already enabled for your Qlik Sense Enterprise on Windows instance, you can proceed to\ngenerating a JWT\n. If Windows authentication is already enabled for your Qlik Sense Enterprise on Windows instance, you can directly proceed to\ncrawling Qlik Sense Enterprise on Windows\n.\n(Optional) Create a virtual proxy\nâ\nOnce you've set permissions for the new user, you can create a\nvirtual proxy\nfor authentication.\nAtlan supports the following authentication methods for Qlik Sense Enterprise on Windows:\nWindows authentication\nâ\nDid you know?\nWhen Qlik Sense Enterprise on Windows is installed, it automatically creates a default virtual proxy called\nCentral\nÂ without a prefix that supports Windows authentication. If it is still available on your instance, you can skip creating a new one and simply edit it.\nTo\ncreate a virtual proxy\nfor Windows authentication:\nLog in to\nQlik Management Console\n(QMC):\nhttps://<QPS server name>/qmc\n.\nIn the left menu under\nConfigure Systems\n, click\nVirtual proxies\n.\nAt the bottom of the\nVirtual proxies\nscreen, click\nCreate new\n.\nIn the\nEdit virtual proxy\nscreen:\nFor\nIdentification\n, enter the following details:\nFor\nDescription\n, add a description for your virtual proxy.\nFor\nPrefix\n, add a path name in the proxyâs URI   -  use only lowercase letters for the prefix.\nFor\nSession cookie header name\n, add the name of the HTTP header used for the session cookie.\nFor\nAuthentication\n,Â enter the following details:\nFor\nAuthentication method\n, select\nTicket\nas the authentication method.Â\nFor\nWindows authentication pattern\n, select\nWindows\n.Â\nClick\nApply\nto save your authentication details.\nJWT authentication\nâ\nTo\ncreate a virtual proxy\nfor JSON Web Token (JWT) authentication:\nLog in to\nQlik Management Console\n(QMC):\nhttps://<QPS server name>/qmc\n.\nIn the left menu under\nConfigure Systems\n, click\nVirtual proxies\n.\nAt the bottom of the\nVirtual proxies\nscreen, click\nCreate new\n.\nIn the\nEdit virtual proxy\nscreen:\nFor\nIdentification\n, enter the following details:\nFor\nDescription\n, add a description for your virtual proxy.\nFor\nPrefix\n, add a path name in the proxyâs URI   -  use only lowercase letters for the prefix.\nFor\nSession cookie header name\n, add the name of the HTTP header used for the session cookie.\nFor\nAuthentication\n,Â enter the following details:\nFor\nAuthentication method\n, select\nJWT\n.Â\nFor\nJWT certificate\n, you can either:\nTo generate a key pair using\nopenssl\n, open the\npublic.key\nfile in a text editor of your choice, copy the key, and paste it_._\nTo use the same certificate as your Qlik Sense Enterprise on Windows instance, you can find it in the path\nC:\\ProgramData\\Qlik\\Sense\\Repository\\Exported Certificates\\.Local Certificates\n. Open the\nserver.pem\nfile in a text editor of your choice, copy the content, and paste it.\nFor\nJWT attribute for user ID\n, add the JWT attribute name for the attribute describing the user ID.Â\nFor\nJWT attribute for user directory\n, add JWT attribute name for the attribute describing the user directory.\n(Optional) Under\nAdvanced\n, for\nHost allow list\n, add the host IP addresses of your Qlik Sense Enterprise on Windows deployment.\nClick\nApply\nto save your authentication details.\n(Optional) Generate a JWT\nâ\nTo generate a JSON Web Token (JWT) for\ncrawling Qlik Sense Enterprise on Windows\n:\nOpen the\nJWT\nwebsite.\nAt the top of the screen, click\nDebugger\n.\nFor\nAlgorithm\n, click the dropdown arrow and select\nRS256\n.\nFor\nPayload\n, add the\nuser ID and directory\nfor your virtual proxy.\nFor\nVerify signature\n, paste the\nserver_key.pem\n(private key) and\nserver.pem\n(public key) pair from\nC:\\ProgramData\\Qlik\\Sense\\Repository\\Exported Certificates\\.Local Certificates\nin the appropriate fields.\nIn the left\nEncoded\nfield, copy the generated token and save it in a temporary location.\ndanger\nTo confirm that you've used the right key pair, navigate to the bottom of the screen and ensure that you can see the\nSignature Verified\nstatus.\nTags:\napi\nrest-api\ngraphql\nPrevious\nQlik Sense Enterprise (Windows)\nNext\nCrawl Qlik Sense Enterprise on Windows\nCreate user in Qlik Sense Enterprise on Windows\nAllocate user access\nSet permissions\n(Optional) Create a virtual proxy\n(Optional) Generate a JWT"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/troubleshooting/troubleshooting-matillion-connectivity",
    "content": "Connect data\nETL Tools\nMatillion\nTroubleshooting\nTroubleshooting Matillion connectivity\nOn this page\nTroubleshooting Matillion connectivity\nWhat are the known limitations of the Matillion connector?\nâ\nAtlan currently only supports the following:\nLineage for asset transformations for Matillion version 1.68 LTS   -  end-to-end lineage is currently not supported.\nColumn-level lineage is currently not supported due to limitations of the\nMatillion ETL Metadata API\n.\nTask history for Matillion\ncomponents\nis calculated based on the last 7 days of metadata crawled from the Matillion APIs.\nTags:\nlineage\ndata-lineage\nimpact-analysis\napi\nrest-api\ngraphql\nPrevious\nWhat lineage does Atlan extract from Matillion?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/troubleshooting/troubleshooting-trino-connectivity",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nTroubleshooting\nTroubleshooting Trino connectivity\nOn this page\nTroubleshooting Trino connectivity\nWhat are the known limitations of the Trino connector?\nâ\nMaterialized views from Trino may be displayed as tables in Atlan due to limitations at source.\nHow to debug test authentication and preflight check errors?\nâ\nTLS/SSL required for basic authentication\nTLS/SSL is required for authentication with username and password.\nEnsure thatÂ TLS/SSLÂ is correctly configured on the Trino server. This typically involves configuring the\nconfig.properties\nfile for each Trino node. Once you have updated the configuration, restart the Trino services to apply the new settings.\nEnsure that the TLS/SSL certificates used by Trino are valid and unexpired.\nDisabled TLS/SSL\nConnection property SSLVerification requires TLS/SSL to be enabled.\nCheck your connection settings to ensure that\nTLS/HTTPS is enabled in the crawler configuration\n.\nIncorrect credentials\nThe username or the password provided to connect to the Trino account is incorrect.\nLog in to your Trino account for the specified host and verify that the username and password are correct.\nYou can also\ncreate a new user\n, if required.\nIncorrect host address\nYour Trino hostname is not correct, please provide a correct hostname and retry.\nThe host address provided by the user is not correct, please check the host address.\nEnsure that the\nhostname you have specified\nis correct   -  it should be a domain name.\nServer connection failure\nThe host or port provided by the user is not correct, please check your host and port.\nEnsure that the\nhostname you have specified\nis correct   -  it should be a domain name.\nVerify the port number.\nAuthentication failed: Unauthorized\nAuthentication failed: Unauthorized\nVerify your credentials in Trino to confirm you have the\nnecessary permissions to connect\n.\nSocketTimeoutException: connection timed out\nConnection timed out. Please check your host and port.\nEnsure that you have\nspecified the hostname and port number\ncorrectly in your connection settings.\nCheck if you can reach the Trino server from your machine. Use\nping <host>\nor\ntelnet <host> <port>\nto test connectivity.\nTags:\napi\nrest-api\ngraphql\nPrevious\nPreflight checks for Trino"
  },
  {
    "url": "https://docs.atlan.com/tags/data-flow",
    "content": "One doc tagged with \"data-flow\"\nView all tags\nLineage\nTrack and visualize data lineage across your data landscape to understand data flow and dependencies."
  },
  {
    "url": "https://docs.atlan.com/tags/dependencies",
    "content": "9 docs tagged with \"dependencies\"\nView all tags\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nIntegrate Atlan with Google Sheets\nThe Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.\nLineage\nTrack and visualize data lineage across your data landscape to understand data flow and dependencies.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?."
  },
  {
    "url": "https://docs.atlan.com/tags/downstream-impact",
    "content": "8 docs tagged with \"downstream-impact\"\nView all tags\nColumn Level Lineage\nData lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage.\nDownload and export lineage\nUnderstanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view](#view-impacted-assets), [download](#download-lineage), and [export](#export-impacted-assets) your impacted assets and share it with others in your organization.\nDownload impacted assets in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).\nIntegrate Atlan with Google Sheets\nThe Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.\nTroubleshooting Amazon MSK connectivity\nLearn about troubleshooting amazon msk connectivity.\nTroubleshooting MongoDB connectivity\nLearn about troubleshooting mongodb connectivity.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/summarize-metadata",
    "content": "Use data\nReporting\nMetadata\nSummarize metadata\nSummarize metadata\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more.\nUse the following dashboards in the reporting center to:\nAssets\n-  monitor your assets\nGlossary\n-  track metrics for your glossaries, categories, and terms\nGovernance\n-  review your governance setup\nInsights\n-  track metrics for your queries\nAutomations\n-  monitor asset enrichment through automation features\nUsage and cost\n-  track asset usage and associated costs\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nReport on governance\nNext\nReport on assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/references/lineage-generator",
    "content": "Use data\nLineage\nReferences\nLineage Generator (no transformations)\nOn this page\nLineage generator (no transformations)\nApp\nThe Lineage Generator (No Transformations) app automatically detects matching or similar assets between two connections in Atlan and generates lineage links between them. This enables teams to establish upstream and downstream data flows without requiring transformation logic, making lineage creation faster and more consistent across your data estate.\nThe app provides three capabilities:\nPreview lineage:\nValidate matches before creating them.\nGenerate lineage:\nBulk-create relationships between matched assets.\nDelete lineage:\nClean up or roll back lineage previously created by the app.\nIt works across multiple asset types, including tables, views, files, BI assets, and more. You can apply custom matching rules using case sensitivity, schema matching, and regex-based name replacements to make sure accurate lineage generation. This reference provides complete configuration details to help you set up and customize lineage generation.\nAccess\nâ\nThe Lineage Generator (No Transformations) app isn't enabled by default. To use this app, contact\nAtlan support\nand request it be added to your tenant.\nConfigurations\nâ\nThis section explains the properties you can configure in the\nLineage Generator (no transformations)\napp.\nWorkflow name\nâ\nSpecifies the display name for the workflow in Atlan. This name is used to identify the workflow in the UI and logs.\nChoose a name that clearly reflects the purpose or scope of the lineage generation run.\nExample:\nSales Data Lineage Generation\nSource asset type\nâ\nSpecifies the type of the input assets from which lineage must originate. This determines which assets in Atlan are scanned and matched with the\nTarget asset type\nto generate lineage.\nSupported types include:\nRelational\n: Table, View, Materialized View, Calculation View, Column\nOther sources\n: MongoDB Collection, Salesforce Object, Salesforce Field, S3 / ADLS / GCS Object, Power BI Table, Power BI Column, Kafka Topic, Looker Field, Looker View\nFor detailed parsing rules, and examples for each supported type, see the\nSource asset type reference\n.\nWhen\nMatch on schema\nis set to\nYes\n, schema names are included in the matching logic. This means that both schema and asset names must align for lineage to be created.\nSchema matching applies only when both the source and target asset types are relational (Table, View, Materialized View, Calculation View, Column) or a MongoDB Collection. If either type is outside these categories, schema matching is ignored.\nExample:\nWith\nMatch on schema = No\n,\nsales.orders\nand\nmarketing.orders\nboth map to a downstream target named\norders\n.\nWith\nMatch on schema = Yes\n, only\nsales.orders\nconnects to\nsales.orders\n, while\nmarketing.orders\nis treated as separate.\nSource qualified name prefix\nâ\nSpecifies the qualified name prefix for the source assets. The qualified name is the unique identifier Atlan assigns to every asset, and the prefix is the first part of this identifier that indicates the connection, environment, and often the schema or directory where the asset resides.\nWhen this property is set, the app only considers source assets whose qualified names begin with the specified prefix. This helps narrow the search scope to a specific connection or subset of assets, improving both accuracy and performance.\nYou can find this prefix in the\nqualifiedName\nfield of an asset in Atlan. Open the asset's details, locate its\nqualifiedName\n, and copy the portion that represents the schema, folder, or relevant path to use as the prefix. Make sure the prefix matches the exact format of the asset's qualified name in Atlan. Incorrect or incomplete prefixes can result in no matches.\nExample:\nSnowflake schema: Use a schema-level prefix when you want to generate or validate lineage only for assets inside a specific Snowflake schema.\ndefault\n/\nsnowflake\n/\n1678901234\n/\nwarehouse_name\n/\ndatabase_name\n/\nschema_name\nS3 bucket folder: Use a folder-level prefix when you want to restrict lineage operations to objects within a particular folder of an S3 bucket.\ndefault/s3/1678904567/bucket-name/folder-name/.../object-name\nTarget asset type\nâ\nSpecifies the type of assets in the destination connection for which lineage is created. The system searches for assets of this type in the target connection and matches them with source assets according to the configured rules.\nThe target asset type usually represents the downstream asset in the lineage relationship. For example, when generating lineage between a staging schema in a data warehouse and a BI tool, the target asset type might be Table for the warehouse layer or Dashboard for the BI layer.\nIf either the\nSource asset type\nor\nTarget asset type\nisn't a relational type (Table, View, Materialized View, Calculation View, or Column) or a MongoDB Collection, the\nMatch on schema\noption is ignored.\nExample: Same technology lineage\nTable as the\nSource asset type\nand Table as the\nTarget asset type\nin the same Snowflake environment, mapping tables from a staging schema to a reporting schema.\nExample: Cross-technology lineage\nTable as the\nSource asset type\nin Snowflake and Dataset as the\nTarget asset type\nin Looker, mapping data warehouse tables to the Looker datasets that use them.\nTarget qualified name prefix\nâ\nSpecifies the qualified name prefix used to identify target assets for lineage generation. Only assets whose qualified names begin with this prefix are considered when matching them to source assets.\nYou can find the qualified name prefix in the\nqualifiedName\nfield of an asset in Atlan. Open the assetâs details, copy its\nqualifiedName\n, and use the portion that represents the schema, folder, or relevant path as the prefix.\nExample:\nIf the target assets are in a Snowflake schema named analytics, the qualified name prefix might look like:\ndefault\n/\nsnowflake\n/\n1234567890\n/\nwarehouse_name\n/\ndatabase_name\n/\nanalytics\nIf the target assets are files in an S3 bucket, the prefix might be:\ndefault\n/\ns3\n/\n1234567890\n/\nbucket\n-\nname\n/\nreports\n/\n.../\nobject\n-\nname\nCase sensitive match\nâ\nDetermines whether asset name matching between the source and target is performed with case sensitivity.\nYes:\nOnly matches assets when the names have exactly the same letter case. For example,\nOrders\nmatches only\nOrders\nand not\norders\n.\nNo (default)\n: Matches assets regardless of letter case. For example,\nOrders\nmatches both\nOrders\nand\norders\n.\nThis setting is useful when working with systems or naming conventions where case differences indicate different assets, or when aligning with case-sensitive data source rules.\nIgnore circular lineage\nâ\nSpecifies whether lineage generation must skip relationships that might create a circular reference between assets.\nYes:\nPrevents the creation of lineage where the source and target assets are connected in a loop, avoiding redundant or misleading paths in lineage graphs.\nNo (default)\n: Enables lineage to be created even if it results in a circular path.\nExample:\nIf you generate lineage between staging and reporting tables in Snowflake, you normally expect data to flow one way (staging â reporting). If reporting tables also reference staging tables, a circular path can appear.\nWhen enabled\n: The lineage is created only in the forward direction, avoiding the circular loop:\nTable A (staging) â Table B (reporting)\nWhen not enabled\n: The lineage includes both directions, creating a loop:\nTable A (staging) â Table B (reporting) â Table A (staging)\ninfo\nThis option blocks self-referencing lineage paths when the same asset is included in both source and target scopes. To establish valid bidirectional lineage between different assets, such as staging and reporting layers, configure and run the workflow separately for each direction.\nMatch on schema\nâ\nControls whether lineage matching compares both the\nschema name\nand the\nasset name\n, or only the asset name, when connecting source and target assets.\nSchema matching applies only when both the\nSource asset type\nand\nTarget asset type\nare relational objects (Table, View, Materialized View, Calculation View, or Column) or MongoDB Collections. If either asset type falls outside these categories, schema names aren't considered in the matching process.\nYes\n: The schema name is included in the matching logic. A source and target asset are considered a match only if both the schema and the asset name align. Useful when the same asset names exist across multiple schemas. Prevents incorrect matches across different domains or functional areas.\nExample:\nIf you're working in a multi-schema warehouse where\nsales.orders\nand\nmarketing.orders\nboth exist, enabling this option makes sure that only\nsales.orders\nmaps to\nsales.orders\n.\nNo (default)\n: Only the asset name is compared between source and target. Schemas are ignored during matching. Useful when assets with the same name are always intended to be connected, regardless of their schema.\nExample:\nIf you're consolidating data and want both\nsales.orders\nand\nmarketing.orders\nto connect to a single downstream target\norders_combined\n, select\nNo\nso that schema differences don't prevent the match.\nOutput type\nâ\nDefines the type of action the app performs after matching source and target assets. You can select one of the following actions:\nPreview Lineage (default)\n: Generates a CSV preview of the potential lineage mappings without applying changes in Atlan. Use this option when validating the configuration before making permanent updates.\nExample:\nIf you configure the app to match orders tables across\nsales\nand\nmarketing\nschemas, the preview generates a file showing which assets might be connected, but no lineage is created in Atlan.\nGenerate Lineage\n: Creates lineage relationships directly in Atlan between the matched source and target assets.\nExample:\nIf the preview showed\nsales.orders\nâ\nwarehouse.orders_combined\nand the configuration is approved, running with\nGenerate Lineage\nadds this lineage in Atlan so it appears in the lineage graph.\nUse this option when confident the mappings are correct and ready to be reflected in the workspace.\nDelete Lineage\n: Removes lineage previously created by this app from Atlan. Only relationships generated by the Lineage Generator (no transformations) app are deleted.\nExample:\nIf\nsales.orders\nwas earlier mapped to\nwarehouse.orders_combined\nbut the connection is no longer valid, selecting\nDelete Lineage\nremoves this relationship from Atlan.\nGenerate lineage on child assets\nâ\nSpecifies whether lineage is also created between the child assets of the selected source and target asset types.\nThis option is relevant only when both the\nSource asset type\nand\nTarget asset type\ninclude child assets, such as tables with columns or Power BI tables with columns. For asset types that don't have a hierarchical structure, the option is ignored.\nYes:\nLineage is created at both the parent level (for example, table-to-table) and the child level (for example, column-to-column).\nExample:\nIf\nsales.orders\n(table) is mapped to\nwarehouse.orders_combined\n(table), enabling this option also generates lineage for the columns under those tables, such as\norders.customer_id\nâ\norders_combined.customer_id\n.\nUseful when column-level lineage is required to understand transformations or dependencies in greater detail.\nNo (default):\nLineage is generated only at the parent level (for example, table-to-table), and no column-level (or child-level) mappings are created.\nExample:\nWith the same configuration, lineage connects only the\nsales.orders\ntable to the\nwarehouse.orders_combined\ntable, without adding relationships between individual columns.\nSuitable when high-level lineage is sufficient or when child asset mappings may add unnecessary noise.\nName transformation (regex)\nâ\nIf you want to replace specific characters or patterns in asset names so they align across systems, configure the regex fields together. When\nMatch on schema = Yes\n, use the schema-specific regex fields to handle schema-level differences. The name-only regex fields apply only to object names, such as tables or columns, and aren't used for schema comparisons.\nRegex to match characters to replace\n: Defines the regex patterns you want to match in the source asset names.\nRegex with replacement characters\n: Defines what those matched patterns must be replaced with.\nExample:\nIf you want to change all\n_id\nsuffixes into\n_identifier\nto match your target system:\nIn\nRegex to match characters to replace\n, enter:\n_id$\nIn\nRegex with replacement characters\n, enter:\n_identifier\nExample:\nTo replace the temporary prefix\ntmp_\nwith a standard prefix\nstg_\n:\nIn\nRegex to match characters to replace\n, enter:\n^tmp_\nIn\nRegex with replacement characters\n, enter:\nstg_\nSchema name transformation (regex)\nâ\nUse these properties when schema names differ between environments or systems and need to be standardized for lineage generation.\nRegex to match characters to replace on the schema\n: Defines the pattern in the schema name to identify the part that must be replaced.\nRegex with replacement characters on the schema\n: Specifies the replacement text to use for each pattern defined earlier.\nBoth fields must be configured together: each regex pattern requires a corresponding replacement. If one is left blank, the transformation doesn't occur.\nExample:\nIf you want to make schema names lowercase across environments:\nIn\nRegex to match characters to replace on the schema\n, enter:\n[A-Z]\nIn\nRegex with replacement characters on the schema\n, enter:\n\\L$0\nThis makes sure schemas like\nFinance\n,\nHR_DATA\n, and\nSALES\nall become lowercase (\nfinance\n,\nhr_data\n,\nsales\n) for consistent matching.\nExample:\nIf you want to ignore version numbers in schema names:\nIn\nRegex to match characters to replace on the schema\n, enter:\n_[0-9]+$\nIn\nRegex with replacement characters on the schema\n, leave the field empty to remove the matched text.\nThis maps schemas like\nanalytics_2023\nand\nanalytics_01\ninto a single logical schema:\nanalytics\n.\nUsing name + schema regex properties\nâ\nIf you want to standardize both schema and object names together, configure the following properties:\nRegex to match characters to replace on the name + schema\n: Defines the pattern across the full schema.object identifier to identify the part that must be replaced.\nRegex with replacement characters on the name + schema\n: Specifies the replacement text to use for each pattern defined earlier.\nThese properties apply only when\nMatch on schema = Yes\n, because the system uses the combined\nschema.object\nkey instead of handling the schema and object name separately. When configured, the\nname + schema\nregex pair takes precedence over\nname-only\nor\nschema-only\nregex pairs.\nExample:\nIf you want to align schema and table names together, you can configure both the schema and object name in a single regex pair. This is useful when naming conventions include environment-specific prefixes or temporary suffixes that need to be normalized across systems.\nIn\nRegex to match characters to replace on the name + schema\n, enter:\n^(raw_|stg_|prod_)|(_tmp$)\nIn\nRegex with replacement characters on the name + schema\n, enter:\n\"\"\nWith this configuration, common environment prefixes (\nraw_\n,\nstg_\n,\nprod_\n) and temporary suffixes (\n_tmp\n) are removed:\nraw_sales.orders_tmp\nâ\nsales.orders\nprod_sales.orders\nâ\nsales.orders\nMatch prefix\nâ\nAdds a fixed string to the beginning of each source asset name before comparing it to target assets.\nThis property is useful when target assets follow a consistent naming convention that uses a prefix (such as\nstg_\n,\nprod_\n, or\nteam_\n), but source assets don't. For example, if the source asset is\norders\nand the target asset is\nstg_orders\n, setting the match prefix to\nstg_\nenables them to match.\nIf both\nMatch prefix\nand\nMatch suffix\nare set, the prefix is applied first, followed by the suffix.\nIf\nMatch on schema = Yes\n, the prefix applies only to the object name, not the schema.\nIf\nregex transformations on name + schema\nare configured, those transformations take precedence, and the prefix is ignored.\nIf\nregex transformations on name only\nare configured, the regex runs first, and then the prefix is added.\nIf the differences between source and target names go beyond a simple prefix, regex transformations are the preferred option.\nExample:\nTo match a source asset named orders with a target asset named\nstg_orders\n, set\nMatch prefix\nto\nstg_\n. This adds the prefix to the source name so both align.\nMatch suffix\nâ\nAppends a fixed string to the end of each source asset name before comparing it to target assets.\nThis property is useful when target systems consistently add a suffix (such as\n_prod\n,\n_stg\n, or\n_2024\n) to asset names, but source systems don't. For example, if the source asset is\norders\nand the target asset is\norders_prod\n, adding the suffix\n_prod\nenables them to match.\nIf both\nMatch prefix\nand\nMatch suffix\nare set, the prefix is applied first, followed by the suffix.\nIf\nMatch on schema = Yes\n, the suffix applies only to the object name, not the schema.\nIf\nregex transformations on name + schema\nare configured, those transformations take precedence, and the suffix is ignored.\nIf\nregex transformations on name only\nare configured, the regex runs first, then the suffix is added.\nIf the naming differences are more complex than a consistent suffix, regex transformations are the preferred option.\nExample\n: To match a source table named\norders\nwith a target table named\norders_prod\n, set\nMatch suffix\nto\n_prod\n. This transforms the source name to\norders_prod\nbefore comparison, creating a correct match.\nFile path segmentation (file-based assets only)\nâ\nFor file-based assets (such as S3, ADLS, or GCS objects), use these properties to extract the meaningful parts of a file path for lineage matching.\nFile advanced separator\n: Defines the character used to split the path into segments (for example,\n/\nin an S3 path or\n\\\nin a Windows-style path).\nFile advanced position\n: Specifies how many segments to keep from the end of the split path. The count is right-to-left (for example,\n3\nkeeps the last three segments).\nThese properties are designed to work\ntogether\n:\nThe separator defines where to cut the path.\nThe position defines which slice of the path to use for matching.\nIf only a separator is set, the path is split but the full string is still compared.\nIf only a position is set, it has no effect because no split occurs.\nFor best results, configure both properties together.\nOther transformations (such as regex, prefix, or suffix) are applied\nafter\nseparator and position logic. If\nMatch on schema = Yes\n, the folder path segments kept by this configuration are treated like a schema. This makes it possible to align files across environments in the same way schemas align tables in databases.\nExample:\nIf your S3 folder paths include the environment (such as\nstaging\nor\nprod\n), you can use the file segmentation properties to focus only on the meaningful parts of the path.\nIn\nFile advanced separator\n, enter:\n/\nIn\nFile advanced position\n, enter:\n3\nThis keeps the last three segments of the path for matching. For example:\narn:aws:s3:::mybucket/staging/customers/data.csv\nbecomes:\ncustomers/data.csv\nso it can correctly align with:\narn:aws:s3:::mybucket/prod/customers/data.csv\nProcess connection\nâ\nThe\nProcess connection\nproperty defines which connection the generated lineage processes belong to. If left blank, processes are automatically assigned to the same connection as the source assets.\nThis property is particularly useful when the lineage logically represents a transformation or movement between two systems and must live in a neutral or dedicated connection, rather than being tied only to the source.\nProcess connection works independently from\nregex\n,\nprefix/suffix\n, and\nfile path segmentation\nproperties. Those affect how matching is computed, while\nProcess connection\naffects only process placement.\nExample:\nIf you want to centralize lineage processes in a dedicated connection called\nlineage_sandbox\n, configure:\nlineage_sandbox\nSee also\nâ\nSource asset type\nGenerate lineage between assets App\nTags:\nlineage\ndata-lineage\nimpact-analysis\nupstream-dependencies\napp\nPrevious\nHow can Atlan generate upstream lineage from the data warehouse layer?\nNext\nSource asset type\nAccess\nConfigurations\nSee also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/field-level-lineage-bi-tools",
    "content": "Use data\nLineage\nFAQ\nDoes Atlan support field-level lineage for BI tools?\nDoes Atlan support field-level lineage for BI tools?\nAtlan supports field-level lineage across most BI sources that have the concept of fields. See\nsupported BI sources\nto drill down further.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nfaq-lineage\nPrevious\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nNext\nDoes lineage only cover calculated fields for Tableau dashboards?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/enable-lineage-for-sources",
    "content": "Use data\nLineage\nFAQ\nHow do you enable data lineage for different data sources?\nHow do you enable data lineage for different data sources?\nThis will vary by connector. For more information, see\nConnectors and capabilities\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nfaq-lineage\nPrevious\nDoes lineage only cover calculated fields for Tableau dashboards?\nNext\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/power-bi-measures-lineage-graph",
    "content": "Use data\nLineage\nFAQ\nHow is the Atlan lineage graph depicted using Power BI measures?\nHow is the Atlan lineage graph depicted using Power BI measures?\nAtlan currently does not support lineage for Microsoft Power BI\nmeasures\n. To learn more about asset types for which lineage is available, see\nWhat does Atlan crawl from Microsoft Power BI?\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nfaq-lineage\nPrevious\nHow does Atlan handle lineage from Spark jobs?\nNext\nHow to obtain upstream lineage if I connect to a Tableau data asset?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/lineage-view-number-meaning",
    "content": "Use data\nLineage\nFAQ\nWhat do the numbers in lineage view mean?\nWhat do the numbers in lineage view mean?\nThe numbers represent the total number of\nlineage processes\n, which are the upstream and downstream transformations of an asset.Â Atlan also displays connector branding for the procedures to help you determine where the transformation originates from, such as Snowflake or dbt.\nTags:\nlineage\ndata-lineage\nimpact-analysis\ndownstream-impact\ndependencies\nupstream-dependencies\ndata-sources\nfaq-lineage\nPrevious\nWhat are Power BI processes on the lineage graph?\nNext\nWhat lineage do you support?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/supported-lineage-types",
    "content": "Use data\nLineage\nFAQ\nWhat lineage do you support?\nWhat lineage do you support?\nFollowing are some examples of built-in\nlineage generation\n:\nInside a data warehouse (table-to-table, view-to-table   -  this goes down to a column level)\nData warehouse   -  Databricks, Snowflake, Amazon Redshift, Google BigQuery, and more\nThe miner packages in the Atlan marketplace bring in lineage\nSnowflake\n,\nGoogle BigQuery\n, and\nAmazon Redshift\nlineage are built by parsing queries in query history\nUnity Catalog APIs and\nsystem tables\nprovide\nDatabricks lineage\n(only if the Databricks workspace is Unity Catalog-enabled   -  which is still in preview from Databricks)\nÂ Lineage from data warehouse to BI tool\nBI tool   -  Microsoft Power BI, Tableau, Looker, and more\nData warehouse   -  all the aforementioned sources\nTable- and column-level lineage from\ndbt\nTo view supported capabilities for Atlan's current integrations, see\nConnectors and capabilities\n.\nIf you see a connector you'd like to manually build lineage for that is not automated, you can do it by\nutilizing our APIs\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nfaq-lineage\nPrevious\nWhat do the numbers in lineage view mean?\nNext\nWhy is lineage available for table level but not column level?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/sql-query-visibility-snowflake",
    "content": "Use data\nLineage\nFAQ\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?\nAtlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage.\nTags:\nmodel\nfaq\nfaq-lineage\nPrevious\nWhy is lineage available for table level but not column level?\nNext\nTroubleshooting lineage"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-lineage",
    "content": "16 docs tagged with \"faq-lineage\"\nView all tags\nCan Atlan integrate with Airflow to generate lineage?\nAtlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage).\nCan Atlan read a dump of SQL statements to create lineage?\nAtlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/.\nCan I be notified if there is a change in downstream dashboards or a schema drift?\nYou can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events.\nDoes Atlan support field-level lineage for BI tools?\nAtlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-.\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.\nHow do you enable data lineage for different data sources?\nLearn about how do you enable data lineage for different data sources?.\nHow does a Snowflake connection know that a table referenced in LookML is actually the same table?\nLearn about how does a snowflake connection know that a table referenced in lookml is actually the same table?.\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nHow is the Atlan lineage graph depicted using Power BI measures?\nLearn about how is the atlan lineage graph depicted using power bi measures?.\nHow to obtain upstream lineage if I connect to a Tableau data asset?\nLearn about how to obtain upstream lineage if i connect to a tableau data asset?.\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nWhat are Power BI processes on the lineage graph?\nNote that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets.\nWhat do the numbers in lineage view mean?\nLearn about what do the numbers in lineage view mean?.\nWhat lineage do you support?\nLearn about what lineage do you support?.\nWhy is lineage available for table level but not column level?\nThe home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset.\nWhy is the SQL query visible only in Snowflake process and not in dbt process nodes?\nAtlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/references/source-asset-type",
    "content": "Use data\nLineage\nReferences\nLineage Generator (no transformations)\nSource asset type\nOn this page\nSource asset type\nApp\nThe\nSource asset type\nproperty is part of the\nLineage Generator (no transformations)\napp. It defines the type of asset from which lineage originates in Atlan. This reference documentation explains what each supported source asset type represents, how the app parses its qualified names, how parsing behavior changes when related properties such as\nMatch on schema\n,\nCase sensitive match\n,\nFile segmentation\n, or\nRegex transformations\nare enabled or disabled.\nThe same parsing rules apply to both Source asset type and target asset type. While this page focuses on source assets, the information also applies when configuring the\nTarget asset type\n.\nADLS object\nâ\nADLS objects represent files stored in Azure Data Lake Storage. Qualified names include the connection, container, folder hierarchy, and file name.\nFile path segmentation can be applied using\nFile advanced separator\nand\nFile advanced position\nto retain folder context.\nMatch on schema\nisn't applied to file-based assets.\nRegex transformations are applied after segmentation to remove suffixes or standardize naming.\nExample:\nIf you want to extract only the file name without any folder context, use the default configuration. This is useful when you need to match files across different folder structures:\ndefault/adls/.../container/folder/data.csv â data.csv\nExample:\nIf you want to retain folder context for better organization, enable file segmentation. This helps when you need to distinguish between files in different environments or stages:\ndefault/adls/.../container/env/staging/customers.csv â staging/customers.csv\nCalculation view\nâ\nCalculation views are relational assets used in systems like SAP HANA. These views combine multiple tables or other views using business logic, calculations, and aggregations to create a unified data model for reporting and analytics.\nDefault parsing strips connection, database, and schema, leaving only the view name.\nIf\nMatch on schema = Yes\n, schema is included in the key for more precise lineage tracking.\nCase sensitivity applies depending on system configuration and can affect matching across different environments.\nRegex transformations can be used to standardize naming conventions or remove system-specific prefixes.\nExample:\nIf you want to match calculation views across different schemas with the same name, use the default configuration. This is useful when you have standardized view names across multiple schemas:\ndefault/sap/.../DATABASE/SCHEMA/VIEW â VIEW\nExample:\nIf you need to distinguish between calculation views with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when views have identical names but different contexts:\ndefault/sap/.../DATABASE/SCHEMA/VIEW â SCHEMA/VIEW\nExample:\nIf you want to clean up SAP-specific naming conventions, apply regex transformations to standardize view names across your lineage:\nSAP_CALC_VIEW_CUSTOMERS â CUSTOMERS\nColumn\nâ\nColumns represent individual fields inside tables or views. They're the fundamental building blocks of data structures and contain the actual data values. Column-level lineage is crucial for understanding how individual data elements flow through your data pipeline, from source systems to final analytics.\nDefault parsing includes table and column names, providing context about which table contains the column.\nIf\nMatch on schema = Yes\n, schema is included for more granular lineage tracking across different database schemas.\nRegex transformations can be used for column name cleanup, standardization, or alignment with business terminology.\nCase sensitivity is important for systems where column names may have different casing conventions.\nExample:\nIf you want to match columns across different schemas with the same table and column names, use the default configuration. This is useful for cross-schema column lineage:\ndefault/snowflake/.../DB/SCHEMA/TABLE/COLUMN â TABLE/COLUMN\nExample:\nIf you need to distinguish between columns with the same name in different schemas, enable schema matching. This ensures accurate lineage when columns exist in multiple schemas:\ndefault/snowflake/.../DB/SCHEMA/TABLE/COLUMN â SCHEMA/TABLE/COLUMN\nExample:\nIf you want to standardize column naming conventions across different systems, apply regex transformations to align warehouse columns with source system fields:\nCUSTOMER_ID_RAW â CUSTOMER_ID\nDynamoDB table\nâ\nDynamoDB tables are schemaless key-value store assets.\nParsing extracts the table name only.\nCase sensitivity determines whether differently cased names are matched separately.\nExample:\nIf you want to extract just the table name for lineage matching, use the default configuration. This simplifies matching across different DynamoDB instances:\ndefault/dynamodb/.../CUSTOMERS â CUSTOMERS\nGCS object\nâ\nGCS objects represent files stored in Google Cloud Storage. Qualified names include the connection, bucket, folder hierarchy, and object.\nDefault parsing extracts only the object (file) name.\nFile path segmentation enables keeping multiple trailing parts of the path.\nRegex transformations apply after segmentation.\nExample:\nIf you want to extract only the file name without folder context, use the default configuration. This is useful for matching files across different bucket structures:\ndefault/gcs/.../bucket/folder/object.json â object.json\nExample:\nIf you need to retain folder context for better file organization, enable file segmentation. This helps when you want to distinguish between files in different environments or project folders:\ndefault/gcs/.../bucket/env/prod/sales/data.json â prod/sales/data.json\nKafka topic\nâ\nKafka topics represent message streams. Qualified names include the connection and topic name.\nParsing extracts only the topic name.\nCase sensitivity determines if topics with the same name but different case are considered matches.\nExample:\nIf you want to extract just the topic name for lineage matching, use the default configuration. This simplifies matching across different Kafka clusters:\ndefault/kafka/.../topic/orders_events â orders_events\nLooker field\nâ\nLooker fields represent individual dimensions or measures in LookML models.\nParsing extracts only the field name portion.\nRegex transformations can align field names with warehouse columns.\nExample:\nIf you want to extract only the field name for lineage matching, use the default configuration. This helps align Looker fields with their corresponding warehouse columns:\ndefault/looker/.../distribution_centers.location â location\nLooker view\nâ\nLooker views represent groupings of fields in LookML models.\nParsing extracts only the view name.\nRegex transformations can clean prefixes or suffixes in view names.\nExample:\nIf you want to extract only the view name for lineage matching, use the default configuration. This simplifies matching Looker views with their underlying data sources:\ndefault/looker/.../distribution_centers â distribution_centers\nMaterialized view\nâ\nMaterialized views are persisted relational query results.\nDefault parsing strips connection, database, and schema.\nIf\nMatch on schema = Yes\n, schema is included.\nCase sensitivity applies depending on system rules.\nExample:\nIf you want to match materialized views across different schemas with the same name, use the default configuration. This is useful when you have standardized view names across multiple schemas:\ndefault/redshift/.../DB/SCHEMA/MV_NAME â MV_NAME\nExample:\nIf you need to distinguish between materialized views with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when views exist in multiple schemas:\ndefault/redshift/.../DB/SCHEMA/MV_NAME â SCHEMA/MV_NAME\nMongoDB collection\nâ\nMongoDB collections represent groups of documents within databases.\nDefault parsing extracts only the collection name.\nIf\nMatch on schema = Yes\n, database and collection are included.\nCase sensitivity can affect results when collection names overlap.\nExample:\nIf you want to match collections across different databases with the same name, use the default configuration. This is useful when you have standardized collection names across multiple databases:\ndefault/mongodb/.../DB/COLLECTION â COLLECTION\nExample:\nIf you need to distinguish between collections with the same name in different databases, enable database matching. This ensures accurate lineage when collections exist in multiple databases:\ndefault/mongodb/.../DB/COLLECTION â DB/COLLECTION\nPower BI column\nâ\nPower BI columns represent fields within BI tables.\nDefault parsing extracts both table and column.\nRegex transformations can rename BI fields to match warehouse columns.\nExample:\nIf you want to extract both table and column names for lineage matching, use the default configuration. This helps align Power BI fields with their corresponding warehouse columns:\ndefault/powerbi/.../TABLE/COLUMN â TABLE/COLUMN\nPower BI table\nâ\nPower BI tables represent datasets inside BI models.\nDefault parsing extracts only the table portion.\nRegex transformations can clean or rename tables to align with upstream assets.\nExample:\nIf you want to extract only the table name for lineage matching, use the default configuration. This simplifies matching Power BI tables with their underlying data sources:\ndefault/powerbi/.../TABLE â TABLE\nS3 object\nâ\nS3 objects represent files stored in Amazon S3 buckets. Qualified names include the connection, bucket, folder hierarchy, and object.\nDefault parsing extracts only the file name.\nFile segmentation properties can keep multiple trailing segments.\nRegex transformations are applied after segmentation to remove environment labels or extensions.\nMatch on schema\nisn't applied to file-based assets.\nExample:\nIf you want to extract only the file name without folder context, use the default configuration. This is useful for matching files across different bucket structures:\ndefault/s3/.../mybucket/folder/data.csv â data.csv\nExample:\nIf you need to retain folder context for better file organization, enable file segmentation. This helps when you want to distinguish between files in different environments or project folders:\ndefault/s3/.../mybucket/env/staging/customers/data.csv â staging/customers/data.csv\nExample:\nIf you want to clean up environment-specific suffixes after segmentation, apply regex transformations. This helps standardize file names while maintaining folder context:\nstaging/customers/data.csv â customers/data.csv\nSalesforce field\nâ\nSalesforce fields represent attributes within Salesforce objects.\nParsing removes connection and org identifiers, leaving only object and field.\nRegex transformations can normalize Salesforce field names to align with warehouse fields.\nExample:\nIf you want to extract object and field names for lineage matching, use the default configuration. This helps align Salesforce fields with their corresponding warehouse columns:\ndefault/salesforce/.../ORG/ACCOUNT/ID â ACCOUNT/ID\nSalesforce object\nâ\nSalesforce objects represent entities such as Accounts or Contacts.\nParsing removes connection and org identifiers, leaving only the object.\nRegex transformations can handle additional suffixes or prefixes across systems.\nExample:\nIf you want to extract only the object name for lineage matching, use the default configuration. This simplifies matching Salesforce objects with their corresponding warehouse tables:\ndefault/salesforce/.../ORG/ACCOUNT â ACCOUNT\nTable\nâ\nTables are standard relational assets in systems like Snowflake, BigQuery, and Redshift.\nDefault parsing removes connection, database, and schema.\nIf\nMatch on schema = Yes\n, schema is included in the parsed key.\nCase sensitivity determines whether same-named tables with different casing are treated separately.\nRegex transformations can clean or align names.\nExample:\nIf you want to match tables across different schemas with the same name, use the default configuration. This is useful when you have standardized table names across multiple schemas:\ndefault/snowflake/.../DB/SCHEMA/TABLE â TABLE\nExample:\nIf you need to distinguish between tables with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when tables exist in multiple schemas:\ndefault/snowflake/.../DB/SCHEMA/TABLE â SCHEMA/TABLE\nView\nâ\nViews are relational query-based objects.\nDefault parsing strips connection, database, and schema.\nIf\nMatch on schema = Yes\n, schema is included.\nCase sensitivity determines whether views with different casing match.\nExample:\nIf you want to match views across different schemas with the same name, use the default configuration. This is useful when you have standardized view names across multiple schemas:\ndefault/bigquery/.../PROJECT/DB/SCHEMA/VIEW â VIEW\nExample:\nIf you need to distinguish between views with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when views exist in multiple schemas:\ndefault/bigquery/.../PROJECT/DB/SCHEMA/VIEW â SCHEMA/VIEW\nTags:\nlineage\nreference\napp\ndata-lineage\nparsing\nPrevious\nLineage Generator (no transformations)\nNext\nCan Atlan integrate with Airflow to generate lineage?\nADLS object\nCalculation view\nColumn\nDynamoDB table\nGCS object\nKafka topic\nLooker field\nLooker view\nMaterialized view\nMongoDB collection\nPower BI column\nPower BI table\nS3 object\nSalesforce field\nSalesforce object\nTable\nView"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-metadata",
    "content": "One doc tagged with \"faq-metadata\"\nView all tags\nTags and Metadata Management\nComplete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nOn this page\nAmazon Redshift\nOverview:\nCatalog Amazon Redshift clusters, databases, schemas, and tables in Atlan. Gain visibility into lineage, usage, and governance for your AWS data warehouse assets.\nGet started\nâ\nFollow these steps to connect and catalog Amazon Redshift assets in Atlan:\nSet up the connector\nCrawl Amazon Redshift assets\nGuides\nâ\nMine Amazon Redshift\n: Extract query history and build lineage for your Redshift assets.\nEnable SSO for Amazon Redshift\n: Set up Okta SSO authentication for Redshift connections.\nSet up a private network link to Amazon Redshift\n: Establish a secure, private network connection to Redshift for metadata extraction.\nReferences\nâ\nWhat does Atlan crawl from Amazon Redshift\n: Learn about the Redshift assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Amazon Redshift\n: Verify prerequisites before setting up the Amazon Redshift connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Amazon Redshift connection issues and errors.\nTags:\namazon\nredshift\nconnector\ndata warehouse\nconnectivity\nNext\nSet up Amazon Redshift\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/troubleshooting/troubleshooting-amazon-redshift-connectivity",
    "content": "Connect data\nData Warehouses\nAmazon Redshift\nTroubleshooting\nTroubleshooting Amazon Redshift connectivity\nOn this page\nTroubleshooting Amazon Redshift connectivity\nWhat are the known limitations of the Amazon Redshift connector?\nâ\nIf you run the\nminer\nfor an\nAmazon Redshift connection\nwith\nServerless\ndeployment type, lineage will be missing for queries with comments included. This is a limitation of AWS   -  queries with comments do not include line breaks when fetched from query history.\nWhy do I get an error when querying an external schema in Amazon Redshift?\nâ\nAtlan currently does not support\nsearch paths\nfor external schemas and tables in Amazon Redshift.\nIf you would like to query an external schema in Amazon Redshift from Atlan, you will need to write the query in Insights with the fully qualified name and keep the editor context empty   -  for example,\nselect * db.schema.table\nand as shown here:\nDoes Atlan support connecting to Amazon Redshift via SSL?\nâ\nYes, Atlan supports connecting to Amazon Redshift over the Secure Sockets Layer (SSL) protocol. If your Amazon Redshift cluster is configured to require an SSL connection, with the\nrequire_SSL\nparameter set to\ntrue\n, Atlan will be able to connect to your cluster.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nPreflight checks for Amazon Redshift"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nAmazon MWAA OpenLineage\nOn this page\nAmazon MWAA OpenLineage\nOverview:\nCapture lineage from Amazon Managed Workflows for Apache Airflow in Atlan. Gain visibility into data transformations, dependencies, and governance for your AWS-managed Airflow DAGs.\nGet started\nâ\nFollow these steps to connect and capture Amazon MWAA lineage in Atlan:\nConfigure OpenLineage integration\nReferences\nâ\nWhat does Atlan capture from Amazon MWAA?\n: Detailed list of MWAA lineage metadata that Atlan can extract and visualize.\nTags:\namazon\nmwaa\nopenlineage\nconnector\nlineage\nconnectivity\nNext\nHow to integrate Amazon MWAA/OpenLineage\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nOn this page\nApache Airflow OpenLineage\nOverview:\nCapture lineage from Apache Airflow workflows in Atlan. Gain visibility into data transformations, dependencies, and governance for your Airflow DAGs.\nGet started\nâ\nFollow these steps to connect and capture Apache Airflow lineage in Atlan:\nSet up the connector\nIntegrate OpenLineage integration\nReferences\nâ\nWhat does Atlan crawl from Apache Airflow OpenLineage\n: Learn about the Airflow lineage metadata that Atlan captures and visualizes.\nPreflight checks for Apache Airflow OpenLineage\n: Verify prerequisites before setting up the Apache Airflow OpenLineage connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Apache Airflow OpenLineage connection issues and errors.\nTags:\napache\nairflow\nopenlineage\nconnector\nlineage\nconnectivity\nNext\nHow to integrate Apache Airflow/OpenLineage\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nApache Spark OpenLineage\nOn this page\nApache Spark OpenLineage\nOverview:\nCapture lineage from Apache Spark jobs in Atlan. Gain visibility into data transformations, dependencies, and governance for your Spark applications.\nGet started\nâ\nFollow these steps to connect and capture Apache Spark lineage in Atlan:\nIntegrate the connector\nReferences\nâ\nWhat does Atlan crawl from Apache Spark?\n: Detailed list of Spark lineage metadata that Atlan can extract and visualize.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Apache Spark, including permissions and network problems.\nTags:\napache\nspark\nopenlineage\nconnector\nlineage\nconnectivity\nNext\nHow to integrate Apache Spark/OpenLineage\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nAstronomer OpenLineage\nOn this page\nAstronomer OpenLineage\nOverview:\nCapture lineage from Astronomer-managed Apache Airflow in Atlan. Gain visibility into data transformations, dependencies, and governance for your Astronomer workflows.\nGet started\nâ\nFollow these steps to connect and capture Astronomer lineage in Atlan:\nIntegrate the connector\nReferences\nâ\nWhat does Atlan crawl from Astronomer?\n: Detailed list of Astronomer lineage metadata that Atlan can extract and visualize.\nTags:\nastronomer\nopenlineage\nconnector\nlineage\nconnectivity\nNext\nHow to integrate Astronomer/OpenLineage\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster",
    "content": "Connect data\nOrchestration & Workflow\nDagster\nOn this page\nDagster\nPrivate Preview\nOverview:\nCapture lineage from Dagster workflows in Atlan. Gain visibility into data transformations, dependencies, and governance for your Dagster assets.\nGet started\nâ\nFollow these steps to connect and capture Dagster lineage in Atlan:\nSet up the connector\n: Configure Dagster integration with Atlan to enable lineage capture\nCrawl Dagster assets\n: Create a crawler workflow in Atlan to capture lineage from Dagster\nReferences\nâ\nWhat does Atlan crawl from Dagster\n: Learn about the Dagster metadata that Atlan captures and visualizes.\nFAQ\nâ\nDagster FAQ\n: Frequently asked questions about Dagster integration with Atlan.\nTags:\ndagster\nconnector\nlineage\nconnectivity\nNext\nSet up Dagster\nGet started\nReferences\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise",
    "content": "Connect data\nDatabases\nNoSQL Databases\nDatastax Enterprise\nOn this page\nDataStax Enterprise\nOverview:\nCatalog DataStax Enterprise keyspaces, tables, materialised views, indexes, and columns in Atlan. Build asset- and column-level lineage for your distributed data.\nGet started\nâ\nFollow these steps to connect and catalog DataStax Enterprise assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from DataStax Enterprise?\n: Detailed list of DataStax Enterprise asset types and metadata fields.\nPreflight checks for DataStax Enterprise\n: Technical checks and requirements.\nTags:\ndatastax\ncassandra\nconnector\ndatabase\nlineage\nconnectivity\nNext\nSet up DataStax Enterprise\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage",
    "content": "Connect data\nOrchestration & Workflow\nGoogle Cloud OpenLineage\nOn this page\nGoogle Cloud Composer OpenLineage\nOverview:\nCapture lineage from Google Cloud Composer in Atlan. Gain visibility into data transformations, dependencies, and governance for your GCP-managed Airflow workflows.\nGet started\nâ\nFollow these steps to connect and capture Google Cloud Composer lineage in Atlan:\nIntegrate the connector\nReferences\nâ\nWhat does Atlan crawl from Google Cloud Composer?\n: Detailed list of Cloud Composer lineage metadata that Atlan can extract and visualize.\nTags:\ngoogle\ncloud\ncomposer\nopenlineage\nconnector\nlineage\nconnectivity\nNext\nHow to integrate Google Cloud Composer/OpenLineage\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/references/preflight-checks-for-apache-airflow",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nReferences\nPreflight checks for Apache Airflow\nOn this page\nPreflight checks for Apache Airflow\nBefore running your DAGs in\nApache Airflow\n,\nAmazon MWAA\n,\nAstronomer\n, or\nGoogle Cloud Composer\n, you can run a preflight check DAG in your Apache Airflow instance to perform the necessary technical validations.\nThe preflight check DAG:\nNeither collects nor transmits any sensitive data during the validation process, ensuring the security of your integration.\nIn case of any errors, it will provide detailed feedback for troubleshooting, including error codes and next steps.\nIncludes a retry mechanism for the API call to handle temporary network issues or server unavailability.\nPreflight checks\nâ\nThe preflight check DAG performs the following steps to validate your Atlan and OpenLineage setup:\nCollects environment variables\n-  to verify OpenLineage-related environment variables set during the configuration of your\nApache Airflow\n,\nAmazon MWAA\n,\nAstronomer\n, or\nGoogle Cloud Composer\ninstance. These variables can vary depending on your Apache Airflow version.\nValidates the OpenLineage library installation\n-  to check whether the\nopenlineage-python\nlibrary has been installed and identify its version. This ensures that the necessary library for sending OpenLineage events to Atlan is in place.\nSends API call for validation\n-  with the information collected in the previous steps, the preflight check DAG makes a\nPOST\nrequest to Atlan's preflight check endpoint. This is to confirm that there are no network issues or configuration errors obstructing the communication.\nFor example, the payload sent for validation looks like this:\n{\n\"connector_type\"\n:\n\"airflow-mwaa\"\n,\n\"version\"\n:\n\"2.5.0\"\n,\n// Airflow version\n\"ol_namespace\"\n:\n\"staging-mwaa\"\n,\n// Environment variable\n\"ol_endpoint\"\n:\n\"https://<host>/events/openlineage/airflow-mwaa/\"\n,\n// Environment variable\n\"ol_version\"\n:\n\"1.8\"\n// Installed OpenLineage library version\n}\nNote that Atlan will conduct some additional validations on the Atlan server using the provided data. This is to ensure that the integration is successful. If successful, the DAG will succeed.\nCheck Apache Airflow DAG logs\nâ\nTo check Apache Airflow DAG logs:\nOpen your Apache Airflow homepage.\nFrom the homepage, navigate to the\nAtlanOpenLineageConnectionTest\nÂ DAG. Under the\nRuns\ncolumn, click the failed run, circled in red.\nOn the\nList Dag Run\npage, under the\nRun Id\ncolumn, click the latest or top most failed run ID.\nFrom the corresponding screen, click the\nrun_ol_preflight_check\ntask.\nFrom the tabs along the top of the\nTask Instance\npopup, click\nLog\nÂ to view logs.\nOn the\nLog\npage, scroll down to the\nException\nsection to view the error code and message. Refer to the troubleshooting guide below to make the necessary changes.\nFor other distributions, refer to\nAmazon MWAA\n,\nAstronomer\n, or\nGoogle Cloud Composer\ndocumentation for more details.\nTroubleshoot errors\nâ\nMissing environment variables\nâ\nEnvironment variable {var} is missing. Please set it before running the DAG.\nEnsure that the required environment variables are set in your environment. These variables can vary depending on your Apache Airflow version:\nFor Apache Airflow versions 2.7.0 onward:\nAIRFLOW__OPENLINEAGE__NAMESPACE\nand\nAIRFLOW__OPENLINEAGE__TRANSPORT\n.\nFor Apache Airflow versions 2.5.0 onward and prior to 2.7.0:\nOPENLINEAGE_URL\n,\nOPENLINEAGE_API_KEY\n, and\nOPENLINEAGE_NAMESPACE\n.\nInvalid transport configuration\nâ\ndanger\nFor all errors related to the environment variable configuration for\nAIRFLOW__OPENLINEAGE__TRANSPORT\n, Atlan recommends paying attention to the JSON structure and adhering to the expected schema. You will need to ensure that the string values are correctly formatted, and the dictionaries contain the correct types and necessary keys.\nThe following errors are related to the\nAIRFLOW__OPENLINEAGE__TRANSPORT\nconfiguration for Apache Airflow versions 2.7.0 onward:\nMissing keys in 'transport_info'\nâ\n\"'{key}' is missing, update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\"\nEnsure that the specified key is present in the\nAIRFLOW__OPENLINEAGE__TRANSPORT\nJSON structure.\nIncorrect type for keys in 'transport_info'\nâ\n\"'{key}' must be of type {expected_type.name}, update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\"\nUpdate the type of specified key in the\nAIRFLOW__OPENLINEAGE__TRANSPORT\nJSON structure to match the expected type.\nWhite space in impermissible field\nâ\n\"'{key}' cannot contain whitespace, update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\"\nRemove any white space from the specified key value in the\nAIRFLOW__OPENLINEAGE__TRANSPORT\nJSON structure.\nEmpty or white space in fields\nâ\n\"'{key}' cannot be empty or contain whitespaces. update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\"\nEnsure that the specified key in the\nAIRFLOW__OPENLINEAGE__TRANSPORT\nJSON structure is neither empty nor does it contain any white space.\nNetwork permission error\nâ\nERROR - Failed to emit OpenLineage eventHTTPSConnectionPool(host='<instance>.atlan.com', port=443):Max retries exceeded with url: /events/openlineage/airflow-cloud-composer/api/v1/lineage\nThis error may result from the firewalls or VPC on which your Apache Airflow instance is hosted. Contact your network team to update the network permissions. This will allow your Airflow instance to make API calls to the URL mentioned in the error message.\nUnsupported Apache Airflow version\nâ\n{\n\"status\"\n:\n\"fail\"\n,\n\"error_code\"\n:\n\"unsupported_airflow_version\"\n,\n\"error_message\"\n:\n\"Minimum supported version is 2.5.0, you are using 2.4\"\n}\nAtlan does not support integrating with Apache Airflow versions older than 2.5.0. Upgrade your Apache Airflow version to 2.5.0 or above. This will allow OpenLineage to push metadata to Atlan.\nConnection not found\nâ\n{\n\"status\"\n:\n\"fail\"\n,\n\"error_code\"\n:\n\"connection_not_found\"\n,\n\"error_message\"\n:\n\"<connection_name> is not present on Atlan.\"\n}\nThe connection name set in your environment variables does not match the connection name created in Atlan. Create a new connection or use an existing connection name.\nInvalid OpenLineage endpoint\nâ\n{\n\"status\"\n:\n\"fail\"\n,\n\"error_code\"\n:\n\"invalid_openlineage_endpoint\"\n,\n\"error_message\"\n:\n\"Your OL endpoint should be: https://<instance>.atlan.com/events/openlineage/airflow/ || received: https://<instance>.atlan.com/events/openlineage/airflow/haha please update this.\"\n}\nUpdate the environment variable containing the OpenLineage URL to the expected URL in the error message.\nUnsupported OpenLineage version\nâ\n{\n\"status\"\n:\n\"fail\"\n,\n\"error_code\"\n:\n\"unsupported_ol_version\"\n,\n\"error_message\"\n:\n\"Minimum supported version is 1.2.0, you are using 1.1\"\n}\nInstall the latest version of the\nopenlineage-airflow\nlibrary   -  1.8.0 and above. The preflight check DAG will fail if you're using any OpenLineage version older than 1.2.0.\nConnection fetching failed\nâ\n{\n\"status\"\n:\n\"fail\"\n,\n\"error_code\"\n:\n\"connection_fetching_failed\"\n,\n\"error_message\"\n:\n\"Atlas Error: Failed to fetch connections - Post \\\"http://localhost:21000/api/atlas/v2/search/indexsearch\\\\\"\n:\ndial tcp\n[\n:\n:\n1\n]\n:\n21000\n:\nconnect\n:\nconnection refused\n}\nContact Atlan support\nto help you debug this error.\nDid you know?\nIf you continue to encounter any issues, Atlan recommends enabling debug logging for OpenLineage using the\nOPENLINEAGE_CLIENT_LOGGING=DEBUG\nenvironment variable. Run your DAGs again and then share the debug log with\nAtlan support\nfor troubleshooting.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nNext\nProvider package versions for OpenLineage\nPreflight checks\nCheck Apache Airflow DAG logs\nTroubleshoot errors"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-glossaries",
    "content": "Use data\nReporting\nGet Started\nReport on glossaries\nOn this page\nReport on glossaries\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe\nglossary\ndashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a\nvariety of filters\nto drill down further.\nDid you know?\nYou can also view and take action on all your\nrequests\nfor updating terms and categories, right from the glossary dashboard.\nFilter glossaries\nâ\nYou can use the filters in the glossary dashboard to customize your view of glossary metrics.\nTo filter glossaries:\nFrom the left menu in Atlan, click\nReporting\nand then click\nGlossary\n.\nUnder\nGlossary\n, for the\nAll glossaries\nfilter, select a glossary   -  for this example, we'll select the\nConcepts\nglossary.\nFor the\nAll Categories\nfilter, select a category   -  for this example, we'll select the\nMarketing\ncategory.\n(Optional) To further\nrefine your search\n, click\nMore filters\n.\nYou will now be able to view metrics for your filtered glossary! ð\nView terms by linked assets\nâ\nYou can view metrics for\nterms with linked assets\nbefore making any changes to your terms. This can help you understand the downstream impact of your modifications in advance.Â\nTo view linked assets for your terms:\nFrom the left menu in Atlan, click\nReporting\nand then click\nGlossary\n.\nFrom the\nGlossary\ndashboard, scroll down to\nTerms by linked assets\n.\nClick a term to view a list of all the linked assets in the sidebar.\n(Optional) In the sidebar, next to\nSearch all assets\n, click the export icon to\nexport linked assets\nfor a term to a spreadsheet.\nUpdate new terms and categories\nâ\nYou can also get to work on recently created terms and categories from the glossary dashboard   -  for example, attach a tag to a recently added term.\nTo update a recently created term:\nFrom the left menu in Atlan, click\nReporting\nand then click\nGlossary\n.\nFrom the\nGlossary\ndashboard, scroll down to\nRecently Created Terms & Categories\n.\nClick any term to view the term details in the sidebar   -  for this example, we'll select\nEconomic Census\n.Â\nIn the term sidebar, navigate to\nTags\nand click the\n+\nsign to\nattach a tag\nto your term   -  for example,\nPublic\n.\nNext to your selected tag in the popup, click the downward arrow to configure\ntag propagation\n.\nClick\nSave\nto save your changes.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nglossary\nbusiness-terms\ndefinitions\nPrevious\nReporting\nNext\nReport on automations\nFilter glossaries\nView terms by linked assets\nUpdate new terms and categories"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-queries",
    "content": "Use data\nReporting\nReport Types\nReport on queries\nOn this page\nReport on queries\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe Insights dashboard in the reporting center helps you track metrics for all your\nqueries\nand query runs.\nYou can also use a\nvariety of filters\nto customize your view of query metrics   -  including\nquery collections\n, folders,\nsaved queries\n, and more.\nDid you know?\nThe default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable.\nFilter by query type\nâ\nAtlan allows you to\nwrite your own SQL queries\nor\nuse the Visual Query Builder\n. This means you can also filter query metrics by query type.\nTo filter queries by type of query:\nFrom the left menu in Atlan, click\nReporting\nand then click\nInsights\n.\nFrom the\nInsights\ndashboard, in the upper right, click\nQuery Type\n.\nFrom the\nQuery Type\ndropdown, click\nSQL Query\nto view metrics for your SQL queries or click\nVisual Query\nto view metrics for your visual queries.\n(Optional) Under\nQueries by certificate\n, click any data point to view query assets in a sidebar. In the top right of the sidebar, click the\nExport\nbutton to\nexport filtered assets\nto a spreadsheet.\nView scheduled queries by user\nâ\nThe Insights dashboard also offers you a breakdown of\nscheduled queries\nby individual users. You can keep track of your scheduled queries as well as view top users.\nTo view scheduled queries by user:\nFrom the left menu in Atlan, click\nReporting\nand then click\nInsights\n.\nFrom the\nInsights\ndashboard, scroll down to\nScheduled Queries\nUnderÂ\nScheduled Queries\n, click\nCreated by all users\n.\nFrom the\nCreated by all users\ndropdown, select the user by whom you want to filter scheduled queries.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nglossary\nbusiness-terms\ndefinitions\nPrevious\nReport on automations\nNext\nReport on usage and cost\nFilter by query type\nView scheduled queries by user"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-usage-and-cost",
    "content": "Use data\nReporting\nReport Types\nReport on usage and cost\nOn this page\nReport on usage and cost\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard.\nThe reporting center in Atlan currently supports\nusage and cost metrics\nfor the following connectors:\nAmazon Redshift\n-  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source.\nDatabricks\n-  tables, views, and columns. Expensive queries for Databricks assets are currently unavailable due to limitations of the\nDatabricks APIs\n.\nGoogle BigQuery\n-  tables, views, and columns\nSnowflake\n-  tables, views, and columns\nUsage and cost metrics for\nMicrosoft Power BI\nwill be added in the future.\nFilter asset usage by users\nâ\nYou can track total assets queried and storage consumption by users in your organization.\nTo filter asset usage by a user:\nFrom the left menu in Atlan, click\nReporting\nand then click\nUsage & Cost\n.\nFrom the\nUsage & Cost\ndashboard, click the\nQueried by\nfilter and select the user by whom you want to filter usage metrics.\n(Optional) To further\nrefine your search\n, click\nMore filters\n.\nYou will now be able to view usage and cost metrics for a specific user! ð\nView least used assets\nâ\nYou can find suggestions for deprecating assets based on the following factors:\nTables and views without any lineage\nAsset size less than 100 GB\nAssets not queried at source in the last 30 days\nLess popular assets not updated at source in the last 3 or 6 months\nTo view least used assets:\nFrom the left menu in Atlan, click\nReporting\nand then click\nUsage & Cost\n.\nFrom the\nUsage & Cost\ndashboard, scroll down to\nReview your tables & views\n.\n(Optional) To further\nrefine your search\n, click\nMore filters\n.\nClick\nView assets\nfor any card to view a list of suggested assets for deprecation in the sidebar.\n(Optional) In the top right of the sidebar, click the\nExport\nbutton to\nexport filtered assets\nto a spreadsheet.\nFilter assets by context\nâ\nDiscover the top 25 most popular, most expensive, and most queried tables and views from the usage and cost dashboard. This can provide you with additional context regarding asset usage across the organization.\nDid you know?\nThe compute cost for an asset is split between read and write queries, allowing you to better understand the cost breakdown for individual assets.\nTo filter assets by context:\nFrom the\nUsage & Cost\ndashboard, scroll down to\nTop tables & views\n.\nIn the\nTop tables & views\nsection, click the filters menu, and depending on the type of asset metrics you'd like to view:\nClick\nMost\nPopular\nÂ to view the top 25 most popular assets.Â\nClick\nMost\nExpensive\nto view the top 25 most expensive assets.\nClick\nMost Queried\nto view the top 25 most queried assets.Â\nDid you know?\nIf you have any questions about usage and popularity metrics, head over\nhere\n.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nglossary\nbusiness-terms\ndefinitions\nPrevious\nReport on queries\nNext\nReport on governance\nFilter asset usage by users\nView least used assets\nFilter assets by context"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata",
    "content": "Connect data\nDatabases\nSQL Databases\nTeradata\nOn this page\nTeradata\nOverview:\nCatalog Teradata schemas, tables, views, and columns in Atlan. Optionally mine query history to build lineage for your Teradata assets.\nGet started\nâ\nFollow these steps to connect and catalog Teradata assets in Atlan:\nSet up the connector\nCrawl Teradata assets\nGuides\nâ\nMine Teradata query history\n: Extract query history and build lineage for your Teradata assets.\nSet up on-premises Teradata miner access\n: Configure access for secure/on-premises Teradata environments.\nReferences\nâ\nWhat does Atlan crawl from Teradata\n: Learn about the Teradata assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Teradata\n: Verify prerequisites before setting up the Teradata connector.\nTags:\nteradata\nconnector\ndatabase\nlineage\nquery history\nconnectivity\nNext\nSet up Teradata\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/troubleshooting/troubleshooting-amazon-msk-connectivity",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nTroubleshooting\nTroubleshooting Amazon MSK connectivity\nOn this page\nTroubleshooting Amazon MSK connectivity\nWhat are the known limitations of the Amazon MSK connector?\nâ\nFollowing are the known limitations of the Amazon MSK connector:\nAtlan currently does not support upstream or downstream lineage for Amazon MSK assets.\nAtlan currently only supports asset-level lineage between\ntopics and consumer groups\n. Column-level lineage is currently not supported.\nSASL/SCRAM and TLS authentication methods are currently not supported. Atlan currently only supports\nIAM role authentication\n.\nServerless deployment for Amazon MSK is currently not supported.\nAtlan currently only supports Apache Kafka version 2.7.1 or higher for Amazon MSK.\nHow to debug test authentication and preflight check errors?\nâ\nInvalid region\nInvalid region. Please provide a valid region.\nEnsure that you have specified the correct\nAWS region in your configuration\n. The region must match where your Amazon MSK cluster is located.\nInvalid role ARN\nInvalid role ARN. Please provide a valid role ARN.\nEnsure that you have specified the correct\nrole ARN in your configuration\n.\nEnsure that the role has the\nrequired permissions\nto perform operations in your Amazon MSK cluster.\nInvalid bootstrap server\nInvalid bootstrap server details. Please provide a valid bootstrap server.\nEnsure that you have specified the correct\nbootstrap server in your configuration\n.\nEnsure that the IAM role has the\nrequired permissions\nto connect to Amazon MSK.\nIf you're also using private network link, ensure that\nprivate network connectivity between Atlan and Amazon MSK\nis properly configured.\nInternal server error\nInternal server error. Please contact Atlan admin.\nThere may be a connectivity issue between Atlan and your Amazon MSK cluster.\nReach out to Atlan support\n.\nTimeout exception\nCaused by: org.apache.kafka.common.errors.TimeoutException: Error listing groups on <broker-address>:<broker-port> : Timed out waiting to send the call.\nEnsure that the port is authorized in the inbound rule of the security group for your Amazon MSK cluster.\nIf you're also using private network link, ensure that\nprivate network connectivity between Atlan and Amazon MSK\nis properly configured.\nEnsure that both your Amazon MSK cluster and Atlan tenant are hosted in the same AWS region.\nTags:\nlineage\ndata-lineage\nimpact-analysis\ndownstream-impact\ndependencies\nupstream-dependencies\ndata-sources\nPrevious\nPreflight checks for Amazon MSK"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/troubleshooting/troubleshooting-apache-airflow-openlineage-connectivity",
    "content": "Connect data\nOrchestration & Workflow\nApache Airflow OpenLineage\nTroubleshooting\nTroubleshooting Apache Airflow/OpenLineage connectivity\nOn this page\nTroubleshooting Apache Airflow/OpenLineage connectivity\nDoes Atlan support Spark SQL?\nâ\nYes, Atlan supports Spark SQL through\nSpark jobs\n.\nHow does Atlan handle multiple DAG owner email addresses?\nâ\nAtlan captures up to the first 10 valid owner email addresses for a DAG. If a DAG has more than 10 owner emails, only the first 10 are stored as âSource Ownersâ in Atlan. To capture DAG owners, make sure to set the owner field when defining the DAG in Apache Airflow. Ensure that the owner emails are comma-separated and valid to be recognized correctly.\nCan I connect multiple Apache Airflow instances to a single Atlan instance?\nâ\nYes. However, you will need to\ncreate a separate connection\nfor each Apache Airflow instance you want to connect to Atlan.\nWhy are some Apache Airflow assets or lineage missing even after the workflow ran successfully?\nâ\nIf OpenLineage has not been configured properly, it may have been unable to send any events while the DAG ran. You can\nview event logs\nin Atlan to track and debug events received from OpenLineage. However, if you have verified that your OpenLineage connection was configured correctly and events are still missing, please reach out to your customer success manager at Atlan or\nraise a support ticket\n.\nDoes Atlan support Column Level Lineage (CLL) for object storage?\nâ\nAtlan currently does not support Column Level Lineage (CLL) for object storage. This is because object storage systems do not have structured schema, unlike relational data sources.\nObject storage systems store unstructured data, unlike relational data sources where columns and relationships are clearly defined. As a result, object storage systems cannot support column level lineage. For example, unstructured data can include a collection of image files stored in an S3 bucket, which doesn't support column-level lineage.\nTo enable CLL for object storage, users must register S3 objects as tables using AWS Glue, Hive, or similar cataloging tools.\nColumn level lineage support is also not available for the following Apache Airflow distributions:\nAmazon MWAA\nAstronomer\nGoogle Cloud Composer\nApache Spark\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nProvider package versions for OpenLineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/troubleshooting/troubleshooting-apache-spark-openlineage-connectivity",
    "content": "Connect data\nOrchestration & Workflow\nApache Spark OpenLineage\nTroubleshooting\nTroubleshooting Apache Spark/OpenLineage connectivity\nOn this page\nTroubleshooting Apache Spark/OpenLineage connectivity\nDoes Atlan support Column Level Lineage (CLL) for object storage?\nâ\nAtlan currently does not support Column Level Lineage (CLL) for object storage. This is because object storage systems do not have structured schema, unlike relational data sources.\nObject storage systems store unstructured data, unlike relational data sources where columns and relationships are clearly defined. As a result, object storage systems cannot support column level lineage. For example, unstructured data can include a collection of image files stored in an S3 bucket, which doesn't support column-level lineage.\nTo enable CLL for object storage, users must register S3 objects as tables using AWS Glue, Hive, or similar cataloging tools.\nColumn level lineage support is also not available for the following Apache Airflow distributions:\nAmazon MWAA\nAstronomer\nGoogle Cloud Composer\nApache Airflow\nTags:\nlineage\ndata-lineage\nimpact-analysis\nschema\nschema-drift\nschema-monitoring\nPrevious\nWhat does Atlan crawl from Apache Spark/OpenLineage?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/troubleshooting/troubleshooting-domo-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nDomo\nTroubleshooting\nTroubleshooting Domo connectivity\nOn this page\nTroubleshooting Domo connectivity\nWhat are the known limitations of the Domo connector?\nâ\nFollowing are the known limitations of the Domo connector:\nColumn-level lineage is currently not supported.\nAdmin privileges are required for the user\ncreating the client credentials\nto crawl both datasets and dataset columns. Otherwise, only datasets will be crawled.\nAll\nthree DomoStats dataset IDs\nare required to run the Domo crawler.\nAn\naccess token\nis required to\ngenerate upstream lineage for datasets\n, currently only supported with Google BigQuery and Snowflake as data sources.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nPreflight checks for Domo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/troubleshooting/troubleshooting-microsoft-azure-cosmos-db-connectivity",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMicrosoft Azure Cosmos DB\nTroubleshooting\nTroubleshooting Microsoft Azure Cosmos DB connectivity\nOn this page\nTroubleshooting Microsoft Azure Cosmos DB connectivity\nWhat are the known limitations of the Microsoft Azure Cosmos DB connector?\nâ\nFollowing are the known limitations:\nAtlan currently does not support asset or column-level lineage for Microsoft Azure Cosmos DB.\nMicrosoft Azure Cosmos DB displays MongoDB views as collections and does not provide metadata for view definition.\nMicrosoft Azure Cosmos DB for NoSQL, Apache Cassandra, Apache Gremlin, Azure Table Storage, and PostgreSQL are currently not supported.\nMicrosoft Azure Cosmos DB for MongoDB deployment currently does not support service principal authentication with role-based access control (RBAC).\nMicrosoft Azure Cosmos DB for MongoDB deployment does not support service principal authentication for vCore cluster-based deployments.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/troubleshooting/troubleshooting-microsoft-azure-data-factory-connectivity",
    "content": "Connect data\nETL Tools\nMicrosoft Azure Data Factory\nTroubleshooting\nTroubleshooting Microsoft Azure Data Factory connectivity\nOn this page\nTroubleshooting Microsoft Azure Data Factory connectivity\nWhat are the known limitations of the Microsoft Azure Data Factory connector?\nâ\nFollowing are the known limitations of the Microsoft Azure Data Factory connector:\nLineage is not supported for data loaded through\ndata flow activity\n.\nThe ability to include or exclude certain pipelines or activities is currently notÂ supported.\nDatasets defined in Microsoft Azure Data Factory are currently not mapped to actual data assets in Atlan.\nAtlan currently does not display pipelines as DAGs on the lineage graph. Only the activities that are part of a pipeline are displayed as related assets.\nLineage is not supported for activities that do not have a defined source and sink attribute in their configuration.\nWhy is lineage missing?\nâ\nRemember that the\nMicrosoft Azure Data Factory\nÂ package only enriches data assets that already exist in Atlan.\nCheck that the data assets corresponding to Microsoft Azure Data Factory's\nsources and sinks\nalready exist in Atlan. For example, that the Microsoft Azure Cosmos DB assets have been\ncrawled from Microsoft Azure Cosmos DB\n, Snowflake assets have been\ncrawled from Snowflake\n, and so on.\nThe fact that a crawler has run (such as Microsoft Azure Cosmos DB or Snowflake) does not mean all assets from that source have been crawled. The filtering configuration of the crawler could have omitted assets. Or the credentials configured in the crawler may not have access to all of the assets.\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/troubleshooting/troubleshooting-mongodb-connectivity",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMongoDB\nTroubleshooting\nTroubleshooting MongoDB connectivity\nOn this page\nTroubleshooting MongoDB connectivity\nDoes Atlan support lineage for MongoDB?\nâ\nAtlan currently does not provide built-in lineage support for MongoDB assets. You can generate and ingest lineage using a custom package. Once lineage is added to an asset, whether upstream or downstream, it appears in Atlanâs lineage view.\nTags:\nlineage\ndata-lineage\nimpact-analysis\ndownstream-impact\ndependencies\nupstream-dependencies\ndata-sources\nPrevious\nWhat does Atlan crawl from MongoDB?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/troubleshooting/troubleshooting-on-premises-looker-connectivity",
    "content": "Connect data\nBI Tools\nCloud-based BI\nLooker\nTroubleshooting\nTroubleshooting on-premises Looker connectivity\nOn this page\nTroubleshooting on-premises Looker connectivity\nHow do I disable cloning Looker projects from git?\nâ\nThe looker-extractor tool clones your Looker git project to parse field-level lineage. By default, the extractor needs the\nlooker_git_private_key\nand\nlooker_git_private_key_passphrase\nsecrets. To disable this behavior, you need to set the\nUSE_FIELD_LEVEL_LINEAGE\nparameter to\n\"false\"\n.\nFor example:\nservices:\n# Example Looker connection\nlooker-example:\n<<: *extract\nenvironment:\n<<: *looker-defaults\nINCLUDE_PROJECTS: \"project-1,project-2\"\nUSE_FIELD_LEVEL_LINEAGE: \"false\"\nvolumes:\n- ./output/looker-example:/output/process\nsecrets:\n- looker_config\nsecrets:\nlooker_config:\nfile: ./looker.ini\nNote that we refer to only the\nlooker-config\nsecret in this\nlooker-example\nservice. This is possible because the\nUSE_FIELD_LEVEL_LINEAGE\nenvironment variable is set to\n\"false\"\n.\nHow do I use different credentials for different connections?\nâ\nYou may define as many Looker connections under the\nservices\nsection as you want. As mentioned before, by default each connection requires the following secrets:\nlooker_config\nlooker_git_private_key\nlooker_git_private_key_passphrase\nYou may want to define many connections with different credentials. For example, you may want to extract field-level lineage for some projects and not for others. To do this, you need to define and refer to those secrets separately for each connection.\nFor example:\nservices:\n# The first-connection uses secrets with default names,\n# so no need to specify its own `secrets` section\nfirst-connection:\n<<: *extract\nenvironment:\n<<: *looker-defaults\nINCLUDE_PROJECTS: \"project-1\"\nUSE_FIELD_LEVEL_LINEAGE: \"true\"\nvolumes:\n- ./output/first-connection:/output/process\n# The second-connection uses alternative secrets,\n# so it defines its own `secrets` section to refer to them\nsecond-connection:\n<<: *extract\nenvironment:\n<<: *looker-defaults\nINCLUDE_PROJECTS: \"project-2\"\nUSE_FIELD_LEVEL_LINEAGE: \"true\"\nvolumes:\n- ./output/first-connection:/output/process\nsecrets:\n- target: looker_config\nsource: alternative_config\n- target: looker_git_private_key\nsource: alternative_private_key\n- target: looker_git_private_key_passphrase\nsource: alternative_key_passphrase\nsecrets:\n# Credentials for the first-connection (default names)\nlooker_config:\nfile: ./looker.ini\nlooker_git_private_key:\nfile: ./id_rsa\nlooker_git_private_key_passphrase:\nfile: ./passphrase.txt\n# Credentials for the second-connection\nalternative_config:\nfile: ./path/to/alternative/looker.ini\nalternative_private_key:\nfile: ./path/to/alternative/id_rsa\nalternative_key_passphrase:\nfile: ./path/to/alternative/passphrase.txt\nIn the\nsecond-connection\nsecrets list, note that:\nthe\ntarget\nattribute gives the default name of the secret the extractor expects\nthe\nsource\nattribute gives the alternative secret name\nTags:\nlineage\ndata-lineage\nimpact-analysis\nPrevious\nTroubleshooting Looker connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/copy-link-share-slack-teams",
    "content": "What is the difference between Copy Link and Share on Slack or Teams?\nWhen using the\nCopy link\nor clipboard button:\nYou must open Slack or Teams and change tabs to paste the URL.\nYou can share the link in any channel you want (including DMs or private chats).\nThe message does not produce a rich embed in Slack or Teams.\nWhen using the\nShare on Slack\nor\nShare on Teams\noption:\nYou can handle the full communication from directly within Atlan, without having to switch to\nSlack\nor\nTeams\n. (You can choose the channel and add a message in Atlan.)\nYou are only able to share on public channels that your Atlan admins have preconfigured.\nYou can also optionally have Atlan add a link to the conversation in Slack or Teams.\nThe message produces a rich embed in your messaging app of choice.\nTags:\nslack\nfaq\nfaq-integrations"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/remote-mcp-overview",
    "content": "Configure Atlan\nAtlan AI\nAtlan MCP\nRemote MCP\nOn this page\nRemote MCP\nPrivate Preview\nThe\nAtlan Remote MCP server\nis a\nhosted, per-tenant implementation\nof the\nModel Context Protocol (MCP)\n. It provides a secure bridge between Atlanâs metadata platform and AI tools, so users can query, search, and update metadata directly from their preferred environments.\nUnlike the\nLocal MCP server\n, which requires you to run a service on your own machine, the Remote MCP server is fully managed and hosted by Atlan. This means:\nNo installation or infrastructure setup is required\nAuthentication and authorization are handled through the same policies and permissions you already use in Atlan\nIntegrations can be enabled quickly across your organization without developer-level setup\nThe Remote MCP server is designed to support both\ninteractive AI experiences\n(for example, searching metadata through Claude or Cursor) and\nautomation workflows\n(for example, using n8n, Windsurf, or Microsoft Copilot Studio).\ninfo\nAn Atlan tenant with Remote MCP enabled is required. If you don't have access, contact\nAtlan Support\nor your Atlan customer team to request enablement.\nAuthentication methods\nâ\nYou can authenticate with the Remote MCP server using two methods. Each method is suited for different tools and use cases.\nOAuth (SSO)\nAPI Key\nOAuth uses your existing Atlan login, including SSO, to authenticate with the MCP server. This method is recommended for interactive tools where users log in with their own identity, since it ensures that roles, policies, and permissions in Atlan are consistently enforced. The OAuth endpoint is available at:\nhttps://<your-tenant>.atlan.com/mcp\nAvailable for Cursor, Claude Connector, and Claude Desktop (via\nmcp-remote\n, intermittent support).\nAPI Key authentication uses a static token generated in Atlan to authenticate with the MCP server. This method is recommended for automation workflows, service-to-service integrations, and other non-interactive environments where OAuth login isn't feasible. The API Key endpoint is available at:\nhttps://<your-tenant>.atlan.com/mcp/api-key\nYou can generate an\nAPI key\nin Atlan by going to\nAdmin Settings â API Keys / Tokens\nand creating a new key. Available for Cursor, n8n, Windsurf, Microsoft Copilot Studio, and Claude Desktop (via\nmcp-remote\n, intermittent support).\nSupported connectors\nâ\nRemote MCP can be integrated with a range of AI assistants and automation frameworks. Each connector has its own setup guide linked below.\nInteractive AI tools\nâ\nCursor\n: Integrate Atlan metadata into the Cursor editor for search, lineage, and updates.\nClaude Connector\n: Use Atlan metadata directly in Claude conversations in the browser.\nClaude Desktop\n: Configure through the\nmcp-remote\npackage (currently intermittent).\nAutomation and workflow tools\nâ\nn8n\n: Automate workflows and service-to-service integrations using Atlan metadata.\nWindsurf\n: Bring Atlan metadata into automation and AI-driven flows.\nMicrosoft Copilot Studio\n: Extend Copilot flows with Atlan metadata context.\nTags:\nAtlan MCP\nremote\nhosted\nAI agents\nPrevious\nAtlan MCP\nNext\nCursor with Remote MCP\nAuthentication methods\nSupported connectors"
  },
  {
    "url": "https://docs.atlan.com/tags/ai",
    "content": "2 docs tagged with \"AI\"\nView all tags\nAtlan AI\nIntegrate and leverage Atlan AI capabilities for enhanced data documentation, and lineage analysis.\nAtlan MCP Overview\nLearn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup."
  },
  {
    "url": "https://docs.atlan.com/tags/atlan-mcp",
    "content": "8 docs tagged with \"Atlan MCP\"\nView all tags\nAtlan MCP Overview\nLearn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup.\nRemote MCP\nLearn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup.\nSet up Claude with Remote MCP\nLearn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access.\nSet up Cursor with Remote MCP\nLearn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access.\nSet up Local MCP Server\nThe Atlan Model Context Protocol (MCP) server enables seamless interaction between external tools and Atlan services. It defines the data structures and conventions used to prepare and transmit context for model execution, making it easier to integrate Atlan into your existing workflows.\nSet up Microsoft Copilot Studio with Remote MCP\nLearn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication.\nSet up n8n with Remote MCP\nLearn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows.\nSet up Windsurf with Remote MCP\nLearn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/tags/tags",
    "content": "One doc tagged with \"tags\"\nView all tags\nTags\nLearn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/classification",
    "content": "One doc tagged with \"classification\"\nView all tags\nTags\nLearn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/categorization",
    "content": "One doc tagged with \"categorization\"\nView all tags\nTags\nLearn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/organization",
    "content": "2 docs tagged with \"organization\"\nView all tags\nDomains\nLearn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way.\nTags\nLearn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/custom-metadata",
    "content": "One doc tagged with \"custom metadata\"\nView all tags\nCustom Metadata\nLearn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information."
  },
  {
    "url": "https://docs.atlan.com/tags/attributes",
    "content": "One doc tagged with \"attributes\"\nView all tags\nCustom Metadata\nLearn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information."
  },
  {
    "url": "https://docs.atlan.com/tags/data-catalog",
    "content": "6 docs tagged with \"data-catalog\"\nView all tags\nCrawl GCS assets\nConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.\nCrawl S3 assets\nConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.\nCustom Metadata\nLearn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information.\nGoogle Cloud Storage\nIntegrate, catalog, and govern Google Cloud Storage assets in Atlan.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/data-products",
    "content": "One doc tagged with \"data-products\"\nView all tags\nData Products\nCreate and manage data products to organize and govern your data assets by domain."
  },
  {
    "url": "https://docs.atlan.com/tags/data-domains",
    "content": "One doc tagged with \"data-domains\"\nView all tags\nData Products\nCreate and manage data products to organize and govern your data assets by domain."
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-members-to-view-reports",
    "content": "Configure Atlan\nAdministration\nGet Started\nAllow members to view reports\nOn this page\nAllow members to view reports\nWho can do this?\nYou will need to be an\nadmin user\nin Atlan to allow\nmember users\nto view the\nreporting center\n.\nAdmin users can control access to the reporting center for member users in their organization. If enabled, member users will be able to view the following dashboards:\nAssets dashboard\nto monitor assets\nGlossary dashboard\nto track metrics for glossaries, categories, and terms\nInsights dashboard\nto track metrics for queries\nUsage and cost dashboard\nto track asset usage and associated costs\ndanger\nPermission to view the\ngovernance\nand\nautomations\ndashboards in the reporting center is reserved for admin users only.\nTo allow member users to view the reporting center\n, follow these steps.\nEnable member users to view reports\nâ\nTo enable member users to view the reporting center:\nFrom the left menu in Atlan, click\nAdmin\n.\nUnder\nWorkspace\n, click\nLabs\n.\nUnder the\nAccess control\nÂ heading of the\nLabs\npage, turn on\nAllow member users to access Reporting Center\n.\nYour member users will now be able to view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards in the reporting center! ð\nIf you would like to revoke access, follow the steps above to turn it off.\nTags:\ndashboards\nvisualization\nanalytics\nPrevious\nAllow guests to request updates\nNext\nDisable user activity\nEnable member users to view reports"
  },
  {
    "url": "https://docs.atlan.com/tags/administration",
    "content": "One doc tagged with \"administration\"\nView all tags\nUsers and groups\nLearn how to manage users and groups in Atlan to control access and organize your data team."
  },
  {
    "url": "https://docs.atlan.com/tags/ai-agents",
    "content": "One doc tagged with \"AI agents\"\nView all tags\nRemote MCP\nLearn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup."
  },
  {
    "url": "https://docs.atlan.com/tags/air-gapped",
    "content": "One doc tagged with \"air-gapped\"\nView all tags\nOn-Premises Databases\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/airflow",
    "content": "One doc tagged with \"airflow\"\nView all tags\nApache Airflow OpenLineage\nIntegrate, catalog, and visualize Apache Airflow lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/aiven",
    "content": "One doc tagged with \"aiven\"\nView all tags\nAiven Kafka\nIntegrate, catalog, and govern Aiven Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/alteryx",
    "content": "2 docs tagged with \"alteryx\"\nView all tags\nAlteryx\nIntegrate, catalog, and govern Alteryx assets in Atlan using OpenLineage.\nSet up Alteryx\nSet up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run."
  },
  {
    "url": "https://docs.atlan.com/tags/amazon",
    "content": "5 docs tagged with \"amazon\"\nView all tags\nAmazon DynamoDB\nIntegrate, catalog, and govern Amazon DynamoDB assets in Atlan.\nAmazon MSK\nIntegrate, catalog, and govern Amazon MSK assets in Atlan.\nAmazon MWAA OpenLineage\nIntegrate, catalog, and visualize Amazon MWAA lineage in Atlan.\nAmazon QuickSight\nIntegrate, catalog, and govern Amazon QuickSight assets in Atlan.\nAmazon Redshift\nIntegrate, catalog, and govern Amazon Redshift assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/amazon-athena",
    "content": "One doc tagged with \"amazon-athena\"\nView all tags\nAmazon Athena\nIntegrate, catalog, and govern Amazon Athena assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/amazon-s-3",
    "content": "4 docs tagged with \"amazon-s3\"\nView all tags\nCrawl S3 assets\nConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/anomalo",
    "content": "One doc tagged with \"anomalo\"\nView all tags\nAnomalo\nIntegrate, catalog, and govern Anomalo assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/apache",
    "content": "3 docs tagged with \"apache\"\nView all tags\nApache Airflow OpenLineage\nIntegrate, catalog, and visualize Apache Airflow lineage in Atlan.\nApache Kafka\nIntegrate, catalog, and govern Apache Kafka assets in Atlan.\nApache Spark OpenLineage\nIntegrate, catalog, and visualize Apache Spark lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/app",
    "content": "6 docs tagged with \"app\"\nView all tags\nAutomatically assign roles based on group names\nLearn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app.\nGenerate lineage between assets App\nLearn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app.\nLineage Generator (no transformations)\nLearn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior.\nSend alerts for workflow events\nLearn how to configure alerts for workflow events in Atlan via email or Google Chat.\nSource asset type\nDetailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app.\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/architecture",
    "content": "One doc tagged with \"architecture\"\nView all tags\nArchitecture\nArchitecture overview and core components of Secure Agent 2.0"
  },
  {
    "url": "https://docs.atlan.com/tags/assests",
    "content": "One doc tagged with \"assests\"\nView all tags\nExport Assets\n:::warning Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to [enable asset export](enable-asset-export)."
  },
  {
    "url": "https://docs.atlan.com/tags/astronomer",
    "content": "One doc tagged with \"astronomer\"\nView all tags\nAstronomer OpenLineage\nIntegrate, catalog, and visualize Astronomer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/atlas",
    "content": "One doc tagged with \"atlas\"\nView all tags\nMongoDB\nCatalog and govern MongoDB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/auto-re-attachment",
    "content": "2 docs tagged with \"auto-re-attachment\"\nView all tags\nEnable auto re-attachment of rules\nLearn how to enable automatic re-attachment of data quality rules to Snowflake tables and views.\nWhat's auto re-attachment\nUnderstand automatic re-attachment of data quality rules to assets that are dropped and recreated."
  },
  {
    "url": "https://docs.atlan.com/tags/aws",
    "content": "4 docs tagged with \"aws\"\nView all tags\nAmazon S3\nIntegrate, catalog, and govern Amazon S3 assets in Atlan.\nAWS Glue\nIntegrate, catalog, and govern AWS Glue assets in Atlan.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler."
  },
  {
    "url": "https://docs.atlan.com/tags/azure",
    "content": "4 docs tagged with \"azure\"\nView all tags\nMicrosoft Azure Cosmos DB\nCatalog and govern Cosmos DB assets in Atlan for discovery and governance.\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan.\nMicrosoft Azure Event Hubs\nIntegrate, catalog, and govern Azure Event Hubs assets in Atlan.\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/bigid",
    "content": "4 docs tagged with \"bigid\"\nView all tags\nBigID\nIntegrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata.\nCrawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nWhat does Atlan crawl from BigID?\nReference guide for BigID metadata crawled by Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/bigquery",
    "content": "One doc tagged with \"bigquery\"\nView all tags\nGoogle BigQuery\nIntegrate, catalog, and govern Google BigQuery assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/calculation-view",
    "content": "One doc tagged with \"calculation-view\"\nView all tags\nSAP HANA\nCatalog and govern SAP HANA assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/cassandra",
    "content": "One doc tagged with \"cassandra\"\nView all tags\nDataStax Enterprise\nCatalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data."
  },
  {
    "url": "https://docs.atlan.com/tags/catalog",
    "content": "7 docs tagged with \"catalog\"\nView all tags\nHow does Atlan handle lineage from Spark jobs?\nLearn about how does atlan handle lineage from spark jobs?.\nIs there a way to build lineage from NetSuite to Snowflake?\nLearn about is there a way to build lineage from netsuite to snowflake?.\nOpenLineage configuration and facets\nLearn about openlineage configuration and facets.\nTroubleshooting Amazon DynamoDB connectivity\nLearn about troubleshooting amazon dynamodb connectivity.\nTroubleshooting IBM Cognos Analytics connectivity\nLearn about troubleshooting ibm cognos analytics connectivity.\nTroubleshooting Tableau connectivity\nLearn about troubleshooting tableau connectivity.\nWhy is my Databricks lineage API not working?\nLearn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/cdi",
    "content": "4 docs tagged with \"cdi\"\nView all tags\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nSet up Informatica CDI\nConfigure authentication and user permissions for Informatica Cloud Data Integration connector\nTask and crawl issues\nTroubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance.\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector"
  },
  {
    "url": "https://docs.atlan.com/tags/claude",
    "content": "One doc tagged with \"Claude\"\nView all tags\nSet up Claude with Remote MCP\nLearn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access."
  },
  {
    "url": "https://docs.atlan.com/tags/cloud",
    "content": "One doc tagged with \"cloud\"\nView all tags\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/cloudera",
    "content": "2 docs tagged with \"cloudera\"\nView all tags\nCloudera Impala\nIntegrate, catalog, and govern Cloudera Impala assets in Atlan.\nCrawl Cloudera Impala\nLearn how to crawl metadata from Cloudera Impala into Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/composer",
    "content": "One doc tagged with \"composer\"\nView all tags\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/concepts",
    "content": "One doc tagged with \"concepts\"\nView all tags\nTransformations\nUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/confluent",
    "content": "2 docs tagged with \"confluent\"\nView all tags\nConfluent Kafka\nIntegrate, catalog, and govern Confluent Kafka assets in Atlan.\nConfluent Schema Registry\nIntegrate, catalog, and govern Confluent Schema Registry assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/connect",
    "content": "2 docs tagged with \"connect\"\nView all tags\nConnectors\nLearn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement."
  },
  {
    "url": "https://docs.atlan.com/tags/container-images",
    "content": "One doc tagged with \"container-images\"\nView all tags\nVerify container images\nVerify the authenticity and integrity of Secure Agent container images with Cosign"
  },
  {
    "url": "https://docs.atlan.com/tags/cosmosdb",
    "content": "One doc tagged with \"cosmosdb\"\nView all tags\nMicrosoft Azure Cosmos DB\nCatalog and govern Cosmos DB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/cratedb",
    "content": "6 docs tagged with \"cratedb\"\nView all tags\nConnection issues\nResolve common connection and authentication issues when setting up CrateDB connector\nCrateDB\nIntegrate, catalog, and govern CrateDB assets in Atlan.\nPermissions and limitations\nFrequently asked questions about CrateDB connector setup, permissions, and limitations\nPreflight checks for CrateDB\nTechnical validations performed before running the CrateDB crawler to verify connectivity and permissions\nSet up CrateDB\nConfigure authentication and connection settings for CrateDB connector\nWhat does Atlan crawl from CrateDB?\nComplete list of CrateDB assets and metadata properties extracted by Atlan during crawling"
  },
  {
    "url": "https://docs.atlan.com/tags/crawling",
    "content": "One doc tagged with \"crawling\"\nView all tags\nCrawl Cloudera Impala\nLearn how to crawl metadata from Cloudera Impala into Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/credentials",
    "content": "One doc tagged with \"credentials\"\nView all tags\nSecret management\nUnderstand how Secure Agent 2.0 handles secrets and why credentials never leave your environment."
  },
  {
    "url": "https://docs.atlan.com/tags/crm",
    "content": "One doc tagged with \"crm\"\nView all tags\nSalesforce\nIntegrate, catalog, and govern Salesforce assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/cursor",
    "content": "One doc tagged with \"Cursor\"\nView all tags\nSet up Cursor with Remote MCP\nLearn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access."
  },
  {
    "url": "https://docs.atlan.com/tags/dagster",
    "content": "4 docs tagged with \"dagster\"\nView all tags\nCrawl Dagster assets\nCreate a crawler workflow in Atlan to capture lineage from Dagster assets\nDagster\nIntegrate, catalog, and visualize Dagster lineage in Atlan.\nSet up Dagster\nConfigure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assets\nWhat does Atlan crawl from Dagster\nLearn about the Dagster metadata that Atlan captures and visualizes"
  },
  {
    "url": "https://docs.atlan.com/tags/dapr",
    "content": "One doc tagged with \"dapr\"\nView all tags\nSecret management\nUnderstand how Secure Agent 2.0 handles secrets and why credentials never leave your environment."
  },
  {
    "url": "https://docs.atlan.com/tags/data-assets",
    "content": "One doc tagged with \"data assets\"\nView all tags\nDomains\nLearn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way."
  },
  {
    "url": "https://docs.atlan.com/tags/data-factory",
    "content": "One doc tagged with \"data factory\"\nView all tags\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-integration",
    "content": "6 docs tagged with \"data integration\"\nView all tags\nAWS Glue\nIntegrate, catalog, and govern AWS Glue assets in Atlan.\nConnectors\nLearn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\nFivetran\nIntegrate, catalog, and govern Fivetran assets in Atlan.\nMatillion\nIntegrate, catalog, and govern Matillion assets in Atlan.\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-lake",
    "content": "One doc tagged with \"data lake\"\nView all tags\nHive\nCatalog and govern Hive assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/data-transformation",
    "content": "One doc tagged with \"data transformation\"\nView all tags\ndbt\nIntegrate, catalog, and govern dbt assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-modeling",
    "content": "One doc tagged with \"data-modeling\"\nView all tags\nData Models\nCreate and manage data models to structure and organize your data assets."
  },
  {
    "url": "https://docs.atlan.com/tags/data-models",
    "content": "One doc tagged with \"data-models\"\nView all tags\nData Models\nCreate and manage data models to structure and organize your data assets."
  },
  {
    "url": "https://docs.atlan.com/tags/data-transfer",
    "content": "One doc tagged with \"data-transfer\"\nView all tags\nData transfer and observability\nUnderstand how metadata moves from your sources to Atlan and what visibility you get into operations."
  },
  {
    "url": "https://docs.atlan.com/tags/database",
    "content": "21 docs tagged with \"database\"\nView all tags\nAmazon Athena\nIntegrate, catalog, and govern Amazon Athena assets in Atlan.\nCloudera Impala\nIntegrate, catalog, and govern Cloudera Impala assets in Atlan.\nConnection issues\nResolve common connection and authentication issues when setting up CrateDB connector\nCrateDB\nIntegrate, catalog, and govern CrateDB assets in Atlan.\nDataStax Enterprise\nCatalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data.\nHive\nCatalog and govern Hive assets in Atlan for discovery and governance.\nMicrosoft Azure Cosmos DB\nCatalog and govern Cosmos DB assets in Atlan for discovery and governance.\nMicrosoft SQL Server\nIntegrate, catalog, and govern Microsoft SQL Server assets in Atlan.\nMongoDB\nCatalog and govern MongoDB assets in Atlan for discovery and governance.\nMySQL\nIntegrate, catalog, and govern MySQL assets in Atlan.\nOn-Premises Databases\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required.\nOracle\nIntegrate, catalog, and govern Oracle assets in Atlan.\nPermissions and limitations\nFrequently asked questions about CrateDB connector setup, permissions, and limitations\nPostgreSQL\nIntegrate, catalog, and govern PostgreSQL assets in Atlan.\nPreflight checks for CrateDB\nTechnical validations performed before running the CrateDB crawler to verify connectivity and permissions\nPrestoSQL\nIntegrate, catalog, and govern PrestoSQL assets in Atlan.\nSAP HANA\nCatalog and govern SAP HANA assets in Atlan for discovery and governance.\nSet up CrateDB\nConfigure authentication and connection settings for CrateDB connector\nTeradata\nCatalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage.\nTrino\nIntegrate, catalog, and govern Trino assets in Atlan.\nWhat does Atlan crawl from CrateDB?\nComplete list of CrateDB assets and metadata properties extracted by Atlan during crawling"
  },
  {
    "url": "https://docs.atlan.com/tags/datastax",
    "content": "One doc tagged with \"datastax\"\nView all tags\nDataStax Enterprise\nCatalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data."
  },
  {
    "url": "https://docs.atlan.com/tags/dbt",
    "content": "One doc tagged with \"dbt\"\nView all tags\ndbt\nIntegrate, catalog, and govern dbt assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/deployment",
    "content": "4 docs tagged with \"deployment\"\nView all tags\nArchitecture\nArchitecture overview and core components of Secure Agent 2.0\nCustomer environment security\nCustomer environment security best practices for deploying and operating Secure Agent 2.0\nDeployment and security\nFrequently asked questions about Secure Agent 2.0 deployment and security\nDeployment options\nUnderstand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/dimensions",
    "content": "One doc tagged with \"dimensions\"\nView all tags\nRules and dimensions\nReference for available data quality rules and classification dimensions in Snowflake data quality."
  },
  {
    "url": "https://docs.atlan.com/tags/docker",
    "content": "One doc tagged with \"docker\"\nView all tags\nDeployment options\nUnderstand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/domains",
    "content": "One doc tagged with \"domains\"\nView all tags\nDomains\nLearn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way."
  },
  {
    "url": "https://docs.atlan.com/tags/domo",
    "content": "One doc tagged with \"domo\"\nView all tags\nDomo\nIntegrate, catalog, and govern Domo assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/dynamodb",
    "content": "One doc tagged with \"dynamodb\"\nView all tags\nAmazon DynamoDB\nIntegrate, catalog, and govern Amazon DynamoDB assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/ecc",
    "content": "One doc tagged with \"ecc\"\nView all tags\nSAP ECC\nIntegrate, catalog, and govern SAP ECC assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/enrichment",
    "content": "One doc tagged with \"enrichment\"\nView all tags\nEnrich Atlan through dbt\nBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property."
  },
  {
    "url": "https://docs.atlan.com/tags/erd",
    "content": "One doc tagged with \"erd\"\nView all tags\nData Models\nCreate and manage data models to structure and organize your data assets."
  },
  {
    "url": "https://docs.atlan.com/tags/erp",
    "content": "6 docs tagged with \"erp\"\nView all tags\nCrawl SAP ECC\nTo crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl SAP S/4HANA\nTo crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nSAP ECC\nIntegrate, catalog, and govern SAP ECC assets in Atlan.\nSAP S/4HANA\nIntegrate, catalog, and govern SAP S/4HANA assets in Atlan.\nSet up SAP ECC\nSet up user accounts and permissions required for SAP ECC metadata extraction in Atlan.\nSet up SAP S/4HANA\nSet up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/etl",
    "content": "7 docs tagged with \"etl\"\nView all tags\nAWS Glue\nIntegrate, catalog, and govern AWS Glue assets in Atlan.\nData Pipelines\nLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.\ndbt\nIntegrate, catalog, and govern dbt assets in Atlan.\nETL tools connectors\nOverview and entry point for all ETL tools connectors in Atlan.\nFivetran\nIntegrate, catalog, and govern Fivetran assets in Atlan.\nMatillion\nIntegrate, catalog, and govern Matillion assets in Atlan.\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/etl-tools",
    "content": "10 docs tagged with \"etl-tools\"\nView all tags\nAlteryx\nIntegrate, catalog, and govern Alteryx assets in Atlan using OpenLineage.\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nInformatica CDI\nIntegrate, catalog, and govern Informatica Cloud Data Integration assets in Atlan.\nSet up Alteryx\nSet up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run.\nSet up Informatica CDI\nConfigure authentication and user permissions for Informatica Cloud Data Integration connector\nSet up Matillion\nConfigure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance.\nTask and crawl issues\nTroubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance.\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector\nTransformations\nUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan\nWhat does Atlan crawl from Informatica CDI\nUnderstand the metadata and assets discovered during crawling from Informatica Cloud Data Integration"
  },
  {
    "url": "https://docs.atlan.com/tags/event-hubs",
    "content": "One doc tagged with \"event hubs\"\nView all tags\nMicrosoft Azure Event Hubs\nIntegrate, catalog, and govern Azure Event Hubs assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-administration",
    "content": "2 docs tagged with \"faq-administration\"\nView all tags\nAdministration and Configuration\nComplete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization.\nUser Management and Access Control\nComplete guide to managing users, configuring access controls, and understanding permissions in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-automation",
    "content": "2 docs tagged with \"faq-automation\"\nView all tags\nAI and Automation Features\nGuide to Atlan's AI capabilities and automation features for enhanced data governance and productivity.\nWorkflows and Data Processing\nEverything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-connections",
    "content": "6 docs tagged with \"faq-connections\"\nView all tags\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nCan the Hive crawler connect to an independent Hive metastore?\nLearn about can the hive crawler connect to an independent hive metastore?.\nData Connections and Integration\nComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.\nHow often does Atlan crawl Snowflake?\nLearn about how often does atlan crawl snowflake?.\nWhat column keys does Atlan crawl?\nLearn about what column keys does atlan crawl?.\nWhat's the difference between connecting to Athena and Glue?\nLearn about what's the difference between connecting to athena and glue?."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-connectors",
    "content": "One doc tagged with \"faq-connectors\"\nView all tags\nPermissions and limitations\nFrequently asked questions about CrateDB connector setup, permissions, and limitations"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-governance",
    "content": "6 docs tagged with \"faq-governance\"\nView all tags\nCan I add duplicate glossary terms?\nEach [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary#term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need.\nCan I create backups of glossaries?\nAtlan also allows you to [export your glossary assets](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) to spreadsheets and keep a record of your contextual information.\nGlossary update request approval issue\nLearn about why am i unable to approve a glossary update request?.\nHow can I use personas to update a term in a glossary?\nBy default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section.\nHow do I fully delete glossary terms or archived items?\nLearn about how do i fully delete glossary terms or archived items?.\nWhat is the default permission for a glossary?\nBy default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data#glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-security",
    "content": "One doc tagged with \"faq-security\"\nView all tags\nSecurity and Compliance\nComplete guide to Atlan's security features, compliance certifications, and data protection capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/firewall",
    "content": "One doc tagged with \"firewall\"\nView all tags\nConfigure network security\nConfigure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services"
  },
  {
    "url": "https://docs.atlan.com/tags/fivetran",
    "content": "One doc tagged with \"fivetran\"\nView all tags\nFivetran\nIntegrate, catalog, and govern Fivetran assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/gcp",
    "content": "2 docs tagged with \"gcp\"\nView all tags\nGoogle Cloud Storage\nIntegrate, catalog, and govern Google Cloud Storage assets in Atlan.\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/gcs",
    "content": "3 docs tagged with \"gcs\"\nView all tags\nCrawl GCS assets\nConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.\nGoogle Cloud Storage\nIntegrate, catalog, and govern Google Cloud Storage assets in Atlan.\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/glue",
    "content": "One doc tagged with \"glue\"\nView all tags\nAWS Glue\nIntegrate, catalog, and govern AWS Glue assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/google",
    "content": "2 docs tagged with \"google\"\nView all tags\nGoogle BigQuery\nIntegrate, catalog, and govern Google BigQuery assets in Atlan.\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/google-gcs",
    "content": "3 docs tagged with \"google-gcs\"\nView all tags\nCrawl GCS assets\nConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan.\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/groups",
    "content": "One doc tagged with \"groups\"\nView all tags\nUsers and groups\nLearn how to manage users and groups in Atlan to control access and organize your data team."
  },
  {
    "url": "https://docs.atlan.com/tags/guides",
    "content": "One doc tagged with \"guides\"\nView all tags\nFrequently Asked Questions\nFind answers to common questions about using Atlan, organized by topic area for quick resolution."
  },
  {
    "url": "https://docs.atlan.com/tags/helm",
    "content": "One doc tagged with \"helm\"\nView all tags\nDeployment options\nUnderstand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/help",
    "content": "One doc tagged with \"help\"\nView all tags\nFrequently Asked Questions\nFind answers to common questions about using Atlan, organized by topic area for quick resolution."
  },
  {
    "url": "https://docs.atlan.com/tags/hive",
    "content": "One doc tagged with \"hive\"\nView all tags\nHive\nCatalog and govern Hive assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/hosted",
    "content": "One doc tagged with \"hosted\"\nView all tags\nRemote MCP\nLearn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup."
  },
  {
    "url": "https://docs.atlan.com/tags/hybrid-bi",
    "content": "5 docs tagged with \"hybrid bi\"\nView all tags\nDomo\nIntegrate, catalog, and govern Domo assets in Atlan.\nMetabase\nIntegrate, catalog, and govern Metabase assets in Atlan.\nMicroStrategy\nIntegrate, catalog, and govern MicroStrategy assets in Atlan.\nSigma\nIntegrate, catalog, and govern Sigma assets in Atlan.\nThoughtSpot\nIntegrate, catalog, and govern ThoughtSpot assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/ibm-cognos",
    "content": "One doc tagged with \"ibm cognos\"\nView all tags\nIBM Cognos Analytics\nIntegrate, catalog, and govern IBM Cognos Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/impala",
    "content": "2 docs tagged with \"impala\"\nView all tags\nCloudera Impala\nIntegrate, catalog, and govern Cloudera Impala assets in Atlan.\nCrawl Cloudera Impala\nLearn how to crawl metadata from Cloudera Impala into Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/informatica",
    "content": "6 docs tagged with \"informatica\"\nView all tags\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nSet up Informatica CDI\nConfigure authentication and user permissions for Informatica Cloud Data Integration connector\nTask and crawl issues\nTroubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance.\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector\nTransformations\nUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan\nWhat does Atlan crawl from Informatica CDI\nUnderstand the metadata and assets discovered during crawling from Informatica Cloud Data Integration"
  },
  {
    "url": "https://docs.atlan.com/tags/inventory-reports",
    "content": "One doc tagged with \"inventory-reports\"\nView all tags\nS3 Inventory Report Structure\nExpected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion."
  },
  {
    "url": "https://docs.atlan.com/tags/kafka",
    "content": "5 docs tagged with \"kafka\"\nView all tags\nAiven Kafka\nIntegrate, catalog, and govern Aiven Kafka assets in Atlan.\nAmazon MSK\nIntegrate, catalog, and govern Amazon MSK assets in Atlan.\nApache Kafka\nIntegrate, catalog, and govern Apache Kafka assets in Atlan.\nConfluent Kafka\nIntegrate, catalog, and govern Confluent Kafka assets in Atlan.\nRedpanda Kafka\nIntegrate, catalog, and govern Redpanda Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/kubernetes",
    "content": "One doc tagged with \"kubernetes\"\nView all tags\nDeployment options\nUnderstand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/logic",
    "content": "One doc tagged with \"logic\"\nView all tags\nTransformations\nUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/looker",
    "content": "One doc tagged with \"looker\"\nView all tags\nLooker\nIntegrate, catalog, and govern Looker assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/matillion",
    "content": "2 docs tagged with \"matillion\"\nView all tags\nMatillion\nIntegrate, catalog, and govern Matillion assets in Atlan.\nSet up Matillion\nConfigure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance."
  },
  {
    "url": "https://docs.atlan.com/tags/messaging",
    "content": "6 docs tagged with \"messaging\"\nView all tags\nAiven Kafka\nIntegrate, catalog, and govern Aiven Kafka assets in Atlan.\nAmazon MSK\nIntegrate, catalog, and govern Amazon MSK assets in Atlan.\nApache Kafka\nIntegrate, catalog, and govern Apache Kafka assets in Atlan.\nConfluent Kafka\nIntegrate, catalog, and govern Confluent Kafka assets in Atlan.\nMicrosoft Azure Event Hubs\nIntegrate, catalog, and govern Azure Event Hubs assets in Atlan.\nRedpanda Kafka\nIntegrate, catalog, and govern Redpanda Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/metabase",
    "content": "One doc tagged with \"metabase\"\nView all tags\nMetabase\nIntegrate, catalog, and govern Metabase assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/metadata-extractor",
    "content": "One doc tagged with \"metadata-extractor\"\nView all tags\nOn-Premises Databases\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/metrics",
    "content": "2 docs tagged with \"metrics\"\nView all tags\nReporting\nGenerate comprehensive reports on your data assets, usage, and governance.\nUsage and Popularity\nTrack and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/microsoft",
    "content": "4 docs tagged with \"microsoft\"\nView all tags\nMicrosoft Azure Data Factory\nIntegrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan.\nMicrosoft Azure Event Hubs\nIntegrate, catalog, and govern Azure Event Hubs assets in Atlan.\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan.\nMicrosoft SQL Server\nIntegrate, catalog, and govern Microsoft SQL Server assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/microsoft-copilot-studio",
    "content": "One doc tagged with \"Microsoft Copilot Studio\"\nView all tags\nSet up Microsoft Copilot Studio with Remote MCP\nLearn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/tags/microsoft-teams",
    "content": "One doc tagged with \"microsoft teams\"\nView all tags\nMicrosoft Teams\nIntegrate Atlan with Microsoft Teams to enable collaboration and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/microstrategy",
    "content": "One doc tagged with \"microstrategy\"\nView all tags\nMicroStrategy\nIntegrate, catalog, and govern MicroStrategy assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/migration",
    "content": "One doc tagged with \"migration\"\nView all tags\nUpgrade to Snowflake data quality studio\nUpdate existing Snowflake data quality integration to the latest version"
  },
  {
    "url": "https://docs.atlan.com/tags/mode",
    "content": "One doc tagged with \"mode\"\nView all tags\nMode\nIntegrate, catalog, and govern Mode assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/mongodb",
    "content": "One doc tagged with \"mongodb\"\nView all tags\nMongoDB\nCatalog and govern MongoDB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/msk",
    "content": "One doc tagged with \"msk\"\nView all tags\nAmazon MSK\nIntegrate, catalog, and govern Amazon MSK assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/mwaa",
    "content": "One doc tagged with \"mwaa\"\nView all tags\nAmazon MWAA OpenLineage\nIntegrate, catalog, and visualize Amazon MWAA lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/mysql",
    "content": "One doc tagged with \"mysql\"\nView all tags\nMySQL\nIntegrate, catalog, and govern MySQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/n-8-n",
    "content": "One doc tagged with \"n8n\"\nView all tags\nSet up n8n with Remote MCP\nLearn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows."
  },
  {
    "url": "https://docs.atlan.com/tags/network",
    "content": "One doc tagged with \"network\"\nView all tags\nConfigure network security\nConfigure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services"
  },
  {
    "url": "https://docs.atlan.com/tags/nosql",
    "content": "3 docs tagged with \"nosql\"\nView all tags\nAmazon DynamoDB\nIntegrate, catalog, and govern Amazon DynamoDB assets in Atlan.\nMicrosoft Azure Cosmos DB\nCatalog and govern Cosmos DB assets in Atlan for discovery and governance.\nMongoDB\nCatalog and govern MongoDB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/oauth",
    "content": "One doc tagged with \"oauth\"\nView all tags\nAuthentication\nUnderstand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure."
  },
  {
    "url": "https://docs.atlan.com/tags/offline",
    "content": "One doc tagged with \"offline\"\nView all tags\nOn-Premises Databases\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/on-premises",
    "content": "One doc tagged with \"on-premises\"\nView all tags\nOn-Premises Databases\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/openlineage",
    "content": "6 docs tagged with \"openlineage\"\nView all tags\nAlteryx\nIntegrate, catalog, and govern Alteryx assets in Atlan using OpenLineage.\nAmazon MWAA OpenLineage\nIntegrate, catalog, and visualize Amazon MWAA lineage in Atlan.\nApache Airflow OpenLineage\nIntegrate, catalog, and visualize Apache Airflow lineage in Atlan.\nApache Spark OpenLineage\nIntegrate, catalog, and visualize Apache Spark lineage in Atlan.\nAstronomer OpenLineage\nIntegrate, catalog, and visualize Astronomer lineage in Atlan.\nGoogle Cloud Composer OpenLineage\nIntegrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/operations",
    "content": "One doc tagged with \"operations\"\nView all tags\nOperations\nAtlan crawls and manages the following data quality operations and results from Snowflake."
  },
  {
    "url": "https://docs.atlan.com/tags/oracle",
    "content": "One doc tagged with \"oracle\"\nView all tags\nOracle\nIntegrate, catalog, and govern Oracle assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/parsing",
    "content": "One doc tagged with \"parsing\"\nView all tags\nSource asset type\nDetailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app."
  },
  {
    "url": "https://docs.atlan.com/tags/policies",
    "content": "One doc tagged with \"policies\"\nView all tags\nStewardship\nLearn how to implement data stewardship in Atlan through automated workflows, policies, and task management."
  },
  {
    "url": "https://docs.atlan.com/tags/popularity",
    "content": "One doc tagged with \"popularity\"\nView all tags\nUsage and Popularity\nTrack and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/postgresql",
    "content": "One doc tagged with \"postgresql\"\nView all tags\nPostgreSQL\nIntegrate, catalog, and govern PostgreSQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/preflight-checks",
    "content": "One doc tagged with \"preflight-checks\"\nView all tags\nPreflight checks for CrateDB\nTechnical validations performed before running the CrateDB crawler to verify connectivity and permissions"
  },
  {
    "url": "https://docs.atlan.com/tags/prestosql",
    "content": "One doc tagged with \"prestosql\"\nView all tags\nPrestoSQL\nIntegrate, catalog, and govern PrestoSQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/privacy",
    "content": "4 docs tagged with \"privacy\"\nView all tags\nBigID\nIntegrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata.\nCrawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nWhat does Atlan crawl from BigID?\nReference guide for BigID metadata crawled by Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/properties",
    "content": "2 docs tagged with \"properties\"\nView all tags\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/qlik-sense",
    "content": "2 docs tagged with \"qlik sense\"\nView all tags\nQlik Sense Cloud\nIntegrate, catalog, and govern Qlik Sense Cloud assets in Atlan.\nQlik Sense Enterprise (Windows)\nIntegrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/query-history",
    "content": "One doc tagged with \"query history\"\nView all tags\nTeradata\nCatalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/quicksight",
    "content": "One doc tagged with \"quicksight\"\nView all tags\nAmazon QuickSight\nIntegrate, catalog, and govern Amazon QuickSight assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/redash",
    "content": "One doc tagged with \"redash\"\nView all tags\nRedash\nIntegrate, catalog, and govern Redash assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/redpanda",
    "content": "One doc tagged with \"redpanda\"\nView all tags\nRedpanda Kafka\nIntegrate, catalog, and govern Redpanda Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/redshift",
    "content": "One doc tagged with \"redshift\"\nView all tags\nAmazon Redshift\nIntegrate, catalog, and govern Amazon Redshift assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/reference",
    "content": "8 docs tagged with \"reference\"\nView all tags\nData quality permissions\nReference for data quality permission scopes and configuration in Atlan.\nOperations\nAtlan crawls and manages the following data quality operations and results from Snowflake.\nRules and dimensions\nReference for available data quality rules and classification dimensions in Snowflake data quality.\nSource asset type\nDetailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app.\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.\nWhat does Atlan crawl from Dagster\nLearn about the Dagster metadata that Atlan captures and visualizes\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/relational",
    "content": "4 docs tagged with \"relational\"\nView all tags\nMicrosoft SQL Server\nIntegrate, catalog, and govern Microsoft SQL Server assets in Atlan.\nMySQL\nIntegrate, catalog, and govern MySQL assets in Atlan.\nOracle\nIntegrate, catalog, and govern Oracle assets in Atlan.\nPostgreSQL\nIntegrate, catalog, and govern PostgreSQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/remote",
    "content": "6 docs tagged with \"remote\"\nView all tags\nRemote MCP\nLearn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup.\nSet up Claude with Remote MCP\nLearn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access.\nSet up Cursor with Remote MCP\nLearn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access.\nSet up Microsoft Copilot Studio with Remote MCP\nLearn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication.\nSet up n8n with Remote MCP\nLearn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows.\nSet up Windsurf with Remote MCP\nLearn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/tags/reporting",
    "content": "One doc tagged with \"reporting\"\nView all tags\nReporting\nGenerate comprehensive reports on your data assets, usage, and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/requests",
    "content": "One doc tagged with \"requests\"\nView all tags\nRequests\nRequest and manage changes to assets that you don't have direct edit access to."
  },
  {
    "url": "https://docs.atlan.com/tags/roles",
    "content": "2 docs tagged with \"roles\"\nView all tags\nAutomatically assign roles based on group names\nLearn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app.\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/rules",
    "content": "2 docs tagged with \"rules\"\nView all tags\nRules and dimensions\nReference for available data quality rules and classification dimensions in Snowflake data quality.\nWhat's Data Quality Studio\nUnderstand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/s-3",
    "content": "4 docs tagged with \"s3\"\nView all tags\nAmazon S3\nIntegrate, catalog, and govern Amazon S3 assets in Atlan.\nCrawl S3 assets\nConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.\nS3 Inventory Report Structure\nExpected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/s-4-hana",
    "content": "One doc tagged with \"s4hana\"\nView all tags\nSAP S/4HANA\nIntegrate, catalog, and govern SAP S/4HANA assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sap",
    "content": "2 docs tagged with \"sap\"\nView all tags\nSAP ECC\nIntegrate, catalog, and govern SAP ECC assets in Atlan.\nSAP S/4HANA\nIntegrate, catalog, and govern SAP S/4HANA assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sap-ecc",
    "content": "One doc tagged with \"sap-ecc\"\nView all tags\nSet up SAP ECC\nSet up user accounts and permissions required for SAP ECC metadata extraction in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sap-hana",
    "content": "One doc tagged with \"sap-hana\"\nView all tags\nSAP HANA\nCatalog and govern SAP HANA assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/sap-s-4-hana",
    "content": "One doc tagged with \"sap-s4hana\"\nView all tags\nSet up SAP S/4HANA\nSet up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/schema",
    "content": "6 docs tagged with \"schema\"\nView all tags\nConfluent Schema Registry\nIntegrate, catalog, and govern Confluent Schema Registry assets in Atlan.\nPreflight Checks for Cloudera Impala\nLearn about preflight checks for cloudera impala.\nTroubleshooting Apache Spark/OpenLineage connectivity\nLearn about troubleshooting apache spark/openlineage connectivity.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Confluent Schema Registry?\nAtlan crawls and maps the following assets and properties from Confluent Schema Registry.\nWhat does Atlan crawl from SAP HANA?\nAtlan crawls and maps the following assets and properties from SAP HANA."
  },
  {
    "url": "https://docs.atlan.com/tags/schema-registry",
    "content": "One doc tagged with \"schema registry\"\nView all tags\nConfluent Schema Registry\nIntegrate, catalog, and govern Confluent Schema Registry assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/schema-drift",
    "content": "5 docs tagged with \"schema-drift\"\nView all tags\nPreflight Checks for Cloudera Impala\nLearn about preflight checks for cloudera impala.\nTroubleshooting Apache Spark/OpenLineage connectivity\nLearn about troubleshooting apache spark/openlineage connectivity.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Confluent Schema Registry?\nAtlan crawls and maps the following assets and properties from Confluent Schema Registry.\nWhat does Atlan crawl from SAP HANA?\nAtlan crawls and maps the following assets and properties from SAP HANA."
  },
  {
    "url": "https://docs.atlan.com/tags/schema-monitoring",
    "content": "5 docs tagged with \"schema-monitoring\"\nView all tags\nPreflight Checks for Cloudera Impala\nLearn about preflight checks for cloudera impala.\nTroubleshooting Apache Spark/OpenLineage connectivity\nLearn about troubleshooting apache spark/openlineage connectivity.\nWhat does Atlan crawl from Cloudera Impala?\nLearn about what does atlan crawl from cloudera impala?.\nWhat does Atlan crawl from Confluent Schema Registry?\nAtlan crawls and maps the following assets and properties from Confluent Schema Registry.\nWhat does Atlan crawl from SAP HANA?\nAtlan crawls and maps the following assets and properties from SAP HANA."
  },
  {
    "url": "https://docs.atlan.com/tags/scopes",
    "content": "One doc tagged with \"scopes\"\nView all tags\nData quality permissions\nReference for data quality permission scopes and configuration in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/secrets",
    "content": "One doc tagged with \"secrets\"\nView all tags\nSecret management\nUnderstand how Secure Agent 2.0 handles secrets and why credentials never leave your environment."
  },
  {
    "url": "https://docs.atlan.com/tags/secure-agent",
    "content": "10 docs tagged with \"secure-agent\"\nView all tags\nArchitecture\nArchitecture overview and core components of Secure Agent 2.0\nAuthentication\nUnderstand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure.\nConfigure network security\nConfigure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services\nCustomer environment security\nCustomer environment security best practices for deploying and operating Secure Agent 2.0\nData transfer and observability\nUnderstand how metadata moves from your sources to Atlan and what visibility you get into operations.\nDeployment and security\nFrequently asked questions about Secure Agent 2.0 deployment and security\nDeployment options\nUnderstand how Secure Agent 2.0 deploys across different container environments.\nSecret management\nUnderstand how Secure Agent 2.0 handles secrets and why credentials never leave your environment.\nSecurity\nSecurity overview and controls for Secure Agent 2.0\nVerify container images\nVerify the authenticity and integrity of Secure Agent container images with Cosign"
  },
  {
    "url": "https://docs.atlan.com/tags/sigma",
    "content": "One doc tagged with \"sigma\"\nView all tags\nSigma\nIntegrate, catalog, and govern Sigma assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sisense",
    "content": "One doc tagged with \"sisense\"\nView all tags\nSisense\nIntegrate, catalog, and govern Sisense assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/soda",
    "content": "One doc tagged with \"soda\"\nView all tags\nSoda\nIntegrate, catalog, and govern Soda assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/spark",
    "content": "One doc tagged with \"spark\"\nView all tags\nApache Spark OpenLineage\nIntegrate, catalog, and visualize Apache Spark lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/spreadsheets",
    "content": "One doc tagged with \"spreadsheets\"\nView all tags\nExport Assets\n:::warning Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to [enable asset export](enable-asset-export)."
  },
  {
    "url": "https://docs.atlan.com/tags/sql",
    "content": "4 docs tagged with \"sql\"\nView all tags\nCloudera Impala\nIntegrate, catalog, and govern Cloudera Impala assets in Atlan.\nCrateDB\nIntegrate, catalog, and govern CrateDB assets in Atlan.\nPrestoSQL\nIntegrate, catalog, and govern PrestoSQL assets in Atlan.\nTrino\nIntegrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sql-server",
    "content": "One doc tagged with \"sql server\"\nView all tags\nMicrosoft SQL Server\nIntegrate, catalog, and govern Microsoft SQL Server assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/stewardship",
    "content": "One doc tagged with \"stewardship\"\nView all tags\nStewardship\nLearn how to implement data stewardship in Atlan through automated workflows, policies, and task management."
  },
  {
    "url": "https://docs.atlan.com/tags/storage",
    "content": "8 docs tagged with \"storage\"\nView all tags\nAmazon DynamoDB\nIntegrate, catalog, and govern Amazon DynamoDB assets in Atlan.\nAmazon S3\nIntegrate, catalog, and govern Amazon S3 assets in Atlan.\nCrawl GCS assets\nConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.\nCrawl S3 assets\nConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.\nGoogle Cloud Storage\nIntegrate, catalog, and govern Google Cloud Storage assets in Atlan.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan.\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler."
  },
  {
    "url": "https://docs.atlan.com/tags/synapse",
    "content": "One doc tagged with \"synapse\"\nView all tags\nMicrosoft Azure Synapse Analytics\nIntegrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/tasks",
    "content": "One doc tagged with \"tasks\"\nView all tags\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector"
  },
  {
    "url": "https://docs.atlan.com/tags/teradata",
    "content": "One doc tagged with \"teradata\"\nView all tags\nTeradata\nCatalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/terminology",
    "content": "One doc tagged with \"terminology\"\nView all tags\nGlossary\nLearn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/thoughtspot",
    "content": "One doc tagged with \"thoughtspot\"\nView all tags\nThoughtSpot\nIntegrate, catalog, and govern ThoughtSpot assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/tokens",
    "content": "One doc tagged with \"tokens\"\nView all tags\nAuthentication\nUnderstand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure."
  },
  {
    "url": "https://docs.atlan.com/tags/transformations",
    "content": "2 docs tagged with \"transformations\"\nView all tags\nTasks, transformations, and lineage\nLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connector\nTransformations\nUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/trino",
    "content": "One doc tagged with \"trino\"\nView all tags\nTrino\nIntegrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/usage",
    "content": "One doc tagged with \"usage\"\nView all tags\nUsage and Popularity\nTrack and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/user-groups",
    "content": "2 docs tagged with \"user groups\"\nView all tags\nAutomatically assign roles based on group names\nLearn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app.\nUser Role Sync\nComplete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/users",
    "content": "One doc tagged with \"users\"\nView all tags\nUsers and groups\nLearn how to manage users and groups in Atlan to control access and organize your data team."
  },
  {
    "url": "https://docs.atlan.com/tags/windsurf",
    "content": "One doc tagged with \"Windsurf\"\nView all tags\nSet up Windsurf with Remote MCP\nLearn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk",
    "content": "Connect data\nEvent/Messaging\nAmazon MSK\nOn this page\nAmazon MSK\nOverview:\nCatalog Amazon MSK clusters, topics, and schemas in Atlan. Gain visibility into event streams, lineage, and governance for your AWS-based messaging platform.\nGet started\nâ\nFollow these steps to connect and catalog Amazon MSK assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Amazon MSK?\n: Detailed list of Amazon MSK asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Amazon MSK\n: Technical checks and requirements needed for a successful Amazon MSK integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Amazon MSK, including permissions and network problems.\nTags:\namazon\nmsk\nkafka\nconnector\nmessaging\nconnectivity\nNext\nSet up Amazon MSK\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server",
    "content": "Connect data\nDatabases\nSQL Databases\nMicrosoft SQL Server\nOn this page\nMicrosoft SQL Server\nOverview:\nCatalog Microsoft SQL Server databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your SQL Server data assets.\nGet started\nâ\nFollow these steps to connect and catalog Microsoft SQL Server assets in Atlan:\nSet up the connector\nCrawl Microsoft SQL Server assets\nGuides\nâ\nSet up a private network link to Microsoft SQL Server on Amazon EC2\n: Configure Atlan to connect to SQL Server in secure, private AWS EC2 environments.\nSet up a private network link to Microsoft SQL Server on Amazon RDS\n: Configure Atlan to connect to SQL Server in secure, private AWS RDS environments.\nReferences\nâ\nWhat does Atlan crawl from Microsoft SQL Server\n: Learn about the Microsoft SQL Server assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Microsoft SQL Server\n: Verify prerequisites before setting up the Microsoft SQL Server connector.\nTags:\nmicrosoft\nsql server\nconnector\ndatabase\nrelational\nconnectivity\nNext\nSet up Microsoft SQL Server\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql",
    "content": "Connect data\nDatabases\nSQL Databases\nMySQL\nOn this page\nMySQL\nOverview:\nCatalog MySQL databases, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your MySQL data assets.\nGet started\nâ\nFollow these steps to connect and catalog MySQL assets in Atlan:\nSet up the connector\nCrawl MySQL assets\nGuides\nâ\nSet up a private network link to MySQL\n: Configure a secure private connection to your MySQL database.\nReferences\nâ\nWhat does Atlan crawl from MySQL\n: Learn about the MySQL assets and metadata that Atlan discovers and catalogs.\nPreflight checks for MySQL\n: Verify prerequisites before setting up the MySQL connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common MySQL connection issues and errors.\nTags:\nmysql\nconnector\ndatabase\nrelational\nconnectivity\nNext\nSet up MySQL\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena",
    "content": "Connect data\nDatabases\nQuery Engines\nAmazon Athena\nOn this page\nAmazon Athena\nOverview:\nCatalog Amazon Athena databases, schemas, tables, views, and columns in Atlan. Organize and govern your Athena data lake assets with rich metadata for discovery.\nGet started\nâ\nFollow these steps to connect and catalog Amazon Athena assets in Atlan:\nSet up the connector\nCrawl Amazon Athena assets\nGuides\nâ\nSet up a private network link to Amazon Athena\n: Configure Atlan to connect to Athena in secure, private AWS environments.\nReferences\nâ\nWhat does Atlan crawl from Amazon Athena\n: Learn about the Amazon Athena assets and metadata that Atlan discovers and catalogs.\nTags:\namazon-athena\nconnector\ndatabase\nconnectivity\nNext\nSet up Amazon Athena\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka",
    "content": "Connect data\nEvent/Messaging\nApache Kafka\nOn this page\nApache Kafka\nOverview:\nCatalog Apache Kafka topics, schemas, and consumer groups in Atlan. Gain visibility into event streams, lineage, and governance for your messaging platform.\nGet started\nâ\nFollow these steps to connect and catalog Apache Kafka assets in Atlan:\nSet up the connector\nCrawl Apache Kafka assets\nGuides\nâ\nSet up on-premises Kafka access\n: Configure Atlan to connect to Kafka environments that are isolated from the public internet.\nCrawl on-premises Kafka\n: Extract metadata from on-premises Kafka instances.\nReferences\nâ\nWhat does Atlan crawl from Apache Kafka\n: Learn about the Apache Kafka assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Apache Kafka\n: Verify prerequisites before setting up the Apache Kafka connector.\nTags:\napache\nkafka\nconnector\nmessaging\nconnectivity\nNext\nSet up Apache Kafka\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala",
    "content": "Connect data\nDatabases\nQuery Engines\nCloudera Impala\nOn this page\nCloudera Impala\nOverview:\nCatalog Cloudera Impala databases, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your Cloudera Impala data assets.\nGet started\nâ\nFollow these steps to connect and catalog Cloudera Impala assets in Atlan:\nSet up the connector\nCrawl Cloudera Impala assets\nReferences\nâ\nWhat does Atlan crawl from Cloudera Impala\n: Detailed list of metadata objects extracted from Cloudera Impala.\nPreflight checks for Cloudera Impala\n: Verify your environment before running crawlers.\nTags:\nconnector\ndatabase\ncloudera\nimpala\nsql\nconnectivity\nNext\nSet up Cloudera Impala\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/how-tos/crawl-cloudera-impala",
    "content": "Connect data\nDatabases\nQuery Engines\nCloudera Impala\nCrawl Cloudera Impala Assets\nCrawl Cloudera Impala\nOn this page\nCrawl Cloudera Impala\nOnce you have\nconfigured the Cloudera Impala user permissions\n, you can establish a connection between Atlan and Cloudera Impala.\nTo crawl metadata from Cloudera Impala, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Cloudera Impala as your source:\nIn the top right of any screen in Atlan, navigate to\n+New\nand click\nNew Workflow\n.\nFrom the\nMarketplace\npage, click\nCloudera Impala Assets\n.\nIn the right panel, click\nSetup Workflow\n.\nProvide your credentials\nâ\nTo enter your Cloudera Impala credentials:\nFor\nExtraction method\n,\nDirect\nis the default selection.\nFor\nHostname\n, enter the host name of your Cloudera Impala coordinator or load balancer.\nFor\nAuthentication\n, select\nLDAP\nas the authentication method.\nFor\nUsername\n, enter the LDAP username that has access to Cloudera Impala.\nFor\nPassword\n, enter the password associated with the LDAP username.\nFor\nSSL\n, keep\nEnabled\nto connect via a Secure Sockets Layer (SSL) channel or click\nDisabled\n.\nClick the\nTest Authentication\nbutton to confirm connectivity to Cloudera Impala.\nOnce authentication is successful, navigate to the bottom of the screen and click\nNext\n.\nConfigure the connection\nâ\nTo complete the Cloudera Impala connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users who are able to manage this connection, change the users or groups listed under\nConnection Admins\n.\nCareful\nIf you do not specify any user or group, no one will be able to manage the connection â not even admins.\nNavigate to the bottom of the screen and click\nNext\nto proceed.\nConfigure the crawler\nâ\nBefore running the Cloudera Impala crawler, you can further configure it.\nOn the\nMetadata Filters\npage, you can override the defaults for any of these options:\nTo include specific assets in crawling, click\nInclude Metadata\n, and select the assets you want. If you don't select any, all assets will be included by default.\nTo exclude specific assets from crawling, click\nExclude Metadata\n, and choose the assets you want to omit. If you don't select any, no assets will be excluded.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Cloudera Impala crawler, after completing the steps above:\nTo run the crawler once, immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule & Run\nbutton.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! ð\nTags:\ncloudera\nimpala\nconnectivity\ncrawling\nPrevious\nSet up Cloudera Impala\nNext\nWhat does Atlan crawl from Cloudera Impala?\nSelect the source\nProvide your credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase",
    "content": "Connect data\nBI Tools\nCloud-based BI\nMetabase\nOn this page\nMetabase\nOverview:\nCatalog Metabase questions, dashboards, and collections in Atlan. Gain visibility into lineage, usage, and governance for your Metabase analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Metabase assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Metabase?\n: Detailed list of Metabase asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Metabase\n: Technical checks and requirements needed for a successful Metabase integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Metabase, including permissions and network problems.\nTags:\nmetabase\nconnector\nbusiness intelligence\nhybrid bi\nconnectivity\nNext\nSet up Metabase\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases",
    "content": "Connect data\nDatabases\nOn-premises\nOn-premises Databases\nOn this page\nOn-Premises Databases\nOverview:\nExtract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required.\nGet started\nâ\nFollow these steps to extract and ingest metadata from on-premises databases:\nSet up on-premises database access\nCrawl on-premises databases\nGuides\nâ\nConnect on-premises databases to Kubernetes\n: For advanced/enterprise environments.\nReferences\nâ\nSupported connections for on-premises databases\n: Full list of supported databases and configuration options.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues and offline extraction scenarios.\nTags:\non-premises\nmetadata-extractor\noffline\ndatabase\nair-gapped\nconnectivity\nNext\nSet up on-premises database access\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql",
    "content": "Connect data\nDatabases\nSQL Databases\nPostgreSQL\nOn this page\nPostgreSQL\nOverview:\nCatalog PostgreSQL databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your PostgreSQL data assets.\nGet started\nâ\nFollow these steps to connect and catalog PostgreSQL assets in Atlan:\nSet up the connector\nCrawl PostgreSQL assets\nGuides\nâ\nSet up a private network link to PostgreSQL\n: Configure a secure private connection to your PostgreSQL database.\nReferences\nâ\nWhat does Atlan crawl from PostgreSQL\n: Learn about the PostgreSQL assets and metadata that Atlan discovers and catalogs.\nPreflight checks for PostgreSQL\n: Verify prerequisites before setting up the PostgreSQL connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common PostgreSQL connection issues and errors.\nTags:\npostgresql\nconnector\ndatabase\nrelational\nconnectivity\nNext\nSet up PostgreSQL\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka",
    "content": "Connect data\nEvent/Messaging\nAiven Kafka\nOn this page\nAiven Kafka\nOverview:\nCatalog Aiven Kafka topics, schemas, and consumer groups in Atlan. Gain visibility into event streams, lineage, and governance for your Aiven-hosted messaging platform.\nGet started\nâ\nFollow these steps to connect and catalog Aiven Kafka assets in Atlan:\nSet up the connector\nCrawl assets\nGuides\nâ\nSet up on-premises Kafka access\nCrawl on-premises Kafka\nReferences\nâ\nWhat does Atlan crawl from Aiven Kafka?\n: Detailed list of Aiven Kafka asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Aiven Kafka\n: Technical checks and requirements needed for a successful Aiven Kafka integration.\nTags:\naiven\nkafka\nconnector\nmessaging\nconnectivity\nNext\nSet up Aiven Kafka\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nAmazon DynamoDB\nOn this page\nAmazon DynamoDB\nOverview:\nCatalog Amazon DynamoDB tables, items, and attributes in Atlan. Gain visibility into NoSQL data structures, access patterns, and governance for your AWS-based storage.\nGet started\nâ\nFollow these steps to connect and catalog Amazon DynamoDB assets in Atlan:\nSet up the connector\nCrawl Amazon DynamoDB assets\nReferences\nâ\nWhat does Atlan crawl from Amazon DynamoDB\n: Learn about the DynamoDB assets and metadata that Atlan discovers and catalogs.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Amazon DynamoDB connection issues and errors.\nTags:\namazon\ndynamodb\nconnector\nstorage\nnosql\nconnectivity\nNext\nSet up Amazon DynamoDB\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3",
    "content": "Connect data\nStorage\nAmazon S3\nOn this page\nAmazon S3\nOverview:\nCatalog Amazon S3 buckets and objects in Atlan. Gain visibility into your S3 data assets and their organization.\nGet started\nâ\nFollow these steps to connect and catalog Amazon S3 assets in Atlan:\nSet up Amazon S3\n: Configure AWS permissions and credentials\nCrawl S3 assets\n: Run the crawler to catalog your S3 assets\nGuides\nâ\nSet up inventory reports for S3\n: Configure inventory reports for efficient large-scale ingestion\nReferences\nâ\nInventory report structure for Amazon S3\n: Required folder structure and format for S3 inventory reports\nWhat does Atlan crawl from S3\n: Complete reference for S3 assets and properties that Atlan crawls\nTags:\ns3\nstorage\naws\nconnectivity\nNext\nSet up Amazon S3\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue",
    "content": "Connect data\nETL Tools\nAWS Glue\nOn this page\nAWS Glue\nOverview:\nCatalog AWS Glue jobs, crawlers, and databases in Atlan. Gain visibility into lineage, transformations, and governance for your AWS ETL assets.\nGet started\nâ\nFollow these steps to connect and catalog AWS Glue assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from AWS Glue?\n: Detailed list of AWS Glue asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to AWS Glue, including permissions and network problems.\nTags:\naws\nglue\nconnector\netl\ndata integration\nconnectivity\nNext\nSet up AWS Glue\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka",
    "content": "Connect data\nEvent/Messaging\nConfluent Kafka\nOn this page\nConfluent Kafka\nOverview:\nCatalog Confluent Kafka topics, schemas, and connectors in Atlan. Gain visibility into event streams, lineage, and governance for your Confluent platform.\nGet started\nâ\nFollow these steps to connect and catalog Confluent Kafka assets in Atlan:\nSet up the connector\nCrawl Confluent Kafka assets\nGuides\nâ\nSet up on-premises Kafka access\n: Configure Atlan to connect to Kafka environments that are isolated from the public internet.\nCrawl on-premises Kafka\n: Extract metadata from on-premises Kafka instances.\nReferences\nâ\nWhat does Atlan crawl from Confluent Kafka\n: Learn about the Confluent Kafka assets and metadata that Atlan discovers and catalogs.\nReferences\nâ\nWhat does Atlan crawl from Confluent Kafka\n: Learn about the Confluent Kafka assets and metadata that Atlan discovers and catalogs.\nTags:\nconfluent\nkafka\nconnector\nmessaging\nconnectivity\nNext\nSet up Confluent Kafka\nGet started\nGuides\nReferences\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry",
    "content": "Connect data\nEvent/Messaging\nConfluent Schema Registry\nOn this page\nConfluent Schema Registry\nOverview:\nCatalog Confluent Schema Registry subjects and schemas in Atlan. Gain visibility into schema evolution, compatibility, and governance for your event data structures.\nGet started\nâ\nFollow these steps to connect and catalog Confluent Schema Registry assets in Atlan:\nSet up the connector\nCrawl Confluent Schema Registry assets\nReferences\nâ\nWhat does Atlan crawl from Confluent Schema Registry\n: Learn about the Schema Registry assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Confluent Schema Registry\n: Verify prerequisites before setting up the Confluent Schema Registry connector.\nTags:\nconfluent\nschema registry\nconnector\nschema\nconnectivity\nNext\nSet up Confluent Schema Registry\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo",
    "content": "Connect data\nBI Tools\nCloud-based BI\nDomo\nOn this page\nDomo\nOverview:\nCatalog Domo dashboards, cards, and datasets in Atlan. Gain visibility into lineage, usage, and governance for your Domo analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Domo assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Domo?\n: Detailed list of Domo asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Domo\n: Technical checks and requirements needed for a successful Domo integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Domo, including permissions and network problems.\nTags:\ndomo\nconnector\nbusiness intelligence\nhybrid bi\nconnectivity\nNext\nSet up Domo\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran",
    "content": "Connect data\nETL Tools\nFivetran\nOn this page\nFivetran\nOverview:\nCatalog Fivetran connectors, destinations, and transformations in Atlan. Gain visibility into lineage, data movement, and governance for your Fivetran ETL assets.\nGet started\nâ\nFollow these steps to connect and catalog Fivetran assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Fivetran?\n: Detailed list of Fivetran asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Fivetran\n: Technical checks and requirements needed for a successful Fivetran integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Fivetran, including permissions and network problems.\nTags:\nfivetran\nconnector\netl\ndata integration\nconnectivity\nNext\nSet up Fivetran\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive",
    "content": "Connect data\nDatabases\nQuery Engines\nHive\nOn this page\nHive\nOverview:\nCatalog Hive databases, schemas, tables, views, materialized views, and columns in Atlan for discovery and governance.\nGet started\nâ\nFollow these steps to connect and catalog Hive assets in Atlan:\nSet up the connector\nCrawl Hive assets\nGuides\nâ\nSet up a private network link to Hive\n: Configure a secure private connection to your Hive metastore.\nReferences\nâ\nWhat does Atlan crawl from Hive\n: Learn about the Hive assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Hive\n: Verify prerequisites before setting up the Hive connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Hive connection issues and metastore limitations.\nTags:\nhive\nconnector\ndatabase\ndata lake\nconnectivity\nNext\nSet up Hive\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi",
    "content": "Connect data\nETL Tools\nInformatica CDI\nOn this page\nInformatica CDI\nOverview:\nConnect to Informatica Cloud Data Integration to automatically discover, catalog, and track lineage for your data integration workflows and assets.\nGet started\nâ\nFollow these steps to connect and catalog Informatica CDI assets in Atlan:\nSet up the connector\n: Configure user authentication and gather required parameter files\nCrawl Informatica CDI assets\n: Create crawler workflow and discover your assets\nConcepts\nâ\nTransformation logic\n: Understand how transformation logic and business rules are discovered and cataloged\nReferences\nâ\nWhat does Atlan crawl from Informatica CDI\n: Understand the metadata and assets discovered during crawling\nTroubleshooting\nâ\nTask and crawl issues\n: Fix connection problems and resolve crawling issues\nFAQ\nâ\nTasks, transformations, and lineage\n: Get answers about tasks, transformations, API calls, parameter files, lineage, and limitations\nTags:\nconnector\netl-tools\nconnectivity\nNext\nSet up Informatica CDI\nGet started\nConcepts\nReferences\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion",
    "content": "Connect data\nETL Tools\nMatillion\nOn this page\nMatillion\nOverview:\nCatalog Matillion jobs, components, and transformations in Atlan. Gain visibility into lineage, data movement, and governance for your Matillion ETL assets.\nGet started\nâ\nFollow these steps to connect and catalog Matillion assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Matillion?\n: Detailed list of Matillion asset types and metadata fields that Atlan can extract and catalog.\nWhat lineage does Atlan extract from Matillion?\n: Learn about supported lineage extraction for Matillion assets.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Matillion, including permissions and network problems.\nTags:\nmatillion\nconnector\netl\ndata integration\nconnectivity\nNext\nSet up Matillion\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMicrosoft Azure Cosmos DB\nOn this page\nMicrosoft Azure Cosmos DB\nOverview:\nCatalog Microsoft Azure Cosmos DB accounts, databases, collections, and columns in Atlan for discovery and governance.\nGet started\nâ\nFollow these steps to connect and catalog Cosmos DB assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\n: Detailed list of Cosmos DB asset types and metadata fields.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues.\nTags:\ncosmosdb\nazure\nconnector\ndatabase\nnosql\nconnectivity\nNext\nSet up Microsoft Azure Cosmos DB\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory",
    "content": "Connect data\nETL Tools\nMicrosoft Azure Data Factory\nOn this page\nMicrosoft Azure Data Factory\nOverview:\nCatalog Microsoft Azure Data Factory pipelines, datasets, and activities in Atlan. Gain visibility into lineage, data movement, and governance for your Azure ETL assets.\nGet started\nâ\nFollow these steps to connect and catalog Microsoft Azure Data Factory assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Microsoft Azure Data Factory?\n: Detailed list of Azure Data Factory asset types and metadata fields that Atlan can extract and catalog.\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\n: Learn about supported lineage extraction for Azure Data Factory assets.\nPreflight checks for Microsoft Azure Data Factory\n: Technical checks and requirements needed for a successful Azure Data Factory integration.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Microsoft Azure Data Factory, including permissions and network problems.\nTags:\nmicrosoft\nazure\ndata factory\nconnector\netl\ndata integration\nconnectivity\nNext\nSet up Microsoft Azure Data Factory\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs",
    "content": "Connect data\nEvent/Messaging\nMicrosoft Azure Event Hubs\nOn this page\nMicrosoft Azure Event Hubs\nOverview:\nCatalog Azure Event Hubs namespaces and event hubs in Atlan. Gain visibility into event streams, lineage, and governance for your Azure-based messaging platform.\nGet started\nâ\nFollow these steps to connect and catalog Azure Event Hubs assets in Atlan:\nSet up the connector\nCrawl assets\nReferences\nâ\nWhat does Atlan crawl from Azure Event Hubs?\n: Detailed list of Azure Event Hubs asset types and metadata fields that Atlan can extract and catalog.\nTags:\nmicrosoft\nazure\nevent hubs\nconnector\nmessaging\nconnectivity\nNext\nSet up Microsoft Azure Event Hubs\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics",
    "content": "Connect data\nData Warehouses\nMicrosoft Azure Synapse Analytics\nOn this page\nMicrosoft Azure Synapse Analytics\nOverview:\nCatalog Microsoft Azure Synapse Analytics workspaces, pools, databases, and tables in Atlan. Gain visibility into lineage, usage, and governance for your Azure data warehouse assets.\nGet started\nâ\nFollow these steps to connect and catalog Microsoft Azure Synapse Analytics assets in Atlan:\nSet up the connector\nCrawl Microsoft Azure Synapse Analytics assets\nGuides\nâ\nMine Microsoft Azure Synapse Analytics\n: Extract query history and build lineage for your Synapse Analytics assets.\nSet up on-premises Microsoft Azure Synapse Analytics miner access\n: Configure Atlan to mine query history from on-premises Synapse Analytics environments.\nReferences\nâ\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics\n: Learn about the Synapse Analytics assets and metadata that Atlan discovers and catalogs.\nWhat lineage does Atlan extract from Microsoft Azure Synapse Analytics\n: Learn about supported lineage extraction for Synapse Analytics assets.\nPreflight checks for Microsoft Azure Synapse Analytics\n: Verify prerequisites before setting up the Microsoft Azure Synapse Analytics connector.\nTags:\nmicrosoft\nazure\nsynapse\nanalytics\nconnector\ndata warehouse\nconnectivity\nNext\nSet up Microsoft Azure Synapse Analytics\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb",
    "content": "Connect data\nDatabases\nNoSQL Databases\nMongoDB\nOn this page\nMongoDB\nOverview:\nCatalog MongoDB databases, collections, and columns in Atlan for discovery and governance.\nGet started\nâ\nFollow these steps to connect and catalog MongoDB assets in Atlan:\nSet up the connector\nCrawl MongoDB assets\nReferences\nâ\nWhat does Atlan crawl from MongoDB\n: Learn about the MongoDB assets and metadata that Atlan discovers and catalogs.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common MongoDB connection issues and errors.\nTags:\nmongodb\natlas\nconnector\ndatabase\nnosql\nconnectivity\nNext\nSet up MongoDB\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle",
    "content": "Connect data\nDatabases\nSQL Databases\nOracle\nOn this page\nOracle\nOverview:\nCatalog Oracle databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your Oracle data assets.\nGet started\nâ\nFollow these steps to connect and catalog Oracle assets in Atlan:\nSet up the connector\nCrawl Oracle assets\nReferences\nâ\nWhat does Atlan crawl from Oracle\n: Learn about the Oracle assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Oracle\n: Verify prerequisites before setting up the Oracle connector.\nTags:\noracle\nconnector\ndatabase\nrelational\nconnectivity\nNext\nSet up Oracle\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql",
    "content": "Connect data\nDatabases\nQuery Engines\nPrestoSQL\nOn this page\nPrestoSQL\nOverview:\nCatalog PrestoSQL databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your PrestoSQL data assets. Atlan currently only supports PrestoSQL up to version 349. PrestoDB is not supported.\nGet started\nâ\nFollow these steps to connect and catalog PrestoSQL assets in Atlan:\nSet up the connector\nCrawl PrestoSQL assets\nReferences\nâ\nWhat does Atlan crawl from PrestoSQL\n: Learn about the PrestoSQL assets and metadata that Atlan discovers and catalogs.\nPreflight checks for PrestoSQL\n: Verify prerequisites before setting up the PrestoSQL connector.\nTags:\nprestosql\nconnector\ndatabase\nsql\nconnectivity\nNext\nSet up PrestoSQL\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows",
    "content": "Connect data\nBI Tools\nOn-premises & Enterprise BI\nQlik Sense Enterprise on Windows\nOn this page\nQlik Sense Enterprise (Windows)\nOverview:\nCatalog apps, sheets, and data sources from Qlik Sense Enterprise on Windows in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Qlik Sense Enterprise on Windows assets in Atlan:\nSet up the connector\n: Configure Atlan to connect to your Qlik Sense Enterprise on Windows environment.\nCrawl assets\n: Extract and catalog Qlik Sense Enterprise on Windows apps, sheets, and data sources.\nReferences\nâ\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\n: Detailed list of Qlik Sense Enterprise on Windows asset types and metadata fields that Atlan can extract and catalog.\nTags:\nqlik sense\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Qlik Sense Enterprise on Windows\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka",
    "content": "Connect data\nEvent/Messaging\nRedpanda Kafka\nOn this page\nRedpanda Kafka\nOverview:\nCatalog Redpanda topics, schemas, and consumer groups in Atlan. Gain visibility into event streams, lineage, and governance for your Redpanda messaging platform.\nGet started\nâ\nFollow these steps to connect and catalog Redpanda Kafka assets in Atlan:\nSet up the connector\nCrawl assets\nGuides\nâ\nSet up on-premises Kafka access\nCrawl on-premises Kafka\nReferences\nâ\nWhat does Atlan crawl from Redpanda Kafka?\n: Detailed list of Redpanda Kafka asset types and metadata fields that Atlan can extract and catalog.\nPreflight checks for Redpanda Kafka\n: Technical checks and requirements needed for a successful Redpanda Kafka integration.\nTags:\nredpanda\nkafka\nconnector\nmessaging\nconnectivity\nNext\nSet up Redpanda Kafka\nGet started\nGuides\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc",
    "content": "Connect data\nERP\nSAP ECC\nOn this page\nSAP ECC\nOverview:\nCatalog SAP ECC modules, tables, and fields in Atlan. Gain visibility into lineage, usage, and governance for your enterprise resource planning data.\nGet started\nâ\nFollow these steps to connect and catalog SAP ECC assets in Atlan:\nSet up SAP ECC\n: Configure user accounts and permissions for metadata extraction\nCrawl SAP ECC assets\n: Extract and catalog your SAP ECC metadata\nReferences\nâ\nWhat does Atlan crawl from SAP ECC\n: Learn about the SAP ECC assets and metadata that Atlan discovers and catalogs.\nTags:\nsap\necc\nconnector\nerp\nconnectivity\nNext\nSet up SAP ECC\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana",
    "content": "Connect data\nDatabases\nSQL Databases\nSAP HANA\nOn this page\nSAP HANA\nOverview:\nCatalog SAP HANA schemas, tables, views, calculation views, columns, and stored procedures in Atlan for discovery and governance.\nGet started\nâ\nFollow these steps to connect and catalog SAP HANA assets in Atlan:\nSet up the connector\nCrawl SAP HANA assets\nReferences\nâ\nWhat does Atlan crawl from SAP HANA\n: Learn about the SAP HANA assets and metadata that Atlan discovers and catalogs.\nPreflight checks for SAP HANA\n: Verify prerequisites before setting up the SAP HANA connector.\nTags:\nsap-hana\nconnector\ndatabase\ncalculation-view\nconnectivity\nNext\nSet up SAP HANA\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana",
    "content": "Connect data\nERP\nSAP S/4HANA\nOn this page\nSAP S/4HANA\nOverview:\nCatalog SAP S/4HANA components, tables, views, CDS views, and more in Atlan. Gain visibility into your enterprise resource planning data and its lineage.\nGet started\nâ\nFollow these steps to connect and catalog SAP S/4HANA assets in Atlan:\nSet up SAP S/4HANA\n: Configure user accounts and permissions for metadata extraction\nCrawl SAP S/4HANA assets\n: Extract and catalog your SAP S/4HANA metadata\nReferences\nâ\nWhat does Atlan crawl from SAP S/4HANA\n: Learn about the SAP S/4HANA assets and metadata that Atlan discovers and catalogs.\nTags:\nsap\ns4hana\nconnector\nerp\nconnectivity\nNext\nSet up SAP S/4HANA\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense",
    "content": "Connect data\nBI Tools\nCloud-based BI\nSisense\nOn this page\nSisense\nOverview:\nCatalog dashboards, widgets, and data models from Sisense in Atlan to enable discovery, lineage, and governance for your analytics assets.\nGet started\nâ\nFollow these steps to connect and catalog Sisense assets in Atlan:\nSet up the connector\n: Configure Atlan to connect to your Sisense environment.\nCrawl assets\n: Extract and catalog Sisense dashboards, widgets, and data models.\nReferences\nâ\nWhat does Atlan crawl from Sisense?\n: Detailed list of Sisense asset types and metadata fields that Atlan can extract and catalog.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Solutions for common issues encountered when connecting Atlan to Sisense, including permissions and network problems.\nTags:\nsisense\nconnector\nbusiness intelligence\nconnectivity\nNext\nSet up Sisense\nGet started\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino",
    "content": "Connect data\nDatabases\nQuery Engines\nTrino\nOn this page\nTrino\nOverview:\nCatalog Trino databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your Trino data assets. Atlan supports Trino with basic authentication and private network options.\nGet started\nâ\nFollow these steps to connect and catalog Trino assets in Atlan:\nSet up the connector\nCrawl Trino assets\nGuides\nâ\nSet up a private network link to Trino\n: Configure Atlan to connect to Trino in secure, private environments.\nReferences\nâ\nWhat does Atlan crawl from Trino\n: Learn about the Trino assets and metadata that Atlan discovers and catalogs.\nPreflight checks for Trino\n: Verify prerequisites before setting up the Trino connector.\nTroubleshooting\nâ\nTroubleshooting connectivity\n: Resolve common Trino connection issues and errors.\nTags:\ntrino\nconnector\ndatabase\nsql\nconnectivity\nNext\nSet up Trino\nGet started\nGuides\nReferences\nTroubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb",
    "content": "Connect data\nDatabases\nSQL Databases\nCrateDB\nOn this page\nCrateDB\nOverview:\nCatalog CrateDB databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your CrateDB data assets.\nGet started\nâ\nFollow these steps to connect and catalog CrateDB assets in Atlan:\nSet up the connector\nCrawl CrateDB assets\nReferences\nâ\nWhat does Atlan crawl from CrateDB\n: Learn about the CrateDB assets and metadata that Atlan discovers and catalogs.\nPreflight checks for CrateDB\n: Verify prerequisites before setting up the CrateDB connector.\nTroubleshooting\nâ\nConnection issues\n: Resolve common CrateDB connection issues and errors.\nFAQ\nâ\nPermissions and limitations\n: Find answers to frequently asked questions about CrateDB setup and limitations.\nTags:\ncratedb\nconnector\ndatabase\nsql\nNext\nSet up CrateDB\nGet started\nReferences\nTroubleshooting\nFAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-governance",
    "content": "Use data\nReporting\nReport Types\nReport on governance\nOn this page\nReport on governance\nWho can do this?\nYou must be an\nadmin user\nin Atlan to view the reporting center. If\nenabled by admins\n,\nmember users\ncan also view the\nassets\n,\nglossary\n,\nInsights\n, and\nusage and cost\ndashboards. Permission to view the\ngovernance\nand\nautomations\ndashboards is reserved for admin users only.\nThe governance dashboard in the reporting center helps you review and report on metrics related to all your\npersonas\n,\npurposes\n,\ntags\n, and\nrequests\n.Â\nTrack query access\nâ\nYou can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup.\nTo view query access for a persona:\nFrom the left menu in Atlan, click\nReporting\nand then click\nGovernance\n.\nFrom the\nGovernance\ndashboard, under\nPersonas\n, navigate to\nQuery Access\n.Â\nUnder\nQuery Access\n,Â click\nPersonas with query access\nto view more details in the governance center.Â\nView assets tagged by propagation\nâ\nApart from viewing the total count of\nassets tagged by propagation\n, you can also view the propagated assets right from the dashboard for lineage analysis.Â\nTo view assets tagged by propagation:Â\nFrom the left menu in Atlan, click\nReporting\nand then click\nGovernance\n.\nFrom the\nGovernance\ndashboard, scroll down to the\nTag by Propagation\nsection.\nClick any tag to view a list of propagated assets in the sidebar.Â\nTrack requests\nâ\nYou can view and take action on all your\nrequests\nÂ from the governance dashboard.\nTo track metadata update requests:\nFrom the left menu in Atlan, click\nReporting\nand then click\nGovernance\n.\nFrom the\nGovernance\ndashboard, scroll down to the\nRequests\nsection.\n(Optional) Under\nRequests\n, click the date selector dropdown to filter requests by a predefined or custom date range.\n(Optional) Click the\nAll asset types\ndropdown to filter requests by a specific asset type.\nUnder\nRequests overview\n, view all requests grouped by request status. Click any request to\ntake action\nin the\nGovernance center\n.\nTags:\nglossary\nbusiness-terms\ndefinitions\nPrevious\nReport on usage and cost\nNext\nSummarize metadata\nTrack query access\nView assets tagged by propagation\nTrack requests"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/troubleshooting/troubleshooting-alteryx-connectivity",
    "content": "Connect data\nETL Tools\nAlteryx\nTroubleshooting\nConnection issues\nOn this page\nConnection issues\nPrivate preview\nThis guide helps you resolve common issues when setting up the Alteryx connector in Atlan.\nUnexpected workflow input-output mapping in technical lineage\nâ\nWhen you are setting up lineage for Alteryx workflows in Atlan, especially during private preview, you may encounter errors related to unexpected input-output mapping. These errors typically surface when Atlan doesnât receive enough information from Alteryx to generate accurate technical lineage.\nError messages\nUnexpected Workflow Input-Output Mapping in Technical Lineage\nCause\nAtlan currently receives limited metadata from Alteryx as part of the private preview integration. This incomplete data results in an inability to correctly determine how input datasets relate to output datasets within workflows. As a result, the generated lineage may be partial, incorrect, or missing entirely.\nHow to fix\nWhile this limitation is inherent to the current private preview, here are some steps you can take:\nVerify all workflow inputs and outputs are explicitly defined with clear and consistent names.\nAvoid using tools like Dynamic Input, Download, or Run Command that make data paths harder to trace.\nUse consistent naming conventions for datasets and tools across all workflows.\nReorganize complex workflows into simpler sections to make lineage easier to infer.\nRe-ingest the workflow in Atlan after making changes to check if the technical lineage appears correctly.\nContact Atlan support\nif the issue continues.\nPrevious\nWhat does Atlan crawl from Alteryx?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs",
    "content": "Connect data\nStorage\nGoogle GCS\nOn this page\nGoogle Cloud Storage\nOverview:\nCatalog Google Cloud Storage buckets and objects in Atlan. Gain visibility into your GCS data assets and their organization.\nGet started\nâ\nFollow these steps to connect and catalog Google Cloud Storage assets in Atlan:\nSet up the connector\nCrawl GCS assets\nReferences\nâ\nWhat does Atlan crawl from GCS\n: Learn about the GCS metadata that Atlan discovers and catalogs.\nTags:\nconnector\ndata-catalog\ngcs\nstorage\ngcp\nNext\nSet up Google Cloud Storage\nGet started\nReferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/references/preflight-checks-for-cloudera-impala",
    "content": "Connect data\nDatabases\nQuery Engines\nCloudera Impala\nReferences\nPreflight Checks for Cloudera Impala\nPreflight Checks for Cloudera Impala\nBefore\nrunning the Cloudera Impala\nyou can run\npreflight checks\nto perform the necessary technical validations.\nThe following preflight checks will be completed:\nAssets\nSchema Permission\nâ Check successful\nâ Check failed. Failed to connect to the Impala cluster.\nTags:\nschema\nschema-drift\nschema-monitoring\nPrevious\nWhat does Atlan crawl from Cloudera Impala?"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/references/architecture",
    "content": "Secure Agent 2.0\nArchitecture & Security\nArchitecture\nOn this page\nArchitecture\nSecure Agent 2.0 enables secure metadata extraction from enterprise data sources and transfers this metadata to Atlan SaaS for processing and asset management. The agent operates entirely within your infrastructure while maintaining secure communication with Atlan's cloud services.\nCore components\nâ\nSecure Agent 2.0 consists of four core components that work together to provide metadata extraction and transfer capabilities:\nTemporal worker\n: Orchestrates extraction workflows through persistent connections\nDapr\n: Makes it easier to connect to back-end services (such as object storage and secret stores) using standard, secure APIs\nFastAPI server\n: Provides operational APIs for monitoring and control\nApplication code\n: Implements source-specific extraction logic\nTemporal worker\nâ\nPurpose:\nOrchestrates metadata extraction workflows through persistent gRPC connections with Atlan's Temporal service.\nTemporal workers are long-running processes that orchestrate the entire metadata extraction workflow. These workers establish persistent gRPC connections with the Temporal service hosted in Atlan's environment and continuously listen on designated task queues.\nKey responsibilities:\nPoll the task queue for available extraction tasks\nExecute the extraction logic for each task\nSend results back to the Temporal service hosted by Atlan\nTechnical details:\nConnection type:\nPersistent gRPC\nAuthentication:\nOAuth 2.0 client credentials flow with unique Client ID and Client Secret per application\nCommunication pattern:\nWorker-initiated (no inbound connections required)\nSecurity model:\nAll connections originate from your infrastructure to Atlan services, ensuring no inbound network access is required\nFor security details, see\nSecurity\n.\nDapr\nâ\nPurpose:\nProvides abstraction layer for secure interaction with back-end services including secret stores, object storage, and Atlan SaaS.\nDapr (Distributed Application Runtime) serves as the abstraction layer between the application and various back-end services. It provides a consistent interface for common distributed application needs, simplifying integration with different infrastructure components.\nCore capabilities:\nRetrieve secrets from secret stores\nWrite data to object stores\nSend events and extracted metadata to Atlan SaaS over HTTPS\nSupported storage targets:\nAWS S3\nAzure Blob Storage\nGoogle Cloud Storage (GCS)\nLocal file systems\nDapr handles all the complexity of different storage APIs and authentication mechanisms, providing a unified interface for the application code.\nLearn more:\nDapr\nFastAPI server\nâ\nPurpose:\nProvides REST APIs for operational control, monitoring, and observability of the Secure Agent.\nEach Secure Agent application hosts its own FastAPI server that exposes REST APIs for operational management.\nAvailable endpoints:\nEndpoint\nPurpose\nReturns\nHealth check\nSystem health monitoring\nPlatform, hostname, IP address, MAC address, processor, RAM\nReadiness\nService availability\nConfirmation if system is ready to handle requests\nObservability\nOperational insights\nMetrics, logs, and runtime status\nThese APIs enable integration with existing enterprise monitoring and alerting systems.\nApplication code for extraction\nâ\nPurpose:\nImplements source-specific application logic (for example metadata extraction and transformation).\nThe application code implements the actual business logic for metadata extraction from various source systems.\nProcess flow: Metadata Extraction\nWorkflow trigerred by user or on schedule\nConnect to source system based on configured secret store\nFetch metadata based on parameters\nTransform metadata into standardized formats (JSON or Parquet)\nWrite transformed data to an object store\nTransfer metadata to Atlan SaaS for further processing\nData flow\nâ\nThe Secure Agent implements a controlled pipeline for metadata extraction and transfer:\nSource Systems â Extraction â Local Storage â Object Store â Atlan SaaS\nâ             â              â             â\nApplication    Temporary      Persistent      Cloud\nCode          Files         Storage       Processing\nHow it works\nâ\nThe complete workflow follows these steps:\nDeploy and register\n: Applications are deployed on your enterprise infrastructure and registered with your Atlan tenant. This establishes the secure connection between your environment and Atlan services using OAuth 2.0 client credentials.\nConfigure workflow\n: A user configures a workflow from the Atlan UI, defining source systems to connect to, extraction schedules or triggers, transformation requirements, and target storage locations.\nExecute extraction\n: The application retrieves relevant job details from Atlan and performs defined actions to extract metadata from the source. This includes:\nConnecting to specified source systems using appropriate protocols\nExecuting extraction logic based on configured parameters\nTransforming metadata into standardized formats (JSON or Parquet)\nTransfer metadata\n: Extracted metadata is first written to configured storage (S3, Azure Blob, GCS, or others), and then securely transferred to the Atlan SaaS tenant using HTTPS.\nProcess and publish\n: Atlan workflows processes the transferred metadata files and publishes them as searchable, governed assets in your Atlan workspace.\nMonitor execution\n: Throughout the execution, logs are collected and sent to Atlan for monitoring and auditing. Status updates are shown in the Atlan UI, and health metrics are exposed via FastAPI endpoints for integration with enterprise monitoring systems.\nSee also\nâ\nAuthentication\n: How OAuth 2.0 authentication works and protects your infrastructure.\nData transfer and observability\n: How metadata moves from sources to Atlan and monitoring capabilities.\nDeployment options\n: How containerization enables flexible deployment across environments.\nSecurity\n: Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0.\nTags:\nsecure-agent\narchitecture\ndeployment\nPrevious\nSecure Agent 2.0\nNext\nSecurity\nCore components\nData flow\nHow it works\nSee also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/data-transfer-and-observability",
    "content": "Secure Agent 2.0\nConcepts\nData transfer and observability\nOn this page\nData transfer and observability\nData transfer in Secure Agent 2.0 moves extracted metadata from your source systems to Atlan through a secure, multi-stage process. Understanding this flow helps you see how your data stays protected and where you get visibility into operations throughout the metadata extraction journey.\nHow metadata moves to Atlan\nâ\nAfter metadata is extracted from source systems, it follows a secure transfer process to Atlan SaaS for further processing:\nAgent App writes output to container local volume\n: The application initially stores extracted metadata in the container's local storage, which uses EBS volumes or node disks.\nOutput moves to persistent storage\n: The metadata is then periodically transferred to your configured persistent storage systems, such as local volume mounts or cloud object storage like AWS S3, Azure Blob Storage, or Google Cloud Storage.\nData replicates to Atlan SaaS tenant storage\n: Using OAuth-based client credentials for secure authentication, the metadata is replicated from your storage to your specific Atlan SaaS tenant's storage infrastructure.\nTransfer happens via Dapr abstraction\n: The actual transfer occurs through Dapr, which handles the complexity of connecting to Atlan's data storage service within your tenant and routing data to Atlan's tenant-configured object storage.\nProcessing and persistence in Atlan\n: Once the metadata reaches your Atlan SaaS tenant, it gets processed by Atlan workflows and persisted in the Atlan metastore where it becomes searchable and governable.\nHow data stays encrypted\nâ\nData transfer to Atlan\n: All communication with Atlan services is encrypted in transit using TLS 1.2+ by default, protecting metadata as it moves from your infrastructure to Atlan.\nYour cloud storage\n: Any data written to your cloud storage systems like S3, Azure buckets, or GCS is encrypted at rest using your configured bucket encryption settings, typically AES-256.\nAtlan storage\n: When data reaches Atlan's infrastructure, Atlan encrypts your metadata using AES-256 encryption in its cloud storage buckets.\nHow you can monitor operations\nâ\nThe Secure Agent provides visibility into all its operations so you can track what's happening, troubleshoot issues, and monitor performance. You can monitor using logs, traces, and metrics that capture detailed information about extraction job executions, interactions with Atlan services, and connections to your data sources and secret stores.\nYou can store these logs in your own infrastructure (for example, using S3 buckets). The logs are already in OpenTelemetry Protocol (OTLP) format, which makes them compatible with popular monitoring systems and security information and event management (SIEM) tools.\nSee also\nâ\nArchitecture\n: System components and overall data flow.\nSecurity - Logging and monitoring\n: Detailed security monitoring and observability features.\nTags:\nsecure-agent\ndata-transfer\nPrevious\nAuthentication\nNext\nDeployment options\nHow metadata moves to Atlan\nHow data stays encrypted\nHow you can monitor operations\nSee also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/deployment-options",
    "content": "Secure Agent 2.0\nConcepts\nDeployment options\nOn this page\nDeployment options\nDeployment in Secure Agent 2.0 leverages containerization to run across any OCI-compliant runtime environment. Since each connector or application is containerized, you can deploy them on your existing container infrastructure without changing your operational practices. Understanding the deployment model helps you choose the right approach for your infrastructure and security requirements.\nDeployment artifacts\nâ\nAtlan provides deployment artifacts for popular runtimes:\nDocker Runtime:\nDocker Compose files for direct container deployment\nKubernetes:\nHelm charts compatible with both managed and self-hosted clusters\nThe deployment chart includes sections where you can modify infrastructure-specific attributes like security context, resource constraints, labels and annotations, and environment variables.\nContainer image\nâ\nEach container image includes four key components that work in collaboration:\nDapr sidecar process for service interaction\nTemporal Python SDK for connecting to Atlan's orchestrator\nFastAPI server for handling requests\nCore application logic implemented as Temporal workers\nFor more details about how these components interact, see the\nArchitecture - Core components\nreference.\nOnce built, images are distributed through Atlan's\nHarbor\nopen-source registry with vulnerability scanning, image signing for integrity verification, and secure distribution capabilities. Access to the Harbor registry requires authentication so only authorized personnel can pull or push images.\nSee also\nâ\nVerify container images\n: Step-by-step guide to verify image signatures and integrity.\nSecurity\n: Security considerations for deployment configurations.\nTags:\nsecure-agent\ndeployment\ndocker\nkubernetes\nhelm\nPrevious\nData transfer and observability\nNext\nSecret management\nDeployment artifacts\nContainer image\nSee also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/secret-management",
    "content": "Secure Agent 2.0\nConcepts\nSecret management\nOn this page\nSecret management\nSecret management in Secure Agent 2.0 ensures that credentials for source systems never leave your environment. By integrating with your existing secret vaults and fetching credentials just-in-time, the agent can authenticate with databases and applications without exposing sensitive credentials to external systems, including Atlan SaaS.\nHow credentials are protected\nâ\nSecure Agent deployed applications integrate with enterprise secret vaults for successful source system authentication. The key principle is that secrets are never stored locally or sent to Atlan and remain entirely within your organization's security perimeter.\nSupported secret stores include:\nAWS Secrets Manager\nAzure Key Vault\nGCP Secret Manager\nHashiCorp Vault\nOthers supported secret stores\nHow it works\nâ\nSecrets (like database credentials) are fetched just-in-time via Dapr when the agent needs to connect to source systems. This dynamic access means credentials are only in memory during the specific authentication operation, then immediately discarded.\nThe secret management makes sure that credentials remain entirely within your organization's security perimeter through:\nNo local storage or transmission\n: Secrets are never stored locally or sent to Atlan\nNetwork isolation\n: Network isolation for secret store access\nAccess controls\n: Access controls with principle of least privilege\nAudit logging\n: Audit logging for all secret access and modifications\nRegular reviews\n: Regular access reviews and backup/disaster recovery for secret stores\nSee also\nâ\nAuthentication\n: How OAuth 2.0 credentials work with Atlan services.\nSecurity\n: Overall security architecture and controls.\nTags:\nsecure-agent\nsecrets\ncredentials\ndapr\nPrevious\nDeployment options\nNext\nDeployment and security\nHow credentials are protected\nHow it works\nSee also"
  }
]