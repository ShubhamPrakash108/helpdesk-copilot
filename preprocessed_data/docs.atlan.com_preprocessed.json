[
  {
    "url": "https://docs.atlan.com/",
    "text": "Discover, trust, and govern your data & AI ecosystem Everything you need to get started with Atlan. Set up Snowflake Set up Databricks Set up PowerÂ BI AtlanÂ Architecture BrowserÂ Extension Get started ð Quick-start guide Step-by-step onboarding ð§ Secure agent Enterprise-grade deployment options ð Playbooks automation Rule-based metadata updates at scale Core features ð Find & understand data Search, discover, and profile assets ð¡ï¸ Govern & manage Create data contracts & policies ð Integrate Automation, collaboration & other integrations Developer hub âï¸ Introductory walkthrough Play with APIs in minutes ð» Client SDKs Java, Python & more ð¦ Packages Developer-built utilities and integrations Atlan University Get started with Atlan by building the right strategy and setting a strong foundation. Atlan Security A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures Help and support Find answers or contact our team for personalized assistance"
  },
  {
    "url": "https://docs.atlan.com/support/submit-request",
    "text": "Submit request Aim to include as much information and detail in your request as possible to reduce delays between replies. Name * Email * Subject * Atlan URL * The URL of your Atlan tenant Severity SEV0 (S0) - System down or critical issue SEV1 (S1) - Major functionality affected SEV2 (S2) - General question or minor issue SEV3 (S3) - Feature request or enhancement Refer to severity and response SLA guidelines as outlined here How is this impacting you? I'm unable to use the product A major feature stopped working An issue is slowing me down I have a non-urgent question I have a suggestion that will help me with my use-case I need help with something Description * Attachments ð Add file or drop files here Submit"
  },
  {
    "url": "https://docs.atlan.com/get-started/what-is-atlan",
    "text": "Get Started What is Atlan? On this page What is Atlan? We are a modern data workspace that makes collaboration among diverse users like business, analysts, and engineers easier, increasing efficiency and agility in data projects. We started out as a data team, solving social good problems using data science. We built Atlan for ourselves over the course of 200 data projects, which included India's national data platform used by the prime minister and monitoring the Sustainable Development Goals with the United Nations. Atlan helped us build India's national data platform with an 8-member team, making it the fastest project of its kind to go live in just 12 months  - instead of the projected three years. Why we built Atlan â Data teams can be diverse: analysts, scientists, engineers, and business users. Diverse people with diverse tools and skillsets mean diverse DNAs. All of it led to chaos, which made our Slack channels look like this... We call this \"collaboration overheard\" â We knew we couldn't scale like this, there had to be a better way. We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams. We then experimented for two years and across 200 data projects to create our own idea of what makes data teams successful. We call this DataOps . How Atlan helps data teams â With Atlan, analyst teams at Unilever have shipped 100+ additional data projects per quarter, while data science teams at Samsung have saved 50% of their time. Create self-service ecosystems by reducing dependencies â Atlan makes all your data assets easily discoverable. No more Slack messages like \"Where's that dataset?\" or long email threads for approvals. With Atlan, you can simply Cmd+K your way to the right data asset. Key capabilities : discovery and search , Visual Query Builder , saved queries , READMEs Improve the agility of your data team â Data practitioners spend 30-50% of their time finding and understanding data. Atlan cuts that time by 95%. Your data team will be shipping 2-3 times more projects in no time. Key capabilities : visibility of data quality tests and observability alerts, automated lineage , Atlan AI Promote governance and a sustainable data culture â Don't lose sleep trying to figure out if your sensitive data is secure. Build ecosystems of trust, make your team happy, and let Atlan manage governance and security behind the scenes. Key capabilities : tag sensitive data , granular access control , data products Tags: atlan documentation Next Administrators Why we built Atlan How Atlan helps data teams"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
    "text": "Connect data Data Warehouses Snowflake Get Started Set up Snowflake On this page Set up Snowflake Who can do this? You need your Snowflake administrator to run these commands   -  you may not have access yourself. Create user and role in Snowflake â Create a role and user in Snowflake using the following commands: Create role â Create a role in Snowflake using the following commands: CREATE OR REPLACE ROLE atlan_user_role ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_user_role ; Replace <warehouse-name> with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user â Create a separate user to integrate into Atlan, using one of the following 3 options: With a public key in Snowflake â See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: CREATE USER atlan_user rsa_public_key = 'MIIBIjANBgkqh...' default_role = atlan_user_role default_warehouse = '<warehouse-name>' display_name = 'Atlan' TYPE = 'SERVICE' Learn more about the SERVICE type property in Snowflake documentation . Did you know? Atlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. With a password in Snowflake â Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace <password> and run the following: CREATE USER atlan_user password = '<password>' default_role = atlan_user_role default_warehouse = '<warehouse-name>' display_name = 'Atlan' TYPE = 'LEGACY_SERVICE' Learn more about the LEGACY_SERVICE type property in Snowflake documentation . Managed through your identity provider (IdP) Private preview â This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user â To grant the atlan_user_role to the new user: GRANT ROLE atlan_user_role TO USER atlan_user ; Configure OAuth (client credentials flow) with Microsoft Entra ID â To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\<AZURE_AD_ISSUER\\>' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\<AZURE_AD_JWS_KEY_ENDPOINT\\>' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\<SNOWFLAKE_APPLICATION_ID_URI\\>' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: <AZURE_AD_ISSUER> â Your tenant's OAuth 2.0 issuer URL <AZURE_AD_JWS_KEY_ENDPOINT> â Azure JWKs URI <SNOWFLAKE_APPLICATION_ID_URI> â Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\<AZURE_AD_CLIENT_OBJECT_ID\\>' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ < ROLE\\ > DEFAULT_WAREHOUSE = \\ < WAREHOUSE\\ > ; Grant the configured role to this user: GRANT ROLE \\ < ROLE\\ > TO USER oauth_svc_user ; Choose metadata fetching method â Atlan supports two methods for fetching metadata from Snowflake   -  account usage and information schema. You should choose one of these two methods to set up Snowflake: Â Account usage Information schema Overview Simplified grants but some limitations in functionality Most comprehensive approach, more grant management required Method Views in the SNOWFLAKE database that display object metadata and usage metrics for your account System-defined views and table functions that provide extensive metadata for objects created in your account Permissions User role and account, single grant for SNOWFLAKE database User role and account, multiple grants per database Data latency 45 minutes to 3 hours (varies by view) None Historical data retention 1 year 7 days to 6 months (varies by view or table function) Asset extraction ACCOUNT_USAGE schema INFORMATION_SCHEMA schema View lineage ACCOUNT_USAGE schema INFORMATION_SCHEMA schema Table lineage ACCOUNT_USAGE schema ACCOUNT_USAGE schema Tag import ACCOUNT_USAGE schema ACCOUNT_USAGE schema Usage and popularity ACCOUNT_USAGE schema ACCOUNT_USAGE schema Metadata extraction time Varies by warehouse size. For example, 8 minutes for 10 million assets (recommended for extracting a large number of assets) Varies by warehouse size. For example, 2+ hours for 10 million assets Extraction limitations External table location data, procedures, and primary and foreign keys None Grant permissions for account usage method â danger If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags â If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role ; The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; Replace <copied-database> with the copied Snowflake database name. Replace <copied-schema> with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams â To crawl streams, provide the following permissions: To crawl current streams: GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the Snowflake database name. To crawl future streams: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the Snowflake database name. (Optional) To preview and query existing assets â To query and preview data within assets that already exist in Snowflake, add these permissions: GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets â To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method â This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets â Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: GRANT USAGE ON ALL FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: GRANT OWNERSHIP ON FUNCTION < schema_name > . < udf_name > TO ROLE < role_name > ; Replace the placeholders with the appropriate values: <schema_name> : The name of the schema that contains the user-defined function (UDF). <udf_name> : The name of the secure UDF that requires ownership permissions. <role_name> : The role that gets assigned ownership of the secure UDF. Did you know? The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets â To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON FUTURE FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: GRANT REFERENCES ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database and <schema-name> with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage â To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role ; To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: GRANT USAGE ON DATABASE \"<cloned-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<cloned-database>\" . \"<cloned-account-usage-schema>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL TABLES IN SCHEMA \"<cloned-database>\" . \"<cloned-account-usage-schema>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL VIEWS IN SCHEMA \"<cloned-database>\" . \"<cloned-account-usage-schema>\" TO ROLE atlan_user_role ; Replace <cloned-database> with the name of the cloned database, and <cloned-account-usage-schema> with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets â To query and preview data within assets that already exist in Snowflake, add these permissions: GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets â To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: GRANT SELECT ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database and <schema-name> with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags â Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role ; The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; Replace <copied-database> with the copied Snowflake database name. Replace <copied-schema> with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake â To push tags updated for assets in Atlan to Snowflake , grant these permissions: GRANT APPLY TAG ON ACCOUNT TO ROLE < role - name > ; You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables â Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: GRANT MONITOR ON ALL DYNAMIC TABLES IN DATABASE \"<DATABASE_NAME>\" TO ROLE atlan_user_role ; Grant permissions at a schema level: GRANT MONITOR ON ALL DYNAMIC TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; To crawl future dynamic tables from Snowflake: Grant permissions at a database level: GRANT MONITOR ON FUTURE DYNAMIC TABLES IN DATABASE \"<DATABASE_NAME>\" TO ROLE atlan_user_role ; Grant permissions at a schema level: GRANT MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database and <schema-name> with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables â Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: GRANT REFERENCES ON ALL ICEBERG TABLES IN DATABASE < database - name > TO ROLE atlan_user_role ; To crawl future Iceberg tables in Snowflake: GRANT REFERENCES ON FUTURE ICEBERG TABLES IN DATABASE < database - name > TO ROLE atlan_user_role ; To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: GRANT USAGE ON INTEGRATION < integration - name > TO ROLE atlan_user_role ; danger You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages â Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: GRANT USAGE ON ALL STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; GRANT READ ON ALL STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; Replace <database_name> with the name of your Snowflake database Replace <atlan_user_role> with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: GRANT USAGE ON FUTURE STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; GRANT READ ON FUTURE STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; Replace <database_name> with the name of your Snowflake database Replace <atlan_user_role> with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP â If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Tags: connectors data crawl Previous Snowflake Next Set up an AWS private network link to Snowflake Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
    "text": "Connect data Data Warehouses Databricks Get Started Set up Databricks On this page Set up Databricks Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication â Who can do this? Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace â To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token â You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster â Did you know? Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: Interactive cluster SQL warehouse (formerly SQL endpoint) Interactive cluster â To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after ... minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . SQL warehouse (formerly SQL endpoint) â To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication â Who can do this? You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID Client secret Create a service principal â You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. Identity federation enabled â To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. Identity federation disabled â To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal â You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Azure service principal authentication â Who can do this? You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Client secret Tenant ID (directory ID) Create a service principal â To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. In_Search resources, services, and docs_, search for and select Microsoft Entra ID . Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . On the application page's_Overview_page, in the_Essentials_section, copy and store the following values in a secure location: Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account â To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace â To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. Identity federation enabled â To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . Identity federation disabled â To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata â You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method â To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction â To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data â danger Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags â To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables â You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : lineage usage and popularity metrics Enable system.access schema â You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table â To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. Grant permissions â Who can do this? You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema â This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . info ðª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions â Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. info ðª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables â When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique namesâfor example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW < cloned - catalog - name > . < cloned - schema - name > . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW < cloned - catalog - name > . < cloned - schema - name > . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace <cloned-catalog-name> and <cloned-schema-name> with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW < cloned - catalog - name > . < cloned - schema - name > . query_history AS SELECT * FROM system . query . history ; Replace <cloned-catalog-name> and <cloned-schema-name> with the catalog and schema names used in your environment. Grant permissions â Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, <cloned-catalog-name> ) USE SCHEMA and SELECT on the schema (for example, <cloned-catalog-name>.<cloned-schema-name> ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID â To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. From the Overview tab of your warehouse page, next to the Name of your warehouse, copy the value for your SQL warehouse ID . For example, example-warehouse (ID: 123ab4c5def67890) , copy the value 123ab4c5def67890 and store it in a secure location. (Optional) Grant view permissions to access Databricks entities via APIs â Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( /api/2.0/workspace/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( /api/2.0/sql/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( /api/2.2/jobs/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( /api/2.0/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views â Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3â6 for each catalog you want to crawl in Atlan. Did you know? SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history â To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . In the Manage permissions dialog, configure the following: In the Type to add multiple users or groups field, search for and select a user or service principal. Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Tags: data authentication Previous Databricks Next Set up cross-workspace extraction Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Get Started Set up Microsoft Power BI On this page Set up Microsoft Power BI Who can do this? Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin â Register application in Microsoft Entra ID â Who can do this? You need your Cloud Application Administrator or Application Administrator to complete these stepsâ> you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Search for Microsoft Entra ID and select it. Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . From the Overview screen, copy and securely store: Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID â Who can do this? You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Search for Microsoft Entra ID and select it. Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Add the appropriate member: For Delegated User authentication : search for the user and select it. For Service Principal authentication : search for the application registration created earlier and select it. Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options â Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) â When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: Admin API only â This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. Who can do this? You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Admin and non-admin APIs â This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. Assign security group to Power BI workspaces in PowerBI service portal â Who can do this? You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Configure admin and non-admin API access in PowerBI Service Portal â Who can do this? You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication â info Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. Fabric administrator role assignment â Who can do this? You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . API permissions â Who can do this? You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. danger The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principalâit's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Search for and select Power BI Service . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). Admin API settings configuration â Who can do this? You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Tags: data authentication Previous Microsoft Power BI Next Crawl Microsoft Power BI Before you begin Configure authentication options"
  },
  {
    "url": "https://docs.atlan.com/platform/references/atlan-architecture",
    "text": "Get Started References Atlan architecture On this page Atlan architecture Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) â Microsoft Azure â Google Cloud Platform (GCP) â The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components â Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Metastore stores metadata as data in a graph store. It is based on Apache Atlas and has fine-grained access control on top. Apache Zookeeper manages consensus and coordination for the metastore services. Elasticsearch indexes data and drives search functionality. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components â Velero performs cluster backups. Kibana explores and filters log data stored in Elasticsearch. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Elasticsearch stores and indexes logs. Central components â Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) â The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Tags: security access-control permissions Previous Incident response plan Next Product release stages Platform components Platform management components Central components Atlan marketplace (not pictured)"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension",
    "text": "Configure Atlan Integrations Automation Browser Extension How-tos How to use the Atlan browser extension On this page Use the Atlan browser extension The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension â To install the Atlan browser extension, first log into your Atlan instance. Atlan saves your Atlan domain in a cookie when you log in. To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https://chrome.google.com/webstore/detail/atlan/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. Did you know? You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension â Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user â To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! ð (Optional) Configure custom domains as an admin â Who can do this? You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In theÂ Browser extension tile, for Set up your custom data source... , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info ðª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Usage â Who can do this? Anyone with access to Atlanâany admin, member, or guest userâand a supported tool can use the browser extension. First, log into Atlan. Did you know? When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tabâno other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow â To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! ð The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Did you know? Your permissions in Atlan control what metadata you can see and change in the extension. Search for metadata â To search for context for any information on any website: Select the text you'd like to search on the web page you're viewing. Right-click, and then select Search in Atlan ð¡ . The extension opens a new browser tab on Atlan's discovery page, with the results for that text! ð Add a resource â You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! ð Did you know? The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools â Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Tags: atlan documentation Previous Configure the extension for managed browsers Next Enable embedded metadata in Tableau Install the extension Configure the extension Usage Supported tools"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins",
    "text": "Get Started Quick Start Guides Administrators On this page Administrators User management â User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center â It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center â The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile â The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. Glossary â The Atlan glossary is a rich tool for defining and organizing your data terminology to improve transparency and share knowledge. No need to ask around for what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one searchable place. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Did you know? The glossary helps power Atlan's powerful search tool , so tagging and defining assets are critical to helping your team find what they need. Discovery â We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms and descriptions you've added to your data assets. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Tags: get-started quick-start Previous What is Atlan? Next Data consumers User management Asset profile Glossary Discovery"
  },
  {
    "url": "https://docs.atlan.com/secure-agent",
    "text": "Connect data Secure Agent On this page Secure Agent The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities â The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture â Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction â A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment â Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations â Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works â The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also â Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. Tags: security access-control permissions Next Install on Virtual Machine (K3s) Key capabilities How it works See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks",
    "text": "Configure Atlan Playbooks On this page Playbooks Overview: Atlan's playbooks provide reusable workflows and automation for common data tasks. Create, share, and execute standardized processes to maintain consistency, reduce manual effort, and enable self-service for data consumers while following governance standards. Get started â How to set up playbooks Guides â Playbook management â How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting â Troubleshooting playbooks : Solutions for common playbook issues. Tags: playbooks automation workflows metadata capabilities Next Set up playbooks Get started Guides Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery",
    "text": "Use data Discovery On this page Discovery Overview: Atlan's discovery capabilities help users find, understand, and use data assets across your organization. With powerful search, filtering, and browsing features, users can quickly locate relevant data assets, explore their context, and access the information they need to make data-driven decisions. Get started â How to search and discover assets For detailed search, filtering, and troubleshooting information, use the sidebar navigation. Tags: discovery search browse capabilities Next Search and discover assets Get started"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts",
    "text": "Build governance Contracts On this page Contracts Overview: Manage data contracts and agreements in Atlan to ensure data quality and compliance. Define and track data quality expectations, service level agreements (SLAs), and data sharing agreements between teams and systems. Get started â Follow these steps to implement contracts in Atlan: Create data contracts Guides â Add contract impact analysis in GitHub : Detailed instructions on adding contracts for impact analysis in GitHub. Tags: contracts agreements data quality governance atlan Next Create data contracts Get started Guides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations",
    "text": "Configure Atlan Integrations Integrations Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts â Integration categories : Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods : Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync : Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks : Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings â âï¸ Automation Connect with platforms like AWS Lambda to automate data workflows and streamline routine tasks. ð¥ Collaboration Integrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing. ð¬ Communication Connect with SMTP for real-time alerts. ð Identity management Integrate with identity providers like Okta and Azure AD for seamless authentication and user management. ð Project management Connect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started â 1 Select an integration Choose from Atlan's available integrations based on your team's tools and workflows. â 2 Configure connection Follow the integration-specific setup guide to establish a secure connection with your tool. â 3 Test and activate Verify the integration is working correctly with a test action, then activate for your organization. ð¡ Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack. Tags: integrations atlan setup Next Automation Integrations"
  },
  {
    "url": "https://docs.atlan.com/support/references/customer-support",
    "text": "Support References Customer support On this page Customer support One of Atlan's core values is to help you and your team do your life's best work. ð That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps/engineering support personnel Vast repository of self-service resources Service-level commitment â Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support â âï¸ Email support at a dedicated customer support email account ( [email protected] ) ð¨âð» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. ð Submit a support request via the online form. To track your support tickets: Navigate to https://atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on -  view and edit the support tickets you either created or were copied on, respectively. Organization requests -  to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation â 24x7 availability for all requests and issues Severity levels â The AtlanÂ Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Severity Description Basic support Advanced support S0 Production software is unavailable; all customers are blocked and productivity halted 2 hours 1 hour S1 Production software is available; functionality or performance is severely impaired 4 hours 2 hours S2 Production software is available and usable with partial, noncritical loss of functionality. Or, production software has an occasional issue that customer requests identification and resolution. Also includes requests for help with administrative tasks 16 hours 4 hours S3 Cosmetic issues or request for general information about the software, documentation, processes, or procedures 24 hours 14 hours Escalation procedure â If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Tags: support Service-level commitment Ways to contact support Hours of operation Severity levels Escalation procedure"
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/authentication-and-authorization",
    "text": "Get Started Core Concepts Authentication and authorization On this page Authentication and authorization Atlan supports the following authentication methods: Basic authentication â Atlan initially comes with basic or username-password authentication. Admins can invite new users to log into Atlan. When a new user opens the invitation link, they will be able to set up their user profile, including username and password. However, Atlan does not recommend using basic authentication. Instead, admins should configure and enforce SSO authentication . SSO authentication â SSO using SAML 2.0 â Atlan supports single sign-on (SSO), allowing admins to configure SSO authentication. Atlan currently supports the following SSO providers: Azure AD Google JumpCloud Okta OneLogin Custom IdP SSO using SCIM â System for Cross-domain Identity ManagementÂ (SCIM) provisioning works in combination with SSO. Atlan currently supports SCIM provisioning for the following SSO providers: Azure AD Okta Authorization â Role-based access control (RBAC) â Atlan implements role-based access control (RBAC) to ensure that users have the minimum level of access required to perform their tasks. Access rights are assigned based on roles, and users are granted permissions according to their responsibilities. A system owner or an authorized party must approve any additional permissions. Atlan adheres to the principle of least privilege, ensuring that users are only granted the level of access necessary to perform their job functions. User access review (UAR) â Atlan recommends that admins performÂ access reviews of users, admins, and service accounts on a quarterly basis to ensure that appropriate access levels are maintained. Access reviews should also be documented. Identity and access management â For centralized management of groups and users, Atlan uses granular access policies . Admins can define policies to control both which actions a user can take and against which assets. These can be as broad as entire databases down to individual columns. Organizations can even build policies based on asset classification. This opens up the ability to restrict access to sensitive data like Personally Identifiable Information (PII)   -  an essential feature in the GDPR era. Atlan denies access by default , and explicit denials override any grants . You can even deny admin users access to assets, if you want. Roles â You must assign every user in Atlan a user role . These control basic levels of access. Groups â You can also add users to groups . Groups provide a more maintainable mechanism for applying access controls. Policies â You can define access policies for both users and groups. Through these policies you can restrict which users can take which actions on which assets. For example, you can set up tags such as PII and apply this to data assets like tables. You can also configure the tag to propagate downstream to any columns or tables created from them. You can then define access controls based on these tags to restrict access to tagged assets. If Atlan propagates tags for you to derived assets, the access control is automatically applied to those derived assets as well. Tags: atlan documentation Previous Atlan's open API Next Data and metadata persistence Basic authentication SSO authentication Authorization Identity and access management"
  },
  {
    "url": "https://docs.atlan.com/platform/references/cloud-logging-and-monitoring",
    "text": "Get Started Administration Cloud logging and monitoring On this page Cloud logging and monitoring Atlan exports IAM service event logs in the OpenTelemetry Protocol (OTLP) specification and securely delivers them to the Amazon S3 or Google Cloud Storage (GCS) bucket of your organization. This enables you to monitor login events and integrate logs with security information and event management (SIEM) systems for real-time security monitoring and alerts. Key aspects â Log format and structure: The OTLP format ensures seamless integration with SIEM systems, and logs are organized by date and event type. Logs are stored in a compressed format in your organization's preferred object storage (S3 or GCS). Once uncompressed, the logs will be available in a JSON file format containing multiple log entries. Each file is saved for an hour in the following folder structure in gzip: /year=YYYY/month=MM/day=DD/hour=HH/logs_<rnd-9-digit-int>.json.gz The JSON file structure is as follows: { \"resourceLogs\" : [ { \"resource\" : { \"attributes\" : [ // k8s metadata ] } , \"scopeLogs\" : [ { \"scope\" : { } , \"logRecords\" : [ { \"timeUnixNano\" : \"1725861538220747913\" , \"observedTimeUnixNano\" : \"1726071786185095727\" , \"body\" : { \"stringValue\" : \"//redacted logline\" } , \"traceId\" : \"\" , \"spanId\" : \"\" } ] } , { ... } , ] } , { \"resource\" : { ... } , \"scopeLogs\" : [ ... ] } ] } Secure delivery- Logs are encrypted in transit and at rest, with mechanisms to validate data integrity. Customer access: Logs are easily accessible through S3 or GCS, allowing for a flexible monitoring and alerting setup. Enabling event logs in AWS â Prerequisites â Enable bucket versioning. Both source and destination buckets must have versioning enabled. See AWS documentation . Customer-provided bucket details: account ID, bucket name, and region. Atlan will use these details to create an IAM role on the Atlan side and then provide you with the bucket policy to be attached. Once you have confirmed that the bucket policy has been attached, Atlan will complete the final step of setting up log replication. Atlan support will complete the configuration on the Atlan side. You will need to attach the following policy to your destination bucket: { \"Version\" : \"2012-10-17\" , \"Id\" : \"\" , \"Statement\" : [ { \"Sid\" : \"Set-permissions-for-objects\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<Atlan Role ARN>\" } , \"Action\" : [ \"s3:ReplicateObject\" , \"s3:ReplicateDelete\" , \"s3:GetBucketVersioning\" , \"s3:PutBucketVersioning\" ] , \"Resource\" : [ \"arn:aws:s3:::<Customer S3 Bucket Name>/*\" , \"arn:aws:s3:::<Customer S3 Bucket Name>\" ] } ] } Continuous replication to S3 bucket â Application audit logs are streamed to Atlan's S3 bucket in near real time â within 10 seconds of being generated. This is a continuous process. Once the logs are available in Atlan's bucket, the logs will be replicated to your organization's S3 bucket within 15 minutes. The replication is ongoing and occurs without delays. This ensures that logs are continuously transferred as they are generated, with no waiting period between replications. Enabling event logs in GCP â For Google Cloud Platform (GCP), Atlan utilizes Logs Router to transfer logs from the GCS bucket of your Atlan tenant to a destination bucket of your choice. The destination must be supported by the Logs Router. The organization must provide details of the destination where the logs should be synced. This destination must be supported by the Logs Router. Atlan will create a Log Router sink and provide you with a service account. Depending on the selected destination, you will need to configure the necessary permissions for the service account as outlined in Google documentation . Once you have configured the permissions, the logs will begin syncing to your preferred destination. New sinks to Cloud Storage buckets may take several hours to start routing log entries. Sinks to Cloud Storage are processed hourly while other destination types are processed in real time. Tags: security monitoring logs compliance siem opentelemetry otlp Previous High availability and disaster recovery (HA/DR) Next Generate HAR files and console logs Key aspects Enabling event logs in AWS Enabling event logs in GCP"
  },
  {
    "url": "https://docs.atlan.com/platform/references/infrastructure-security",
    "text": "Get Started Security & Compliance Infrastructure security On this page Infrastructure security See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Atlan is deployed using Kubernetes in an Atlan-managed VPC (virtual private cloud). Atlan also carries out: Vulnerability management through frequent releases Â   -  Atlan makes weekly releases to minimize vulnerability at a product and operating system level. Application Penetration Testing (APT) -  Atlan uses a third-party toolÂ to conduct industry standard APT. A penetration test is an authorized simulated cyber attack on a computer system, performed to evaluate the security of the system. The test is performed to identify both weaknesses (including the potential for unauthorized parties to gain access to the system's features and data) and strengths, enabling a full risk assessment to be completed. Event logging and monitoring Â   -  Atlan has many tools to support monitoring and event logging: Prometheus and Grafana for monitoring Fluent Bit and Loki for event logging Network access to the control plane â We restrict access to the Kubernetes control plane by IP address to cluster administrators. We deny public internet access to the control plane. Network access to nodes â Nodes are configured to only accept connections (via network access control lists): from the control plane on the specified ports for services in Kubernetes of type NodePort and LoadBalancer Each component of the Kubernetes cluster has security measures configured. These security measures are at the following levels: Cluster security Node security Pod security Container security Network security Code security Secret management Data encryption in transit Tags: integration connectors security access-control permissions Previous Tenant offboarding Next How are resources isolated? Network access to the control plane Network access to nodes"
  },
  {
    "url": "https://docs.atlan.com/faq/getting-started-and-onboarding",
    "text": "Get Started FAQs Getting Started and Onboarding On this page Getting Started and Onboarding Everything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements. Is there a trial version of Atlan that I can use to learn more on my own? â You can sign up for the product tour with your business email to learn more about Atlan. Do you have customers in my industry? â Atlan has customers across a wide variety of industries, including highly regulated industries like financial services and healthcare. Feel free to review the customers page , or reach out to discuss your use case in more detail. What are the implementation and maintenance requirements? â There are no additional costs incurred during implementation. To support the user/process transformation within your organization, Atlan provides an optional Accelerator Package with cultural enablement services. In terms of individuals and responsibilities required to run and maintain Atlan, both may vary depending on the following two factors: Size of your organization (10, 100, 1000 users). Organizational structure (data mesh vs. data stewardship model). Typically, the application is owned by at least 1 data engineer persona and 1 persona responsible for data governance, data enablement, and information architecture. You'd only be in charge of managing data source integrations because Atlan is deployed as software as a service (SaaS). Where's Atlan deployed? â Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) For more information, see Atlan architecture . What cloud providers do you support? â Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) What's the maintenance window for managing updates? â In addition to regular feature rollouts , Atlan performs monthly tenant infrastructure upgrades to enhance the performance, stability, and security of the tenant. Unless explicitly outlined otherwise in your organization's contract, these upgrades are performed for a period of 2-4 hours during a predetermined low usage window, usually over the weekend in your timezone. For any upgrades requiring downtime, Atlan provides notice to organizations at least 1 week in advance. For any further questions about the maintenance window, please reach out to Atlan support . Tags: atlan documentation faq-platform Previous Quality assurance framework Next Basic Platform Usage"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/the-dataops-culture-code",
    "text": "Get Started References The DataOps Culture Code On this page The DataOps Culture Code We experimented for two years, across 200 data projects, to create our own viewpoint of what makes data teams successful. We've codified these learnings into what we call the \"DataOps Culture Code\". The data team is the most interdisciplinary team in any organization. Data Scientists, Analysts, Engineers, Business Users ... These are diverse people, with diverse tools, skillsets, and DNA. All doing diverse things. Sometimes they're asking open-ended questions to get to the bottom of âwhyâ, just like a scientist in a research lab. Sometimes they're working on scaling petabyte-sized data processing systems, like a software engineer. Add to all this the living and breathing thing that is... data . Unlike code or design, it's constantly changing. How do you make a data team successful? â There's no easy answer. We started as a data team ourselves, on a quest to make ourselves as agile as we could. We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams. We then experimented for two years, across 200 data projects, to create our own idea of what makes data teams successful. These principles are the foundation of everything we build at Atlan. The DataOps Culture Code â ð¤Â  Itâs a team sport, and collaboration is key â Data teams will always have a variety of roles, each with their own skills, favorite tools and DNA. Embrace the diversity, and create mechanisms for effective collaboration. ð Treat data, code, models and dashboards as assets. â All data assets   -  from code and models to data and dashboards   -  are assets, and they should be treated like assets. Assets should be easily discoverable. Assets should be maintained. Assets should be easily reusable. ðÂ  Optimize for agility â In todayâs world, as business needs evolve rapidly, data teams need to be a step ahead, not deluged with three months of backlog. Constantly measure your teamâs velocity, and invest in foundational initiatives to improve cycle times. Reduce dependencies between business, analysts and engineers. Enable a documentation-first culture. Automate whatever is repetitive.Â ð¥Â  Create systems of trust â With the inherent diversity of data teams, it's all too easy to misunderstand other team members' roles. But that creates trust deficiencies   -  especially when things go wrong! Intentionally create systems of trust in your team. Make everyoneâs work accessible and discoverable to break down \"tool\" silos. Create transparency in data pipelines and lineage so everyone can see and troubleshoot issues. Set up monitoring and alerting systems to proactively know when things break. ðï¸ Create a plug-and-play data stack â The data ecosystem will rapidly evolve. The tools, technology and infrastructure you use today will (and should) be different from the tools you use two years later. Your data stack should allow your team to experiment and innovate as technology evolves, without creating lock-ins. Embrace tools that are open and extensible. Leverage a strong metadata layer to tie diverse tooling together. â¨Â  User experience defines adoption velocity â Employees at Airbnb famously said, \"Designing the interface and user experience of a data tool should not be an afterthought.\" Without good user experience, the best tools or most thoughtful processes won't be adopted in your team. Invest in user experience, even for internal tools. It will define adoption velocity! Invest in simple and intuitive tools. Software shouldn't need training programs. Tags: atlan documentation Previous Our 3 pro tips for saving time with Atlan Next How are product updates deployed? How do you make a data team successful? The DataOps Culture Code"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/search-and-discover-assets",
    "text": "Use data Discovery Get Started Search and discover assets On this page Search and discover assets Atlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context. Its Amazon-like search and filtering experience isn't just for data tables. It also extends to a variety of data assets, like columns, databases, SQL queries, BI dashboards, and much more. To ensure a high-quality search experience, Atlan recommends the following: Certify your assets Link terms to your assets to add business context Enrich your assets with descriptions Star your assets for easy access Did you know? You can bookmark your search results with applied filters or share them with other Atlan users in your organization for quick and easy access. Search superpowers â Let's find out what makes Atlan's search intuitive and super quick. Intelligent keyword recognition Atlan supports powerful, intelligent search.Â When you search using keywords, the keywords in the matching search results will be highlighted for easy recognition. Even if your keyword contains an underscore _ or a period . -  for example, instacart_order -  both keywords will be highlighted across all search results. For keyword-based search: If the keywords you're searching by is present in the asset name, description , or linked term , only then will the asset appear in your search results.Â Atlan displays search results based on asset names   -  technical name and alias -  that match your keyword(s). If the keyword is a glossary term linked to assets or present in asset descriptions , such assets will be boosted in search results. Whether your search query is incomplete ( insta ) or misspelled ( instacrt ordr ), Atlan's powerful search can still help you discover exactly what you need. Search from anywhere â There are multiple ways to start your search: Click the Search assets across Atlan bar on the homepage. Click Assets in the left-side panel. Use Cmd/Ctrl + K to open the search page from anywhere in Atlan. Search using context â The Assets section offers a variety of filters to narrow down your search. Here are the different types of filters that you can use: Source : Search by connectors , chosen from a list of connections within Atlan. Domains : Filter assets by domains , such as a single domain, multiple domains, or no domain. Certificate : Search based on the certificate attached to data assets, such as Verified , Draft , Deprecated , and No certificate . Owners : Filter by selecting one or more users. You can also toggle between users and groups Â to filter based on a group of users. Tags : Filter by user-generated tags, such as public , PII , and more. Terms : Filter by terms from your glossaries, such as cost , revenue , or P&L . Properties : Filter assets by other properties, like technical name or alias , description , last updated, and so on. Atlan's search results include a quick count of all the resulting data assets grouped by type. As you apply the filters, you'll see these counts change in real time. You can also enter a keyword in the search bar and filter your results by a specific type of data asset. For instance, enter the keyword order in the search bar and then click the Column checkbox to view column results for your searched keyword. Sort search results â Atlan allows you to sort your search results in different ways. This helps you quickly find the assets you're interested in. Sorting options include: Relevance : Sort by how closely the search results match your searched keywords. Name : Sort by the asset name in an alphabetical or a reverse alphabetical order. Updated on Atlan : Sort by the newest or oldest updated assets. Star count : SortÂ assets by most or fewest stars . Order : Sort the search results in an ascending or descending order. Popularity : Sort Snowflake and Google BigQuery assets by the most or least popular assets . Did you know? The sorting options may vary depending on the asset type selected. For example, if you are viewing the results while filtering by the Table tab, you'll also have the option of sorting by the most or fewest number of rows and columns. Search with patterns â You can refine your search in Atlan with the following patterns: Exact match search : Wrap the keywords within single '' or double \"\" quotation marks when typing them in the search bar   -  for example, \"instacart_total_users\" . Only the asset names with case-insensitive exact match and following the order of the keywords will be boosted in the search results   -  for example, instacart_total_users or Instacart_Total_Users . If the keywords are contained in the asset description or linked terms , such assets will show up next. Additionally, you can use exact match to search by the qualifiedName or globally unique identifier (GUID) of an asset. Combined string of database, schema, and table : For a more data-friendly search experience, copy the combined string of database.schema.table (or schema.table ) from your SQL editor and paste it in the search bar   -  for example, atlan_db.public.instacart_total_orders . Multiple phrase match : When you enter two or more keywords, Atlan will find assets with asset names that partially match the keywords or a combination of them to narrow down the search results. See only what you want to see â Atlan gives you the option to customize your search. Want to show or hide certain fields in your search results? Click the 3-dot icon next to the search bar to set display preferences for each field: Description Terms Tags Connection Tags: data asset-profile Previous Discovery Next Access archived assets Search superpowers"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/query-data",
    "text": "Use data Insights Get Started How to query data On this page query data There are two ways to query data in Atlan: writing your own SQL using the Visual Query Builder Did you know? Atlan pushes all queries to the source (no data is stored in Atlan). In addition, Atlan applies access policies to the results before displaying them. Write your own SQL â Who can do this? Anyone with the knowledge to write SQL. Any Atlan user with data access to the asset can query data. To query an asset with your own SQL: From the left menu of any screen, click Insights . Under the Explorer tab, find the asset you want to query: Use the Select database dropdown to choose another database, if necessary. Search for the asset by name in the search bar, or browse for it in the tree structure. Hover over the table or view, and click the play icon. This writes and runs a basic preview query. (Optional) Click the open asset sidebar icon to view more details in the asset sidebar. (Optional) Click the eye icon to view a preview of the query results. (Optional) Click the 3-dot icon for more options: Click Set editor context to set the same connection, database, and schema name in the query editor as selected in the Explorer tab. Click Place name in editor to view the asset name in the query editor. Click Copy path to copy the full path of the asset, including database and schema names. Under the Untitled tab on the right, change the sample query or write your own   -  separate multiple queries with a semicolon ; . Click the Run button in the upper right to test your query as you write it. (Optional) Click the downward arrow next to the Run button to export query results via email or schedule the query . (Optional) If you have multiple tabs open in the query editor, right-click a tab to open the tabs menu. You can close a specific tab or all tabs, or duplicate the query. (Optional) From the top right of the query editor, click the 3-dot icon for additional query editor actions or to customize it further: Click or hover over Duplicate query to create a duplicate version of your query. Click or hover over Open command palette to view the actions you can run inside the query editor. Click or hover over Themes and then select your preferred theme for the query editor. Click or hover over Tab spacing to change the tab spacing for your queries. Click or hover over Font size to change the font size for your queries. Click or hover over Cursor to change the cursor position in the query editor. Click or hover over Autosuggestions to turn off autosuggestions for assets in the query editor. The editor supports all read-based SQL statements, including JOIN . The editor will not run any write-based statements. The following SQL statements are not supported: UPDATE DELETE CREATE ALTER DROP TRUNCATE INSERT INTO Did you know? You can select the context for your query to the left of the Run button. Then you won't need to fully qualify table names with schema and database names. Use the Visual Query Builder â Who can do this? Any Atlan user with data access to the asset . No SQL knowledge required! To query an asset using the Visual Query Builder: From the left menu of any screen, click Insights . At the top of the screen, to the right of the Untitled tab, click the + button and select New visual query . Under Select from choose the table or view you want to query. (Optional) In the column selector to the right, select the column you want to query. Then develop your query: Click the Run button to run the query and preview its results. Click the blue circular + button to add an action to the query. Repeat these steps until your query is complete. (Optional) If there are any errors in your query, click Auto fix for Atlan to recommend a fix.Â (Optional) In the query results set, click Copy to copy the query results or click Download to export them. Did you know? You can learn more about the query builder actions in this example . Tags: atlan documentation Previous Insights Next Save and share queries Write your own SQL Use the Visual Query Builder"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/save-and-share-queries",
    "text": "Use data Insights Query Management Save and share queries On this page Save and share queries You can save queries to re-run them later, schedule them, or share them. Did you know? You can only save queries through a collection . You can share collections with others, to share your queries. Save a query â To save a query: Open the query in Insights. At the top right of the query, click the Save button. In the resulting Save query dialog, enter the following details: For Query name , enter a name for the query. (Optional) For Description , add a description for the query. For Collection , you can either: If you have access to existing collections, click the Choose collection dropdown to select an existing collection. If you do not have any existing collections, click the Create Collection button. In the Create collection dialog, enter the following details: For Name , enter a name for the collection. (Optional) To the left of the name, click the image icon to choose an icon for the collection. (Optional) For Description , describe the collection. (Optional) For Share , select other users or groups that can access the collection. (See below for more details.) At the bottom of the Create Collection dialog, click Create . (Optional) For Certificate , click the No certification dropdown to assign a certificate to the query. (Optional) For Linked terms , click the Select terms dropdown to assign a term to the query. At the bottom of the dialog: To only save your query, click Save . To save and share your query, click Save and share . In the Query saved dialog, enter the following details: For Add users or groups , select other users or groups that can access the saved query. (Optional) To the right of the user or group, click the Editor dropdown to change the sharing permissions: Viewer allows users to view and run all queries in the collection, but not edit them. Editor allows users to view, run, and edit all queries in the collection. Click Invite to invite the users or groups. (Optional) Click Copy Link to copy the link for the saved query to share with others in your team. (Optional) Click the Slack or Teams button to share directly on Slack or Microsoft Teams , respectively. Click Done to confirm your selections. Did you know? Atlan currently supports a query length of 2 million characters for saved queries. Share a query collection â A collection helps you organize saved queries in Atlan. A collection could represent a topic, department, or team with similar saved queries under one roof. Within each collection, you can have a folder that contains multiple saves queries of a similar type. To share a collection of queries: Open Insights. In the upper left, click the papers-in-a-box icon. Under the icon, click the name of the selected query collection. From the resulting list of collections, hover over the collection you want to share. Click the 3-dot icon to the right of the collection name and choose Edit collection . In the Edit collection dialog, under Share : Search for users or groups with whom to share the collection. (Optional) To the right of the user or group, click the Can edit dropdown to change the sharing permissions: Can edit allows users to view, run, and edit all queries in the collection. Can view allows users to view and run all queries in the collection, but not edit them. Repeat these steps for each user or group with whom you want to share the collection. At the bottom of the Edit collection dialog, click Update . Did you know? Users with only Can view permissions will still be able to change the interactive part of interactive queries . Move a saved query â To move a saved query to another query collection: Open Insights. In the upper left, click the papers-in-a-box icon. Under the icon, click the name of the selected query collection. From the resulting list of collections, hover over the collection from which you want to move a query. Click the 3-dot icon to the right of the saved query name, and then click Move to . In the Move to dialog, select the query collection to which you want to move your saved query. Click Move to complete moving the saved query. To duplicate, rename, edit, or delete your saved query, click the 3-dot icon to the right of the saved query name and select the relevant option. Did you know? If you add a Slack channel to the Query output share channels field in your Slack integration , you will be able to share your saved query and query results directly on that Slack channel. Atlan will deliver the query results as a CSV file on the same Slack thread. View query sidebar â Once you've saved a query , you can access the query sidebar to view additional context for your saved queries. To open the query sidebar for a saved query: In the left Explorer panel in Insights , hover over a saved query and click the Open query sidebar icon. From the saved query sidebar in the right, you can: View details about your saved query in Overview , including the actual query. (Optional) You can add more details to your saved query: Click the star button to star the query for quick access. For Description , add a description to your saved query. Click +Add README to add a README to your saved query and provide more context. For Collection , click the collection name to view the query collection. Copy the SQL query or expand the query view to fullscreen. For Terms , add a term to link to your saved query . For Owners , update the owner of the saved query or add more owners. For Tags , attach a tag to your saved query. For Certificate , update the certification status of your saved query. Click the Relations tab to view queried assets. (Optional) Select a related asset to open the asset sidebar. From the asset sidebar, click the Queries tab to view the saved query auto-linked to the asset. Click the Activity tab to view the activity log for the saved query. Click the Schedules tab to view associated schedules for the saved query, if any. Click the Resources tab to view any linked resources . Click the Requests tab to view any requests on the saved query. Click the Properties tab to view query properties. Click the Slack or Teams tab to view Slack or Teams messages pertaining to the query. Link saved queries â Once you have saved a query, the saved query will be auto-linked to all the assets queried or referenced in the SQL query. Linked queries are displayed in the asset profile and sidebar . You might want to link it to other assets, too. This can help you provide additional context on the assets and quickly find the saved query. For such assets, you can link the saved query as a resource . To link a saved query to an asset: Open Insights. In the upper left, click the papers-in-a-box icon. Under the icon, click the name of a query collection. From the resulting list of collections, select a collection and then select the saved query you want to link as a resource . To copy the link for a saved query, you can either: Click the 3-dot icon to the right of the saved query name and then click Copy link . In the top right of the query editor, click the 3-dot icon and then click Share . From the Share menu, click Copy link . From the left of menu, click Assets to navigate to your assets. From the Assets page, select an asset to open the asset sidebar. From the asset sidebar on the right, click the Resources tab and then click + Add resource . In the Add Resource dialog, enter the following details: For Link , paste the saved query link you copied in Insights. For Title , add a title for your saved query. Click Add to add the saved query as a resource to the asset. Did you know? Any user in Atlan will be able to preview saved queries for auto-linked or manually linked assets from the asset sidebar   -  unless there are access policies prohibiting them. Tags: atlan documentation Previous How to query data Next Make a query interactive Save a query Share a query collection Move a saved query View query sidebar Link saved queries"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo",
    "text": "Connect data Data Quality & Observability Monte Carlo Get Started Set up Monte Carlo On this page Set up Monte Carlo Who can do this? You will probably need your Monte Carlo account owner to complete these steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Monte Carlo. This method uses an API key ID and secret to fetch metadata. Create an account-service API key â Did you know? Atlan does not make any API requests or queries that will update the objects in your Monte Carlo environment. You will need to create an account-service API key in Monte Carlo for integration with Atlan. To create an account-service API key for crawling Monte Carlo : Log in to your Monte Carlo instance. In the top header of your Monte Carlo instance, click Settings . In the left menu under Settings , click API Access and then click Account Service Keys . From the Account Service Keys page, click the Create Key button. In the Create Account Service Key dialog, enter the following details: For Description , add a meaningful description for your API key   -  for example, Atlan connection . From the Authorization Groups dropdown, select Viewers (All) to provide minimum permissions for crawling Monte Carlo. (Optional) For Expires After , keep the default selection or select a preferred option. Click Create to finish creating the account-service API key. From the corresponding screen, copy the Key ID and Secret and store them in a secure location. danger The API secret cannot be retrieved later. Tags: connectors data integration crawl api authentication Previous Monte Carlo Next Crawl Monte Carlo Create an account-service API key"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-is-lineage",
    "text": "Use data Lineage Concepts What is lineage? On this page Lineage Data lineage captures how data moves across your data landscape. This information is useful to: Trace data's origins, to assist with root cause analysis Trace data's destinations, to assist with impact analysis Automate the propagation of metadata to derived assets Did you know? Tag propagation is disabled by default in Atlan. You can enable tag propagation to child and downstream assets. Root cause analysis â Root cause analysis is about identifying the underlying causes of a data problem. You want to know where the data came from and what happened to it before it got to you. With root cause analysis, your focus is on these upstream sources and transformations. Impact analysis â Impact analysis is about identifying potential consequences of changes. You want to know where the data is going and what could happen to others if you change it. With impact analysis, the primary focus is on these downstream systems and consumers. Did you know? When viewing lineage in Atlan, hover over any asset to view a metadata popover. The metadata popovers display relevant metadata for the asset, providing you with more context for your analysis. For example, database and schema names for Snowflake assets, project names for dbt models, and more. How does it work? â Atlan constructs lineage by combining assets and processes: Assets represent the inputs and outputs of processes   -  databases, dashboards, and so on. Processes represent the activities that move or transform data between the assets. (Processes are the lines between the assets in Atlan's graphical view.) Atlan chains these together into a flow of data from various resources: SQL parsing â Atlan parses SQL queries to determine how data stores have created or transformed assets. Examples of this include: Amazon Redshift dbt Generic query logs (via S3 objects) Google BigQuery Snowflake API crawling â Atlan also retrieves lineage information for assets from APIs. Examples of this include: Databricks (Unity Catalog) Looker Microsoft Power BI Tableau API ingestion â Atlan provides built-in lineage extraction for the tools above. But you can also extend lineage with your own information using Atlan's open APIs . You can use these to integrate lineage from your own home-grown tools or orchestration suites like Apache Airflow and Dagster . Tags: lineage data-lineage impact-analysis faq troubleshooting Previous What is column-level lineage? Next What are partial assets? How does it work?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/concepts/what-is-atlan-ai",
    "text": "Configure Atlan Atlan AI Concepts What's Atlan AI? On this page Atlan AI â Available to customers in Enterprise and Business-Critical platform editions You can use Atlan AI to supercharge the documentation of your data assets and gain meaningful insights from your data estate in Atlan. Did you know? Atlan uses Azure OpenAI Service to power Atlan AI. Atlan does not send any data to the AI service and only uses metadata for supported capabilities. For questions about data security, see Atlan AI security . Enable Atlan AI â Who can do this? You will need to be an admin user in Atlan to enable Atlan AI. Only admin users in Atlan can enable Atlan AI for their organization. Once enabled, each user's existing permissions and access policies in Atlan will determine how they can use Atlan AI. For example, a user must have the permission to edit metadata to use Atlan AI for updating asset descriptions. To enable Atlan AI for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, under the Atlan AI heading: Turn on Enrich metadata to enable your users to use Atlan AI for documenting assets , explaining lineage transformations , and generating aliases . (Optional) For Customize Atlan AI , click the Add instructions button to make Atlan AI suggestions more relevant to your organization. In the Enhance suggestions dialog, for General instructions , describe your organization and add details about your product, mission, and more, and then click Save . You have now unleashed the power of Atlan AI for your users! ð If you'd like to disable Atlan AI from your organization's Atlan workspace, follow the steps above to turn it off. Did you know? If Atlan AI is disabled , the feature will no longer be available in your workspace. However, any descriptions previously generated by Atlan AI and added to your assets or saved SQL queries will still be available. Current capabilities â Use Atlan AI to document assets â Atlan AI puts you in control of your data estate in Atlan, helping you curate meaningful context for your data assets. Accept, reject, or edit any AI-powered suggestions, the choice is yours . You can use Atlan AI to: Document tables and views with AI-generated descriptions Document columns with AI-generated descriptions Document terms and categories with AI-generated descriptions Document terms with AI-generated READMEs Add an Atlan AI-generated alias to supported assets Did you know? To ensure full transparency, any changes made using Atlan AI will be marked as Updated using Atlan AI in the activity log . Use Atlan AI for lineage analysis â Atlan AI can help you understand lineage transformations using natural language. You can use Atlan AI to create a natural language explanation for assets with SQL attributes and help you better understand the transformation logic. You can use Atlan AI to: Explain lineage transformations Tags: atlan-ai Previous Atlan AI security Enable Atlan AI Current capabilities"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/attach-a-tag",
    "text": "Build governance Tags Tag Management Attach a tag On this page Attach a tag Atlan allows users to add tags to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection. Atlan also supports attaching tags imported from the following supported sources: Databricks dbt Google BigQuery Snowflake For tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar. Did you know? Tag propagation is disabled by default in Atlan. You can enable tag propagation to child and downstream assets. Directly tag an asset â To directly tag an asset: In the left menu from any screen in Atlan, click Assets . On the Assets page, click an asset to view its asset profile. Under Tags Â in the right menu, click the + icon. In the popup, check the boxes to select one or more tags for the asset. No propagation is the default setting. Next to your selected tag(s) in the popup, click Edit Â to configure the propagation of tags: Click Hierarchy & lineage to allow propagation of tags to the child and downstream assets.Â Click Hierarchy only (no lineage) to allow propagation of tags to the child assets only. Click No propagation to disallow any propagation of tags. (Optional) For tags imported from supported sources, you can configure the following: For Snowflake assets , you can attach a Snowflake tag . If reverse sync is enabled , any updates made in Atlan will also be synced to Snowflake. If reverse sync is disabled, updates will be restricted to Atlan. Under Snowflake tags , select a synced Snowflake tag and then: Click the Select tag value dropdown to attach an allowed value from a predefined list, if available. For Add value , enter a tag value of your choice, if no predefined allowed values are present. Tag values added in Atlan are case-sensitive. For dbt Cloud or dbt Core assets, you can attach a dbt tag . For Google BigQuery assets, you can attach a Google BigQuery tag . For Databricks assets, you can attach a Databricks tag and tag values. If reverse sync is enabled , any updates made in Atlan will also be synced to Databricks. If reverse sync is disabled, updates will be restricted to Atlan. danger If there are multiple synced tags mapped to an Atlan tag , you will only be able to select one synced tag. You can also only select imported tags that belong to the same connection as the selected asset. Click Update to confirm your selections. Click Save to save the tag(s) to your asset. (Optional) Hover over the attached tag to view tag propagation details in a popover, including username of the user who applied the tag, mode of tag propagation, and when the tag was configured. (Optional) Filter tagged assets by attached tags , including tags imported from supported sources. For reverse sync to work for tags imported from Snowflake and Databricks , first ensure that reverse sync is enabled on the imported tag and then you must attach the imported tag to the asset (complete step 6 above). Did you know? You can remove tags from your tagged assets. You can also add tags to your column assets directly from Google Sheets. Tags: connectors data Previous Delete a tag Next Remove a tag Directly tag an asset"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data",
    "text": "Build governance Custom Metadata Access Management Control access to metadata and data? On this page Control access to metadata and data? You can customize access for users through several mechanisms. User roles â The most general mechanism is aÂ user role . These define the very broad permissions a user has in Atlan   -  for example, whether they can administer other users, or only discover metadata. When it comes to what metadata and data a user can access, though, we need to use the additional mechanisms below. Connection admins â Connection admins are users who manage connectivity to a data source. By default, these users can: Read and write all metadata on assets from that connection. Preview and query the data in all data assets from that connection. Manage access policies to grant others access to the assets from that connection. You define the connection admin when crawling a new data source for the first time. A connection admin can also extend the list of connection admins on their connection at any time. Access policies â Who can do this? A user must be both an admin user and a connection admin to define access policies for the connection's assets. Access policies either allow or restrict access to certain assets. These allow you to be much more creative (and granular) about access than the all-or-nothing privileges of connection admins. You start by defining which assets to control with each policy. There are two complementary mechanisms to do this in Atlan   - personas and purposes . Once you have defined the subset of assets, you can then define granular access to both metadata and data: Metadata policies â Metadata policies control what users can do with the assets' metadata. Through them, you can control who can: Read : view an asset's activity log, custom metadata, and SQL queries Update : change asset metadata, including description, certification, owners, README, and resources Update Custom Metadata Values for the assets Add Tags to the assets Remove Tags from the assets Add Terms to the assets Remove Terms from the assets Create : create new assets within the selected connection (via API) Delete : delete assets within the selected connection (via API) Data policies â Data policies control what users can do with the assets' data. Through them, you can control who can: Query and preview the data within the assets Whether to hide any data, through various masking techniques: Show first 4 : replaces all the data with X except the first 4 characters of data. For example 1234 5678 9012 3456 would become 1234XXXX . Show last 4 : replaces all the data with X except the last 4 characters of data. For example 1234 5678 9012 3456 would become XXXX3456 . Hash : replaces the data with a consistent hashed value. Because the hash is consistent you can still join on it across assets. For example 1234 5678 9012 3456 would become f43jknscakc12nk21ak . Nullify : replaces the data with the null value. For example 1234 5678 9012 3456 would become null . Redact : replaces all alphabetic data with x and all numeric data with 0. For example 1234 Street Name would become 0000 Xxxxxx Xxxx . Glossary policies â Glossary policies control what users can do with glossary metadata   -  terms and categories. Through them, you can control who can do the following against each glossary: Read permission on terms, categories, and glossaries exists by default and cannot be modified. Glossary policies do not restrict users from viewing any glossary and its contents within the Glossary section. Create terms and categories inside the glossary Update descriptions, certification, owners, READMEs, and resources for the glossary, terms and categories Link terms in the glossary with all other assets Delete terms and categories inside the glossary Add tags to the terms Remove tags from the terms Update custom metadata values for the terms and categories inside the glossary Glossary policies can only be defined through personas. Interactions â All the mechanisms above can coexist. This is powerful, but can also be a bit overwhelming to think about.Â What takes priority when a user is under the control of all these mechanisms? ðµâð« It's actually not as bad as you might think   -  only these three rules: Access is denied by default (implicitly) â By default, users will not have the permissions listed above. This remains true until you explicitly grant a user a permission. For example, imagine you have not set up any access policies and a new user joins. They will not have any of the permissions above against any assets in Atlan. Did you know? Users have read permission on terms, categories, and glossaries by default in Atlan. Explicit grants (allows) are combined â When you grant a user a permission, this is combined with all other permissions you have granted the user. Continuing our example, imagine you add the new user to a group defined as the connection admins for Snowflake. The user will now have full read/write access to all metadata for Snowflake assets, and be able to query and preview the data in those assets. Then you add the user to a persona that gives read/write access to a Looker project. The user will now have access to all Snowflake assets and a Looker project's assets. Explicit restrictions (denies) take priority â danger When you explicitly deny a user a permission, this takes priority over all other permissions you have granted the user. Continuing our example, imagine you define a purpose with a data policy that masks PII data. The user will still have full read/write access to all metadata for Snowflake assets and a Looker project's assets. In general, they will still be able to query and preview the data in the Snowflake assets. However, any PII data in Snowflake will now be masked. Then you add a metadata policy to the purpose that denies permission to remove the PII tag. The user will no longer have full read/write access to all metadata for Snowflake assets and a Looker project's assets. The user can no longer remove the PII tag from any of these assets. Did you know? The combination of mechanisms in the example above shows their power. Through a small number of controls we can define wide-ranging but granular access permissions. Tags: atlan documentation Previous Add options Next Disable data access User roles Connection admins Access policies Interactions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/concepts/what-are-data-products",
    "text": "Configure Atlan Data Products Concepts What are data products? On this page Data Products â Available via the Data Marketplace package From a single data table to a collection of data assets, anything can be a data product in Atlan. Data products provide a framework for your teams to curate assets specific to a domain, business unit, region of operation, brand, and more. These curated data products then empower your data consumers to easily discover data assets, quickly get the context they need, and collaborate more efficiently. As organizations shift from centralized data architectures, build a new paradigm of governance with data products in Atlan. Enable products module â Who can do this? You will need to be an admin user in Atlan to enable the products module for your organization. To enable data products for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, turn on Products module to enable your users to create and manage data domains and products . In the Who can access Products module dialog, you can configure access to the module for the following sets of users: Click Only admins to enable the products module for admin users only. Click Selected personas to enable the products module for specific personas with domain policies . Select the persona(s) to which you want to limit usage of the products module. If there are no personas with domain policies, you can either create a new persona with a domain policy or add a domain policy to an existing persona. Include all admins is selected by default. This allows any admin user to access and manage the module irrespective of whether they belong to the specified personas. (Optional) Uncheck the Include all admins checkbox to remove the default selection. Click All users and personas to enable the products module for all your Atlan users and personas. (Optional) Click Configure to update your user selections for access to the module. (Optional) To hide the product scorecard on your data products, turn off Product score . (Optional) To enable your users to search for data products from asset discovery , turn on Show products in asset discovery . If you'd like to disable the Products module from your organization's Atlan workspace, follow the steps above to turn it off. Once enabled, you can also temporarily disable the module and turn it on again as needed. For any domains and products you may have created, this will not result in any data loss. Order of operations â To start using data products, you will need to: Create a data domain (Optional) Add data subdomains Create domain policies Create data products within the data domain Discover and collaborate on data products Track and monitor domain usage Stakeholders â Atlan currently supports adding predefined stakeholders and creating custom ones for your data domains and subdomains. These are responsibilities you can assign to your users based on their function within a specific domain or subdomain. Stakeholders do not enforce access control, but are meant to help your data consumers understand the organizational structure and responsibilities. Atlan provides the following options: Domain owner -  overall domain management and reporting. Architect -  design and deployment of domains and subdomains. Data product owner -  creation, management, and documentation of data products. Data engineer -  creation and management of data pipelines. Create new stakeholders that better reflect your organizational structure and functions. Components of a data product â To search for a data product: From the left menu of any screen in Atlan, you can either: Click Products to search for data products from the products homepage: From the left navigation menu, use the search bar or select the relevant domain and then select a data product. In the Data products section, select a trending or recently viewed data product. The list of Trending products is sorted by the total count of views on each product, with the most viewed product listed at the top. From the top right of any screen in Atlan, click the star icon . From the Starred assets popup, select a starred data product. If your Atlan admin has enabled the Show products in asset discovery toggle, click Assets to search for data products from asset discovery : Click the Asset type dropdown and then select Product to filter for data products. Use the Filters menu on the left to further refine your search. Click any data product to view the product sidebar or open the product profile. In addition to the factors documented here , Atlan uses product score to determine the most relevant results for your product search. Use the search bar to search for products using keyword-based search. Overview â This section displays important details about the data product: Data product status   -  current status of the data product: Draft -  data product is in draft state and only visible to product owners Published -  data product is active for consumption Sunset -  data product is planned for retirement Archived -  data product is archived and will be no longer available to users Domain name   -  view and navigate to the data domain that the data product belongs to Criticality   -  view business criticality rating: High -  high business impact _Medium   - _ moderate business impact Low -  internal or non-business impact Sensitivity   -  view sensitivity score for data product classification: Public -  may be freely accessible Internal -  may only be distributed within the organization Confidential -  may only be limited to a specific domain or team within an organization Freshness   -  timestamp for when the data product was last updated in Atlan. This only includes metadata updates made on the data product and not on any underlying assets. Description of the data product Linked assets at a glance List of assets designated as output ports Output ports â This section displays a list of assets that allow users to consume the data product across multiple domains. A data product can have multiple output ports. Click the output port asset to open the asset sidebar and view more details. README and resources â This section allows you to add a README and resources to your data product. READMEs can help you provide detailed documentation about the product to your data consumers. Resources enable you to add links to internal or external URLs for more context. Product score â Based on the principles of data as a product, product scores can help you signal the accuracy and completeness of your data products, helping build trust in them. Atlan calculates and assigns a product score to your data products based on a preset criteria of metadata completeness. Details sidebar â The sidebar to the right of the product profile allows you to view and add metadata, depending on your domain permissions : Visibility helps you determine who can access and monitor the data product throughout its entire lifecycle: Private to members of this domain -  only members of a specific domain can access the data product. Private to selected members -  only members of a specific domain and other selected users or groups can access the data product. Public -  everyone in the organization can access the data product. Under Terms , click + to add terms and offer contextual information for your data product.Â Under Owners , click + to assign owners to the data product. Under Tags , click + to attach a tag and configure tag propagation for all assets in the data product. Under Certificate , click + to update the certification status. Choose from four certificateÂ options - Draft , Verified , Deprecated , and No certificate . Product profile header â This section helps you perform quick actions. From the top right of the product profile: Click the user avatars to view a list of recently visited users, total views on your product, total number of unique visitors, and total views by user. Use the days filter to filter product views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your product and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement to your product. Assets â The Assets tab provides a comprehensive list of assets included in the data product: Search for specific assets in the sidebar Filter assets by input and output ports Select an asset to view more details in the asset sidebar View queried at source information for all assets Lineage â The Lineage tab provides a visual representation of the provenance of and relationships between your data products in Atlan. Activity log â The Activity tab provides a changelog for your data product. Activity -  view details about changes made to the data product and filter for specific types of metadata changes Views -  view top and recent users of the data product Producers -  view information about when the data product was created and last updated and by whom Contracts â The Contracts tab displays any linked contracts for the output ports in your data product. You can view contract specifications, track the evolution of your contract over time, and compare and contrast multiple versions. Tags: atlan documentation Previous What is a product score? Enable products module Order of operations Stakeholders Components of a data product"
  },
  {
    "url": "https://docs.atlan.com/tags/atlan",
    "text": "133 docs tagged with \"atlan\" View all tags Access Control Learn how to manage user permissions and access to data assets in Atlan for security and compliance. Add a resource Need to redirect users to important information that's outside Atlan? Add an alias An alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use [Atlan AI](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai) to do the same, if [Atlan AI is enabled in your Atlan workspace](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai). This can help you improve the readability of your asset names while providing useful context to your users. Add certificates How many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data! Add custom metadata badges Bringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges. Add owners Atlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data. Add stakeholders Stakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams. Add users to groups Atlan supports configuring SSO group mappings. You will first need to [create groups](/product/capabilities/governance/users-and-groups/how-tos/create-groups) in Atlan that correspond to the groups you want to map from your SSO provider to Atlan. Additional connectivity to data sources Learn about additional connectivity to data sources. Administration Learn about administration. Allow guests to request updates If you'd like to disable this option for your guest users, follow the steps above and then turn it off. Are there any limits on concurrent queries? Learn about are there any limits on concurrent queries?. Authenticate SSO credentials to query data Learn about authenticate sso credentials to query data. Authenticate SSO credentials to view sample data Learn about authenticate sso credentials to view sample data. Authentication and authorization Learn about authentication and authorization. Automate policy compliance âAvailable via the Advanced Policy & Compliances package Basic Platform Usage Essential information about using Atlan's core features, from browser requirements to data querying and asset management. Can Atlan integrate with multiple Azure AD tenants within a single instance? Learn about can atlan integrate with multiple azure ad tenants within a single instance?. Can we restrict who can query our data warehouse? Learn about can we restrict who can query our data warehouse?. Configure custom domains for Microsoft Excel If your Atlan tenant is hosted on a custom domain   -  for example, `https://<your-tenant-name>.mycompany.com`Â   -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel. Configure language settings How does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level. Configure the extension for managed browsers If you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script. Connectors Learn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management. Contracts Learn how to manage data contracts and agreements in Atlan to ensure data quality and compliance. Control access to metadata and data? Learn about control access to metadata and data?. Create a new tag For tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar. Create data contracts Create data contracts <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Create data domains Data domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization. Create domain policies Domain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more. Create forms You can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more. Create groups :::warning Who can do this? You will need to be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to cr. Create persona :::warning Who can do this? You will need to be an admin user to create personas. Create policies âAvailable via the Advanced Policy & Compliances package Create purpose :::warning Who can do this? You will need to be an admin user to create purposes. :::. Custom Metadata Atlan provides basic metadata for assets like certification, owners, and descriptions. But since every data team has their own unique needs, Atlan allows you to extend these with your own unique properties. Custom Metadata Learn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. Data Products From a single data table to a collection of data assets, anything can be a data product in Atlan. Data products provide a framework for your teams to curate assets specific to a domain, business unit, region of operation, brand, and more. These curated data products then empower your data consumers to easily discover data assets, quickly get the context they need, and collaborate more efficiently. Databricks Data Quality Studio Set up and configure Databricks for data quality monitoring through Atlan. Delete a tag If a tag is attached to assets, you will need to [remove the tag](/product/capabilities/governance/tags/how-tos/remove-a-tag) from the [tagged assets](/product/capabilities/governance/tags/how-tos/attach-a-tag)Â before deleting it. Disable user activity You can [view recently visited users](/product/capabilities/discovery/concepts/what-are-asset-profiles) and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps. Domains Learn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way. Enable  Azure AD for SCIM provisioning You can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Enable  discovery of process assets To create a more customizable experience for your users, you can turn on discovery and tracking of process assets. Enable  sample data download Atlan allows admin users to enable or disable downloading [sample data](/product/capabilities/discovery/concepts/what-are-asset-profiles sample-data). This can help you enforce better governance across your organization. Enable  scheduled queries To enable scheduled queries, follow these steps. Enable data quality on connection Enable and configure data quality for your Databricks connection in Atlan. Enable data quality on connection Enable and configure data quality for your Snowflake connection in Atlan. Getting Started and Onboarding Everything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements. Glossary Learn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization. Google Dashboard login error Learn about why do i get an error while logging in via google dashboard?. How are resources isolated? Learn about how are resources isolated?. How do I see views instead of materialized views in the reporting center? On the _Assets_ dashboard in the [reporting center](/product/capabilities/reporting/references/how-to-report-on-assets), click **View** in the _All Asset Types_ dropdown menu toÂ only see views. You can further filter by selecting a specific connector or connection as well. How do I use the filters menu? Learn about how do i use the filters menu?. How to interpret timestamps Learn about how to interpret timestamps. How to use parameterized queries? Learn about how to use parameterized queries?. Insights tips and tricks At Atlan, we are committed to powering your user experience. Here are a few tips and tricks to help you get the most out of Insights for you and your team! Install on Virtual Machine (K3s) This page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying [K3s in a rootless execution mode](https://docs.k3s.io/advanced running-rootless-servers-experimental:~:text=to%20take%20effect.-,Running%20Rootless%20Servers,-\\(Experimental\\)%E2%80%8B). Integrations Learn how to integrate Atlan with project management, communication, collaboration, automation, and identity management tools. Invite new users Note that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users. Is there a dashboard to see how my metadata is populated? Learn about is there a dashboard to see how my metadata is populated?. Limit SSO automatically creating users when they log in Only users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan. Make a query interactive If you want to share a query with others, but limit how they can change the query, you can make it _interactive_. Manage domains Most importantly, domains help promote shared ownership and domain-level governance in your organization. Manage playbooks Once you've [created a playbook](/product/capabilities/playbooks/how-tos/set-up-playbooks), you can monitor, modify, or delete it at any time. You can also [enable notifications](/product/capabilities/playbooks/how-tos/manage-playbooks) to monitor your playbook runs directly in Slack or Microsoft Teams. Manage policies You must be an admin user in Atlan to enable, create, manage, and approve data governance policies. Manage system announcements Have you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape. Manage users :::warning Who can do this? You will need to be an admin user in Atlan to manage other users. Monitor for runaway queries? Learn about monitor for runaway queries?. Organize assets Learn about organize assets. Our 3 pro tips for saving time with Atlan Learn about our 3 pro tips for saving time with atlan. Product release stages Learn about product release stages. Quality assurance framework Learn about quality assurance framework. query data Learn about query data. query without shared credentials Don't want to use a single shared service account to access data? Remove a tag Atlan allows you to remove [tags](/product/capabilities/governance/tags/concepts/what-are-tags) from a [tagged asset](/product/capabilities/discovery/asset-prof. Save and share queries You can save queries to re-run them later, schedule them, or share them. Set up a private network link to Amazon MSK :::warning Who can do this? You will need your Amazon MSK or AWS administrator involved - you may not have access to run these tasks. Set up a private network link to Amazon Redshift :::warning Who can do this? You will need your Amazon Redshift administrator or AWS administrator involved - you may not have access to run these tasks. Set up a private network link to Microsoft SQL Server on Amazon EC2 :::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself. Set up a private network link to Microsoft SQL Server on Amazon RDS :::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself. Set up a private network link to MySQL :::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself. Set up a private network link to PostgreSQL Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This [method](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-setup.html rds-proxy-secrets-arns) uses a username and password to connect to the RDS database. Set up a private network link to Tableau server As a prerequisite for TLS configuration on Tableau Server only, ensure that the health check _Protocol_ of the target group is set to **HTTPS** or [modify the health check settings](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html modify-health-check-settings) as required. Set up Amazon Athena Learn about set up amazon athena. Set up an AWS private network link to Snowflake Atlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you. Set up an Azure private network link to Snowflake Atlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request. Set up Apache Kafka :::warning Who can do this? You will probably need your Apache Kafka administrator to run these commands - you may not have access yourself. Set up AWS Lambda Learn about set up aws lambda. Set up Cloudera Impala Learn about set up cloudera impala. Set up dbt Core Learn about set up dbt core. Set up Metabase :::warning Who can do this? You will probably need your Metabase administrator to follow the below steps - you may not have access yourself. Set up playbooks Learn about set up playbooks. Snowflake Data Quality Studio Set up and configure Snowflake for data quality monitoring through Atlan. Stewardship Learn how to implement data stewardship in Atlan through automated workflows, policies, and task management. Suggestions from similar assets Learn about suggestions from similar assets. Tags For details on tagging assets, see [How to attach a tag](/product/capabilities/governance/tags/how-tos/attach-a-tag). Tags Learn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance. Tenant access management Learn about tenant access management. Tenant monitoring Learn about tenant monitoring. Tenant offboarding Learn about tenant offboarding. The DataOps Culture Code Learn about the dataops culture code. Troubleshoot permission issues Learn about troubleshoot permission issues. Troubleshooting bring your own credentials Learn about troubleshooting bring your own credentials. Troubleshooting exporting large query results Learn about troubleshooting exporting large query results. Troubleshooting Google BigQuery connectivity Learn about troubleshooting google bigquery connectivity. Troubleshooting Hive connectivity Learn about troubleshooting hive connectivity. Troubleshooting MicroStrategy connectivity Learn about troubleshooting microstrategy connectivity. Troubleshooting MySQL connectivity Learn about troubleshooting mysql connectivity. Troubleshooting on-premises database connectivity Learn about troubleshooting on-premises database connectivity. Troubleshooting playbooks Learn about troubleshooting playbooks. Troubleshooting policies Learn about troubleshooting policies. Troubleshooting PostgreSQL connectivity Learn about troubleshooting postgresql connectivity. Troubleshooting Snowflake connectivity Learn about troubleshooting snowflake connectivity. Troubleshooting Snowflake tag management Learn about troubleshooting snowflake tag management. Update input type for existing custom metadata Learn about update input type for existing custom metadata. Use Atlan AI for documentation â Available to customers in Enterprise and Business-Critical platform editions Use the Atlan browser extension The Atlan browser extension provides metadata context directly in your [supported data tools]( supported-tools). You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. User Management and Access Control Complete guide to managing users, configuring access controls, and understanding permissions in Atlan. Users and groups Learn how to manage users and groups in Atlan to control access and organize your data team. What are asset profiles? Learn about what are asset profiles?. What are groups? Learn about what are groups?. What are partial assets? Learn about what are partial assets?. What are personas? Learn about what are personas?. What are preflight checks? Learn about what are preflight checks?. What are purposes? Learn about what are purposes?. What are the query builder actions? Learn about what are the query builder actions?. What are the sidebar tabs? Learn about what are the sidebar tabs?. What column keys does Atlan crawl? Learn about what column keys does atlan crawl?. What controls the frequency of queries? Learn about what controls the frequency of queries?. What happens when users do not have access to metadata? Learn about what happens when users do not have access to metadata?. What is a product score? Learn about what is a product score?. What is Atlan? Learn about what is atlan?."
  },
  {
    "url": "https://docs.atlan.com/tags/documentation",
    "text": "117 docs tagged with \"documentation\" View all tags Add a resource Need to redirect users to important information that's outside Atlan? Add an alias An alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use [Atlan AI](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai) to do the same, if [Atlan AI is enabled in your Atlan workspace](/product/capabilities/atlan-ai/concepts/what-is-atlan-ai). This can help you improve the readability of your asset names while providing useful context to your users. Add certificates How many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data! Add custom metadata badges Bringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges. Add owners Atlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data. Add stakeholders Stakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams. Add users to groups Atlan supports configuring SSO group mappings. You will first need to [create groups](/product/capabilities/governance/users-and-groups/how-tos/create-groups) in Atlan that correspond to the groups you want to map from your SSO provider to Atlan. Additional connectivity to data sources Learn about additional connectivity to data sources. Administration Learn about administration. Allow guests to request updates If you'd like to disable this option for your guest users, follow the steps above and then turn it off. Are there any limits on concurrent queries? Learn about are there any limits on concurrent queries?. Authenticate SSO credentials to query data Learn about authenticate sso credentials to query data. Authenticate SSO credentials to view sample data Learn about authenticate sso credentials to view sample data. Authentication and authorization Learn about authentication and authorization. Automate policy compliance âAvailable via the Advanced Policy & Compliances package Basic Platform Usage Essential information about using Atlan's core features, from browser requirements to data querying and asset management. Can Atlan integrate with multiple Azure AD tenants within a single instance? Learn about can atlan integrate with multiple azure ad tenants within a single instance?. Can we restrict who can query our data warehouse? Learn about can we restrict who can query our data warehouse?. Configure custom domains for Microsoft Excel If your Atlan tenant is hosted on a custom domain   -  for example, `https://<your-tenant-name>.mycompany.com`Â   -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel. Configure language settings How does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level. Configure the extension for managed browsers If you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script. Control access to metadata and data? Learn about control access to metadata and data?. Create a new tag For tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar. Create data contracts Create data contracts <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Create data domains Data domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization. Create domain policies Domain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more. Create forms You can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more. Create groups :::warning Who can do this? You will need to be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to cr. Create persona :::warning Who can do this? You will need to be an admin user to create personas. Create policies âAvailable via the Advanced Policy & Compliances package Create purpose :::warning Who can do this? You will need to be an admin user to create purposes. :::. Custom Metadata Atlan provides basic metadata for assets like certification, owners, and descriptions. But since every data team has their own unique needs, Atlan allows you to extend these with your own unique properties. Data Products From a single data table to a collection of data assets, anything can be a data product in Atlan. Data products provide a framework for your teams to curate assets specific to a domain, business unit, region of operation, brand, and more. These curated data products then empower your data consumers to easily discover data assets, quickly get the context they need, and collaborate more efficiently. Delete a tag If a tag is attached to assets, you will need to [remove the tag](/product/capabilities/governance/tags/how-tos/remove-a-tag) from the [tagged assets](/product/capabilities/governance/tags/how-tos/attach-a-tag)Â before deleting it. Disable user activity You can [view recently visited users](/product/capabilities/discovery/concepts/what-are-asset-profiles) and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps. Enable  Azure AD for SCIM provisioning You can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Enable  discovery of process assets To create a more customizable experience for your users, you can turn on discovery and tracking of process assets. Enable  sample data download Atlan allows admin users to enable or disable downloading [sample data](/product/capabilities/discovery/concepts/what-are-asset-profiles sample-data). This can help you enforce better governance across your organization. Enable  scheduled queries To enable scheduled queries, follow these steps. Getting Started and Onboarding Everything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements. Google Dashboard login error Learn about why do i get an error while logging in via google dashboard?. How are resources isolated? Learn about how are resources isolated?. How do I see views instead of materialized views in the reporting center? On the _Assets_ dashboard in the [reporting center](/product/capabilities/reporting/references/how-to-report-on-assets), click **View** in the _All Asset Types_ dropdown menu toÂ only see views. You can further filter by selecting a specific connector or connection as well. How do I use the filters menu? Learn about how do i use the filters menu?. How to interpret timestamps Learn about how to interpret timestamps. How to use parameterized queries? Learn about how to use parameterized queries?. Insights tips and tricks At Atlan, we are committed to powering your user experience. Here are a few tips and tricks to help you get the most out of Insights for you and your team! Install on Virtual Machine (K3s) This page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying [K3s in a rootless execution mode](https://docs.k3s.io/advanced running-rootless-servers-experimental:~:text=to%20take%20effect.-,Running%20Rootless%20Servers,-\\(Experimental\\)%E2%80%8B). Invite new users Note that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users. Is there a dashboard to see how my metadata is populated? Learn about is there a dashboard to see how my metadata is populated?. Limit SSO automatically creating users when they log in Only users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan. Make a query interactive If you want to share a query with others, but limit how they can change the query, you can make it _interactive_. Manage domains Most importantly, domains help promote shared ownership and domain-level governance in your organization. Manage playbooks Once you've [created a playbook](/product/capabilities/playbooks/how-tos/set-up-playbooks), you can monitor, modify, or delete it at any time. You can also [enable notifications](/product/capabilities/playbooks/how-tos/manage-playbooks) to monitor your playbook runs directly in Slack or Microsoft Teams. Manage policies You must be an admin user in Atlan to enable, create, manage, and approve data governance policies. Manage system announcements Have you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape. Manage users :::warning Who can do this? You will need to be an admin user in Atlan to manage other users. Monitor for runaway queries? Learn about monitor for runaway queries?. Organize assets Learn about organize assets. Our 3 pro tips for saving time with Atlan Learn about our 3 pro tips for saving time with atlan. Quality assurance framework Learn about quality assurance framework. query data Learn about query data. query without shared credentials Don't want to use a single shared service account to access data? Remove a tag Atlan allows you to remove [tags](/product/capabilities/governance/tags/concepts/what-are-tags) from a [tagged asset](/product/capabilities/discovery/asset-prof. Save and share queries You can save queries to re-run them later, schedule them, or share them. Set up a private network link to Amazon MSK :::warning Who can do this? You will need your Amazon MSK or AWS administrator involved - you may not have access to run these tasks. Set up a private network link to Amazon Redshift :::warning Who can do this? You will need your Amazon Redshift administrator or AWS administrator involved - you may not have access to run these tasks. Set up a private network link to Microsoft SQL Server on Amazon EC2 :::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself. Set up a private network link to Microsoft SQL Server on Amazon RDS :::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself. Set up a private network link to MySQL :::warning Who can do this? You will need your AWS administrator to complete these tasks - you may not have access yourself. Set up a private network link to PostgreSQL Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This [method](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-setup.html rds-proxy-secrets-arns) uses a username and password to connect to the RDS database. Set up a private network link to Tableau server As a prerequisite for TLS configuration on Tableau Server only, ensure that the health check _Protocol_ of the target group is set to **HTTPS** or [modify the health check settings](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html modify-health-check-settings) as required. Set up Amazon Athena Learn about set up amazon athena. Set up an AWS private network link to Snowflake Atlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you. Set up an Azure private network link to Snowflake Atlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request. Set up Apache Kafka :::warning Who can do this? You will probably need your Apache Kafka administrator to run these commands - you may not have access yourself. Set up AWS Lambda Learn about set up aws lambda. Set up Cloudera Impala Learn about set up cloudera impala. Set up dbt Core Learn about set up dbt core. Set up Metabase :::warning Who can do this? You will probably need your Metabase administrator to follow the below steps - you may not have access yourself. Set up playbooks Learn about set up playbooks. Suggestions from similar assets Learn about suggestions from similar assets. Tags For details on tagging assets, see [How to attach a tag](/product/capabilities/governance/tags/how-tos/attach-a-tag). Tenant access management Learn about tenant access management. Tenant monitoring Learn about tenant monitoring. Tenant offboarding Learn about tenant offboarding. The DataOps Culture Code Learn about the dataops culture code. Troubleshoot permission issues Learn about troubleshoot permission issues. Troubleshooting bring your own credentials Learn about troubleshooting bring your own credentials. Troubleshooting exporting large query results Learn about troubleshooting exporting large query results. Troubleshooting Google BigQuery connectivity Learn about troubleshooting google bigquery connectivity. Troubleshooting Hive connectivity Learn about troubleshooting hive connectivity. Troubleshooting MicroStrategy connectivity Learn about troubleshooting microstrategy connectivity. Troubleshooting MySQL connectivity Learn about troubleshooting mysql connectivity. Troubleshooting on-premises database connectivity Learn about troubleshooting on-premises database connectivity. Troubleshooting playbooks Learn about troubleshooting playbooks. Troubleshooting policies Learn about troubleshooting policies. Troubleshooting PostgreSQL connectivity Learn about troubleshooting postgresql connectivity. Troubleshooting Snowflake connectivity Learn about troubleshooting snowflake connectivity. Troubleshooting Snowflake tag management Learn about troubleshooting snowflake tag management. Update input type for existing custom metadata Learn about update input type for existing custom metadata. Use Atlan AI for documentation â Available to customers in Enterprise and Business-Critical platform editions Use the Atlan browser extension The Atlan browser extension provides metadata context directly in your [supported data tools]( supported-tools). You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. User Management and Access Control Complete guide to managing users, configuring access controls, and understanding permissions in Atlan. What are asset profiles? Learn about what are asset profiles?. What are groups? Learn about what are groups?. What are partial assets? Learn about what are partial assets?. What are personas? Learn about what are personas?. What are preflight checks? Learn about what are preflight checks?. What are purposes? Learn about what are purposes?. What are the query builder actions? Learn about what are the query builder actions?. What are the sidebar tabs? Learn about what are the sidebar tabs?. What column keys does Atlan crawl? Learn about what column keys does atlan crawl?. What controls the frequency of queries? Learn about what controls the frequency of queries?. What happens when users do not have access to metadata? Learn about what happens when users do not have access to metadata?. What is a product score? Learn about what is a product score?. What is Atlan? Learn about what is atlan?."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake",
    "text": "Connect data Data Warehouses Snowflake On this page Snowflake Overview: Catalog Snowflake databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your cloud data warehouse assets. Get started â Follow these steps to connect and catalog Snowflake assets in Atlan: Set up the connector Crawl Snowflake assets Guides â Authentication â Enable Snowflake OAuth : Set up OAuth authentication for Snowflake connections. Metadata & lineage â Mine Snowflake : Learn how to mine query history and construct lineage for Snowflake assets. Tag management â Manage Snowflake tags : Configure and manage tags and policy tags in Snowflake. Advanced features â Configure Snowflake data metric functions : Configure and use data metric functions in Snowflake. Private networking â Set up an AWS private network link to Snowflake : Establish a secure, private network connection to Snowflake on AWS. Set up an Azure private network link to Snowflake : Establish a secure, private network connection to Snowflake on Azure. References â What does Atlan crawl from Snowflake : Learn about the Snowflake assets and metadata that Atlan discovers and catalogs. Preflight checks for Snowflake : Verify prerequisites before setting up the Snowflake connector. Troubleshooting â Troubleshooting connectivity : Resolve common Snowflake connection issues and errors. Best practices â Snowflake warehouse configuration : Recommended Snowflake warehouse configuration to enable reliable Atlan workflow execution. Tags: connectivity snowflake Next Set up Snowflake Get started Guides References Troubleshooting Best practices"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-aws-private-network-link-to-snowflake",
    "text": "Connect data Data Warehouses Snowflake Get Started Set up an AWS private network link to Snowflake On this page Set up an AWS private network link to Snowflake AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Snowflake and Atlan, when you use our Single Tenant SaaS deployment. Who can do this? You will need Snowflake Support, and probably your Snowflake administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites â Snowflake must be setup with Business Critical Edition (or higher). Open a ticket with Snowflake Support to enable PrivateLink for your Snowflake account. Snowflake support will take 1-2 days to review and enable PrivateLink. If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support request to do so. (For all details, see the Snowflake documentation .) Fetch PrivateLink information â Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin; select system$get_privatelink_config(); This will produce output like the following (formatted here for readability): { \"privatelink-account-name\":\"abc123.ap-south-1.privatelink\", \"privatelink-vpce-id\":\"com.amazonaws.vpce.ap-south-1.vpce-svc-257a4d536bd8e3594\", \"privatelink-account-url\":\"abc123.ap-south-1.privatelink.snowflakecomputing.com\", \"regionless-privatelink-account-url\":\"xyz789-abc123.privatelink.snowflakecomputing.com\", \"privatelink_ocsp-url\":\"ocsp.abc123.ap-south-1.privatelink.snowflakecomputing.com\", \"privatelink-connection-urls\":\"[]\" } Share details with Atlan support team â Share the following values with the Atlan support team: privatelink-account-name privatelink-vpce-id privatelink-account-url privatelink_ocsp-url Atlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you. When you use this endpoint in the configuration for crawling and mining , Atlan will connect to Snowflake over the PrivateLink. Tags: atlan documentation Previous Set up Snowflake Next Set up an Azure private network link to Snowflake Prerequisites Fetch PrivateLink information Share details with Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-azure-private-network-link-to-snowflake",
    "text": "Connect data Data Warehouses Snowflake Get Started Set up an Azure private network link to Snowflake On this page Set up an Azure private network link to Snowflake Azure Private Link creates a secure, private connection between services running in Azure. This document describes the steps to set this up between Snowflake and Atlan. Who can do this? You will need Snowflake Support, and probably your Snowflake administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites â Snowflake must be set up with Business Critical Edition (or higher). Open a ticket with Snowflake Support to enable Azure Private Link for your Snowflake account. Snowflake support will take 1-2 days to review and enable Azure Private Link. If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support request to do so. (For all details, see the Snowflake documentation .) Fetch Private Link information â Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin; select system$get_privatelink_config(); This will produce an output like the following (formatted here for readability): { \"regionless-snowsight-privatelink-url\": \"abc123.privatelink.snowflakecomputing.com\", \"privatelink-account-name\": \"abc123.west-europe.privatelink\", \"snowsight-privatelink-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-account-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-connection-ocsp-urls\": \"[]\", \"privatelink-pls-id\": \"abc123.westeurope.azure.privatelinkservice\", \"regionless-privatelink-account-url\": \"abc123.privatelink.snowflakecomputing.com\", \"privatelink_ocsp-url\": \"ocsp.abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-connection-urls\": \"[]\" } Share details with Atlan support team â Share the following values with the Atlan support team : regionless-snowsight-privatelink-url privatelink-account-name snowsight-privatelink-url privatelink-account-url privatelink-connection-ocsp-urls privatelink-pls-id regionless-privatelink-account-url privatelink_ocsp-url privatelink-connection-urls Atlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request. Approve the endpoint connection request â Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin; SELECT SYSTEM$AUTHORIZE_PRIVATELINK ( '/subscriptions/26d.../resourcegroups/sf-1/providers/microsoft.network/privateendpoints/test-self-service', 'eyJ...' ); Snowflake will return an AccountÂ isÂ authorizedÂ forÂ PrivateLink. message to confirm successful authorization. The status of the private endpoint in Atlan will then change to Approved . When you use this endpoint in the configuration for crawling and mining Snowflake, Atlan will connect to Snowflake over the Private Link. (Optional) Configure private endpoint for internal stages â This is only required if you're using Snowflake internal stages. To enable Atlan to securely access your Snowflake internal stages, Atlan will require a private endpoint to your Azure storage account. Refer to Snowflake documentation to learn more. To configure an Azure private endpoint to access Snowflake internal stages: Open the Azure portal and navigate to your Azure Storage account. On the Storage accounts page, select the storage account to connect. From the storage account menu, click Overview . In the Resource JSON form, for Resource ID , click the clipboard icon to copy the value and contact Atlan support to share the value . (Atlan support will finish the configuration on the Atlan side using the Resource ID value and contact you to confirm endpoint creation.) From the storage account menu, click Security + networking and then click Networking . On the Networking page, change to the Private endpoint connections tab and then approve the endpoint connection request from Atlan. Tags: atlan documentation Previous Set up an AWS private network link to Snowflake Next How to enable Snowflake OAuth Prerequisites Fetch Private Link information Share details with Atlan support team Approve the endpoint connection request (Optional) Configure private endpoint for internal stages"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/enable-snowflake-oauth",
    "text": "Connect data Data Warehouses Snowflake Get Started How to enable Snowflake OAuth On this page Enable  Snowflake OAuth Atlan supports Snowflake OAuth-based authentication Â for Snowflake connections. Once the integration has been completed, Atlan will generate a trusted secure token with Snowflake. This will allow Atlan to authenticate users with Snowflake on their behalf to: Query data with Snowflake OAuth credentials View sample data with Snowflake OAuth credentials Configure Snowflake OAuth in Atlan â Who can do this? You will need to be a connection admin in Atlan to complete these steps. You will also needÂ inputs and approval from your Snowflake account administrator . To configure Snowflake OAuth on a Snowflake connection, from Atlan: From the left menu of any screen, click Assets . From the Assets page, click the Connector filter, and from the dropdown, click Snowflake . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select a Snowflake connection to enable Snowflake OAuth-based authentication. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , click Snowflake OAuth to enforce Snowflake OAuth credentials for querying data : For Authentication Required , click Copy Code to copy a security authorization code to execute it in Snowflake . UnderÂ Display sample data , for Source preview , click Snowflake OAuth to enforce Snowflake OAuth credentials for viewing sample data : If Snowflake OAuth-based authentication is enabled for querying data, the same connection details will be reused for viewing sample data. If a different authentication method is enabled for querying data, click Copy Code to copy a security authorization code to execute it in Snowflake . (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Did you know? The refresh token does not expire by default. Create a security integration in Snowflake â Who can do this? You will need your Snowflake account administrator to run these commands. You will also need to have an existing Snowflake connection in Atlan. To create a security integration in Snowflake: Log in to your Snowflake instance. From the top right of your Snowflake instance, click the + button, and then from the dropdown, click SQL Worksheet to open a new worksheet. In the query editor of your Snowflake SQL worksheet, paste the security authorization code you copied in Atlan . See a representative example below: CREATE SECURITY INTEGRATION < name > TYPE = EXTERNAL_OAUTH ENABLED = TRUE EXTERNAL_OAUTH_TYPE = OKTA EXTERNAL_OAUTH_ISSUER = 'https://<COMPANY>.okta.com/oauth2/<ID>' EXTERNAL_OAUTH_JWS_KEYS_URL = 'https://<COMPANY>.okta.com/oauth2/<ID>/v1/keys' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '<snowflake_account_url' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_ANY_ROLE_MODE = 'ENABLE' ; EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'EMAIL_ADDRESS' Run the security integration in Snowflake. (Optional) To allow the ACCOUNTADMIN , ORGADMIN , or SECURITYADMIN role to query with Snowflake OAuth-based authentication, add and run the following command to set account-level permissions: ALTER ACCOUNT SET EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST = FALSE ; Your users will now be able to run queries and view sample data using their Snowflake OAuth credentials! ð Did you know? You can refer to troubleshooting connector-specific SSO authentication to troubleshoot any errors. Tags: connectors data integration authentication Previous Set up an Azure private network link to Snowflake Next Crawl Snowflake Configure Snowflake OAuth in Atlan Create a security integration in Snowflake"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
    "text": "Connect data Data Warehouses Snowflake Crawl Snowflake Assets Crawl Snowflake On this page Crawl Snowflake Once you have configured the Snowflake user permissions , you can establish a connection between Atlan and Snowflake. (If you are also using AWS PrivateLink or Azure Private Link for Snowflake, you will need to set that up first, too.) To crawl metadata from Snowflake, review the order of operations and then complete the following steps. Select the source â To select Snowflake as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Snowflake Assets and click on Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . This is currently only supported when using the information schema extraction method to fetch metadata with basic authentication . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Snowflake credentials: For Account Identifiers (Host) , enter the hostname, AWS PrivateLink endpoint , or Azure Private Link endpoint for your Snowflake instance. For Authentication , choose the method you configured when setting up the Snowflake user : For Basic authentication, enter the Username and Password you configured in either Snowflake or the identity provider. info ðª Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. For Keypair authentication, enter the Username , Encrypted Private Key , and Private Key Password you configured. Atlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to Snowflake documentation . For Okta SSO authentication,Â enter the Username ,Â Password , and Authenticator you configured. The Authenticator will be the Okta URL endpoint of your Okta account , typically in the form of https://<okta_account_name>.okta.com . For Role , select the Snowflake role through which the crawler should run. For Warehouse , select the Snowflake warehouse in which the crawler should run. Click Test Authentication to confirm connectivity to Snowflake using these details. Once successful, at the bottom of the screen, click Next . Offline extraction method â Atlan supports the offline extraction method for fetching metadata from Snowflake. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank. ForÂ Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the Snowflake connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Snowflake data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Snowflake data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Snowflake. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the crawler â danger When modifying an existing Snowflake connection, switching to a different extraction method will delete and recreate all assets in the existing connection. If you'd like to change the extraction method, contact Atlan support Â for assistance. Before running the Snowflake crawler, you can further configure it. You must select the Extraction method you configured when you set up Snowflake : For Information Schema Â method , keep the default selection. Change to Account Usage method Â and specify the following: Database Name of the copied Snowflake database Schema Name of the copied ACCOUNT_USAGE schema Incremental extraction Public preview - Toggle incremental extraction for faster and more efficient metadata extraction. You can override the defaults for any of the remaining options: For Asset selection , select a filtering option: To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This will default to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression   -  for example, specifying ATLAN_EXAMPLE_DB.* for Databases will include all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This will default to no assets, if none are specified.)Â To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression   -  for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views will exclude all the matching tables and views. Click + to add more filters. If you add multiple filters, assets will be crawled based on matching all the filtering conditions you have set. To exclude lineage for views in Snowflake, change View Definition Lineage to No . To import tags from Snowflake to Atlan , change Import Tags to Yes . Note the following: If using the Account Usage extraction method, grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. If using the Information Schema extraction method, note that Snowflake stores all tag objects in the ACCOUNT_USAGE schema. You will need to grant permissions on the account usage schema instead to import tags from Snowflake. danger Object tagging in Snowflake currently requires Enterprise Edition or higher . If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error   -  unable to retrieve tags. For Control Config , keep Default for the default configuration or click Custom to further configure the crawler: If you have received a custom crawler configuration from Atlan support, for Custom Config , enter the value provided. You can also: Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. For Enable Source Level Filtering , click True to enable schema-level filtering at source or keep False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Exclude tables with empty data , change to Yes to exclude any tables and corresponding columns without any data. For Exclude views , change to Yes to exclude all views from crawling. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Snowflake crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Note that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run. Tags: connectors data crawl Previous How to enable Snowflake OAuth Next Mine Snowflake Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags",
    "text": "Connect data Data Warehouses Snowflake Manage Snowflake in Atlan Manage Snowflake tags On this page Manage Snowflake tags Note that object tagging in Snowflake currently requires Enterprise Edition or higher . Atlan enables you to import your Snowflake tags , update your Snowflake assets with the imported tags, and push the tag updates back to Snowflake: Import tags   -  crawl Snowflake tags from Snowflake to Atlan Reverse sync   -  sync Snowflake tag updates from Atlan to Snowflake Once you've imported your Snowflake tags to Atlan: Your Snowflake assets in Atlan are automatically enriched with their Snowflake tags. Imported Snowflake tags are mapped to corresponding Atlan tags through case-insensitive name match   -  multiple Snowflake tags can be matched to a single tag in Atlan. You can also attach Snowflake tags , including tag values, to your Snowflake assets in Atlan   -  allowing you to categorize your assets at a more granular level. Atlan supports: Allowed values : attach an allowed value from a predefined list of values imported from Snowflake. Tag values: enter any value in Atlan while attaching or editing imported Snowflake tags on an asset. You can enable reverse sync to push any tag updates for your Snowflake assets back to Snowflake   -  including allowed and tag values added to assets in Atlan. You can filter your assets by Snowflake tags and tag and allowed values. Did you know? Enabling reverse sync only updates existing tags in Snowflake. It neither creates nor deletes any tags in Snowflake. Prerequisites â Did you know? Additional privileges are only required when using the information schema method for fetching metadata. This is because Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , any permissions required are already set. Account usage method â Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. Information schema method â Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant additional permissions to import tags from Snowflake. Grant additional permissions to push updated tags to Snowflake. Import Snowflake tags to Atlan â Who can do this? You need to be an admin user in Atlan to import Snowflake tags to Atlan. You also need to work with your Snowflake administrator to grant additional permissions to import tags from Snowflake   -  you may not have access yourself. You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags are matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets are enriched with their synced tags from Snowflake. To import Snowflake tags to Atlan, you can either: Create a new Snowflake workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Snowflake workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan preserves those tags. Once the crawler has completed running, tags imported from Snowflake are available to use for tagging assets! ð View Snowflake tags in Atlan â Once you've imported your Snowflake tags, you can view and manage your Snowflake tags in Atlan. To view Snowflake tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. From the left menu under Tags , select a synced tag   -  synced tags display the Snowflake âï¸ icon next to the tag name. In the Overview section, you can view a total count of synced Snowflake tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Snowflake tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . You can't rename tags synced from Snowflake. Push tag updates to Snowflake â Who can do this? Any admin or member user in Atlan can configure reverse sync for tag updates to Snowflake. You also need to work with your Snowflake administrator to grant additional permissions to push updates -  you may not have access yourself. Did you know? Reverse sync is currently only available for imported Snowflake tags in Atlan. The imported tags display a Snowflake âï¸ icon next to the tag name. If using the account usage method , expect a data latency of up to 3 hours for reverse tag sync to be successful. You can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source. Once you have enabled reverse sync, any Snowflake assets with tags updated in Atlan are also updated in Snowflake. To enable reverse sync for imported Snowflake tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. In the left menu under Tags , select a synced Snowflake tag   -  synced tags display the Snowflake âï¸ icon next to the tag name. On your selected tag page, to the right of Overview , click Synced tags . Under Synced tags , in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Snowflake. In the advanced settings, you can also enable concatenation to support multiple tag values for a single column. For detailed information about multiple tag values and concatenation, see Multiple tag values and concatenation . In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel . Now when you attach Snowflake tags to your Snowflake assets in Atlan, these tag updates are also pushed to Snowflake! ð Did you know? Enabling reverse sync won't trigger any updates in Snowflake until synced tags are attached to Snowflake assets in Atlan. For any questions about managing Snowflake tags, head over here . Tags: connectors data crawl Previous Mine Snowflake Next Configure Snowflake data metric functions Prerequisites Import Snowflake tags to Atlan View Snowflake tags in Atlan Push tag updates to Snowflake"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/multiple-tag-values-and-concatenation",
    "text": "Connect data Data Warehouses Snowflake References Multiple tag values and concatenation On this page Multiple tag values and concatenation Atlan supports assigning multiple tag values to a single Snowflake object. When multiple tag values are assigned, Atlan concatenates them into a single string using a configurable delimiter. Requirements â Both reverse sync and concatenation must be enabled for multi-value synchronization to work. Constraints â When configuring multiple tag values, keep the following constraints in mind: The chosen delimiter can't appear inside any tag value to prevent parsing errors. The concatenated tag values length must not exceed 256 characters . If the allowed list is enabled for a tag in Snowflake, concatenated tag values that you attach to Snowflake objects must come from the tagâs predefined list. To use a new value, add it to the list. Each tag supports up to 300 values . For more information, see Snowflake tag quota for objects . How concatenation works â Important Tag concatenation is an Atlan feature. Concatenated values created in Atlan are synced to Snowflake. However, if you concatenate tag values in Snowflake workflows, those concatenated references won't be synced back to Atlan. Atlan manages multiple tag values by concatenating them into a single string that can be synchronized back to Snowflake. The process involves sorting, concatenation, and synchronization behaviors described below. Single values â When only one value is assigned to a tag, no concatenation occurs. The single value is sent as-is to Snowflake. For example, if you have a tag cost_center and assign only the value finance to an object, the result in Snowflake is the single value finance without any concatenation. Multiple values â You can assign multiple values to a single tag for any object in Atlan. When multiple values are assigned, they're concatenated into a single string using a delimiter character. For example, if you have a tag cost_center and assign the values finance , engineering , and sales to an object with comma as the delimiter, the result is the concatenated string engineering,finance,sales . Sorting â Tag values are sorted alphabetically before concatenation to maintain consistent ordering. For example, if you have a tag environment with values production , development , and staging assigned to an object and use a comma ( , ) as your delimiter, Atlan sorts them alphabetically ( development , production , staging ) and concatenates them as: development,production,staging . Reverse sync â Concatenation and reverse sync apply at the schema level for imported Snowflake tags. When reverse sync is enabled, the concatenated tag values are synchronized back to the corresponding objects in Snowflake. Updates and removals â When you update or remove tag assignments in Atlan, these changes can be synchronized back to Snowflake and depend on the combination of reverse sync and concatenation settings: When reverse sync is OFF : Snowflake isn't updated When reverse sync is ON but concatenation is OFF : Only one tag value (typically the latest) is sent to Snowflake When both reverse sync and concatenation are ON : All tag values are concatenated and sent as a single string See also â Snowflake object tagging introduction - Learn about Snowflake's tag capabilities, quotas, and supported objects Tags: multiple-concatenation snowflake Previous Configure Snowflake data metric functions Next What does Atlan crawl from Snowflake? Requirements Constraints How concatenation works See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity",
    "text": "Connect data Data Warehouses Snowflake Troubleshooting Troubleshooting Snowflake connectivity On this page Troubleshooting Snowflake connectivity How to debug test authentication and preflight check errors? â Missing warehouse grants The user doesnât have USAGE and OPERATE grants on a warehouse. Grant warehouse access to the role : GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse>\" TO ROLE atlan_user_role ; Then, ensure that you grant the role to the new user : GRANT ROLE atlan_user_role TO USER atlan_user ; Missing authorized access to SNOWFLAKE.ACCOUNT_USAGE schema The user doesnât have authorized access to the SNOWFLAKE.ACCOUNT_USAGE database Reach out to your account admin to grant imported privileges on the Snowflake database to the role: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role ; If using a copied database , you'll need to grant the following permissions: GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; Missing usage grants on databases and/or schemas The user doesn't have usage grants to the databases ` $missingDatabases ` and schemas ` $missingSchemas Grant missing permissions listed here for information schema extraction method. Atlan IP not allowlisted Atlan's current location or network isn't recognized by Snowflake's security settings. This can happen if Atlan's IP address isn't on the list of allowed addresses in Snowflake's network policies. If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist . Contact Atlan support to obtain Atlan's IP addresses. Incorrect credentials The username or the password provided to connect to the Snowflake account is incorrect. Sign into the Snowflake account for the specified host and verify that the username and password are correct. You can also create a new user, if required, by following the steps here . Missing or unauthorized role The role specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this role. If the role does not exist or is missing the required grants, create a role and then grant the role to the user . User account locked The user account you're using to connect to Snowflake has been locked temporarily because of multiple incorrect login attempts. Wait for the user account to unlock or create a different user account to continue. Missing or unauthorized warehouse The warehouse specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this warehouse. Ensure that the warehouse name is configured correctly. Update the warehouse name in the configuration if your account is using a different warehouse. Create a role and then grant the role to the user for the updated warehouse. Missing access to non-system databases or schemas The configured user doesn't have usage grants to any database or schema. or The configured user doesn't have usage grants to any non-system database or schema. This pertains to the information schema method of fetching metadata. Ensure that the user has authorized access to the databases and schemas to be crawled. Grant the requisite permissions as outlined here . Why are some assets from a database or schema missing? â Check the grants on the role attached to the user defined for the crawler. Ensure the missing database or schema is present in these grants. SHOW GRANTS TO ROLE atlan_user_role ; Why are new tables or views missing? â When using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata. Make sure the role attached to the user defined for the crawler has grants for future tables and views being created in the database: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Make sure you run the below commands as well so that new tables and views you've created in-between are also visible to the user: GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO role atlan_user_role ; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO role atlan_user_role ; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO role atlan_user_role ; Why is some lineage missing? â The query miner only mines query history for up to the previous two weeks. The miner will not mine any queries that ran before that time window. If the queries that created your assets ran before that time window, lineage for those assets will not be present. To mine more than the previous two weeks of query history, either use S3-based query mining or contact Atlan support . Note that Snowflake itself only retains query history for so long as well, though. Once Snowflake itself no longer contains the query history we will be unable to mine it for lineage. Lineage is unsupported for parameterized queries. Snowflake currently does not resolve values for parameterized queries before logging them in query history. This limits Atlan from generating lineage in such cases. Missing attributes and lineage â When using the account usage extraction method, there are currently some limitations. We are working with Snowflake to find workarounds for crawling the following: External table location data Procedures Primary key designation Furthermore, only database-level filtering is currently possible. What views does Atlan require access to for the account usage method? â When using the account usage method for fetching metadata, Atlan requires access to the following views in Snowflake: For the crawler: DATABASES , SCHEMATA , TABLES , VIEWS , COLUMNS , and PIPES For the miner and popularity metrics : QUERY_HISTORY , ACCESS_HISTORY , and SESSIONS Why am I getting a destination URL mismatch error when authenticating via Okta SSO? â This error can occur when you're connecting to Snowflake through Okta SSO and enter the URL of your Snowflake instance in a format different from the one used in Okta. Snowflake follows two URL formats: Legacy format   - Â <AccountLocator>.<Region>.snowflakecomputing.com or <AccountLocator>.<Region>.<cloud>.snowflakecomputing.com New URL format   - <Orgname>-<AccountName>.snowflakecomputing.com Ensure that you're using the same Snowflake URL format in Snowflake and Okta. Refer to Snowflake documentation to learn more. Why am I getting a 'name or service not known' error when connecting via private link? â If you're getting the following error messages   - java.net.UnknownHostException and Name or service not known -  this is a known error for users who have upgraded to the Snowflake JDBC driver version 3.13.25., have underscores in their account name, and connect to their Snowflake accounts over private link (for example, https://my_account.us-west-2.privatelink.snowflakecomputing.com ). If your Snowflake account name has an underscore   -  for example, my_account - Â the updated JDBC driver will automatically convert underscores to dashes or hyphens - . This does not affect normal URLs because Snowflake accepts URLs with both hyphens and underscores. For private link users, however, the JDBC driver will return an error if there are underscores present in the account name and the connection will fail. To troubleshoot further, refer to Snowflake documentation . Tags: atlan documentation Previous Preflight checks for Snowflake Next Troubleshooting Snowflake tag management"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/best-practices/snowflake-warehouse-configuration",
    "text": "Connect data Data Warehouses Snowflake Best Practices Snowflake warehouse configuration On this page Snowflake warehouse configuration Configure your Snowflake warehouses following these best practices to achieve optimal performance and reliability for Atlan data workflows. These recommendations establish predictable resource allocation and maximize workflow efficiency. Configure warehouse allocation â Use a dedicated warehouse for Atlan workflows : Assign a dedicated warehouse exclusively for Atlan operations. This approach separates warehouse performance from other workloads, enables precise cost tracking for Atlan operations, and provides consistent workflow performance. One warehouse per connection : Provision one Snowflake warehouse for each Atlan connection to maintain scoped capacity and predictable resource allocation. Configure statement timeout â Set appropriate timeout values : If your account enforces timeouts, configure both STATEMENT_TIMEOUT_IN_SECONDS and STATEMENT_QUEUED_TIMEOUT_IN_SECONDS to at least 6 hours (21,600 seconds) for the Atlan user to accommodate comprehensive data cataloging workflows. Default values work well : By default, both parameters are set to 0 (no limit), which is optimal for Atlan operations. Only adjust if your organization requires specific timeout enforcement. Apply at user level : Configure timeouts at the user level for consistent behavior across all Atlan sessions rather than at warehouse or session level. Tags: snowflake warehouse configuration Previous Troubleshooting Snowflake tag management Configure warehouse allocation Configure statement timeout"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/manage-connectivity",
    "text": "Connect data Connectivity Framework Connector Framework How-tos Manage connectivity On this page Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Modify connectivity â To modify the configuration of an existing workflow, complete the following steps. On the left of any screen, navigate toÂ Workflow . Under Monitor select an existing workflow tile. (You may need to expand the run history or filter first.) From the Workflow Run History table, click on the previous run of the workflow you want to modify. In the upper left of the screen, change to the Config tab. Modify the parts of the workflow configuration you require: Under <Connector> Credential , use the Edit Credentials button to change the credentials for the source. danger If you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically. Under Connection settings , use the Edit Â button to change the connection details: Modify whether or not querying or data previews are allowed for the source. Modify the query row limit to enable exporting large query results via email . Modify the query timeout limit   -  expandable up to 60 minutes. Under Connection Admins , click the pencil icon to add or remove connection admins. danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Under Metadata , use the selectors to modify which metadata to include and exclude. To check for any permissions or other configuration issues before running the workflow, click Preflight checks . Once you've made your updates, click theÂ Update button to save the changes. You can optionally run the workflow with the new configuration immediately. You will need to confirm your changes by clicking theÂ Yes button. Note that some workflow changes may take a few minutes to come into effect. That's it   -  next time you run the workflow, or it runs on its schedule, it will use your changes! ð danger If you modify the Metadata portion, any previously crawled metadata that is now excluded will be archived on the next workflow run. Tags: integration connectors workflow automation orchestration Previous Connectors Next Monitor connectivity Modify connectivity"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/mine-snowflake",
    "text": "Connect data Data Warehouses Snowflake Crawl Snowflake Assets Mine Snowflake On this page Mine Snowflake Once you have crawled assets from Snowflake , you can mine its query history to construct lineage. To mine lineage from Snowflake, review the order of operations and then complete the following steps. Select the miner â To select the Snowflake miner: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the filters along the top, click Miner . From the list of packages, select Snowflake Miner and then click Setup Workflow . Configure the miner â To configure the Snowflake miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , select Source , Agent , or see the separate instructions for the S3 miner . For Snowflake Database : If the connection is configured with access to the snowflake database , choose Default . If the connection can only access a separate cloned database , choose Cloned Database . If you are using a cloned database, enter the name of the cloned database in Database Name and the name of the cloned schema in Schema Name . For Start time , choose the earliest date from which to mine query history. info ðª Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. To check for any permissions or other configuration issues before running the miner, click Preflight checks . At the bottom of the screen, click Next to proceed. Agent extraction method â Atlan supports using a Secure Agent for mining query history from Snowflake. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the connection configuration used when crawling Snowflake . Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. danger If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Configure the miner behavior â To configure the Snowflake miner behavior: (Optional) For Calculate popularity , keep True to retrieve usage and popularity metrics for your Snowflake assets from query history. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Snowflake assets. Press Enter after each name to add more names.Â (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the miner: If Atlan support has provided you with a custom control configuration,Â enter the configuration into theÂ Custom Config box. You can also enter {âignore-all-caseâ: true} to enable crawling assets with case-sensitive identifiers. For Popularity Window (days) , 90 days is the maximum limit. You can set a shorter popularity window of less than 90 days. Run the miner â To run the Snowflake miner, after completing the steps above: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Snowflake assets that were created in Snowflake between the start time and when the miner ran! ð Tags: connectors data crawl setup Previous Crawl Snowflake Next Manage Snowflake tags Select the miner Configure the miner Configure the miner behavior Run the miner"
  },
  {
    "url": "https://docs.atlan.com/tags/connectors",
    "text": "299 docs tagged with \"connectors\" View all tags Add impact analysis in GitHub Learn about add impact analysis in github. Add impact analysis in GitLab Learn about add impact analysis in gitlab. Atlan browser extension security Learn about atlan browser extension security. Attach a tag Atlan allows users to add [tags](/product/capabilities/governance/tags/concepts/what-are-tags) to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection. Automate data profiling âAvailable via the Data Quality Studio package Bulk enrich metadata Atlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan. Can Atlan integrate with Airflow to generate lineage? Atlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage). Can I add Atlan's browser extension for everyone in my organization? Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension). Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case. Can I turn off sample data preview for the entire organization? Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data. Can site renaming affect the Jira integration? Learn about can site renaming affect the jira integration?. Can the Hive crawler connect to an independent Hive metastore? Learn about can the hive crawler connect to an independent hive metastore?. Can we use a Microsoft SSO login? Learn about can we use a microsoft sso login?. Configure workflow execution Learn about configure workflow execution. Connect data sources for Azure-hosted Atlan instances This document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:. Connect on-premises databases to Kubernetes You can configure and use [Atlan's metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access) to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose. Connection issues Resolve common connection and authentication issues when setting up CrateDB connector Connectors Learn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management. Connectors and capabilities Learn about connectors and capabilities. Crawl Aiven Kafka Once you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka. Crawl Amazon Athena To crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Amazon DynamoDB Once you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB. Crawl Amazon MSK To crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Amazon QuickSight Once you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),. Crawl Amazon Redshift Once you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift. Crawl Apache Kafka Learn about crawl apache kafka. Crawl AWS Glue Once you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue. Crawl BigID Configure and run the Atlan BigID workflow to crawl metadata from BigID. Crawl Confluent Kafka Learn about crawl confluent kafka. Crawl Confluent Schema Registry Once you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry. Crawl CrateDB Configure and run the CrateDB crawler to extract metadata from your database Crawl Dagster assets Create a crawler workflow in Atlan to capture lineage from Dagster assets Crawl Databricks To crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl DataStax Enterprise Crawl DataStax Enterprise Crawl dbt Once you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan. Crawl Domo Once you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo. Crawl Fivetran Learn about crawl fivetran. Crawl Google BigQuery Once you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery. Crawl Hive To crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl IBM Cognos Analytics Once you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics. Crawl Informatica CDI assets Configure and run the crawler to discover and catalog your Informatica CDI assets Crawl Looker Once you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker. Crawl Matillion Once you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion. Crawl Metabase Once you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase. Crawl Microsoft Azure Cosmos DB Once you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB. Crawl Microsoft Azure Data Factory Once you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-. Crawl Microsoft Azure Event Hubs Once you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs. Crawl Microsoft Azure Synapse Analytics Once you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics. Crawl Microsoft Power BI Once you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI. Crawl Microsoft SQL Server Once you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),. Crawl MicroStrategy Once you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy. Crawl Mode Once you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode. Crawl MongoDB Once you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB. Crawl Monte Carlo Once you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo. Crawl MySQL To crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl on-premises databases Once you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps. Crawl on-premises Databricks Once you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps. Crawl on-premises IBM Cognos Analytics Once you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps. Crawl on-premises Kafka Once you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps. Crawl on-premises Looker Once you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps. Crawl on-premises Tableau Once you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps. Crawl on-premises ThoughtSpot Once you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),. Crawl Oracle Once you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle create-user-in-oracle), you can establish a connection between Atlan and Oracle. Crawl PostgreSQL To crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl PrestoSQL Once you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL. Crawl Qlik Sense Cloud Once you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud. Crawl Qlik Sense Enterprise on Windows Once you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-. Crawl Redash Once you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash. Crawl Redpanda Kafka Once you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka. Crawl Salesforce Once you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce. Crawl SAP HANA Once you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA. Crawl Sigma Once you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma. Crawl Sisense Once you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense. Crawl Snowflake To crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Soda Once you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda. Crawl Tableau To crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Teradata Once you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata. Crawl ThoughtSpot Once you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot. Crawl Trino To crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Create README templates Admin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and [enrich their assets with READMEs](/product/integrations). They will also be able to see a rich preview of each template before adding the relevant documentation. Custom solutions Learn about custom solutions. Dagster integration Frequently asked questions about Dagster integration with Atlan Data Connections and Integration Complete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. Delete a connection Learn about delete a connection. Deployment architecture The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. Does Atlan require an admin user in Salesforce? No. However, it is recommended that a Salesforce administrator establishes a [connection between Atlan and Salesforce](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce). To learn more, see [here](/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity). Does lineage only cover calculated fields for Tableau dashboards? Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Enable  Azure AD for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  Google for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  JumpCloud for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  Okta for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  OneLogin for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  SAML 2.0 for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  Snowflake OAuth Atlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware. Enable  SSO for Amazon Redshift You will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift). Enable  SSO for Google BigQuery Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan. Enable Okta for SCIM provisioning You can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Enrich Atlan through dbt Beyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property. ETL tools connectors Overview and entry point for all ETL tools connectors in Atlan. extract lineage and usage from Databricks Once you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal. extract on-premises Databricks lineage Once you have [set up the databricks-extractor tool](/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction), you can extract lineage from your on-premises Databricks instances by completing the following steps. Find assets by usage Data teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance. How are product updates deployed? Learn about how are product updates deployed?. How can I identify an Insights query in my database access log? Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. Implement OpenLineage in Airflow operators If you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage. Infrastructure security Learn about infrastructure security. Integrate Amazon MWAA/OpenLineage To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Anomalo Once you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo. Integrate Apache Airflow/OpenLineage To integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Astronomer/OpenLineage To integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/. Integrate Atlan with Google Sheets The Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan. Integrate Jira Cloud You must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work. Integrate Microsoft Teams Once you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams. Interpret usage metrics Atlan currently supports usage and popularity metrics for the following connectors: Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Link your Jira account To create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that [set up the Jira integration](/product/integrations/project-management/jira/how-tos/integrate-jira-cloud), but not for other users. Link your Microsoft Teams account To get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users. Link your Slack account To see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that [set up the Slack integration](/product/integrations/collaboration/slack/how-tos/integrate-slack), but not for other users. Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Manage Databricks tags You must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan. Manage dbt tags Atlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags. Manage Google BigQuery tags Atlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro editions_features). Manage requests If your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected. Manage Snowflake tags You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake. Migrate from dbt to Atlan action The dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/. Mine Amazon Redshift Once you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics). Mine Google BigQuery Once you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage. Mine Microsoft Azure Synapse Analytics Learn about mine microsoft azure synapse analytics. Mine Microsoft Power BI Once you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics. Mine queries through S3 Once you have crawled assets from a supported connector, you can mine query history. Mine Snowflake Once you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage. Mine Teradata Once you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage. Monitor connectivity Atlan runs its crawlers through an orchestrated set of automated tasks. Okta first-time login authentication error Learn about why do i get an authentication error when logging in via okta for the first time?. OpenLineage configuration and facets Learn about openlineage configuration and facets. order workflows The [order of operations](/product/connections/how-tos/order-workflows order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers. Permissions and limitations Frequently asked questions about CrateDB connector setup, permissions, and limitations Preflight checks for Aiven Kafka Before [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne. Preflight checks for Amazon MSK Before [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti. Preflight checks for Amazon QuickSight The [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission. Preflight checks for Amazon Redshift Before [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec. Preflight checks for Anomalo This check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid. Preflight checks for Apache Kafka Before [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection. Preflight checks for Confluent Schema Registry Before [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca. Preflight checks for CrateDB Technical validations performed before running the CrateDB crawler to verify connectivity and permissions Preflight checks for Databricks Before [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co. Preflight checks for DataStax Enterprise Preflight checks for DataStax Enterprise Preflight checks for dbt This checks if manifest files are present in the provided bucket and prefix. Preflight checks for Domo Atlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo. Preflight checks for Fivetran Learn about preflight checks for fivetran. Preflight checks for Google BigQuery Each request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication service-accounts). Preflight checks for Hive Before [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a. Preflight checks for Looker First, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management). Preflight checks for Metabase Before [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co. Preflight checks for Microsoft Azure Data Factory Before [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact. Preflight checks for Microsoft Azure Synapse Analytics This check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method. Preflight checks for Microsoft Power BI Before [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run. Preflight checks for Microsoft SQL Server Before [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli. Preflight checks for MicroStrategy First, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html /Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions. Preflight checks for Mode Before [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co. Preflight checks for Monte Carlo Before [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c. Preflight checks for MySQL Before [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha. Preflight checks for Oracle Before [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/. Preflight checks for PostgreSQL Before [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio. Preflight checks for PrestoSQL Before [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/. Preflight checks for Qlik Sense Cloud This check tests for access to datasets and other Qlik objects. Preflight checks for Redash Before [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti. Preflight checks for Redpanda Kafka Before [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod. Preflight checks for Salesforce Before [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co. Preflight checks for SAP S/4HANA Preflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Preflight checks for Sigma First, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission. Preflight checks for Sisense Atlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6 /folders) to check if it's responding with a response status code 200. Preflight checks for Snowflake Before [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne. Preflight checks for Tableau The [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm server_info) REST API is used to fetch the `restApiVersion` value. Preflight checks for Teradata Before [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con. Preflight checks for Trino Before [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha. Provide credentials to query data Learn about provide credentials to query data. Provide credentials to view sample data Learn about provide credentials to view sample data. provide SSL certificates SSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau). Provider package versions for OpenLineage Learn about provider package versions for openlineage. Report on assets Learn about report on assets. Report on automations You can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions. S3 Inventory Report Structure Expected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion. Set default user roles for SSO :::warning Who can do this? You will need to be an admin user and [configure SSO](/product/integrations/identity-management/sso) with a provider first. Set up a private network link to Amazon Athena :::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps. Set up a private network link to Hive Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Set up a private network link to Trino :::warning Who can do this? You will need your AWS administrator involved - you may not have access to run these tasks yourself. Set up Aiven Kafka Atlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Set up Alteryx Set up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run. Set up Amazon DynamoDB Learn about set up amazon dynamodb. Set up Amazon S3 Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects. Set up Anomalo Atlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata. Set up AWS Glue Learn about set up aws glue. Set up BigID Create a BigID system user and API token for Atlan integration. Set up client credentials flow Configure Salesforce for OAuth 2.0 client credentials authentication in Atlan. Set up Confluent Schema Registry :::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself. Set up CrateDB Configure authentication and connection settings for CrateDB connector Set up Dagster Configure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assets Set up dbt Cloud :::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself. Set up Domo :::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself. Set up Google BigQuery You must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access). Set up Google Cloud Storage Configure Google Cloud Storage for secure metadata ingestion with Atlan. Set up IBM Cognos Analytics :::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself. Set up Informatica CDI Configure authentication and user permissions for Informatica Cloud Data Integration connector Set up Inventory reports Create Inventory report for Amazon S3 in case of inventory based ingestion through the crawler. Set up JWT bearer flow Configure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan. Set up Matillion Configure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance. Set up Microsoft Azure Event Hubs Atlan supports the following authentication methods for Microsoft Azure Event Hubs:. Set up Microsoft Azure Synapse Analytics Atlan supports crawling the following with the Microsoft Azure Synapse Analytics package:. Set up MicroStrategy Atlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata. Set up Mode If you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email. Set up MongoDB Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password]( create-database-user-in-mongodb) to fetch metadata. Set up Monte Carlo :::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups). Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Microsoft Azure Synapse Analytics miner access In some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to [mine query history from the Query Store](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics). For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Teradata miner access In some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up PrestoSQL Learn about set up prestosql. Set up Redpanda Kafka Atlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Set up Salesforce Learn about setting up Salesforce authentication for Atlan. Set up Sisense Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Set up Snowflake :::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::. Set up Teradata :::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself. Set up ThoughtSpot :::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself. Set up username-password flow Configure Salesforce username-password flow for Atlan integration. Supported connections for on-premises databases The metadata-extractor tool supports the following connection types. Supported sources Learn about supported sources. Task and crawl issues Troubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance. Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector Transformations Understand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan Troubleshooting AWS Glue connectivity Learn about troubleshooting aws glue connectivity. Troubleshooting connector-specific SSO authentication Learn about troubleshooting connector-specific sso authentication. Troubleshooting Metabase connectivity Learn about troubleshooting metabase connectivity. Troubleshooting Microsoft Teams Why do I get an error while adding Atlan to Microsoft Teams? Troubleshooting Mode connectivity Learn about troubleshooting mode connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity. Troubleshooting SCIM provisioning Learn about troubleshooting scim provisioning. Troubleshooting ServiceNow Why is the security\\_admin role required to complete the ServiceNow integration? Troubleshooting Sisense connectivity Learn about troubleshooting sisense connectivity. Troubleshooting Slack What do the colors in Slack notifications for modified assets mean? Troubleshooting spreadsheets Why do I need admin consent for exporting assets to Microsoft Excel? Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. update column metadata in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets. Update column metadata in Microsoft Excel Once you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel. View event logs Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. What are user roles? Learn about what are user roles?. What does Atlan crawl from Aiven Kafka? Atlan crawls and maps the following assets and properties from Aiven Kafka. What does Atlan crawl from Amazon MSK? Atlan crawls and maps the following assets and properties from Amazon MSK. What does Atlan crawl from Amazon MWAA/OpenLineage? Once you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [. What does Atlan crawl from Amazon QuickSight? Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:. What does Atlan crawl from Anomalo? Once you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check. What does Atlan crawl from Apache Airflow/OpenLineage? Once you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),. What does Atlan crawl from Apache Kafka? Atlan crawls and maps the following assets and properties from Apache Kafka. What does Atlan crawl from Apache Spark/OpenLineage? Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports. What does Atlan crawl from Astronomer/OpenLineage? Atlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html). What does Atlan crawl from BigID? Reference guide for BigID metadata crawled by Atlan. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Confluent Kafka? Atlan crawls and maps the following assets and properties from Confluent Kafka. What does Atlan crawl from Confluent Schema Registry? Atlan crawls and maps the following assets and properties from Confluent Schema Registry. What does Atlan crawl from CrateDB? Complete list of CrateDB assets and metadata properties extracted by Atlan during crawling What does Atlan crawl from Dagster Learn about the Dagster metadata that Atlan captures and visualizes What does Atlan crawl from Fivetran? Learn about what does atlan crawl from fivetran?. What does Atlan crawl from Google BigQuery? Atlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery materialized-views). What does Atlan crawl from Google Cloud Composer/OpenLineage? Atlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html). What does Atlan crawl from IBM Cognos Analytics? Atlan crawls and maps the following assets and properties from IBM Cognos Analytics. What does Atlan crawl from Informatica CDI Understand the metadata and assets discovered during crawling from Informatica Cloud Data Integration What does Atlan crawl from Matillion? Atlan crawls and maps the following assets and properties from Matillion. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase. What does Atlan crawl from Microsoft Azure Cosmos DB? Once you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [. What does Atlan crawl from Microsoft Azure Event Hubs? Atlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs. What does Atlan crawl from Microsoft Power BI? Atlan crawls and maps the following assets and properties from Microsoft Power BI. What does Atlan crawl from MicroStrategy? Atlan crawls and maps the following assets and properties from MicroStrategy. What does Atlan crawl from Mode? Atlan crawls and maps the following assets and properties from Mode. What does Atlan crawl from MongoDB? Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets. What does Atlan crawl from Monte Carlo? What does Atlan crawl from Monte Carlo? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from MySQL? Atlan crawls and maps the following assets and properties from MySQL. What does Atlan crawl from PostgreSQL? Atlan crawls and maps the following assets and properties from PostgreSQL. What does Atlan crawl from Qlik Sense Cloud? Atlan crawls and maps the following assets and properties from Qlik Sense Cloud. What does Atlan crawl from Qlik Sense Enterprise on Windows? Atlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows. What does Atlan crawl from Redash? Atlan crawls and maps the following assets and properties from Redash. What does Atlan crawl from Redpanda Kafka? Atlan crawls and maps the following assets and properties from Redpanda Kafka. What does Atlan crawl from Sisense? Atlan crawls and maps the following assets and properties from Sisense. What does Atlan crawl from Snowflake? Atlan crawls and maps the following assets and properties from Snowflake. What does Atlan crawl from Soda? Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset. What does Atlan crawl from Tableau? Atlan crawls and maps the following assets and properties from Tableau. What does Atlan crawl from ThoughtSpot? Once you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters]. What is the crawler logic for a deprecated asset? Learn about what is the crawler logic for a deprecated asset?. What lineage does Atlan extract from Matillion? Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools. What lineage does Atlan extract from Microsoft Azure Data Factory? Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01). What lineage does Atlan extract from Microsoft Azure Synapse Analytics? Learn about what lineage does atlan extract from microsoft azure synapse analytics?. What lineage does Atlan extract from Microsoft Power BI? This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation. What type of user provisioning does Atlan support for SSO integrations? Atlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:. What's the difference between connecting to Athena and Glue? Learn about what's the difference between connecting to athena and glue?. Why did my users not receive an invite email from Atlan? If you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:. Why do I get an error message when I click on Atlan's browser extension? Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension). Why does the description from Salesforce not show up in Atlan? Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce). Why is Atlan's browser extension not loading? Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension)."
  },
  {
    "url": "https://docs.atlan.com/tags/data",
    "text": "255 docs tagged with \"data\" View all tags Access archived assets Learn about access archived assets. Add contract impact analysis in GitHub Add contract impact analysis in GitHub <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Add custom metadata <div style={{position: \"relative\", paddingBottom: \"calc(66.33333333333333% + 41px)\", height: 0}}> <iframe src=\"https://demo.arcade.software/1dT1bPneM5fp1O71lb. Add descriptions You can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information. Add impact analysis in GitLab Learn about add impact analysis in gitlab. Add options :::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties. AI and Automation Features Guide to Atlan's AI capabilities and automation features for enhanced data governance and productivity. Atlan AI security Atlan uses [Azure OpenAI Service](https://azure.microsoft.com/en-in/products/cognitive-services/openai-service) to power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model. Attach a tag Atlan allows users to add [tags](/product/capabilities/governance/tags/concepts/what-are-tags) to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection. Automate data profiling âAvailable via the Data Quality Studio package Can Atlan read a dump of SQL statements to create lineage? Atlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/. Can I be notified if there is a change in downstream dashboards or a schema drift? You can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events. Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case. Can I query any DW/DL? You can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature. Can I turn off sample data preview for the entire organization? Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data. Configure Snowflake data metric functions Configure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Connect data sources for Azure-hosted Atlan instances This document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:. Connect on-premises databases to Kubernetes You can configure and use [Atlan's metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access) to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose. Crawl Aiven Kafka Once you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka. Crawl Amazon Athena To crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Amazon DynamoDB Once you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB. Crawl Amazon MSK To crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Amazon QuickSight Once you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),. Crawl Amazon Redshift Once you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift. Crawl Apache Kafka Learn about crawl apache kafka. Crawl AWS Glue Once you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue. Crawl BigID Configure and run the Atlan BigID workflow to crawl metadata from BigID. Crawl Confluent Kafka Learn about crawl confluent kafka. Crawl Confluent Schema Registry Once you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry. Crawl CrateDB Configure and run the CrateDB crawler to extract metadata from your database Crawl Databricks To crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl DataStax Enterprise Crawl DataStax Enterprise Crawl dbt Once you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan. Crawl Domo Once you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo. Crawl Fivetran Learn about crawl fivetran. Crawl Google BigQuery Once you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery. Crawl Hive To crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl IBM Cognos Analytics Once you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics. Crawl Looker Once you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker. Crawl Matillion Once you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion. Crawl Metabase Once you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase. Crawl Microsoft Azure Cosmos DB Once you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB. Crawl Microsoft Azure Data Factory Once you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-. Crawl Microsoft Azure Event Hubs Once you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs. Crawl Microsoft Azure Synapse Analytics Once you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics. Crawl Microsoft Power BI Once you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI. Crawl Microsoft SQL Server Once you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),. Crawl MicroStrategy Once you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy. Crawl Mode Once you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode. Crawl MongoDB Once you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB. Crawl Monte Carlo Once you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo. Crawl MySQL To crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl on-premises databases Once you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps. Crawl on-premises Databricks Once you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps. Crawl on-premises IBM Cognos Analytics Once you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps. Crawl on-premises Kafka Once you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps. Crawl on-premises Looker Once you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps. Crawl on-premises Tableau Once you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps. Crawl on-premises ThoughtSpot Once you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),. Crawl Oracle Once you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle create-user-in-oracle), you can establish a connection between Atlan and Oracle. Crawl PostgreSQL To crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl PrestoSQL Once you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL. Crawl Qlik Sense Cloud Once you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud. Crawl Qlik Sense Enterprise on Windows Once you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-. Crawl Redash Once you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash. Crawl Redpanda Kafka Once you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka. Crawl Salesforce Once you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce. Crawl SAP HANA Once you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA. Crawl Sigma Once you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma. Crawl Sisense Once you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense. Crawl Snowflake To crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Soda Once you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda. Crawl Tableau To crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Teradata Once you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata. Crawl ThoughtSpot Once you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot. Crawl Trino To crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Create announcements Adding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions. Data Connections and Integration Complete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues. Data Models Data models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. Disable data access :::warning Who can do this? You will need to be an admin user in Atlan to configure these options. Discovery FAQs Frequently asked questions about Atlan's Discovery capabilities. Does lineage only cover calculated fields for Tableau dashboards? Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f. Download impacted assets in Microsoft Excel Once you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Enable  Snowflake OAuth Atlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware. Enable  SSO for Amazon Redshift You will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift). Enable  SSO for Google BigQuery Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan. Enrich Atlan through dbt Beyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property. extract lineage and usage from Databricks Once you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal. extract on-premises Databricks lineage Once you have [set up the databricks-extractor tool](/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction), you can extract lineage from your on-premises Databricks instances by completing the following steps. Find assets by usage Data teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance. How can I identify an Insights query in my database access log? Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs. How can I use personas to update a term in a glossary? By default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section. Implement OpenLineage in Airflow operators If you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage. Integrate Amazon MWAA/OpenLineage To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Apache Airflow/OpenLineage To integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Apache Spark/OpenLineage Atlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Atlan with Microsoft Excel The Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel. Integrate Jira Data Center You will need to [configure an incoming link](https://confluence.atlassian.com/adminjiraserver/configure-an-incoming-link-1115659067.html) with an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider. Integrate ServiceNow If your Atlan admin has [enabled the governance workflows and inbox module](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) in your Atlan workspace, you can create a ServiceNow integration to allow your users to [grant or revoke data access](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) for governed assets in Atlan or any other data source. Interpret usage metrics Atlan currently supports usage and popularity metrics for the following connectors: Link your account To [export assets to and bulk enrich metadata from](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) a supported spreadsheet tool,. Link your ServiceNow account To request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that [set up the ServiceNow integration](/product/integrations/project-management/servicenow/how-tos/integrate-servicenow), but not for other users. Manage custom metadata structures :::warning Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones. Manage Databricks tags You must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan. Manage Google BigQuery tags Atlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro editions_features). Manage Snowflake tags You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake. Migrate from dbt to Atlan action The dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/. Mine Amazon Redshift Once you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics). Mine Google BigQuery Once you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage. Mine Microsoft Azure Synapse Analytics Learn about mine microsoft azure synapse analytics. Mine queries through S3 Once you have crawled assets from a supported connector, you can mine query history. Mine Snowflake Once you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage. Mine Teradata Once you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage. order workflows The [order of operations](/product/connections/how-tos/order-workflows order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers. Preflight checks for Amazon Redshift Before [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec. Preflight checks for Anomalo This check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid. Preflight checks for Databricks Before [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co. Preflight checks for DataStax Enterprise Preflight checks for DataStax Enterprise Preflight checks for Domo Atlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo. Preflight checks for Fivetran Learn about preflight checks for fivetran. Preflight checks for Google BigQuery Each request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication service-accounts). Preflight checks for Hive Before [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a. Preflight checks for Metabase Before [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co. Preflight checks for Microsoft Azure Data Factory Before [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact. Preflight checks for Microsoft Azure Synapse Analytics This check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method. Preflight checks for Microsoft SQL Server Before [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli. Preflight checks for Mode Before [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co. Preflight checks for MySQL Before [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha. Preflight checks for Oracle Before [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/. Preflight checks for PostgreSQL Before [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio. Preflight checks for PrestoSQL Before [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/. Preflight checks for Qlik Sense Cloud This check tests for access to datasets and other Qlik objects. Preflight checks for SAP S/4HANA Preflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Preflight checks for Snowflake Before [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne. Preflight checks for Soda Learn about preflight checks for soda Preflight checks for Teradata Before [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con. Preflight checks for Trino Before [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha. provide SSL certificates SSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau). Schedule a query You must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients. Search and discover assets Atlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context. Security The Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring. Security and Compliance Complete guide to Atlan's security features, compliance certifications, and data protection capabilities. Set up a private network link to Amazon Athena :::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps. Set up Aiven Kafka Atlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Set up Amazon DynamoDB Learn about set up amazon dynamodb. Set up Amazon MSK Learn about set up amazon msk. Set up Amazon QuickSight Learn about set up amazon quicksight. Set up Amazon Redshift :::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself. Set up Amazon S3 Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects. Set up an Azure private network link to Databricks For all details, see [Databricks documentation](https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/private-link-simplified?source=recommendations create-the-workspace-and-private-endpoints-in-the-azure-portal-ui). Set up Anomalo Atlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata. Set up AWS Glue Learn about set up aws glue. Set up BigID Create a BigID system user and API token for Atlan integration. Set up Confluent Kafka Atlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata. Set up Confluent Schema Registry :::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself. Set up Databricks Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods:. Set up DataStax Enterprise Set up DataStax Enterprise Set up dbt Cloud :::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself. Set up Domo :::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself. Set up Fivetran Learn about set up fivetran. Set up Google BigQuery You must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access). Set up Google Cloud Storage Configure Google Cloud Storage for secure metadata ingestion with Atlan. Set up Hive :::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself. Set up IBM Cognos Analytics :::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself. Set up Inventory reports Create Inventory report for Amazon S3 in case of inventory based ingestion through the crawler. Set up Microsoft Azure Cosmos DB If your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db). Set up Microsoft Azure Data Factory Atlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata. Set up Microsoft Azure Event Hubs Atlan supports the following authentication methods for Microsoft Azure Event Hubs:. Set up Microsoft Azure Synapse Analytics Atlan supports crawling the following with the Microsoft Azure Synapse Analytics package:. Set up Microsoft Power BI This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Set up Microsoft SQL Server :::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself. Set up MicroStrategy Atlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata. Set up MongoDB Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password]( create-database-user-in-mongodb) to fetch metadata. Set up Monte Carlo :::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups). Set up MySQL :::warning Who can do this? You will probably need your MySQL administrator to run these commands - you may not have access yourself. Set up on-premises database access In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Set up on-premises Databricks access In some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises IBM Cognos Analytics access :::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,. Set up on-premises Kafka access In some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Looker access In some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Microsoft Azure Synapse Analytics miner access In some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to [mine query history from the Query Store](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics). For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Tableau access In some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Teradata miner access In some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises ThoughtSpot access In some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up Oracle :::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself. Set up PostgreSQL :::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself. Set up PrestoSQL Learn about set up prestosql. Set up Redash :::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself. Set up Redpanda Kafka Atlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Set up SAP HANA :::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself. Set up Sisense Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Set up Snowflake :::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::. Set up Soda :::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -. Set up Tableau :::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself. Set up Teradata :::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself. SSO integration with PingFederate using SAML To use both IdP- and SP-initiated SSO, add both the URLs mentioned above. Star assets :::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can star assets. Supported connections for on-premises databases The metadata-extractor tool supports the following connection types. Tags and Metadata Management Complete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization. Troubleshooting data models What are the known limitations of data models in Atlan? Troubleshooting Jira What fields are supported when creating tickets or requesting access? Troubleshooting lineage So you've crawled your source, and mined the queries, but lineage is missing. Why? update column metadata in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets. Update column metadata in Microsoft Excel Once you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel. Use the filters menu You can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you. view data models Once you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:. View query logs You can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization. What are Power BI processes on the lineage graph? Note that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets. What does Atlan crawl from Amazon Athena? Atlan crawls and maps the following assets and properties from Amazon Athena. What does Atlan crawl from Amazon DynamoDB? Atlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran. What does Atlan crawl from Amazon QuickSight? Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:. What does Atlan crawl from Amazon Redshift? Atlan crawls and maps the following assets and properties from Amazon Redshift. What does Atlan crawl from Anomalo? Once you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check. What does Atlan crawl from Apache Spark/OpenLineage? Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports. What does Atlan crawl from AWS Glue? Atlan crawls and maps the following assets and properties from AWS Glue. What does Atlan crawl from BigID? Reference guide for BigID metadata crawled by Atlan. What does Atlan crawl from Databricks? Atlan crawls and maps the following assets and properties from Databricks. What does Atlan crawl from DataStax Enterprise? What does Atlan crawl from DataStax Enterprise? What does Atlan crawl from Domo? Atlan supports lineage for the following asset types:. What does Atlan crawl from Fivetran? Learn about what does atlan crawl from fivetran?. What does Atlan crawl from Google BigQuery? Atlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery materialized-views). What does Atlan crawl from Hive? Atlan crawls and maps the following assets and properties from Hive. What does Atlan crawl from Microsoft Azure Cosmos DB? Once you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [. What does Atlan crawl from Microsoft Azure Data Factory? Atlan crawls and maps the following assets and properties from Microsoft Azure Data Factory. What does Atlan crawl from Microsoft Azure Synapse Analytics? Atlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources. What does Atlan crawl from Microsoft SQL Server? Atlan crawls and maps the following assets and properties from Microsoft SQL Server. What does Atlan crawl from MongoDB? Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets. What does Atlan crawl from MySQL? Atlan crawls and maps the following assets and properties from MySQL. What does Atlan crawl from Oracle? Atlan crawls and maps the following assets and properties from Oracle. What does Atlan crawl from PostgreSQL? Atlan crawls and maps the following assets and properties from PostgreSQL. What does Atlan crawl from PrestoSQL? Atlan crawls and maps the following assets and properties from PrestoSQL. What does Atlan crawl from Qlik Sense Cloud? Atlan crawls and maps the following assets and properties from Qlik Sense Cloud. What does Atlan crawl from SAP ECC? What does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from SAP S/4HANA? What does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from Sisense? Atlan crawls and maps the following assets and properties from Sisense. What does Atlan crawl from Snowflake? Atlan crawls and maps the following assets and properties from Snowflake. What does Atlan crawl from Soda? Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset. What does Atlan crawl from Tableau? Atlan crawls and maps the following assets and properties from Tableau. What does Atlan crawl from Teradata? Atlan crawls and maps the following assets and properties from Teradata. What does Atlan crawl from Trino? Atlan crawls and maps the following assets and properties from Trino. What is included in the Jira integration? With two of your most important workspaces connected, you can save time and improve the way you track issues for your data. What is included in the Microsoft Teams integration? With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team. What is included in the Slack integration? Learn about the features and capabilities of the Slack integration with Atlan. What is the default permission for a glossary? By default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access. What is the difference between a Power BI data source and dataflow? Learn about what is the difference between a power bi data source and dataflow?. What lineage does Atlan extract from Matillion? Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools. What lineage does Atlan extract from Microsoft Azure Data Factory? Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01). What lineage does Atlan extract from Microsoft Azure Synapse Analytics? Learn about what lineage does atlan extract from microsoft azure synapse analytics?. What lineage does Atlan extract from Microsoft Power BI? This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation. When does Atlan become a personal data processor or subprocessor? Atlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively. Why do I only see tables from the same schema to join from in a visual query? When [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder. Why does the description from Salesforce not show up in Atlan? Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce). Why is lineage available for table level but not column level? The home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset. Workflows and Data Processing Everything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/crawl",
    "text": "227 docs tagged with \"crawl\" View all tags Add descriptions You can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information. Add options :::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties. Automate data profiling âAvailable via the Data Quality Studio package Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case. Can I turn off sample data preview for the entire organization? Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data. Crawl Aiven Kafka Once you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka. Crawl Amazon Athena To crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Amazon DynamoDB Once you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB. Crawl Amazon MSK To crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Amazon QuickSight Once you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),. Crawl Amazon Redshift Once you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift. Crawl Apache Kafka Learn about crawl apache kafka. Crawl AWS Glue Once you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue. Crawl BigID Configure and run the Atlan BigID workflow to crawl metadata from BigID. Crawl Confluent Kafka Learn about crawl confluent kafka. Crawl Confluent Schema Registry Once you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry. Crawl CrateDB Configure and run the CrateDB crawler to extract metadata from your database Crawl Databricks To crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl DataStax Enterprise Crawl DataStax Enterprise Crawl dbt Once you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan. Crawl Domo Once you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo. Crawl Fivetran Learn about crawl fivetran. Crawl GCS assets Configure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan. Crawl Google BigQuery Once you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery. Crawl Hive To crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl IBM Cognos Analytics Once you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics. Crawl Informatica CDI assets Configure and run the crawler to discover and catalog your Informatica CDI assets Crawl Looker Once you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker. Crawl Matillion Once you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion. Crawl Metabase Once you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase. Crawl Microsoft Azure Cosmos DB Once you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB. Crawl Microsoft Azure Data Factory Once you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-. Crawl Microsoft Azure Event Hubs Once you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs. Crawl Microsoft Azure Synapse Analytics Once you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics. Crawl Microsoft Power BI Once you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI. Crawl Microsoft SQL Server Once you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),. Crawl MicroStrategy Once you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy. Crawl Mode Once you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode. Crawl MongoDB Once you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB. Crawl Monte Carlo Once you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo. Crawl MySQL To crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl on-premises databases Once you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps. Crawl on-premises Databricks Once you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps. Crawl on-premises IBM Cognos Analytics Once you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps. Crawl on-premises Kafka Once you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps. Crawl on-premises Looker Once you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps. Crawl on-premises Tableau Once you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps. Crawl on-premises ThoughtSpot Once you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),. Crawl Oracle Once you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle create-user-in-oracle), you can establish a connection between Atlan and Oracle. Crawl PostgreSQL To crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl PrestoSQL Once you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL. Crawl Qlik Sense Cloud Once you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud. Crawl Qlik Sense Enterprise on Windows Once you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-. Crawl Redash Once you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash. Crawl Redpanda Kafka Once you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka. Crawl S3 assets Configure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan. Crawl Salesforce Once you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce. Crawl SAP ECC To crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl SAP HANA Once you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA. Crawl SAP S/4HANA To crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Sigma Once you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma. Crawl Sisense Once you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense. Crawl Snowflake To crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Soda Once you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda. Crawl Tableau To crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Teradata Once you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata. Crawl ThoughtSpot Once you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot. Crawl Trino To crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Disable data access :::warning Who can do this? You will need to be an admin user in Atlan to configure these options. Does lineage only cover calculated fields for Tableau dashboards? Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f. Enrich Atlan through dbt Beyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property. extract lineage and usage from Databricks Once you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal. Manage Databricks tags You must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan. Manage dbt tags Atlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags. Manage Google BigQuery tags Atlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro editions_features). Manage Snowflake tags You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake. Mine Amazon Redshift Once you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics). Mine Google BigQuery Once you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage. Mine Microsoft Azure Synapse Analytics Learn about mine microsoft azure synapse analytics. Mine Microsoft Power BI Once you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics. Mine queries through S3 Once you have crawled assets from a supported connector, you can mine query history. Mine Snowflake Once you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage. Mine Teradata Once you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage. order workflows The [order of operations](/product/connections/how-tos/order-workflows order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers. Preflight checks for Aiven Kafka Before [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne. Preflight checks for Amazon MSK Before [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti. Preflight checks for Amazon QuickSight The [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission. Preflight checks for Amazon Redshift Before [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec. Preflight checks for Apache Kafka Before [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection. Preflight checks for Confluent Schema Registry Before [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca. Preflight checks for Databricks Before [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co. Preflight checks for DataStax Enterprise Preflight checks for DataStax Enterprise Preflight checks for dbt This checks if manifest files are present in the provided bucket and prefix. Preflight checks for Domo Atlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo. Preflight checks for Fivetran Learn about preflight checks for fivetran. Preflight checks for Google BigQuery Each request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication service-accounts). Preflight checks for Hive Before [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a. Preflight checks for Looker First, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management). Preflight checks for Metabase Before [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co. Preflight checks for Microsoft Azure Data Factory Before [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact. Preflight checks for Microsoft Azure Synapse Analytics This check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method. Preflight checks for Microsoft Power BI Before [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run. Preflight checks for Microsoft SQL Server Before [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli. Preflight checks for MicroStrategy First, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html /Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions. Preflight checks for Mode Before [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co. Preflight checks for Monte Carlo Before [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c. Preflight checks for MySQL Before [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha. Preflight checks for Oracle Before [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/. Preflight checks for PostgreSQL Before [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio. Preflight checks for PrestoSQL Before [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/. Preflight checks for Qlik Sense Cloud This check tests for access to datasets and other Qlik objects. Preflight checks for Redash Before [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti. Preflight checks for Redpanda Kafka Before [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod. Preflight checks for Salesforce Before [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co. Preflight checks for SAP S/4HANA Preflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Preflight checks for Sigma First, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission. Preflight checks for Sisense Atlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6 /folders) to check if it's responding with a response status code 200. Preflight checks for Snowflake Before [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne. Preflight checks for Soda Learn about preflight checks for soda Preflight checks for Tableau The [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm server_info) REST API is used to fetch the `restApiVersion` value. Preflight checks for Teradata Before [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con. Preflight checks for Trino Before [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha. provide SSL certificates SSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau). Set up a private network link to Amazon Athena :::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps. Set up Amazon Redshift :::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself. Set up Amazon S3 Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects. Set up AWS Glue Learn about set up aws glue. Set up BigID Create a BigID system user and API token for Atlan integration. Set up Confluent Schema Registry :::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself. Set up DataStax Enterprise Set up DataStax Enterprise Set up dbt Cloud :::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself. Set up Domo :::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself. Set up Fivetran Learn about set up fivetran. Set up Google BigQuery You must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access). Set up Google Cloud Storage Configure Google Cloud Storage for secure metadata ingestion with Atlan. Set up Hive :::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself. Set up IBM Cognos Analytics :::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself. Set up Inventory reports Create Inventory report for Amazon S3 in case of inventory based ingestion through the crawler. Set up Looker :::warning Who can do this? You will probably need your Looker administrator to run these commands - you may not have access yourself. Set up Microsoft Azure Cosmos DB If your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db). Set up Microsoft Azure Synapse Analytics Atlan supports crawling the following with the Microsoft Azure Synapse Analytics package:. Set up Microsoft SQL Server :::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself. Set up MicroStrategy Atlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata. Set up Mode If you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email. Set up MongoDB Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password]( create-database-user-in-mongodb) to fetch metadata. Set up Monte Carlo :::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups). Set up on-premises database access In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Set up on-premises Databricks access In some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises IBM Cognos Analytics access :::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,. Set up on-premises Kafka access In some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Looker access In some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises Tableau access In some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up on-premises ThoughtSpot access In some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up Oracle :::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself. Set up PostgreSQL :::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself. Set up SAP HANA :::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself. Set up Sisense Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Set up Snowflake :::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::. Set up Tableau :::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself. Set up Teradata :::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself. Set up ThoughtSpot :::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself. Set up Trino :::warning Who can do this? You will probably need your Trino administrator to run these commands - you may not have access yourself. Troubleshooting data models What are the known limitations of data models in Atlan? Troubleshooting lineage So you've crawled your source, and mined the queries, but lineage is missing. Why? update column metadata in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets. Update column metadata in Microsoft Excel Once you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel. view data models Once you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:. What does Atlan crawl from Aiven Kafka? Atlan crawls and maps the following assets and properties from Aiven Kafka. What does Atlan crawl from Amazon Athena? Atlan crawls and maps the following assets and properties from Amazon Athena. What does Atlan crawl from Amazon DynamoDB? Atlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran. What does Atlan crawl from Amazon MSK? Atlan crawls and maps the following assets and properties from Amazon MSK. What does Atlan crawl from Amazon MWAA/OpenLineage? Once you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [. What does Atlan crawl from Amazon QuickSight? Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:. What does Atlan crawl from Amazon Redshift? Atlan crawls and maps the following assets and properties from Amazon Redshift. What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging. What does Atlan crawl from Anomalo? Once you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check. What does Atlan crawl from Apache Airflow/OpenLineage? Once you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),. What does Atlan crawl from Apache Kafka? Atlan crawls and maps the following assets and properties from Apache Kafka. What does Atlan crawl from Apache Spark/OpenLineage? Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports. What does Atlan crawl from Astronomer/OpenLineage? Atlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html). What does Atlan crawl from AWS Glue? Atlan crawls and maps the following assets and properties from AWS Glue. What does Atlan crawl from BigID? Reference guide for BigID metadata crawled by Atlan. What does Atlan crawl from Confluent Kafka? Atlan crawls and maps the following assets and properties from Confluent Kafka. What does Atlan crawl from CrateDB? Complete list of CrateDB assets and metadata properties extracted by Atlan during crawling What does Atlan crawl from Databricks? Atlan crawls and maps the following assets and properties from Databricks. What does Atlan crawl from DataStax Enterprise? What does Atlan crawl from DataStax Enterprise? What does Atlan crawl from Domo? Atlan supports lineage for the following asset types:. What does Atlan crawl from Fivetran? Learn about what does atlan crawl from fivetran?. What does Atlan crawl from Google BigQuery? Atlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery materialized-views). What does Atlan crawl from Google Cloud Composer/OpenLineage? Atlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html). What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging. What does Atlan crawl from Hive? Atlan crawls and maps the following assets and properties from Hive. What does Atlan crawl from IBM Cognos Analytics? Atlan crawls and maps the following assets and properties from IBM Cognos Analytics. What does Atlan crawl from Looker? Atlan crawls and maps the following assets and properties from Looker. What does Atlan crawl from Matillion? Atlan crawls and maps the following assets and properties from Matillion. What does Atlan crawl from Microsoft Azure Cosmos DB? Once you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [. What does Atlan crawl from Microsoft Azure Data Factory? Atlan crawls and maps the following assets and properties from Microsoft Azure Data Factory. What does Atlan crawl from Microsoft Azure Event Hubs? Atlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs. What does Atlan crawl from Microsoft Azure Synapse Analytics? Atlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources. What does Atlan crawl from Microsoft Power BI? Atlan crawls and maps the following assets and properties from Microsoft Power BI. What does Atlan crawl from Microsoft SQL Server? Atlan crawls and maps the following assets and properties from Microsoft SQL Server. What does Atlan crawl from MicroStrategy? Atlan crawls and maps the following assets and properties from MicroStrategy. What does Atlan crawl from MongoDB? Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets. What does Atlan crawl from Monte Carlo? What does Atlan crawl from Monte Carlo? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from MySQL? Atlan crawls and maps the following assets and properties from MySQL. What does Atlan crawl from Oracle? Atlan crawls and maps the following assets and properties from Oracle. What does Atlan crawl from PostgreSQL? Atlan crawls and maps the following assets and properties from PostgreSQL. What does Atlan crawl from PrestoSQL? Atlan crawls and maps the following assets and properties from PrestoSQL. What does Atlan crawl from Qlik Sense Cloud? Atlan crawls and maps the following assets and properties from Qlik Sense Cloud. What does Atlan crawl from Qlik Sense Enterprise on Windows? Atlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows. What does Atlan crawl from Redash? Atlan crawls and maps the following assets and properties from Redash. What does Atlan crawl from Redpanda Kafka? Atlan crawls and maps the following assets and properties from Redpanda Kafka. What does Atlan crawl from Salesforce? Atlan only performs GET requests on these five endpoints:. What does Atlan crawl from SAP ECC? What does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from SAP S/4HANA? What does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from Sisense? Atlan crawls and maps the following assets and properties from Sisense. What does Atlan crawl from Snowflake? Atlan crawls and maps the following assets and properties from Snowflake. What does Atlan crawl from Soda? Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset. What does Atlan crawl from Tableau? Atlan crawls and maps the following assets and properties from Tableau. What does Atlan crawl from Teradata? Atlan crawls and maps the following assets and properties from Teradata. What does Atlan crawl from ThoughtSpot? Once you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters]. What does Atlan crawl from Trino? Atlan crawls and maps the following assets and properties from Trino. What lineage does Atlan extract from Matillion? Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools. What lineage does Atlan extract from Microsoft Azure Data Factory? Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01). What lineage does Atlan extract from Microsoft Power BI? This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation. When does Atlan become a personal data processor or subprocessor? Atlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively. Why does the description from Salesforce not show up in Atlan? Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce)."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks",
    "text": "Connect data Data Warehouses Databricks On this page Databricks Overview: Catalog Databricks workspaces, databases, schemas, and tables in Atlan. Gain visibility into lineage, usage, and governance for your Databricks assets. Get started â Follow these steps to connect and catalog Databricks assets in Atlan: Set up the connector Crawl Databricks assets Guides â Cross-workspace setup â Set up cross-workspace extraction : Configure a single service principal to crawl metadata from all workspaces within a Databricks metastore. Lineage & usage â Extract lineage and usage from Databricks : Extract lineage and usage metrics from your Databricks assets. Tag management â Manage Databricks tags : Configure and manage tags in Databricks. On-premises â Set up on-premises Databricks access : Configure Atlan to access on-premises Databricks environments. Set up on-premises Databricks lineage extraction : Prepare for offline lineage extraction from on-premises Databricks. Extract on-premises Databricks lineage : Step-by-step instructions for extracting lineage from on-premises Databricks. Crawl on-premises Databricks : Crawl metadata from on-premises Databricks environments. Private networking â Set up an AWS private network link to Databricks : Establish a secure, private network connection to Databricks on AWS. Set up an Azure private network link to Databricks : Establish a secure, private network connection to Databricks on Azure. References â What does Atlan crawl from Databricks : Learn about the Databricks assets and metadata that Atlan discovers and catalogs. Preflight checks for Databricks : Verify prerequisites before setting up the Databricks connector. Troubleshooting â Troubleshooting connectivity : Resolve common Databricks connection issues and errors. Tags: databricks connector data warehouse connectivity Next Set up Databricks Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-cross-workspace-extraction",
    "text": "Connect data Data Warehouses Databricks Cross-workspace Setup Set up cross-workspace extraction On this page Set up cross-workspace extraction Eliminate the need for separate crawler configurations by using a single service principal to crawl metadata from all workspaces within a Databricks metastore. This guide walks you through configuring the necessary permissions to enable cross-workspace extraction. Important! Cross-workspace extraction isn't supported for REST API or JDBC extraction methods. Prerequisites â Before you begin, make sure you have: A Unity Catalog-enabled Databricks workspace Account admin access to create and manage service principals Workspace admin access to grant permissions across all target workspaces At least one active SQL warehouse in each workspace you intend to crawl Set up Databricks authentication completed with one of the supported authentication methods System table extraction enabled for lineage and usage extraction Permissions required â The service principal needs the following permissions to enable cross-workspace extraction: CAN_USE on SQL warehouses in each workspace SELECT on system.access.workspace_latest table USE CATALOG , BROWSE , and SELECT on all catalogs you want to crawl Add service principal to all workspaces â You must use a single, common service principal that has been granted access to all Databricks workspaces you intend to crawl within the metastore. Log in to your Databricks account console as an account admin From the left menu, click Workspaces and select a workspace From the tabs along the top, click the Permissions tab In the upper right, click Add permissions In the Add permissions dialog: For User, group, or service principal , select your service principal For Permission , select workspace User Click Add Repeat steps 2-5 for each workspace you intend to crawl Grant permissions â Configure the necessary permissions for the service principal to access and extract metadata from all workspaces within the metastore. SQL workspace permissions: The service principal must have usage permissions on at least one active SQL warehouse within each workspace . The extractor uses the smallest available warehouse to run its discovery queries. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command for each workspace, replacing the placeholders: GRANT CAN_USE ON WAREHOUSE < warehouse_name > TO ` <service_principal_id> ` ; Replace <warehouse_name> with your actual warehouse name Replace <service_principal_id> with your service principal's application ID Example GRANT CAN_USE ON WAREHOUSE production - warehouse TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click SQL Warehouses On the Compute page, for each SQL warehouse, click the 3-dot icon and then click Permissions In the Manage permissions dialog: In the Type to add multiple users or groups field, search for and select your service principal Select Can use permission Click Add to assign the permission System table permissions: Access to the system schema is essential for workspace and lineage discovery. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command, replacing the placeholder: GRANT SELECT ON TABLE system . access . workspace_latest TO ` <service_principal_id> ` ; Replace <service_principal_id> with your service principal's application ID Example GRANT SELECT ON TABLE system . access . workspace_latest TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to system > access Click on the workspace_latest table Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check SELECT Click Grant to apply the permissions Asset permissions: The service principal requires permissions to \"see\" and \"read\" the metadata for all data assets you wish to extract. These grants must be applied to all private, public, and shared catalogs that are in scope for crawling. Important! For private catalogs, grant permissions from each workspace. For public catalogs, grant from any workspace. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Grant catalog-level permissions (required even when using BROWSE - BROWSE automatically grants access to all schemas and tables): GRANT USE CATALOG ON CATALOG < catalog_name > TO ` <service_principal_id> ` ; GRANT BROWSE ON CATALOG < catalog_name > TO ` <service_principal_id> ` ; Replace <catalog_name> with your actual catalog name Replace <service_principal_id> with your service principal's application ID If not using BROWSE, along with catalog permissions, grant additional permissions: Grant schema-level permissions: GRANT USE SCHEMA ON SCHEMA < catalog_name > . < schema_name > TO ` <service_principal_id> ` ; Replace <catalog_name> and <schema_name> with your actual values Replace <service_principal_id> with your service principal's application ID Example GRANT USE CATALOG ON CATALOG main TO ` 12345678-1234-1234-1234-123456789012 ` ; GRANT BROWSE ON CATALOG main TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to the catalog you want to grant permissions on (for example, main ) Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check the following permissions: USE CATALOG USE SCHEMA BROWSE SELECT Click Grant to apply the permissions Repeat steps 3-5 for each catalog you want to crawl in Atlan Need help? â Check Troubleshooting Databricks connectivity for common issues Contact Atlan support for help with setup or integration Next steps â Crawl Databricks Tags: databricks setup cross-workspace-extraction Previous Set up Databricks Next Crawl Databricks Prerequisites Permissions required Add service principal to all workspaces Grant permissions Need help? Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks",
    "text": "Connect data Data Warehouses Databricks Crawl Databricks Assets Crawl Databricks On this page Crawl Databricks Once you have configured the Databricks access permissions , you can establish a connection between Atlan and your Databricks instance. (If you are also using AWS PrivateLink or Azure Private Link for Databricks, you will need to set that up first, too.) To crawl metadata from your Databricks instance, review the order of operations and then complete the following steps. Select the source â To select Databricks as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Databricks Assets , and click Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. Next, select an authentication method: In JDBC , you will need a personal access token and HTTP path for authentication . In AWS Service , you will need a client ID and client secret for AWS service principal authentication . In Azure Service , you will need a tenant ID, client ID, and client secret for Azure service principal authentication . In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method â JDBC â To enter your Databricks credentials: For Host , enter the hostname, AWS PrivateLink endpoint , or Azure Private Link endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Personal Access Token , enter the access token you generated when setting up access . For HTTP Path , enter one of the following: A path starting with /sql/1.0/warehouses to use the Databricks SQL warehouse . A path starting with sql/protocolv1/o to use the Databricks interactive cluster . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . danger Make sure your Databricks instance (SQL warehouse or interactive cluster) is up and running, otherwise the Test Authentication step times out. AWS service principal â To enter your Databricks credentials: For Host , enter the hostname or AWS PrivateLink endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Client ID , enter the client ID for your AWS service principal . For Client Secret , enter the client secret for your AWS service principal . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . Azure service principal â To enter your Databricks credentials: For Host , enter the hostname or Azure Private Link endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Client ID , enter the application (client) ID for your Azure service principal . For Client Secret , enter the client secret for your Azure service principal . For Tenant ID , enter the directory (tenant) ID for your Azure service principal . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . Offline extraction method â Atlan supports the offline extraction method for fetching metadata from Databricks. This method uses Atlan's databricks-extractor tool to fetch metadata. You need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output/databricks-example/catalogs/success/result-0.json , output/databricks-example/schemas/{{catalog_name}}/success/result-0.json , output/databricks-example/tables/{{catalog_name}}/success/result-0.json , and similar files. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Databricks. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Databricks data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the Databricks connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. (Optional) To prevent users from querying any Databricks data, change Enable SQL Query to No . (Optional) To prevent users from previewing any Databricks data, change Enable Data Preview to No . (Optional) To prevent users from running large queries, change Max Row Limit or keep the default selection. At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the Databricks crawler, you can further configure it. System tables extraction method â The system metadata extraction method is only available for Unity Catalog-enabled workspaces . It provides access to detailed metadata from system tables and supports all three authentication types. You can extract metadata from your Databricks workspace using this method. Follow these steps: Set up authentication using one of the following: Personal access token AWS service principal Azure service principal The default options can work as is. You may choose to override the defaults for any of the remaining options: For Asset selection , select a filtering option: For SQL warehouse , click the dropdown to select the SQL warehouse you want to configure. To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases includes all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views excludes all the matching tables and views. Click + to add more filters. If you add multiple filters, assets are crawled based on matching all the filtering conditions you have set. To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Incremental extraction Public preview â Toggle incremental extraction, for a faster and more efficient metadata extraction. JDBC extraction method â The JDBC extraction method uses JDBC queries to extract metadata from your Databricks instance. This was the original extraction method provided by Databricks. This extraction method is only supported for personal access token authentication . You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For View Definition Lineage , keep the default Yes to generate upstream lineage for views based on the tables referenced in the views or click No to exclude from crawling. For Advanced Config , keep Default for the default configuration or click Advanced to further configure the crawler: To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select True to enable it or False to disable it. REST API extraction method â The REST API extraction method uses Unity Catalog to extract metadata from your Databricks instance. This extraction method is supported for all three authentication options: personal access token , AWS service principal , and Azure service principal . This method is only supported by Unity Catalog-enabled workspaces. If you enable an existing workspace, you also need to upgrade your tables and views to Unity Catalog . While REST APIs are used to extract metadata, JDBC queries are still used for querying purposes. You can override the defaults for any of these options: Change the extraction method under Extraction method to REST API . To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. For SQL warehouse , click the dropdown to select the SQL warehouse you have configured. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â Follow these steps to run the Databricks crawler: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up cross-workspace extraction Next Set up on-premises Databricks access Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-access",
    "text": "Connect data Data Warehouses Databricks On-premises Setup Set up on-premises Databricks access On this page Set up on-premises Databricks access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Databricks instance details, including credentials. In some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your on-premises Databricks instance, you will need to use Atlan's databricks-extractor tool. Did you know? Atlan uses exactly the same databricks-extractor behind the scenes when it connects to Databricks in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the databricks-extractor tool â To get the databricks-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Databricks: sudo docker load -i /path/to/databricks-extractor-master.tar Get the compose file â Atlan provides you with a Docker compose file for the databricks-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Databricks instance. The file is docker-compose.yaml . Define Databricks connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Databricks connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises Databricks instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract environment: <<: *databricks-defaults INCLUDE_FILTER: '{\"DB_1\": [], \"DB_2\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' EXCLUDE_FILTER: '{\"DB_1\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' TEMP_TABLE_REGEX: '.*temp.*|.*tmp.*|.*TEMP.*|.*TMP.*' SYSTEM_SCHEMA_REGEX: '^information_schema$' volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *extract tells the databricks-extractor tool to run. environment contains all parameters for the tool. INCLUDE_FILTER -  specify the databases and schemas from which you want to extract metadata. Remove this line if you want to extract metadata from all databases and schemas. EXCLUDE_FILTER -  specify the databases and schemas you want to exclude from metadata extraction. This will take precedence over INCLUDE_FILTER . Remove this line if you do not want to exclude any databases or schemas. TEMP_TABLE_REGEX -  specify a regular expression for excluding temporary tables. Remove this line if you do not want to exclude any temporary tables. SYSTEM_SCHEMA_REGEX -  specify a regular expression for excluding system schemas. If unspecified, INFORMATION_SCHEMA will be excluded from the extracted metadata by default. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Databricks connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your Databricks connections, you will need to provide a Databricks configuration file. The Databricks configuration is a .ini file with the following format: [DatabricksConfig] host = <host> port = <port>   seconds to wait for a response from the server timeout = 300   Databricks authentication type. Options: personal_access_token, aws_service_principal auth_type = personal_access_token   Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] personal_access_token = <personal_access_token>   Required only if auth_type is aws_service_principal. [AWSServicePrincipalAuth] client_id = <client_id> client_secret = <client_secret> Secure credentials â Using local files â danger If you decide to keep Databricks credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: databricks_config: file: ./databricks.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets â To create and use Docker secrets: Store the Databricks configuration file: sudo docker secret create databricks_config path/to/databricks.ini At the top of your compose file, add a secrets element to access your secret: secrets: databricks_config: external: true name: databricks_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Databricks configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets: databricks_config: external: true name: databricks_config x-templates:   ... services: databricks-example: <<: *extract environment: <<: *databricks-defaults INCLUDE_FILTER: '{\"DB_1\": [], \"DB_2\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' EXCLUDE_FILTER: '{\"DB_1\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' TEMP_TABLE_REGEX: '.*temp.*|.*tmp.*|.*TEMP.*|.*TMP.*' SYSTEM_SCHEMA_REGEX: '^information_schema$' volumes: - ./output/databricks-example:/output secrets: - databricks_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The databricks_config refers to an external Docker secret created using the docker secret create command. The name of this service is databricks-example . You can use any meaningful name you want. The <<: *databricks-defaults sets the connection type to Databricks. The ./output/databricks-example:/output Â line tells the extractor where to store results. In this example, the extractor will store results in theÂ ./output/databricks-example directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Crawl Databricks Next Crawl on-premises Databricks Prerequisites Get the compose file Define Databricks connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-an-aws-private-network-link-to-databricks",
    "text": "Connect data Data Warehouses Databricks Private Network Setup Set up an AWS private network link to Databricks On this page Set up an AWS private network link to Databricks AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Databricks and Atlan. Who can do this? You will need Databricks support, and probably your Databricks administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites â Databricks must be set up on the E2 version of the platform and Enterprise pricing tier. Your Databricks workspace must be in an AWS region that supports the E2 version of the platform, and not the us-west-1 region. Your Databricks workspace must also be hosted in the same region as Atlan. Your Databricks workspace must use customer-managed VPC. (Note that you cannot update an existing Databricks-managed VPC to a customer-managed VPC.) For all details, see Databricks documentation . Notify Atlan support â Once setup is completed, provide Atlan support with the following information: The AWS region of your Databricks instance. There are additional steps that Atlan will need to complete: Creating a security group Creating an endpoint Once the Atlan team has confirmed that the configuration is ready, please continue with the remaining steps. Accept the endpoint connection request â You can either: Accept the endpoint connection request from Atlan via API . Accept the endpoint connection request from Atlan from the Databricks console . Once the endpoint connection is accepted, Atlan support will finish the configuration on the Atlan side. When you use this endpoint in the configuration for crawling Databricks , Atlan will connect to Databricks over AWS PrivateLink. Tags: api rest-api graphql Previous Set up on-premises Databricks lineage extraction Next Set up an Azure private network link to Databricks Prerequisites Notify Atlan support Accept the endpoint connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/extract-lineage-and-usage-from-databricks",
    "text": "Connect data Data Warehouses Databricks Lineage and Usage How to extract lineage and usage from Databricks On this page extract lineage and usage from Databricks Once you have crawled assets from Databricks , you can retrieve lineage from Unity Catalog and usage and popularity metrics from query history or system tables. This is supported for all three authentication methods : personal access token, AWS service principal, and Azure service principal. Both Atlan and Databricks strongly recommend using the system tables method to extract lineage and usage and popularity metrics from Databricks. danger Usage and popularity metrics can be retrieved for all Databricks users. However, your Databricks workspace must be Unity Catalog-enabled for the retrieval of lineage and usage and popularity metrics to succeed. You may also need to upgrade existing tables and views to Unity Catalog , as well as reach out to your Databricks account executive to enable lineage in Unity Catalog. (As of publishing, the feature is still in preview from Databricks on AWS and Azure.) To retrieve lineage and usage from Databricks, rev iew the order of operations and then complete the following steps. Select the extractor â To select the Databricks lineage and usage extractor: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the filters along the top, click Miner . From the list of packages, select Databricks Miner and click on Setup Workflow . Configure the lineage extractor â Choose your lineage extraction method: In REST API , Atlan connects to your database and extracts lineage directly. In Offline , you will need to first extract lineage yourself and make it available in S3 . In System Table , Atlan connects to your database and queries system tables to extract lineage directly. REST API â To configure the Databricks lineage extractor: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) Click Next to proceed. Offline extraction method â Atlan supports the offline extraction method for extracting lineage from Databricks This method uses Atlan's databricks-extractor tool to extract lineage. You will need to first extract lineage yourself and make it available in S3 . To enter your S3 details: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include extracted-lineage/result-0.json , extracted-query-history/result-0.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . System table â To configure the Databricks lineage extractor: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) * Extraction Catalog Type : Default : Select to fetch lineage from the system catalog and access schema. Cloned_catalog : Select to fetch lineage from a cloned catalog and schema. Before proceeding, make sure the following prerequisites are met: You have already created cloned views named column_lineage and table_lineage in your schema. If not, follow the steps in Create cloned views of system tables . The atlan-user must have SELECT permissions on both views to access lineage data. Then, provide values for the following fields: Cloned Catalog Name â Catalog containing the cloned views. Cloned Schema Name â Schema containing the cloned views. For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Click Next to proceed. (Optional) Configure the usage extractor â Atlan extracts usage and popularity metrics from: Query history System tables This feature is currently limited to queries on SQL warehouses   -  queries on interactive clusters are not supported. Additionally, expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . To configure the Databricks usage and popularity extractor: For Fetch Query History and Calculate Popularity , click Yes to retrieve usage and popularity metrics for your Databricks assets. For Popularity Extraction Method : Choose one of the following methods to extract usage and popularity metrics:: Click REST API to extract usage and popularity metrics from query history. Click System table to extract metrics directly from system tables: Extraction catalog type for popularity : Choose where to fetch popularity data from: Default : Uses the system catalog and query schema to fetch popularity metrics. Cloned_catalog : Select to fetch popularity from cloned views in a separate catalog and schema. Before proceeding: The query_history view must exist in the provided schema. The atlan-user must have SELECT permission on the view. Then provide: Cloned Catalog Name â The catalog that contains the query_history view. Cloned Schema Name â The schema that contains the query_history view. For more information, see Create cloned views of system tables . For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Configure the usage extractor: Â Â For Popularity Window (days) , 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days. For Start time , choose the earliest date from which to mine query history. If you're using the offline extraction method to extract query history from Databricks, skip to the next step. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Databricks assets. Press enter after each name to add more names.Â danger If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run the extractor â To run the Databricks lineage and popularity extractor, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . This isÂ currently only supported when using REST API and offline extraction methods. If you're using system tables, skip to step 2. You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the extractor has completed running, you will see lineage for Databricks assets! ð Tags: connectors data crawl api authentication Previous Set up an Azure private network link to Databricks Next How to extract on-premises Databricks lineage Select the extractor Configure the lineage extractor (Optional) Configure the usage extractor Run the extractor"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/manage-databricks-tags",
    "text": "Connect data Data Warehouses Databricks Tag management Manage Databricks tags On this page Manage Databricks tags You must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Atlan enables you to import your Databricks tags , update your Databricks assets with the imported tags, and push the tag updates back to Databricks: Import tags -  crawl Databricks tags from Databricks to Atlan Reverse sync -  sync Databricks tag updates from Atlan to Databricks Once you've imported your Databricks tags to Atlan: Your Databricks assets in Atlan will be automatically enriched with their Databricks tags. Imported Databricks tags will be mapped to corresponding Atlan tags through case-insensitive name match   -  multiple Databricks tags can be matched to a single tag in Atlan. You can also attach Databricks tags , including tag values, to your Databricks assets in Atlan   -  allowing you to categorize your assets at a more granular level. You can filter your assets by Databricks tags and tag values. You can enable reverse sync to push any tag updates for your Databricks assets back to Databricks   -  including tag values added to assets in Atlan. Did you know? Enabling reverse sync will only update existing tags in Databricks. It will neither create nor delete any tags in Databricks. Prerequisites â You must have a Unity Catalog-enabled workspace and SQL warehouse configured to import Databricks tags in Atlan. Before you can import tags from andÂ push tag updates to Databricks using personal access token , AWS service principal , or Azure service principal authentication, you will need to do the following: Ensure that you have a Unity Catalog-enabled workspace and a SQL warehouse configured. Create tags or have existing tags in Databricks. Grant permissions to import tags from and push tag updates to Databricks. Import Databricks tags to Atlan â Who can do this? You will need to be an admin user in Atlan to import Databricks tags to Atlan. You will also need to work with your Databricks administrator to grantÂ permissions to import tags from Databricks   -  you may not have access yourself. You can import your Databricks tags to Atlan through one-way tag sync. The synced Databricks tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Databricks assets will be enriched with their synced tags from Databricks. To import Databricks tags to Atlan, you can either: Create a new Databricks workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Databricks workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags imported from Databricks will be available to use for tagging assets ! ð View Databricks tags in Atlan â Once you've imported your Databricks tags, you will be able to view and manage your Databricks tags in Atlan. To view Databricks tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. From the left menu under Tags , select a synced tag. In the Overview section, you can view a total count of synced Databricks tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Databricks tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . You cannot rename tags synced from Databricks. Push tag updates to Databricks â Who can do this? Any admin or member user in Atlan can configure reverse sync for tag updates to Databricks. You will also need to work with your Databricks administrator to grant additional permissions to push updates   -  you may not have access yourself. You can enable reverse sync for your imported Databricks tags in Atlan and push all tag updates for your Databricks assets back to source. Once you have enabled reverse sync, any Databricks assets with tags updated in Atlan will also be updated in Databricks. To enable reverse sync for imported Databricks tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. In the left menu under Tags , select a synced Databricks tag   -  synced tags will display the Databricks icon next to the tag name.Â On your selected tag page, to the right of Overview , click Synced tags . Under Synced tags , in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Databricks. In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel . Now when you attach Databricks tags to your Databricks assets in Atlan, these tag updates will also be pushed to Databricks! ð Did you know? Enabling reverse sync will not trigger any updates in Databricks until synced tags are attached to Databricks assets in Atlan. Tags: connectors data crawl Previous How to extract on-premises Databricks lineage Next What does Atlan crawl from Databricks? Prerequisites Import Databricks tags to Atlan View Databricks tags in Atlan Push tag updates to Databricks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/references/what-does-atlan-crawl-from-databricks",
    "text": "Connect data Data Warehouses Databricks References What does Atlan crawl from Databricks? On this page What does Atlan crawl from Databricks? Atlan crawls and maps the following assets and properties from Databricks. info The following properties aren't crawled by the System tables extraction method: Table properties : partitionList , partitionCount Column properties : maxLength , precision Databases â Atlan maps databases from Databricks to its Database asset type. Source property Atlan property Where in Atlan TABLE_CATALOG name Asset profile and overview sidebar SCHEMA_COUNT schemaCount API only Schemas â Atlan maps schemas from Databricks to its Schema asset type. Source property Atlan property Where in Atlan TABLE_SCHEMA name Asset profile and overview sidebar TABLE_COUNT tableCount Asset preview and profile VIEW_COUNT viewsCount Asset preview and profile TABLE_CATALOG databaseName Asset preview and profile Tables â Atlan maps tables from Databricks to its Table asset type. Source property Atlan property Where in Atlan TABLE_NAME name Asset profile and overview sidebar REMARKS, DESCRIPTION description Asset profile and overview sidebar COLUMN_COUNT columnCount Asset profile and overview sidebar LOCATION externalLocation Overview sidebar FORMAT externalLocationFormat Overview sidebar OWNER Created (in Databricks) Properties sidebar CREATEDAT sourceCreatedAt Properties sidebar UPDATED_BY Last updated Properties sidebar LASTMODIFIED sourceUpdatedAt Properties sidebar PARTITIONS isPartitioned , partitionCount , partitionList API only Views â Atlan maps views from Databricks to its View asset type. Source property Atlan property Where in Atlan TABLE_NAME name Asset profile and overview sidebar REMARKS description Asset profile and overview sidebar COLUMN_COUNT columnCount Asset profile and overview sidebar CREATETAB_STMT definition Asset profile and overview sidebar OWNER Created (in Databricks) Properties sidebar CREATEDAT sourceCreatedAt Properties sidebar UPDATED_BY Last updated Properties sidebar LASTMODIFIED sourceUpdatedAt Properties sidebar Materialized views â Atlan maps materialized views from Databricks to its MaterialisedView asset type. Source property Atlan property Where in Atlan TABLE_NAME name Asset profile and overview sidebar REMARKS description Asset profile and overview sidebar COLUMN_COUNT columnCount Asset profile and overview sidebar CREATETAB_STMT definition Asset profile and overview sidebar OWNER Created (in Databricks) Properties sidebar CREATEDAT sourceCreatedAt Properties sidebar UPDATED_BY Last updated Properties sidebar LASTMODIFIED sourceUpdatedAt Properties sidebar Columns â Did you know? To help you work seamlessly with STRUCT data types, Atlan supports nested columns up to level 30 in Databricks. You can view these columns in the Tree view or the asset sidebar of your table assets, and also explore child columns of STRUCTs nested within MAPs or ARRAYs. However, lineage for nested columns isn't supported. Atlan maps columns from Databricks to its Column asset type. Source property Atlan property Where in Atlan PRIMARY KEY isPrimary Asset preview and filter, overview sidebar FOREIGN KEY isForeign Asset preview and filter, overview sidebar COLUMN_NAME name Asset profile and overview sidebar REMARKS description Asset profile and overview sidebar ORDINAL_POSITION order Asset profile TYPE_NAME dataType Asset profile and overview sidebar PARTITION_INDEX isPartition Asset preview and profile NULLABLE isNullable API only CHAR_OCTET_LENGTH maxLength API only DECIMAL_DIGITS precision API only Tags: data crawl api Previous Manage Databricks tags Next Preflight checks for Databricks Databases Schemas Tables Views Materialized views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/troubleshooting/troubleshooting-databricks-connectivity",
    "text": "Connect data Data Warehouses Databricks Troubleshooting Troubleshooting Databricks connectivity On this page Troubleshooting Databricks connectivity Did you know? The documentation refers to both SQL endpoint and interactive cluster as compute engine below. Does Atlan consider expensive queries and compute costs? â No, Atlan doesn't factor in expensive queries or compute costs due to limitations in the Databricks APIs, which don't expose this information. How does Atlan calculate popularity for Databricks assets? â Atlan calculates popularity for tables , views , and columns in Databricks by analyzing query execution data. It retrieves query history from the system.query.history table and specifically filters for execution_status = 'FINISHED' and statement_type = 'SELECT' to determine how frequently assets are accessed. How to debug test authentication and preflight check errors? â Hostname resolution error Provided Host name cannot be resolved via DNS, please check and try again. The hostname you have provided can't be resolved through DNS. Check that the hostname is correct. Verify that the DNS settings have been configured properly. Invalid client ID or secret Provided Client ID is invalid, please check and try again. The client ID or secret you have provided is either invalid or no longer working. Follow the steps for AWS or Azure setup to generate new credentials. Invalid tenant ID Provided tenant ID is invalid, please check and try again. The tenant ID you have provided is incorrect. Ensure that the tenant ID you have provided corresponds to the one in your Microsoft Entra ID application . Unity Catalog not linked Configured Databricks instance doesn't have Unity Catalog linked. Please choose JDBC extraction instead of REST API in Atlan. If you have not set up Unity Catalog in your Databricks workspace, you can change the extraction method to JDBC instead of REST API to crawl your Databricks assets in Atlan. Connection timeout Failed to connect to Databricks (connection timed out). Please check your host and port and try again. The connection to the Databricks instance has timed out. Verify that the host and port are correct. Check that no firewall rules or network issues are blocking the connection. Invalid HTTP path Provided HTTP path is invalid, please check and try again. The HTTP path you have provided is invalid. Ensure that the endpoint is properly configured and accessible, and the warehouse ID in the HTTP path is correct. Invalid personal access token PAT token is invalid, please check and try again. The personal access token used for authentication is invalid. Ensure that the token is valid and neither deleted nor expired. You can also generate a new personal access token , if needed. Insufficient permisions for crawling metadata User doesn't have access to any schemas / dbs, please check the accesses provided to the atlan user and try again. Check that the service principal or the user who's PAT token is being used has the necessary permissions provided. Refer to the setup doc to understand permissions required for different auth types. Insufficient permisions for some of the included crawling metadata Warning, user doesn't have access to the following objects anymore, or the objects no longer exist on the source!, check failed for ... user doesn't have access to one or more db objects from the include filter, (such as catalogs / schemas). You can either remove these objects from the include filter if they no longer exist on the source. Or check that the service principal or the user who's PAT token is being used has the necessary permissions provided. Refer to the setup doc to understand permissions required for different auth types. Insufficient permisions to crawl tags User doesn't have access to the following system tables Check that you have sufficient permissions provided for the tags extraction . User doesn't have permission to access warehouses please check your credentials and warehouse access Check that the configured user / service principal has CAN_USE on the configured SQL warehouse. Unable to access query history from the source, user doesn't have the access Check the permissions required for the system tables based lineage extraction are provided. System table extraction checks failing with User doesn't have access to the following system tables Check the permissions required for the system tables based extraction. General connection failure Unable to connect to the configured Databricks instance, please check your credentials and configs and then try again. If the problem persists, contact [email protected] . Check that you have entered the host and port correctly. Verify that the credentials for the connection are correct. Check that your Databricks instance is properly configured and available. If the problem still persists after verifying all of the previous steps, contact Atlan support . Why does the workflow take longer than usual in the extraction step? â Certain Databricks runtime versions don't have an easy way to extract some metadata (for example partitioning, table_type, and format). Extra operations must be performed to retrieve these, resulting in slower performance. If you aren't already, you may want to try the Unity Catalog extraction method . Why is some metadata missing? â When using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata. Currently, some metadata can't be extracted from Databricks: Metadata JDBC REST API System Tables ViewCount and TableCount (on schemas) â â â RowCount (on tables and views) â â â TABLE_KIND (on tables and views) â â â PARTITION_STRATEGY (on tables and views) â â â CONSTRAINT_TYPE (on columns) â â â Partition key (on columns) â â â Table partitioning information â â â BYTES , SIZEINBYTES (table size) â â â The team is exploring ways to bring this metadata into Atlan if Databricks supports extraction of the metadata. Why doesn't my SQL work when querying Databricks? â Atlan currently supports SparkSQL on Databricks runtime 7.x and above . Can I use Atlan when the Databricks compute engine isn't running? â Atlan needs the Databricks compute engine to be running for two activities: Crawling assets (normal and scheduled run) Querying assets (including data previews) If you don't need to perform the activities listed, your experience shouldn't be affected. In any other case, you'll get a downgraded experience on Atlan if the compute engine isn't running. Queries won't work as expected and a scheduled workflow might fail after a couple of retries. The team recommends turning off the Terminate after x minutes of inactivity option in your cluster to avoid these problems. If you have this turned on, any of the listed activities triggers the cluster to come back online within about 30 seconds. Why can't I see all the assets on Atlan that are available in Databricks? â Have you excluded the database or schema when crawling ? Does the Databricks user you configured for crawling have access to these other assets? Why is the test authentication taking so long? â Please check the state of the compute engine. It must be in a running state for all operations, including authentication. What limitations are there with the REST API (Unity Catalog) extraction method? â Currently, schema-level filtering and retrieving table partitioning information aren't supported. Why has my workflow started to fail when it worked before? â This can happen if the PAT you configured the workflow with has since expired. You will need to create a new PAT in Databricks, and then modify the workflow configuration in Atlan with this new PAT. If you are unable to update the PAT, pause the workflow and reach out to us . How do I migrate to Unity Catalog? â Currently Unity Catalog is in a public preview state. The Databricks team is working on an automated migration to Unity Catalog. Currently you must migrate individual tables manually. Why are some notebooks missing from metadata extraction? â Notebooks stored inside hidden directories (names starting with \".\" such as .hidden_dir/ ) are generally not returned by the /api/2.0/workspace/list API endpoint. This may cause missing notebook details in Atlan. Why is metadata missing for some Databricks entities? â The Databricks APIs used provide data only within a single configured workspace. If an entity used in lineage creation exists outside this workspace, its details won't be available via these APIs. Does Atlan support nested columns beyond level 30? â Atlan doesn't support nested columns beyond 30 levels for complex types such as Struct, Array, and Map. Columns exceeding this nesting depth aren't parsed. Instead, the deepest column level gets assigned the data type string, and its value contains a string representation of the remaining nested structure. For example, LEVEL_31 has the data type <LEVEL_32:STRUCT<LEVEL_33:STRUCT<...>>> . What happens if the service principal loses access to one workspace? â The crawler is resilient to this scenario. During the discovery phase, it fails to connect to that specific workspace, logs it as inaccessible, and simply skips it. The process continues for all other available workspaces without failing the entire run. Why are assets from a specific catalog not appearing in Atlan? â This is almost always a permission issue. Verify that the service principal has been granted USE CATALOG , BROWSE , and SELECT permissions on the catalog and its contents (schemas, tables). Why is my lineage view incomplete? â Check the source and target tables of the missing lineage link. The service principal must have SELECT permissions on both tables for lineage to be captured. For more information on cross-workspace extraction setup, see Set up cross-workspace extraction . Tags: api rest-api graphql Previous Preflight checks for Databricks"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/references/provide-credentials-to-view-sample-data",
    "text": "Use data Discovery References Provide credentials to view sample data On this page Provide credentials to view sample data Once your connection admins have configured bring your own credentials (BYOC) in Atlan, users will need to provide their own credentials before they can view the sample data in the asset profile. This will help you enforce better governance across your organization. Who can do this? Any Atlan user with data access to the asset and their own credentials for the data store. Atlan will display a 100-row sample of the data . Use your own credentials to view sample data â Atlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports SSO authentication . To set up your own credentials for viewing sample data: On the Assets page, click on an asset to view its asset profile. In the asset profile, click Sample Data . To set up your credentials for viewing the sample data, click Get Started . In the popup window, click Get Started once again to proceed. In the User credential setup dialog box, Basic is selected as the default authentication option. Enter the following: For Username , enter the username for the connection. For Password , enter the password for that connection. For Role , enter your role for that connection. For Warehouse , enter the name of the warehouse. Click the Test Authentication button to confirm your credentials. Once authentication is successful, click Done . You can now view sample data using your own credentials! ð When using the key pair method, you'll need to enter your encrypted private key and the private key password to complete the authentication process. Did you know? Once you've set up your credentials for viewing sample data, you can also manage your credentials . If your admin has enabled sample data download , you can export sample data in a CSV file. Tags: integration connectors Previous What are asset profiles? Next Discovery FAQs Use your own credentials to view sample data"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics",
    "text": "Use data Usage & Popularity Analysis How to interpret usage metrics On this page Interpret usage metrics Atlan currently supports usage and popularity metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Microsoft Power BI -  reports and dashboards Snowflake -  tables, views, and columns Powered by Atlan's enhanced query-mining capabilities, you can view popularity metrics for supported assets: The popularity score of an asset is computed using both the number of queries and the number of users who have queried that asset in the last 30 days. The popularity score of an asset helps determine its relative popularity. All assets with a popularity score are then slotted into one of four percentile groups   - Least popular , Less popular , Popular , and Most popular . Popularity score is calculated using the following formula: number of distinct users * log (total number of read queries) Time period = 30 days The popularity indicator is displayed for all supported assets that have been queried in the last 30 days. This indicator visualizes the relative popularity of an asset on a scale of 1 to 4 blue bars   -  1 being the lowest score and 4 being the highest. A popularity popover will appear when hovering over the popularity indicator. It displays additional information pertaining to an asset, such as a graph for trends in the data, last queried and by whom, and when the data was last updated. View popularity metrics â To view popularity metrics for your assets, complete these steps. Identify popular assets â Being able to identify your most relevant and trusted data assets can help you increase their adoption and drive usage within your organization. To view popularity metrics for an asset: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Your assets will now have a popularity indicator. To view the popularity popover for an asset, click or hover over the popularity indicator .Â You'll now be able to see all the relevant popularity metrics for your asset! ð View usage metrics in the asset sidebar â The new Usage tab in the asset sidebar helps you view usage metadata for your assets. For example, if you'd like to appoint a data steward for your data assets, you'll be able to determine the right candidate based on the top users for that asset. You'll also be able to review popular queries or users for a particular table while checking for data compliance. To view usage details for an asset: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. In the bottom right of any asset card, click or hover over the popularity indicator to open the popularity popover.Â In the popularity popover, click View usage details to view the following: For Usage , view top and recent users in the last 30 days. For Queries , view top five queries by context   - Popular , Slow , and Expensive . Only read queries or SELECT statements are shown for these queries. For Compute , view the total compute cost for an asset. The compute cost is split between read and write queries, allowing you to better understand the cost breakdown for individual assets: Read queries   - SELECT statements. Write queries   -  all non- SELECT statements, for example, UPDATE , INSERT , CREATE , and more. The usage details for the asset will now appear in the asset sidebar! ð View and sort columns by popularity â For any Snowflake, Databricks, or Google BigQuery table or view sorted by popularity, you'll also be able to view and sort the columns by popularity in the asset profile. To view column assets by popularity: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Click any asset to open to its asset profile. In the Column preview tab of the asset profile, hover over the popularity indicator to view the popularity popover for your columns. (Optional) In the search bar under Column preview , click the sort icon and then click Most popular or Least popular to sort columns by popularity. You'll now be able to view the popularity score, number of queries and users, and timestamp for last queried for your columns! ð View queries by context â Get the context you need before querying an asset to help you optimize your queries. Query popular, slow, or expensive queries from the Usage tab directly in Insights. To view and work with queries by context: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. In the bottom right of any asset card, click or hover over the popularity indicator to open the popularity popover.Â In the popularity popover, click View usage details . In the Usage tab in the asset sidebar, navigate to Queries and depending on the type of query you'd like to see: Click Popular Â to see the top five most popular queries.Â Click Slow to see queries sorted by average duration and last run. Click Expensive to see the top five most expensive queries.Â Once you've selected the relevant query type, hover over a query card to: Click the expand icon to see the query details. Click the copy icon to copy the query and use it as a template for writing your own queries. Click the code icon to open the query directly in Insights and run it. Did you know? If you have any questions about usage and popularity metrics, head over here . Tags: connectors data api Previous How to find assets by usage Next Troubleshooting usage and popularity metrics View popularity metrics View queries by context"
  },
  {
    "url": "https://docs.atlan.com/tags/authentication",
    "text": "53 docs tagged with \"authentication\" View all tags Add impact analysis in GitHub Learn about add impact analysis in github. Add impact analysis in GitLab Learn about add impact analysis in gitlab. Authentication Understand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure. Create an AWS Lambda trigger Once you have configured the [AWS Lambda permissions](/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda), you can run an AWS Lambda function. Enable  Snowflake OAuth Atlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware. Enable  SSO for Amazon Redshift You will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift). Enable  SSO for Google BigQuery Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan. extract lineage and usage from Databricks Once you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal. Integrate Anomalo Once you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo. Integrate Apache Spark/OpenLineage Atlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). PingFederate SSO 404 error If you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion. Preflight checks for Google BigQuery Each request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication service-accounts). Preflight checks for Microsoft Azure Synapse Analytics This check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method. Security The Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring. Security and Compliance Complete guide to Atlan's security features, compliance certifications, and data protection capabilities. Set up Amazon DynamoDB Learn about set up amazon dynamodb. Set up Amazon MSK Learn about set up amazon msk. Set up Amazon QuickSight Learn about set up amazon quicksight. Set up Amazon Redshift :::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself. Set up Anomalo Atlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata. Set up client credentials flow Configure Salesforce for OAuth 2.0 client credentials authentication in Atlan. Set up Confluent Kafka Atlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata. Set up Confluent Schema Registry :::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself. Set up Databricks Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods:. Set up Domo :::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself. Set up Hive :::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself. Set up IBM Cognos Analytics :::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself. Set up Informatica CDI Configure authentication and user permissions for Informatica Cloud Data Integration connector Set up JWT bearer flow Configure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan. Set up Microsoft Azure Cosmos DB If your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db). Set up Microsoft Azure Data Factory Atlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata. Set up Microsoft Azure Event Hubs Atlan supports the following authentication methods for Microsoft Azure Event Hubs:. Set up Microsoft Azure Synapse Analytics Atlan supports crawling the following with the Microsoft Azure Synapse Analytics package:. Set up Microsoft Power BI This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Set up Microsoft SQL Server :::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself. Set up MicroStrategy Atlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata. Set up MongoDB Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password]( create-database-user-in-mongodb) to fetch metadata. Set up Monte Carlo :::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups). Set up MySQL :::warning Who can do this? You will probably need your MySQL administrator to run these commands - you may not have access yourself. Set up Oracle :::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself. Set up PrestoSQL Learn about set up prestosql. Set up Redash :::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself. Set up Salesforce Learn about setting up Salesforce authentication for Atlan. Set up SAP HANA :::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself. Set up Sisense Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Set up Soda :::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -. Set up Teradata :::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself. Set up ThoughtSpot :::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself. Set up Trino :::warning Who can do this? You will probably need your Trino administrator to run these commands - you may not have access yourself. Set up username-password flow Configure Salesforce username-password flow for Atlan integration. Supported connections for on-premises databases The metadata-extractor tool supports the following connection types. Users and groups Learn how to manage users and groups in Atlan to control access and organize your data team. What does Atlan crawl from Domo? Atlan supports lineage for the following asset types:."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI On this page Microsoft Power BI Overview: Catalog workspaces, reports, dashboards, and datasets from Microsoft Power BI in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Power BI assets in Atlan: Set up the connector Crawl Microsoft Power BI assets Guides â Mine Power BI usage : Extract usage metrics and activity events from Power BI. References â What does Atlan crawl from Microsoft Power BI : Learn about the Power BI assets and metadata that Atlan discovers and catalogs. What lineage does Atlan extract from Microsoft Power BI : Learn about supported lineage extraction for Power BI assets. Preflight checks for Microsoft Power BI : Verify prerequisites before setting up the Microsoft Power BI connector. Troubleshooting â Troubleshooting connectivity : Resolve common Power BI connection issues and errors. Tags: power bi connector business intelligence connectivity Next Set up Microsoft Power BI Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Crawl Power BI Assets Crawl Microsoft Power BI On this page Crawl Microsoft Power BI Once you have configured the Microsoft Power BI user permissions , you can establish a connection between Atlan and Microsoft Power BI. To crawl metadata from Microsoft Power BI, review the order of operations and then complete the following steps. Select the source â To select Microsoft Power BI as your source: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the list of packages, selectÂ Power BI Assets and click onÂ Setup Workflow . Provide credentials â To enter your Microsoft Power BI credentials: For Authentication, choose the method you want to use to access Microsoft Power BI: For Service Principal authentication, enter the Tenant Id , Client Id , and Client Secret you configured when setting up Microsoft Power BI . Use the Enable Only Admin API Access option to control how metadata is extracted. When enabled, the crawler uses only admin APIs. If disabled, both admin and non-admin APIs are used. For Delegated User authentication, enter the Username , Password , Tenant Id , Client Id , and Client Secret you configured when setting up Microsoft Power BI . At the bottom of the form, click the Test Authentication button to confirm connectivity to Microsoft Power BI using these details. Once successful, at the bottom of the screen click the Next button. Configure connection â To complete the Microsoft Power BI connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection   -  not even admins. At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the Microsoft Power BI crawler, configure metadata extraction and advanced options. You can override the default settings for the following fields. Configure metadata â Include Workspaces : Select Microsoft Power BI workspaces to include. Defaults to all workspaces when left blank. Use Advanced Search to filter workspaces using the following options: Contains : Matches workspaces that contain the given substring. Starts with : Matches workspaces that begin with the specified text. Ends with : Matches workspaces that end with the specified text. Regex pattern : Matches workspaces based on a regular expression. All selected filters apply using an AND condition. Exclude Workspaces : Select workspaces to exclude. No workspaces are excluded by default. Advanced Search is also available for exclusion, with the same filtering options as mentioned previously. Include Dashboard and Reports Regex : Use a regular expression to include dashboards and reports based on naming patterns. Includes all by default. Exclude Dashboard and Reports Regex : Use a regular expression to exclude dashboards and reports based on naming patterns. Excludes none by default. Attach Endorsements from Power BI : Automatically certify assets endorsed in Power BI. To manually review before applying, change this setting to Send a Request . For more details, see What does Atlan crawl from Microsoft Power BI? Configure advanced settings â Source Connections : When your tenant has multiple connections available for the same source system that share the similar metadata, confirm the advanced options and choose the correct connections from the Source Connections list drop down to avoid creating duplicate lineage to such connections. Enable ODBC DSN Connectivity Mapping : Power BI provides multiple ways of connecting to a SQL source, including ODBC connectivity for building Reports and Dashboards. When datasets are populated using ODBC, provide a mapping of the DSN ( Data Source Name ) names to their appropriate database qualified names after enabling this toggle. Did you know? If a workspace appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Microsoft Power BI crawler: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you can see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up Microsoft Power BI Next Mine Microsoft Power BI Select the source Provide credentials Configure connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-does-atlan-crawl-from-microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI References What does Atlan crawl from Microsoft Power BI? On this page What does Atlan crawl from Microsoft Power BI? Atlan crawls and maps the following assets and properties from Microsoft Power BI. Once you've crawled Microsoft Power BI , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Measures -  External measure filter danger Currently Atlan only represents the assets marked with ð in lineage. For your Microsoft Power BI reports , Atlan also provides asset previews to help with quick discovery and give you the context you need. Apps â Atlan maps Apps from Microsoft Power BI to its PowerBIApp asset type. Source Property Atlan Property Where in Atlan name name asset profile and overview sidebar id powerBIAppId properties sidebar description description asset profile and overview sidebar publishedBy sourceOwners overview sidebar users   ( displayName, appUserAccessRight ) powerBIAppUsers asset profile groups   ( displayName, appUserAccessRight ) powerBIAppGroups asset profile Workspaces â Atlan maps workspaces from Microsoft Power BI to its PowerBIWorkspace asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar reportCount reportCount asset preview and profile dashboardCount dashboardCount asset profile datasetCount datasetCount asset profile dataflowCount dataflowCount asset profile webUrl sourceURL overview sidebar Dashboards ð â Atlan maps dashboards from Microsoft Power BI to its PowerBIDashboard asset type. Source property Atlan property Where in Atlan displayName name asset profile and overview sidebar workspace_name workspaceName API only description description asset profile and overview sidebar tileCount tileCount asset profile webUrl sourceURL overview sidebar Data sources â Atlan maps data sources from Microsoft Power BI to its PowerBIDatasource asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar connectionDetails connectionDetails overview sidebar Datasets ð â Atlan maps datasets from Microsoft Power BI to its PowerBIDataset asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar workspace_name workspaceName API only description description asset profile and overview sidebar webUrl sourceURL overview sidebar configuredBy sourceOwners asset profile and properties sidebar createdDate sourceCreatedAt asset profile and properties sidebar endorsementDetails certificateStatus (VERIFIED) asset profile and overview sidebar endorsementDetails (endorsement) certificateStatusMessage , powerBIEndorsement asset profile and overview sidebar endorsementDetails (certifiedBy) certificateUpdatedBy asset profile and overview sidebar Dataflows ð â Atlan maps dataflows from Microsoft Power BI to its PowerBIDataflow asset type. Atlan currently only supports dataflow lineage for the following SQL sources: Microsoft SQL Server Oracle SAP HANA Snowflake Source property Atlan property Where in Atlan name name asset profile and overview sidebar workspace_name workspaceName API only description description asset profile and overview sidebar webUrl sourceURL overview sidebar configuredBy sourceOwners asset profile and properties sidebar modifiedDateTime sourceUpdatedAt asset profile and properties sidebar endorsementDetails certificateStatus (VERIFIED) asset profile and overview sidebar endorsementDetails (endorsement) certificateStatusMessage , powerBIEndorsement asset profile and overview sidebar endorsementDetails (certifiedBy) certificateUpdatedBy asset profile and overview sidebar modifiedBy sourceUpdatedBy asset profile and properties sidebar days powerBIDataflowRefreshScheduleFrequency properties sidebar times powerBIDataflowRefreshScheduleTimes properties sidebar localTimeZoneId powerBIDataflowRefreshScheduleTimeZone properties sidebar Dataflow entity columns ð â Atlan maps attributes of dataflow entities from Microsoft Power BI to its PowerBIDataflowEntityColumn asset type. Source property Atlan property Where in Atlan attrbutes.name name asset profile and overview sidebar entities.name powerBIDataflowEntityName overview sidebar attributes.$type powerBIDataflowEntityColumnDataType asset profile and overview sidebar Reports ð â Atlan maps reports from Microsoft Power BI to its PowerBIReport asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar workspace_name workspaceName API only description description asset profile and overview sidebar pageCount pageCount asset profile webUrl sourceURL overview sidebar createdDateTime sourceCreatedAt asset profile and properties sidebar modifiedDateTime sourceUpdatedAt asset profile and properties sidebar createdBy sourceCreatedBy , sourceOwners asset profile and properties sidebar modifiedBy sourceUpdatedBy asset profile and properties sidebar endorsementDetails certificateStatus (VERIFIED) asset profile and overview sidebar endorsementDetails (endorsement) certificateStatusMessage , powerBIEndorsement asset profile and overview sidebar endorsementDetails (certifiedBy) certificateUpdatedBy asset profile and overview sidebar Pages ð â Atlan maps pages from Microsoft Power BI to its PowerBIPage asset type. Source property Atlan property Where in Atlan displayName name asset profile and overview sidebar workspace_name workspaceName API only report_name reportName API only Tiles ð â Atlan maps tiles from Microsoft Power BI to its PowerBITile asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar subTitle description asset profile and overview sidebar workspace_name workspaceName API only dashboard_name dashboardName API only Tables ð â Atlan maps tables from Microsoft Power BI to its PowerBITable asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar isHidden isHidden API only sourceExpressions powerBITableSourceExpressions API only columnCount powerBITableColumnCount asset profile and preview measureCount powerBITableMeasureCount asset profile and preview Columns ð â Atlan maps columns from Microsoft Power BI to its PowerBIColumn asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar isHidden powerBIIsHidden API only dataCategory powerBIColumnDataCategory API only dataType powerBIColumnDataType asset preview and overview sidebar formatString powerBIFormatString API only sortByColumn powerBISortByColumn API only summarizeBy powerBIColumnSummarizeBy API only Measures ð â Atlan maps measures from Microsoft Power BI to its PowerBIMeasure asset type. Atlan supports PowerBI Measures for downstream lineage to a PowerBI Page. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar isHidden powerBIIsHidden API only expression powerBIMeasureExpression overview sidebar isExternalMeasure powerBIIsExternalMeasure asset filter formatString powerBIFormatString API only Tags: connectors crawl Previous Mine Microsoft Power BI Next Preflight checks for Microsoft Power BI Apps Workspaces Dashboards ð Data sources Datasets ð Dataflows ð Dataflow entity columns ð Reports ð Pages ð Tiles ð Tables ð Columns ð Measures ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/troubleshooting/troubleshooting-microsoft-power-bi-connectivity",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Troubleshooting Troubleshooting Microsoft Power BI connectivity On this page Troubleshooting Microsoft Power BI connectivity What are the known limitations of the Microsoft Power BI connector? â Atlan currently doesn't support the following: Filtering hidden pages in reports -  known limitation of PowerBI reports API Crawling reports developed and published in Power BI Report Server What are the limitations of the Microsoft PowerBI Connector, when only admin APIs are used? â When the Microsoft PowerBI Connector is configured to use only admin APIs, it results in reduced metadata extraction and limited lineage capabilities compared to using a combination of admin & non-admin APIs . The following limitations apply as mentioned below: PowerBI Pages (which are a part of PowerBI Report) won't be catalogued. Downstream Lineage from PowerBI Table Column / Measure -> PowerBI Page won't be available. Can users who don't have access to a report still see the preview? â Users can only see asset previews if the following conditions are met: They have the necessary permissions in both Microsoft Power BI and Atlan. They're logged into Atlan and Microsoft Power BI on the same browser. Therefore, if a user lacks the permission to view a report in Microsoft Power BI, they won't be able to see the report preview in Atlan. Even if they do have the necessary permissions, they need to be logged into Microsoft Power BI on the same browser as their Atlan instance for asset previews to work. Why can I not see previews for my Microsoft Power BI assets? â Your Microsoft Power BI assets are updated with previews during the next run of your Microsoft Power BI workflow. If you have run the workflow and still don't see the previews, rerun the workflow. Once you've rerun the workflow, the previews are visible to all eligible users. How does Atlan calculate views for Microsoft Power BI reports and dashboards? â Atlan calculates Usage views based on the number of times users open a report or dashboard in Microsoft Power BI. Tags: lineage data-lineage impact-analysis upstream-dependencies data-sources Previous What lineage does Atlan extract from Microsoft Power BI? Next What is the difference between a Power BI data source and dataflow?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/faq/power-bi-dataflow-datasource",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI FAQ What is the difference between a Power BI data source and dataflow? What is the difference between a Power BI data source and dataflow? In Microsoft Power BI: Data source   -  a data source is any source that fuels a dataset. You can connect to different types of data sources, such as databases, files, Microsoft Fabric, and more, to fetch data directly into Microsoft Power BI for creating reports and dashboards. For a full list, refer to Microsoft Power BI documentation . Dataflows   -  data flows help you encode the process of extracting and transforming data from a data source to produce a dataset. These have a refresh frequency that keeps the dataset in sync with the data source. To learn more, refer to Microsoft Power BI documentation . Tags: data model Previous Troubleshooting Microsoft Power BI connectivity"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/product-release-stages",
    "text": "Get Started References Product release stages On this page Product release stages The following release stages are part of the lifecycle of an Atlan feature: Stage Access Status Alpha By invitation only to few data teams Experimental Private preview By invitation only to a larger cohort of data teams Near production grade Public preview Open to all data teams Production grade with known limitations General availability Open to all data teams Production grade Atlan release stages â While features are stable, Atlan may on occasion deprecate legacy functionality to support product updates. Alpha â Enabled for select customers on request to test, validate, and provide feedback. Experimental in nature, user feedback critical in preparing for a wider release. Formal support and documentation unavailable. Standard SLAs, terms, and warranties don't apply. Private preview â Enabled for a larger cohort of customers on request to test, validate, and provide feedback. Partially production ready   -  breaking changes may occur. Informal support and documentation from product and engineering teams. Standard SLAs, terms, and warranties don't apply. To get access to private preview features, raise a support request . Public preview â Enabled for all customers, usually includes a Preview label in the product or requires admins to enable the feature from Labs in the admin center. Production grade is high, but with known limitations. Formal support and documentation provided on best available basis. Feature release may be announced on the Shipped channel . Standard SLAs, terms, and warranties do not apply. General availability â Enabled for all users. Production grade is of the highest quality to ensure the best possible user experience. Formal support and documentation available. Feature release announced on the Shipped channel . Standard SLAs, terms, and warranties apply. Tags: atlan releases Previous Atlan architecture Next Our 3 pro tips for saving time with Atlan Atlan release stages Alpha Private preview Public preview General availability"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/our-3-pro-tips-for-saving-time-with-atlan",
    "text": "Get Started References Our 3 pro tips for saving time with Atlan On this page Our 3 pro tips for saving time with Atlan There are a lot of incredibly time-saving functionalities built into Atlan. Here are three pro tips that you can use to save time for you and your team! Search from anywhere in Atlan â Atlan supports powerful, intelligent search. Start your search from anywhere in Atlan to find the data asset you're looking for. You can also enter a keyword in the search bar and filter your results by a specific type of asset. For instance, enter the keyword product in the search bar and then click the Table filter to view table results for your searched keyword. Did you know? You can use Cmd/Ctrl + K to open the search page from anywhere in Atlan. Collaborate with your team â Use Atlan's seamless Slack or Microsoft Teams integration to share updates with your team. Don't waste time adding the correct link   -  just click the Slack or Teams icon in Atlan to post directly on a Slack or Microsoft Teams channel. You can also add your message as a resource to the asset. Refine your search using filters â Use the filters in the left panel of the Assets workspace to refine your search. Check out our three best pro tips for using filters! Search by certification status â Certificates in Atlan are a useful tool for letting your team know the status of an asset. You can search by certification status to only find the assets you need. Search by owners or data contributors â Want to find out what your teammate is working on? Looking for a dashboard that your manager is delegating to you? Use the Owners filter to search by asset owner in Atlan. You can also search by other user roles. To do this, you first need to set up custom metadata properties to represent those other roles. Then you can filter by those custom metadata properties to find assets with any Atlan user who has that role for the asset. Search by lineage â Want to only find assets with a completed lineage ? You can filter your search by assets that have data lineage using the Has lineage filter listed under Properties .Â Try using these 3 pro tips to make Atlan a powerful part of your team's tool kit. Tags: atlan documentation Previous Product release stages Next The DataOps Culture Code Search from anywhere in Atlan Collaborate with your team Refine your search using filters"
  },
  {
    "url": "https://docs.atlan.com/get-started/faqs/how-are-product-updates-deployed",
    "text": "Get Started References How are product updates deployed? On this page How are product updates deployed? As the control plane for your entire data stack, Atlan's product release strategy is centered on being fast, iterative, and adaptive to the needs of our partners. All the while ensuring that product releases are mature and stable to truly empower users. Atlan follows a standard CI/CD model   -  continuous integration (CI) and continuous delivery (CD)   -  for deploying product updates . This includes bug fixes and making improvements regularly.Â To minimize risks and enhance security, each component in Atlan is tracked and released individually. When developers make changes, a series of automated validations are initiated, including building component images, running tests, and verifying that no vulnerabilities have been introduced. Peer reviews further ensure high-quality standards. Only after the changes have been approved, the new release will go through the following stages: Staging   -  for pre-production testing Beta   -  for limited user testing Production   -  live environment release Atlan's engineering team actively reviews any issues with new releases to extract valuable insights and learning opportunities toward offering a better service. Accordingly, Atlan recommends that any connections or integrations you would like to make must be approved by Atlan unless already tested and documented. Note that any unauthorized integration may lead to breakdown of service and impact end user experience. Significant feature updates are always highlighted within the application. You can also subscribe to our Shipped channel for updates on product rollouts. View product updates â To view product updates in Atlan: From the top right of any screen in Atlan, click the gift box icon.Â In the Product Updates sidebar, view the latest product releases. You can also: (Optional) Click Subscribe to subscribe to our Shipped channel . (Optional) Type a keyword in the search bar to search for specific product releases. Tags: integration connectors faq-integrations Previous The DataOps Culture Code Next Quality assurance framework View product updates"
  },
  {
    "url": "https://docs.atlan.com/platform/references/quality-assurance-framework",
    "text": "Get Started References Quality assurance framework On this page Quality assurance framework Atlan has a robust quality assurance (QA) framework for delivering reliable, high-quality products that exceed user expectations and build trust. The QA strategies and processes implemented throughout the product development lifecycle have the following benefits to offer: Rigorous testing and quality checks ensure that the product is reliable, functional, and user-friendly. Thorough testing helps identify and mitigate potential defects and risks early in the development process   -  minimizing rework and reducing the time and effort for bug fixes. Clear communication, sharing test results, and involving all stakeholders throughout the process fosters effective collaboration and alignment on quality goals. Manual testing process â Atlan uses Testmo for the manual testing process and Jira Â for project management: Test planning â The objectives, scope, and test approach are defined for the manual testing phase. Testable requirements are identified and test cases prioritized based on risk analysis. Test case design â Once a feature is ready for testing, it is handed over to the QA team. Test scenarios are created based on user requirements. The product owner reviews these test scenarios and approves them for testing. Manual testing execution â Manual testing is performed using the approved test scenarios. Testing is conducted in different environments, including beta , staging , and production . Comprehensive testing takes place in the staging environment. Any issues identified during testing are documented using a bug reporting tool. Reported issues are then assigned to the development team for resolution. Manual testing covers the following areas: Functional testing -  to verify that the feature functions according to specified requirements. Security testing -  to identify vulnerabilities and ensure the security of user data and system resources. Integration testing -  to validate the seamless integration of the feature with other components or systems. Usability testing -  to evaluate the user-friendliness and intuitiveness of the feature. Defect reporting â Any deviations or defects encountered during testing are recorded. Steps to reproduce the issue are documented with screenshots or logs, if applicable, and a severity level is assigned to each issue. Issue retesting and sanity testing â After the development team has fixed the reported issues, the QA team retests the scenarios. A round of sanity tests is conducted to ensure the overall stability of the feature. Test cycle closure â Test results are evaluated, including defect metrics, test coverage, and overall quality. A test summary report is prepared that highlights the testing activities, challenges faced, and recommendations for improvement. Automation testing process â Atlan uses mabl for the test automation process: Increased test coverage â Automation testing allows Atlan to execute a large number of test cases across different scenarios, configurations, and environments. By automating repetitive and time-consuming tasks, Atlan is able to achieve a broader test coverage, ensuring that critical functionalities and edge cases are thoroughly validated. Faster time-to-market â Automation testing significantly reduces the time required to execute test cases compared to manual testing. Atlan can accelerate the testing process and obtain quicker feedback on the quality of the product. This helps in meeting tight deadlines and release features faster. Improved accuracy and consistency â Automated tests are executed with precision and consistency, minimizing human errors and ensuring accurate results. By eliminating manual intervention, Atlan reduces the risk of human-induced mistakes, resulting in reliable and repeatable test outcomes. Regression testing â Automation testing is particularly effective for regression testing, which involves retesting previously validated functionalities to ensure that new changes or fixes do not introduce unintended issues. By automating regression tests, Atlan can quickly and accurately verify that existing features are working as expected after modifications. Continuous integration and deployment â Automation testing seamlessly integrates with Atlan's continuous integration and continuous delivery (CI/CD) pipeline. This enables Atlan to automate the execution of tests at various stages of the development process, such as after each code commit or prior to a deployment. By automating tests as part of the CI/CD process, Atlan ensures that software updates are thoroughly validated before being released. Maintenance and reusability â Automation testing scripts can be maintained and reused across multiple test cycles, saving time and effort. As the product evolves, the scripts can be updated to accommodate changes. This ensures that the automation suite is up-to-date and aligned with the latest functionalities. Label-based testing â Label-based testing incorporates mabl's automated testing capabilities into the CI/CD pipeline using labels. Here's how it works: Triggering tests based on labels -  tests can be triggered based on specific labels in the CI/CD pipeline. For example, a particular set of mabl tests labeled as Regression or Smoke can be executed during different stages of the pipeline, such as pre-production or post-deployment. Test result reporting -  after executing the mabl tests, results are reported back to the CI/CD pipeline. This information includes the test outcomes, such as pass, fail, or blocked, along with any associated logs, screenshots, or other artifacts for further analysis. Deployment process â Once a feature has successfully passed manual and automation testing, it is ready for deployment: Environment setup â Target environments required for testing are set up and a base user is created for the environment to perform the test cases.Â Each environment closely resembles the intended production environment to ensure accurate testing and validation. Build generation â Once the code changes are ready for deployment, a build or package is generated that encapsulates the necessary files and resources for the software update. The build ensures that all components are packaged correctly and ready for deployment. Automation testing â Before deploying the software update to the target environment, a suite of automated tests is executed. These tests validate the functionality, performance, and stability of the software, ensuring that it meets Atlan's quality standards. Monitoring and rollback â Once the deployment is completed, the application is closely monitored in the target environment. Monitoring tools and logs are used to identify any issues or anomalies. In case of critical failures or any unexpected issues, there is a rollback plan in place to revert to the previous version. Continuous improvement â The deployment process is continuously evaluated to identify areas for improvement. This includes capturing feedback, analyzing deployment metrics, and implementing changes to enhance efficiency, reliability, and quality. Daily test reports â Tests are executed on a daily schedule to ensure the proper functioning of the product and promptly address any issues. The daily test reports include the following: Test case execution status â Overall status of the test case execution for the day   -  including the number of cases that were executed, passed, failed, or blocked. Test coverage â Insights into the coverage of test cases across different functional areas, features, or modules. These help stakeholders understand the parts of the system that were tested and to what extent. Defects or issues â Information on any defects or issues discovered during testing. This can encompass newly identified defects, severity and priority levels, steps to reproduce the issues, and any relevant supporting details. Metrics and statistics â Metrics and statistics related to test case execution, such as the pass/fail ratio, defect density, test coverage percentage, or any other relevant metrics agreed upon by the QA team. Summary and recommendations â Key findings, observations, and recommendations for further actions or improvements to help with decision making and prioritizing testing efforts. Protocol for test failures â When a test fails, Atlan adheres to a specific protocol to address and resolve the failure: Failure identification â Once a test fails, the specific test case or test suite that encountered the failure is promptly identified. This involves reviewing test logs, error messages, and any available diagnostic information to pinpoint the cause of the failure. Root cause analysis â A thorough root cause analysis is conducted to determine why the test failed. This may involve examining the test environment, reviewing test data, analyzing code changes, or investigating any external factors that could have influenced the failure. Bug reporting â If the test failure is determined to be due to a software defect, the bug is reported following a standard bug reporting process. The QA team provides detailed information regarding the failure, including steps to reproduce, relevant test data, and any supporting evidence or screenshots. Bug resolution â Once the bug is reported, the development team takes appropriate action to address and resolve the defect. This includes analyzing the bug report, reproducing the issue, and implementing the necessary fixes. The bug resolution process follows Atlan's established software development lifecycle and bug prioritization guidelines. Retesting â After the bug fixes are implemented, the QA team retests to verify that the issue has been resolved and the test case has passed successfully. This is to ensure that the fix does not introduce any new issues or regressions. Tags: atlan documentation Previous How are product updates deployed? Next Getting Started and Onboarding Manual testing process Automation testing process Deployment process Daily test reports Protocol for test failures"
  },
  {
    "url": "https://docs.atlan.com/platform/references/how-are-resources-isolated",
    "text": "Get Started Security & Compliance How are resources isolated? How are resources isolated? Each Atlan customer has their own isolated set of nodes within Kubernetes. Deployments isolate all running services, including the metastore and its persistence in Cassandra and Elasticsearch. The underlying Kubernetes control plane and networking layer (coredns) are shared between tenants. To achieve logical isolation, Loftâs virtual clusters are implemented. The compute resources (nodes, nodegroups) and storage are physically isolated between tenants. Only Atlan's cloud team is able to manage the AWS , Azure , and GCP resources across these levels of isolation. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Tags: atlan documentation Previous Infrastructure security Next Security monitoring"
  },
  {
    "url": "https://docs.atlan.com/tags/security",
    "text": "24 docs tagged with \"security\" View all tags Access Control Learn how to manage user permissions and access to data assets in Atlan for security and compliance. Atlan architecture Learn about atlan architecture. Atlan browser extension security Learn about atlan browser extension security. BigID Integrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata. Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring. Compliance standards and assessments Learn about compliance standards and assessments. Configure network security Configure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services Customer environment security Customer environment security best practices for deploying and operating Secure Agent 2.0 Deployment and security Frequently asked questions about Secure Agent 2.0 deployment and security Deployment architecture The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. Encryption and key management Learn about encryption and key management. Incident response plan Learn about incident response plan. Infrastructure security Learn about infrastructure security. Install on AWS EKS This guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster. Manage user authentication When users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication. Microsoft Defender SSO error Learn about unable to log into atlan via sso due to an \"internal error\" from microsoft defender. Roles and permissions Explanation of Snowflake's security model and role requirements for data quality operations. Secure Agent The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Security Security overview and controls for Secure Agent 2.0 Security monitoring Learn about security monitoring. Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Troubleshooting Salesforce connectivity Learn about troubleshooting salesforce connectivity. Troubleshooting ServiceNow Why is the security\\_admin role required to complete the ServiceNow integration? Verify container images Verify the authenticity and integrity of Secure Agent container images with Cosign"
  },
  {
    "url": "https://docs.atlan.com/tags/access-control",
    "text": "19 docs tagged with \"access-control\" View all tags Access Control Learn how to manage user permissions and access to data assets in Atlan for security and compliance. Atlan architecture Learn about atlan architecture. Atlan browser extension security Learn about atlan browser extension security. Automatically assign roles based on group names Learn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app. Compliance standards and assessments Learn about compliance standards and assessments. Deployment architecture The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. Encryption and key management Learn about encryption and key management. Incident response plan Learn about incident response plan. Infrastructure security Learn about infrastructure security. Install on AWS EKS This guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster. Manage user authentication When users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication. Microsoft Defender SSO error Learn about unable to log into atlan via sso due to an \"internal error\" from microsoft defender. Requests Request and manage changes to assets that you don't have direct edit access to. Secure Agent The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Security monitoring Learn about security monitoring. Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Troubleshooting Salesforce connectivity Learn about troubleshooting salesforce connectivity. Troubleshooting ServiceNow Why is the security\\_admin role required to complete the ServiceNow integration? User Role Sync Complete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/permissions",
    "text": "20 docs tagged with \"permissions\" View all tags Access Control Learn how to manage user permissions and access to data assets in Atlan for security and compliance. Atlan architecture Learn about atlan architecture. Atlan browser extension security Learn about atlan browser extension security. Compliance standards and assessments Learn about compliance standards and assessments. Data quality permissions Reference for data quality permission scopes and configuration in Atlan. Deployment architecture The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. Encryption and key management Learn about encryption and key management. Incident response plan Learn about incident response plan. Infrastructure security Learn about infrastructure security. Install on AWS EKS This guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster. Manage user authentication When users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication. Microsoft Defender SSO error Learn about unable to log into atlan via sso due to an \"internal error\" from microsoft defender. Roles and permissions Explanation of Snowflake's security model and role requirements for data quality operations. Secure Agent The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesnât require inbound connectivity. Running within an organizationâs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Security monitoring Learn about security monitoring. Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up SAP ECC Set up user accounts and permissions required for SAP ECC metadata extraction in Atlan. Set up SAP S/4HANA Set up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan. Troubleshooting Salesforce connectivity Learn about troubleshooting salesforce connectivity. Troubleshooting ServiceNow Why is the security\\_admin role required to complete the ServiceNow integration?"
  },
  {
    "url": "https://docs.atlan.com/platform/references/incident-response-plan",
    "text": "Get Started Security & Compliance Incident response plan On this page Incident response plan Atlan's incident response plan for any potential network security incidents is as follows: Incident response process â For any critical issues, the Incident Response Team at Atlan will follow a structured process designed to investigate, contain, and remediate the threat as well as recover systems and services. The process includes: Event reported   -  initial notification of the incident. Triage and analysisÂ   -  assessment of the incident's severity and potential impact. InvestigationÂ   -  detailed examination to understand the cause and scope. Containment and neutralizationÂ   -  actions to limit the impact and prevent further exploitation. Recovery and vulnerability remediationÂ   -  restoration of systems and addressing vulnerabilities. Hardening and detection improvementsÂ   -  enhancing security measures and detection capabilities to prevent future incidents. Key details about this process are as follows: Incident managerÂ   -  oversees incident response efforts. War roomÂ   -  a central location, either physical or virtual (for example, Slack), dedicated to managing the incident. Recurring meetingsÂ   -  regular meetings to review the incident status until resolution. NotificationÂ   -  legal and executive staff will be informed as required. Incident severity levels â Severity Category Description P0 Critical Actively exploited risk involves the engagement of a malicious actor. Identifying such active exploitation is essential. Major data breach, widespread system outage, critical vulnerability being actively exploited. P1 High Active exploitation is not yet confirmed but is highly probable. The vulnerability presents a high risk, potentially causing severe performance degradation or unauthorized access to sensitive data. P2/P3 Medium/Low Suspicious or unusual behavior that has not yet been verified and requires further investigation. This includes moderate performance issues, non-critical vulnerabilities, and isolated incidents affecting a small group of users. Incident reporting â Atlan will report any breaches to customers, consumers, data subjects, and regulators without undue delay and in accordance with allÂ contractual commitments and applicable legislation. If any users become aware of an information security incident, potential incident, imminent incident, unauthorized access, policy violation, security weakness, or suspicious activity, please notify Atlan support immediately. Tags: security access-control permissions Previous Compliance standards and assessments Next Atlan architecture Incident response process Incident severity levels Incident reporting"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation",
    "text": "Configure Atlan Integrations Automation Automation Integrations Integrate Atlan with automation tools such as AWS Lambda, Connections, Webhooks, Browser Extension, and Always On to automate workflows and extend Atlan's capabilities. Tags: integrations automation lambda webhooks browser extension Previous Integrations Next AWS Lambda"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda",
    "text": "Configure Atlan Integrations Automation AWS Lambda On this page AWS Lambda Overview: Connect Atlan with AWS Lambda to automate workflows, create triggers, and extend Atlan's automation capabilities. Get started â Follow these steps to connect and integrate AWS Lambda with Atlan: Set up AWS Lambda Create an AWS Lambda trigger Guides â Set up AWS Lambda : Step-by-step instructions to set up AWS Lambda integration with Atlan. Create an AWS Lambda trigger : How to create triggers for automated workflows using AWS Lambda. Tags: aws lambda integration automation Previous Automation Integrations Next Set up AWS Lambda Get started Guides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/always-on",
    "text": "Configure Atlan Integrations Automation Always On On this page Always On Overview: Connect Atlan with Always On to enable continuous automation, tag propagation, and receive suggestions from similar assets. References â Suggestions from similar assets : Learn how Atlan provides suggestions based on similar assets. Tag propagation : Understand how tags are propagated automatically in Atlan. Tags: always on integration automation Previous Create an AWS Lambda trigger Next Suggestions from similar assets References"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension",
    "text": "Configure Atlan Integrations Automation Browser Extension On this page Browser Extension Overview: Connect Atlan with the Browser Extension to enhance your data catalog experience, streamline workflows, and access Atlan features directly from your browser. Guides â Configure the extension for managed browsers : How to configure the Atlan browser extension for managed environments. Use the Atlan browser extension : Step-by-step instructions to use the Atlan browser extension. Concepts â Atlan browser extension security : Learn about the security features of the Atlan browser extension. Troubleshooting â Troubleshooting Atlan browser extension : Solutions for common issues encountered when using the Atlan browser extension. FAQ â Add the browser extension for everyone in my organization Browser extension shows an error message Browser extension not loading Tags: browser extension integration automation Previous Tag propagation Next Configure the extension for managed browsers Guides Concepts Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/configure-the-extension-for-managed-browsers",
    "text": "Configure Atlan Integrations Automation Browser Extension How-tos Configure the extension for managed browsers On this page Configure the extension for managed browsers If you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script. Atlan supports managing the Atlan browser extension for the following: Operating systems: macOS and Microsoft Windows Browsers: Google Chrome and Microsoft Edge The deployment scripts   -  .mobileconfig file for macOS and PowerShell script for Microsoft Windows   -  are designed to make only the most necessary modifications required for the Atlan browser extension to function properly. Both deployment methods adhere to the principle of least privilege: The .mobileconfig file for macOS only includes the configuration settings required to install and operate the Atlan browser extension. The PowerShell script creates essential registry keys required for the Atlan browser extension to operate on Microsoft Windows systems. To configure the Atlan browser extension for a managed browser, you must complete these steps in the following order: Configure the browser extension Bulk install the browser extension Deploy the configuration script (Optional) Verify and monitor the installation Configure the browser extension â Who can do this? You will need to be an admin in Atlan to configure the browser extension for users in your organization. You will also need inputs and approval from the IT administrator of your organization. You can configure the browser extension and then download a configuration script to bulk install and deploy it for everyone in your organization. To configure the browser extension, from within Atlan: From the left menu on any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In theÂ Browser extension tile, for Bulk install the browser extension , click the Set up now button. In the Set up browser extension form, enter the following details: For Choose browser , the browser and operating system values will be prefilled based on what you're currently using   -  you can modify the fields, if required. For Your Atlan domain , enter the URL of your Atlan instance   -  for example, https://(instance_name).atlan.com . info ðª Did you know? If you enable multiple Atlan domains, your users will be able to select the most relevant Atlan domain from a dropdown menu while using the browser extension. The default value in the dropdown will be the Atlan instance entered as Your Atlan domain . If your organization does not have multiple Atlan domains, only the default selection will be displayed. (Optional) For Advanced settings , you can configure the following: If you have multiple Atlan instances, toggle on Multiple Atlan domains and then enter the URLs of your additional Atlan instances. Click + Add to add more Atlan domains. If your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on Custom data source domain . Click + Add Â to add more custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of your custom data source domain. info ðª Did you know? For any supported tools configured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources. Click the Download Script button to download the corresponding configuration script. The IT administrator(s) in your organization will need to install this configuration file in your organization's devices using a mobile device management (MDM) software. Administrative permissions to the MDM platform are required to complete the setup. Based on your operating system, the downloaded file can be one of the following two types: .mobileconfig -  use this file to configure profiles with specific settings in macOS devices . .ps1 -  use this PowerShell script to create registry keys in Microsoft Windows devices . Bulk install the browser extension â Who can do this? You will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin. You will need to configure the ExtensionInstallForcelist browser policy for either Google Chrome or Microsoft Edge to force-install the extension for everyone in your organization. The ExtensionInstallForcelist browser policy: Governs extensions that can be silently installed and automatically enabled for all users. Provides extension IDs that the browser will automatically install and enable when a user logs in. Google Chrome â To bulk install the Atlan browser extension in Google Chrome, follow the steps in Google documentation: Force install apps and extensions . Microsoft Edge â To bulk install the Atlan browser extension in Microsoft Edge, follow the steps in Microsoft documentation: Force-install an extension . For the Extension/App IDs and update URLs to be silently installed (Device) field, copy and paste the following value: fipjfjlalpnbejlmmpfnmlkadjgaaheg;https://clients2.google.com/service/update2/crx fipjfjlalpnbejlmmpfnmlkadjgaaheg is the extension-id for the Atlan browser extension. https://clients2.google.com/service/update2/crx indicates that it needs to be installed from the Chrome Web Store. Deploy the configuration script â Who can do this? You will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin. The browser extension relies on managed storage for configuring domains in the Atlan extension. The values for managed storage can be configured through: A configuration profile in macOS Registry keys in Microsoft Windows Although Atlan's solution is platform-agnostic, the following example pertains to Microsoft Intune . macOS â You will need to create a custom managed profile to configure the domains for the Atlan browser extension. To deploy the .mobileconfig file for your organization, you can use any MDM platform. For example: Microsoft Intune   -  follow the steps in Custom configuration profile settings . Microsoft Windows â You will need to create registry keys to deploy the extension. You can create the required registry keys with a PowerShell script, which can then be deployed to your usersâ devices using an MDM software. To deploy the PowerShell configuration script for your organization, you can use any MDM platform. For example: Microsoft Intune   -  follow the steps in Create a script policy and assign it . For Script settings , enter the following details: Script location -  upload the .ps1 configuration script downloaded from Atlan. Run this script using the logged on credentials -  change to No . Enforce script signature check Â   -  change to No . Run script in 64 bit PowerShell Host Â   -  change to Yes . Verify and monitor the installation â To ensure that the Atlan browser extension has been successfully deployed across all selected devices in your organization, you can: Verify the installation   -  after you have deployed the policies, check a few target devices to ensure that the extension was installed and configured correctly. Monitor compliance   -  monitor the compliance status of the policy and troubleshoot any issues. Your users will now be able to use the Atlan browser extension in a managed browser! ð Once the managed browser has synced with the latest configuration changes for your organization, the Atlan browser extension will be automatically installed and a new tab will open to indicate that the Atlan browser extension is now active. Tags: atlan documentation Previous Browser Extension Next How to use the Atlan browser extension Configure the browser extension Bulk install the browser extension Deploy the configuration script Verify and monitor the installation"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/enable-embedded-metadata-in-tableau",
    "text": "Configure Atlan Integrations Automation Browser Extension How-tos Enable embedded metadata in Tableau On this page Enable embedded metadata in Tableau Atlan metadata layers Atlan context directly onto the source application, rather than in a sidebar. This embedded experience provides users with immediate access to data lineage, quality metrics, and governance information without leaving their Tableau workflow. Prerequisites â Before enabling embedded metadata in Tableau: You must have the Atlan browser extension installed. If not, see the How to use the Atlan browser extension guide for instructions. Tableau dashboards must be available through the browser Permissions required â To enable this feature, you need: Admin role in Atlan Access to the Labs settings Enable embedded metadata in Tableau â To enable the embedded Atlan experience in Tableau for your users: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn on View embedded metadata in Tableau Once enabled, users with the extension installed can view Atlan metadata in Tableau dashboards. See also â Troubleshoot the Atlan browser extension Use the Atlan browser extension Tableau connector documentation Tags: integrations tableau browser-extension metadata embedded automation how-to Previous How to use the Atlan browser extension Next Atlan browser extension security Prerequisites Permissions required Enable embedded metadata in Tableau See also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/concepts/atlan-browser-extension-security",
    "text": "Configure Atlan Integrations Automation Browser Extension Concepts Atlan browser extension security On this page Atlan browser extension security Atlan adheres to strict security standards for the browser extension . Atlan mandates security throughout the extension coding lifecycle: Hardening configurations through content security policies, Validating all inputs, Requiring least privileges, Employing defense-in-depth techniques like code obfuscation to frustrate reverse engineering, Accessing customer resources over secure HTTPS channels after SSL certificate verification to prevent tampering. Atlan follows proven CI/CD methodologies used for our SaaS application, enabling rapid and frequent updates to Atlan's Chrome extension. This allows: Patching identified vulnerabilities faster through new releases while simultaneously upholding stability. Mandatory code reviews specifically focused on analyzing security to help with identifying issues before these can impact customers. Once ready, both static and dynamic scanning tools rigorously test the extension codebase for any weaknesses before distribution. Atlan is committed to transparency. If any post-deployment points of concern arise, Atlan will notify impacted customers promptly and address their concerns responsibly. By incorporating security into each phase   -  secure architecture, peer reviews, robust testing, and responsible disclosure   -  Atlan strives to build browser extensions with both user needs and enterprise risks top of mind. Reach out to Atlan support for any questions. Permissions â When using Atlan's browser extension in a supported tool , Atlan will read: the URL of your browser tab Document Object Model (DOM) elements such as asset title, hierarchy information, text, data-test-id attributes, and more to locate an asset in a supported source tool . This list may vary depending on the source tool. If you're using Atlan's browser extension on any website , it will only read the favicon, page title, and URL of your browser tab. Atlan uses the following permissions for the browser extension to work in a supported tool: activeTab -  the activeTab permission allows the browser extension to temporarily access the content of the active tab as you interact with the extension. This enables Atlan to display the Atlan badge and read the URL and DOM elements to locate the asset for displaying asset metadata in the sidebar. storage -  the storage permission allows Atlan to store information about the locally domains configured. This enables the browser extension to remember the sites that you want to use it on, even when you close and reopen your browser. cookies -  the cookies permission allows Atlan to manage cookies for maintaining session states or user preferences for a supported tool. These login cookies are only shared between your Atlan tenant and the browser extension. contextMenus -  the contextMenus permission allows Atlan to add context menu options (for example, right-click menus) to help you interact with the extension's features, namely search in Atlan , directly from any website. host_permissions -  the host permissions allow the browser extension to work specifically with Atlan tenants, which is the host in this case. For example, https://atlan.com/* , https://atlan.dev/* . \"content_scripts\": [ { \"matches\": [\"http://*/*\", \"https://*/*\"] -  the content_scripts key allows Atlan to inject Atlan's content script to any website you visit. Although this content script will be injected into all webpages, it will neither be executed nor any DOM elements captured if the webpage is not a supported tool. Tags: integration connectors security access-control permissions Previous Enable embedded metadata in Tableau Next Troubleshooting Atlan browser extension Permissions"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension",
    "text": "Configure Atlan Integrations Automation Browser Extension Troubleshooting Troubleshooting Atlan browser extension On this page Troubleshooting Atlan browser extension Can I add the browser extension for everyone in my organization? â Yes! To install the Atlan browser extension at the workspace level, follow the instructions in this guide . You will need to be an administrator or have access to the admin console of your organization's Google account for this setup. Once installed, users in your organization can start using Atlan's browser extension . If your organization uses managed browsers, refer to How to configure a managed browser extension . Once you have installed the browser extension for your organization, anyone with access to Atlan and a supported tool can use the browser extension. You will first need to log into Atlan. For users in your organization who do not have access to Atlan and attempt to use the extension, they will receive an error message notifying them that they do not have an Atlan account. Why is the extension not loading? â If a blank page appears while loading the Atlan browser extension or the page is continuously loading, update your browser's security settings to Block third party cookies in Incognito . As long as you have access to the data you are trying to view within Atlan, you should be able to use the browser extension . Why do I get an error message when I click on the extension in Brave? â Users of the Brave browser may receive an error message showing that the content is blocked when they click on the Atlan browser extension. In that case, turning off the Brave Shields feature can help you access the content. Why can't I see the Atlan logo in my data tool? â If your data tool is hosted on a custom domain and you're not seeing the Atlan logo on the bottom right of your BI tool, such as Tableau, you'll need to map your data tool's custom URL in the extension . Tags: integration setup Previous Atlan browser extension security Next Can I add Atlan's browser extension for everyone in my organization?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/faq/add-browser-extension",
    "text": "Configure Atlan Integrations Automation Browser Extension FAQ Can I add Atlan's browser extension for everyone in my organization? Can I add Atlan's browser extension for everyone in my organization? Refer to Troubleshooting the Atlan browser extension . Tags: integration connectors Previous Troubleshooting Atlan browser extension Next Why do I get an error message when I click on Atlan's browser extension?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/connections",
    "text": "Configure Atlan Integrations Automation Connections On this page Connections Integration Overview: Connect Atlan with Connections to create webhooks, automate notifications, and streamline your data workflows. Guides â How to create webhooks : Step-by-step instructions to create webhooks and automate notifications in Atlan. Tags: integrations automation connections webhooks Previous Why is Atlan's browser extension not loading? Next Delete a connection Guides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/webhooks",
    "text": "Configure Atlan Integrations Automation Webhooks On this page Webhooks Integration Overview: Connect Atlan with Webhooks to automate actions, trigger notifications, and integrate with external systems. Guides â How to delete a connection : Step-by-step instructions to delete a connection and its assets using Webhooks in Atlan. Tags: integrations automation webhooks Previous Delete a connection Next Create webhooks Guides"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration",
    "text": "Configure Atlan Integrations Collaboration Collaboration Integrations Integrate Atlan with collaboration tools such as Microsoft Teams and Slack to streamline communication and enhance teamwork around your data assets. Tags: integrations collaboration teams slack Previous Create webhooks Next Microsoft Teams"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication",
    "text": "Configure Atlan Integrations Communication Communication Integrations Integrate Atlan with communication tools such as SMTP and Announcements to automate notifications and keep your teams informed. Tags: integrations communication smtp announcements Previous Send alerts for workflow events Next SMTP and Announcements Integration"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management",
    "text": "Configure Atlan Integrations Identity Management Identity Management Integrations Integrate Atlan with identity management tools such as SCIM and SSO to automate user provisioning, authentication, and access control. Tags: integrations identity management scim sso Previous Manage system announcements Next SCIM Integration"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management",
    "text": "Configure Atlan Integrations Project Management Project Management Integrations Integrate Atlan with popular project management tools such as Jira and ServiceNow. Automate ticket creation, link your accounts, and streamline your data operations with these integrations. Tags: integrations project management jira servicenow Previous Why did my users not receive an invite email from Atlan? Next Jira"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-a-resource",
    "text": "Use data Discovery Asset Management Add a resource Add a resource Need to redirect users to important information that's outside Atlan? You can add links to internal or external URLs wit hin an asset profile. These can help your team better understand the contextual information for your asset. To add resources to your assets, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to open the asset profile. In the navigation bar to the right of the asset profile, click Resources . Click +Add Resource and paste your URL. For Title , type a title for your resource. Your resource is now ready to use! ð Once you've added a resource, click +Add in the Resource menu to add more resources for your asset. You can also edit or delete them to curate your list of resources.Â Did you know? You can also add any web page as a resource to your assets using Atlan's Chrome extension . Tags: atlan documentation Previous Add certificates Next Configure language settings"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/references/what-does-atlan-crawl-from-amazon-quicksight",
    "text": "Connect data BI Tools Cloud-based BI Amazon QuickSight References What does Atlan crawl from Amazon QuickSight? On this page What does Atlan crawl from Amazon QuickSight? Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources: Amazon Athena Amazon Redshift Microsoft SQL Server MySQL PostgreSQL Salesforce Snowflake Atlan crawls and maps the following assets and properties from Amazon QuickSight. Analyses â Atlan maps analyses from Amazon QuickSight to its QuickSightAnalysis asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt status quickSightAnalysisStatus calculatedFields quickSightAnalysisCalculatedFields parameterDeclarations quickSightAnalysisParameterDeclarations filterGroups quickSightAnalysisFilterGroups Dashboards â Atlan maps dashboards from Amazon QuickSight to its QuickSightDashboard asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt publishedVersionNumber quickSightDashboardPublishedVersionNumber lastPublishedTime quickSightDashboardLastPublishedTime analysisName quickSightAnalysisQualifiedName Datasets â Atlan maps datasets from Amazon QuickSight to its QuickSightDataset asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt ImportMode quickSightDatasetImportMode outputColumnCount quickSightDatasetColumnCount Folders â Atlan maps folders from Amazon QuickSight to its QuickSightFolder asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt FolderType quickSightFolderType SharingModel quickSightFolderSharingModel folderHierarchy quickSightFolderHierarchy Dataset fields â Atlan maps dataset fields from Amazon QuickSight to its QuickSightDatasetField asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt type quickSightDatasetFieldType datasetQualifiedName quickSightDatasetQualifiedName Analysis visuals â Atlan maps analysis visuals from Amazon QuickSight to its QuickSightAnalysisVisual asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt Id quickSightId analysisQualifiedName quickSightAnalysisQualifiedName sheetId quickSightSheetId sheetName quickSightSheetName Dashboard visuals â Atlan maps dashboard visuals from Amazon QuickSight to its QuickSightDashboardVisual asset type. Source property Atlan property name name createdAt sourceCreatedAt updatedAt sourceUpdatedAt Id quickSightId dashboardQualifiedName quickSightDashboardQualifiedName sheetId quickSightSheetId sheetName quickSightSheetName Tags: connectors data crawl salesforce Previous Crawl Amazon QuickSight Next Preflight checks for Amazon QuickSight Analyses Dashboards Datasets Folders Dataset fields Analysis visuals Dashboard visuals"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud",
    "text": "Connect data ETL Tools dbt References What does Atlan crawl from dbt Cloud? On this page What does Atlan crawl from dbt Cloud? Atlan crawls and maps the following assets and properties from dbt Cloud. Atlan also supports lineage between the following: dbt models dbt seeds dbt sources SQL tables and views materialized by dbt models, dbt seeds, dbt sources Column-level lineage for these entities warning Atlan only crawls dbt assets that are in the âappliedâ (built) state in dbt Cloud. Models must be part of a successful run to be picked up during crawling; models that are only defined in your project files but havenât been executed wonât be included. For more information about project state, see Project states in dbt Cloud. Once you've crawled dbt , you can use dbt-specific filters for quick asset discovery: Test status -  filter dbt tests that passed, failed, or have a warning or error Alias -  filter by the name of a dbt model's identifier in the dbt project Unique id -  filter by the unique node identifier of a dbt model Project name -  filter by dbt project name, only supported for dbt Core version 1.6+ Environment name -  filter by dbt environment name Job status -  filter by dbt job status Last job run -  filter by the last run of the dbt job Atlan's dbt connectivity also populates custom metadata to further enrich the assets in Atlan. The Atlan dbt-specific property column in the tables below gives the name of the mapped custom metadata property in Atlan. Did you know? Atlan enables you to sync your dbt tags and update your dbt assets with the synced tags. It's also possible to map other metadata on Atlan's assets through your dbt models . Tables â Atlan maps tables from dbt Cloud to its Table asset type. Source property Atlan property Where in Atlan description description asset profile and overview sidebar [collected via REST API] assetDbtTestStatus API only alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar accountName assetDbtAccountName asset filter projectName assetDbtProjectName asset filter and overview sidebar packageName assetDbtPackageName asset filter and properties sidebar tags assetDbtTags asset filter and overview sidebar environment.name (collected via REST API) assetDbtEnvironmentName API only Columns â Atlan maps columns from dbt Cloud to its Column asset type. Source property Atlan property Where in Atlan description description asset profile and overview sidebar [collected via REST API] assetDbtTestStatus API only alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar accountName assetDbtAccountName asset filter projectName assetDbtProjectName asset filter and overview sidebar packageName assetDbtPackageName asset filter and properties sidebar tags assetDbtTags asset filter and overview sidebar Models â Atlan maps models from dbt Cloud to its Model asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar owner sourceCreatedBy asset profile and properties sidebar [dynamically generated using accountId, projectId, and uniqueId] sourceURL overview sidebar alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar accountName assetDbtAccountName asset filter projectName assetDbtProjectName asset filter and overview sidebar packageName assetDbtPackageName asset filter and properties sidebar rawCode (available via REST API) dbtRawSQL overview sidebar compiledCode (available via REST API) dbtCompiledSQL overview sidebar tags assetDbtTags asset filter and overview sidebar materializedType dbtMaterializationType API only stats dbtStats API only executionInfo.lastRunStatus dbtJobRuns.dbtModelRunStatus overview sidebar job.status (available via REST API) dbtJobRuns.dbtJobRunStatus overview sidebar job.name (available via REST API) dbtJobRuns.dbtJobName overview sidebar executionInfo.lastJobDefinitionId dbtJobRuns.dbtJobId overview sidebar executionInfo.lastRunId dbtJobRuns.dbtJobRunId API only executionInfo.lastRunGeneratedAt dbtJobRuns.dbtJobRunCompletedAt overview sidebar environmentId dbtJobRuns.dbtEnvironmentId API only environment.name (available via REST API) dbtJobRuns.dbtEnvironmentName overview sidebar compiledCode dbtJobRuns.dbtCompiledCode API only Sources â Atlan maps sources from dbt Cloud to its DbtSource asset type. Source property Atlan property Where in Atlan name name Asset profile and overview sidebar description description Asset profile and overview sidebar owner sourceCreatedBy Asset profile and properties sidebar [dynamically generated using accountId, projectId, and uniqueId] sourceURL Overview sidebar alias assetDbtAlias Asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId Asset filter and overview sidebar accountName assetDbtAccountName Asset filter projectName assetDbtProjectName Asset filter and overview sidebar packageName assetDbtPackageName Asset filter and properties sidebar tags assetDbtTags Asset filter and overview sidebar stats dbtStats API only freshness assetDbtSourceFreshnessCriteria Overview sidebar environmentId dbtJobRuns.dbtEnvironmentId API only environment.name (available via REST API) dbtJobRuns.dbtEnvironmentName Overview sidebar Tests â Atlan maps tests from dbt Cloud to its Test asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar [dynamically generated using accountId, projectId, and uniqueId] sourceURL overview sidebar alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar account (name) assetDbtAccountName asset filter project (name) assetDbtProjectName asset filter, overview and properties sidebar packageName assetDbtPackageName asset filter and properties sidebar rawCode (available via REST API) dbtTestRawCode overview sidebar compiledCode (available via REST API) dbtTestCompiledCode overview sidebar tags assetDbtTags asset filter and overview sidebar stats dbtStats API only executionInfo.lastRunError dbtTestError overview sidebar executionInfo.lastRunStatus dbtJobRuns.dbtTestRunStatus overview sidebar job.status (available via REST API) dbtJobRuns.dbtJobRunStatus overview sidebar job.name (available via REST API) dbtJobRuns.dbtJobName overview sidebar executionInfo.lastJobDefinitionId dbtJobRuns.dbtJobId overview sidebar executionInfo.lastRunId dbtJobRuns.dbtJobRunId API only executionInfo.lastRunGeneratedAt dbtJobRuns.dbtJobRunCompletedAt overview sidebar environmentId dbtJobRuns.dbtEnvironmentId API only environment.name (available via REST API) dbtJobRuns.dbtEnvironmentName overview sidebar compiledCode dbtJobRuns.dbtCompiledCode API only Seeds â Atlan maps models from dbt Core to its Seed asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar executeCompletedAt sourceUpdatedAt asset profile and properties sidebar owner sourceCreatedBy asset profile and properties sidebar status dbtJobRuns.dbtModelRunStatus overview sidebar Source property Atlan dbt-specific property Where in Atlan alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar stats dbtSeedStats API only filePath dbtSeedfilePath asset profile and overview sidebar Previous Add impact analysis in GitLab Next What does Atlan crawl from dbt Core? Tables Columns Models Sources Tests Seeds"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery References What does Atlan crawl from Google BigQuery? On this page What does Atlan crawl from Google BigQuery? Once you have crawled Google BigQuery , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Tables -  BigQuery labels and Is sharded filters Atlan doesn't run any table scans. Atlan leverages the table preview options from Google BigQuery Â that enable you to view data for free and without affecting any quotas using the tabledata.list API. Hence, table asset previews in Atlan are already cost-optimized. However, this doesn't apply to views and materialized views . For Google BigQuery views and materialized views , Atlan sends you a cost nudge before viewing a sample data preview. This informs you about the precise bytes that are spent during the execution of the query, helping you decide if you still want to run the preview. Did you know? You also receive a cost nudge before querying your Google BigQuery assets . Atlan crawls and maps the following assets and properties from Google BigQuery. Databases â Atlan maps projects from Google BigQuery to its Database asset type. Source property Atlan property Where in Atlan Project ID name asset preview, profile, and filter, overview sidebar Schemas â Atlan maps datasets from Google BigQuery to its Schema asset type. Source property Atlan property Where in Atlan TABLE_SCHEMA name asset preview and profile, overview sidebar TABLE_COUNT tableCount asset preview and profile VIEW_COUNT viewsCount asset preview and profile, overview sidebar TABLE_CATALOG databaseName asset preview REMARKS description asset preview and profile, overview sidebar CREATED sourceCreatedAt asset profile and properties sidebar MODIFIED sourceUpdatedAt asset profile and properties sidebar Tables â Did you know? Table asset previews are already cost-optimized. Google BigQuery enables you to use the table preview options to view data for free and without affecting any quotas. Note that this isn't currently supported for Google BigQuery views and materialized views in Atlan. Atlan maps tables from Google BigQuery to its Table asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview and profile, overview sidebar REMARKS description asset preview and profile, overview sidebar COLUMN_COUNT columnCount asset preview and profile, overview sidebar ROW_COUNT rowCount asset preview, profile, and filter, overview sidebar SIZE_BYTES sizeBytes asset filter and overview sidebar TABLE_TYPE subType asset preview and profile LABELS assetTags overview sidebar CREATED sourceCreatedAt asset profile and properties sidebar MODIFIED sourceUpdatedAt asset profile and properties sidebar OPTION_NAMES (require_partition_filter) isPartitioned API only Views â Atlan maps views from Google BigQuery to its View asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview and profile, overview sidebar REMARKS description asset preview and profile, overview sidebar COLUMN_COUNT columnCount asset preview and profile, overview sidebar TABLE_TYPE subType asset preview and profile CREATED sourceCreatedAt asset profile and properties sidebar MODIFIED sourceUpdatedAt asset profile and properties sidebar OPTION_NAMES (require_partition_filter) isPartitioned API only DDL definition asset profile and overview sidebar Materialized views â Atlan maps materialized views from Google BigQuery to its MaterialisedView asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview and profile, overview sidebar REMARKS description asset preview and profile, overview sidebar COLUMN_COUNT columnCount asset preview and profile, overview sidebar ROW_COUNT rowCount asset preview, profile, and filter, overview sidebar SIZE_BYTES sizeBytes asset filter and overview sidebar TABLE_TYPE subType asset preview and profile CREATED sourceCreatedAt asset profile and properties sidebar MODIFIED sourceUpdatedAt asset profile and properties sidebar OPTION_NAMES (require_partition_filter) isPartitioned API only DDL definition asset profile and overview sidebar Columns â Atlan supports nested columns up to level 1 for Google BigQuery to help you enrich your semi-structured data types. You can view nested columns in the asset sidebar for your table assets. Atlan maps columns from Google BigQuery to its Column asset type. Important Atlan doesn't crawl primary key (PK) and foreign key (FK) information from Google BigQuery. Source property Atlan property Where in Atlan COLUMN_NAME name asset preview and profile, overview sidebar REMARKS, DESCRIPTION description asset preview and profile, overview sidebar ORDINAL_POSITION order asset profile TYPE_NAME dataType asset preview, profile, and filter, overview sidebar IS_NULLABLE isNullable API only IS_PARTITIONING_COLUMN isPartition asset preview, profile, and filter CLUSTERING_COLUMN_LIST isClustered asset preview, profile, and filter Stored procedures â Atlan maps stored procedures from Google BigQuery to its Procedure asset type. Source property Atlan property Where in Atlan PROCEDURE_NAME name API only REMARKS description API only PROCEDURE_TYPE subType API only ROUTINE_DEFINITION definition API only CREATED sourceCreatedAt API only MODIFIED sourceUpdatedAt API only Tags: connectors data crawl Previous Manage Google BigQuery tags Next Preflight checks for Google BigQuery Databases Schemas Tables Views Materialized views Columns Stored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/references/what-does-atlan-crawl-from-ibm-cognos-analytics",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics References What does Atlan crawl from IBM Cognos Analytics? On this page What does Atlan crawl from IBM Cognos Analytics? Atlan crawls and maps the following assets and properties from IBM Cognos Analytics. Atlan also supports lineage: For packages, files, reports, and modules. To upstream sources   -  Microsoft SQL Server and Snowflake. Field-level lineage is currently not supported. Atlan generates the sourceURL property for IBM Cognos Analytics assets using a combination of the host, port, and id of the asset. This allows Atlan to help you view your assets directly in IBM Cognos Analytics from the asset profile. Direct extraction -  in addition to id , Atlan obtains the host and port values from the credentials you provided while setting up a crawler workflow. Offline extraction -  in addition to id , Atlan obtains the host and port values from the parameters with which the offline extractor is executed. Important Assets marked with ð includes lineage and column-level lineage. Assets marked with ð display column information. Folders â Atlan maps folders from IBM Cognos Analytics to its CognosFolder asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners Dashboards ð â Atlan maps dashboards from IBM Cognos Analytics to its CognosDashboard asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners Packages ð â Atlan maps packages from IBM Cognos Analytics to its CognosPackage asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners Explorations ð â Atlan maps explorations from IBM Cognos Analytics to its CognosExploration asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners Reports ð â Atlan maps reports from IBM Cognos Analytics to its CognosReport asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners Files ð â Atlan maps files from IBM Cognos Analytics to its CognosFile asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners Data sources â Atlan maps data sources from IBM Cognos Analytics to its CognosDatasource asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion hidden cognosIsHidden creationTime sourceCreatedAt modificationTime sourceUpdatedAt owner sourceOwners connectionString cognosDatasourceConnectionString Modules ð â Atlan maps modules from IBM Cognos Analytics to its CognosModule asset type. Source property Atlan property defaultName name defaultDescription description id cognosId searchPath cognosPath type cognosType version cognosVersion modificationTime sourceUpdatedAt Columns â Atlan maps fields from IBM Cognos Analytics to its CognosColumns asset type. Based on the asset type, some attributes may not be extracted: cognosColumnRegularAggregate appears only for reports and datasets. cognosColumnDatatype appears only for modules. Source property Atlan property label name datatype cognosDatatype regularAggregate cognosRegularAggregate Tags: connectors crawl Previous Crawl on-premises IBM Cognos Analytics Next Troubleshooting IBM Cognos Analytics connectivity Folders Dashboards ð Packages ð Explorations ð Reports ð Files ð Data sources Modules ð Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/references/what-does-atlan-crawl-from-looker",
    "text": "Connect data BI Tools Cloud-based BI Looker References What does Atlan crawl from Looker? On this page What does Atlan crawl from Looker? Atlan crawls and maps the following assets and properties from Looker. Atlan also supports the following lineage: Asset-level lineage for views, models, looks, dashboards, tiles, and explores. Field-level lineage for views, explores, looks, tiles, and dashboards. Lineage between explore fields and dashboards. This allows you to view all the fields used in a given dashboard and trace their upstream lineage to SQL columns. Cross-project lineage for Looker assets. For example, if an explore includes a view from an imported project, Atlan will parse project manifest files to generate lineage. Looker refinements for views and explores. Atlan will parse project manifest files to generate lineage. Refined fields for views and explores are displayed with a Refinement label in Atlan. danger Currently Atlan only represents the assets marked with ð in lineage. Connections â Atlan maps connections from Looker to its Connection asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar host host API only port port API only database database API only schema schema API only dialect_name dialect_name API only created_at sourceCreatedAt asset profile and properties sidebar Projects â Atlan maps projects from Looker to its LookerProject asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar Views ð â Atlan maps views from Looker to its LookerView asset type. To trace the upstream lineage of these views, Atlan currently supports SQL-based derived tables . Persistent derived tables (PDTs) and Liquid parameterized tables are currently not supported. However, Atlan will always catalog the associated views. Atlan also supports view refinements . Atlan includes the fields from refinements in the parent view asset, and marks the fields with a Refinement label. You can hover over the label to view the file path and line number where the refinement is defined. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar project_name projectName API only Models ð â Atlan maps models from Looker to its LookerModel asset type. Source property Atlan property Where in Atlan label name asset profile and overview sidebar project_name projectName API only Project not found certificateStatus (DEPRECATED) asset profile and overview sidebar Project not found certificateStatusMessage (\"Project attached to this model was not found by the workflow in Looker.\") asset profile and overview sidebar Folders â Atlan maps folders from Looker to its LookerFolder asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar content_metadata_id sourceContentMetadataId properties sidebar creator_id sourceCreatorId API only child_count sourceChildCount asset preview and profile parent_id sourceParentID API only created_at sourceCreatedAt asset profile and properties sidebar Fields ð â For explores â Atlan maps fields for explores from Looker to its LookerField asset type. Source property Atlan property Where in Atlan label name asset profile and overview sidebar description description asset profile and overview sidebar category subType asset preview and profile project_name projectName API only model_name modelName API only parsed from LookML files lookerFieldIsRefined asset preview and profile, overview sidebar parsed from LookML files lookerFieldRefinementFilePath asset preview and profile, overview sidebar parsed from LookML files lookerFieldRefinementLineNumber asset preview and profile, overview sidebar For views â Atlan maps fields for views from Looker to its LookerField asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar category subType asset preview and profile project_name projectName API only parsed from LookML files lookerFieldIsRefined asset preview and profile, overview sidebar parsed from LookML files lookerFieldRefinementFilePath asset preview and profile, overview sidebar parsed from LookML files lookerFieldRefinementLineNumber asset preview and profile, overview sidebar For looks â Atlan maps fields for looks from Looker to its LookerField asset type. Source property Atlan property Where in Atlan name name asset preview, profile, and filter, and overview sidebar look look API only For tiles â Atlan maps fields for tiles from Looker to its LookerField asset type. Source property Atlan property Where in Atlan name name asset preview, profile, and filter, and overview sidebar tile tile API only For dashboards â Atlan maps fields for dashboards from Looker to its LookerField asset type. Source property Atlan property Where in Atlan name name asset preview, profile, and filter, and overview sidebar dashboard dashboard API only Looks ð â Atlan maps looks from Looker to its LookerLook asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar description description asset profile and overview sidebar folder_name folderName API only user_id sourceUserId API only view_count sourceViewCount asset preview and profile, overview sidebar last_updater_id sourceLastUpdaterId API only last_updater_name sourceUpdatedBy asset profile and properties sidebar user_name sourceOwners asset profile and properties sidebar content_metadata_id sourceContentMetadataId properties sidebar query_id lookerSourceQueryId API only last_accessed_at sourceLastAccessedAt API only last_viewed_at sourceLastViewedAt API only created_at sourceCreatedAt asset profile and properties sidebar updated_at sourceUpdatedAt asset profile and properties sidebar Dashboards ð â Atlan maps dashboards from Looker to its LookerDashboard asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar description description asset profile and overview sidebar slug lookerSlug API only folder_name folderName API only user_id sourceUserId API only view_count sourceViewCount asset preview and profile, overview sidebar last_updater_id sourceLastUpdaterId API only last_updater_name sourceUpdatedBy asset profile and properties sidebar user_name sourceOwners asset profile and properties sidebar content_metadata_id sourceMetadataId properties sidebar last_accessed_at sourceLastAccessedAt API only last_viewed_at sourceLastViewedAt API only created_at sourceCreatedAt asset profile and properties sidebar updated_at sourceUpdatedAt asset profile and properties sidebar Tiles ð â Atlan maps tiles from Looker to its LookerTile asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar title of Look Deleted certificateStatus (DEPRECATED) asset profile and overview sidebar title of Look Deleted certificateStatusMessage (\"Look attached to this tile has been deleted. This tile might be unusable.\") asset profile and overview sidebar body_text description asset profile and overview sidebar lookml_link_id lookml_link_id API only merge_result_id merge_result_id API only note_text noteText overview sidebar query_id lookerQueryID API only result_maker_id resultMakerID properties sidebar look_id lookId API only subtitle_text subtitleText overview sidebar type subType asset preview and profile Explores ð â Atlan maps explores from Looker to its LookerExplore asset type. Atlan also supports explore refinements : For explores defined in the same model, Atlan includes the fields from refinements in the parent explore asset. For explores with the same name that are defined in a different model, Atlan will create a new explore asset. In both cases, Atlan marks the fields with a Refinement label. You can hover over the label to view the file path and line number where the refinement is defined. Source property Atlan property Where in Atlan title , name , or id name asset profile and overview sidebar description description asset profile and overview sidebar model_name modelName API only connection_name sourceConnectionName API only user_name sourceOwners asset profile and properties sidebar view_name viewName asset profile and properties sidebar sql_table_name sqlTableName API only project_name projectName API only Tags: crawl model Previous Crawl on-premises Looker Next Preflight checks for Looker Connections Projects Views ð Models ð Folders Fields ð Looks ð Dashboards ð Tiles ð Explores ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/references/what-does-atlan-crawl-from-mode",
    "text": "Connect data BI Tools Cloud-based BI Mode References What does Atlan crawl from Mode? On this page What does Atlan crawl from Mode? Atlan crawls and maps the following assets and properties from Mode. Workspaces â Atlan maps workspaces from Mode to its ModeWorkspace asset type. Source property Atlan property name name id modeId token modeToken username modeWorkspaceUsername space_count modeCollectionCount _links.web.href sourceURL created_at sourceCreatedAt Collections â Atlan maps collections from Mode to its ModeCollection asset type. Source property Atlan property name name description description id modeId token modeToken space_type modeCollectionType state modeCollectionState _links.creator sourceCreatedBy _links.web sourceURL extras.workspace.name modeWorkspaceName Array of reports modeReports Reports â Atlan maps reports from Mode to its ModeReport asset type. Source property Atlan property name name description description id modeId token modeToken created_at sourceCreatedAt updated_at sourceUpdatedAt _links.creator sourceCreatedBy _links.web sourceURL space_token modeCollectionToken account_username modeWorkspaceUsername published_at modeReportPublishedAt query_count modeQueryCount chart_count modeChartCount query_preview modeQueryPreview public modeIsPublic shared modeIsShared view_count popularityScore archived modeIsArchived Array of collections modeCollections Queries â Atlan maps queries from Mode to its ModeQuery asset type. Source property Atlan property name name id modeId token modeToken created_at sourceCreatedAt updated_at sourceUpdatedAt _links.creator.href sourceCreatedBy raw_query modeRawQuery explorations_count modeExplorationCount report_imports_count modeReportImportCount data_source_id modeDatasourceId datasource.name or extras.datasource.name modeDatasourceName extras.report.name modeReportName Charts â Atlan maps charts from Mode to its ModeChart asset type. Source property Atlan property name name description description token modeToken view_vegas.chartType modeChartType created_at sourceCreatedAt updated_at sourceUpdatedAt _links.creator.href sourceCreatedBy _links.report_viz_web.href sourceURL extras.query.name modeQueryName Tags: integration connectors Previous Crawl Mode Next Preflight checks for Mode Workspaces Collections Reports Queries Charts"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/references/what-does-atlan-crawl-from-qlik-sense-cloud",
    "text": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud References What does Atlan crawl from Qlik Sense Cloud? On this page What does Atlan crawl from Qlik Sense Cloud? Atlan crawls and maps the following assets and properties from Qlik Sense Cloud. Once you've crawled Qlik Sense Cloud , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Apps Sheets Datasets Lineage â Atlan only supports asset-level lineage for the following asset types: Datasets --> Charts --> Sheets --> Apps Spaces â Atlan maps spaces from Qlik Sense Cloud to its QlikSpace asset type. Source property Atlan property type qlikSpaceType ownerId qlikOwnerId id qlikId createdAt sourceCreatedAt updatedAt sourceUpdatedAt Apps â Atlan maps apps from Qlik Sense Cloud to its QlikApp asset type. !important Only the app resource type is retrieved. Other types, such as qvapp or qlikview , are not crawled. Source property Atlan property attributes.name name attributes.description description attributes.resourceId qlikId static_byte_size qlikAppStaticByteSize attributes.spaceId qlikSpaceId attributes.resourceCreatedAt sourceCreatedAt attributes.resourceUpdatedAt sourceUpdatedAt attributes.ownerId qlikOwnerId attributes.resourceAttributes.originAppId qlikOriginAppId attributes.resourceAttributes.hasSectionAccess qlikHasSectionAccess attributes.resourceAttributes.directQueryMode qlikIsDirectQueryMode attributes.resourceAttributes.published qlikIsPublished Sheets â Atlan maps sheets from Qlik Sense Cloud to its QlikSheet asset type. Source property Atlan property qProperty.qMetaDef.title name qProperty.qMetaDef.description description qProperty.qInfo.qId qlikId spaceId qlikSpaceId appId qlikAppId approved qlikIsApproved published qlikIsPublished Charts â Atlan maps charts from Qlik Sense Cloud to the QlikChart asset type and catalogs only those linked to dataset fields. For example, table charts are crawled because their columns represent dataset dimensions or measures. UI elements that do not reference dataset fields  - such as filters, buttons, and text elements  - are ignored. danger These elements are not considered charts and are not crawled: filterpane , qlik-button-for-navigation , VizlibAdvancedTextObject , listbox , action-button , VizlibFilter , variable , text-image , VizlibLineObject . Source property Atlan property qProperty.qInfo.qId qlikId qProperty.subtitle qlikChartSubtitle qProperty.footnote qlikChartFootnote qProperty.qInfo.qType qlikChartType qProperty.options.dimensionsOrientation qlikChartOrientation Datasets â Atlan maps datasets from Qlik Sense Cloud to the QlikDataset asset type. Datasets loaded through the Data Load Editor are called implicit datasets in Atlan and appear under this type. Source property Atlan property id qlikId resourceAttributes.technicalName qlikDatasetTechnicalName resourceAttributes.dataStoreType qlikDatasetType resourceAttributes.uri qlikDatasetUri resourceAttributes.secureQri qlikQRI resourceSubType qlikDatasetSubtype ownerId qlikOwnerId resourceCreatedAt sourceCreatedAt resourceUpdatedAt sourceUpdatedAt spaceId qlikSpaceId Tags: connectors data crawl Previous Crawl Qlik Sense Cloud Next Preflight checks for Qlik Sense Cloud Lineage Spaces Apps Sheets Charts Datasets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/references/what-does-atlan-crawl-from-redash",
    "text": "Connect data BI Tools On-premises & Enterprise BI Redash References What does Atlan crawl from Redash? On this page What does Atlan crawl from Redash? Atlan crawls and maps the following assets and properties from Redash. Once you've crawled Redash , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Queries -  Schedule, Is Published, and Redash tags filters Visualizations -  Type filter Dashboards -  Redash tags filter danger Currently, Atlan only represents the assets marked with ð in lineage. Queries ð â Atlan maps queries from Redash to its RedashQuery asset type. Source property Atlan property name name created_at sourceCreatedAt updated_at sourceUpdatedAt query query Dashboards ð â Atlan maps dashboards from Redash to its RedashDashboard asset type. Source property Atlan property widgets widget_count Visualizations ð â Atlan maps visualization elements from Redash to its RedashVisualization asset type. Source property Atlan property name name type type Tags: connectors crawl Previous Crawl Redash Next Preflight checks for Redash Queries ð Dashboards ð Visualizations ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce",
    "text": "Connect data CRM Salesforce References What does Atlan crawl from Salesforce? On this page What does Atlan crawl from Salesforce? Atlan only performs GET requests on these five endpoints: sObject Basic Information Query Reports Dashboards Folders Did you know? Each endpoint will be set in its own OAuth client session. For every API request, it will hit the Salesforce login endpoint, which means there will be at least five (same as the number of endpoints above) login entries in your Salesforce account's login history within the duration of the scheduled workflow run. Atlan crawls and maps the following assets and properties from Salesforce. Once you've crawled Salesforce , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Fields -  Is encrypted and Is required filters Organizations â Atlan maps organizations from Salesforce to its SalesforceOrganization asset type. Source property Atlan property Where in Atlan id sourceId overview sidebar name name asset preview and profile, overview sidebar description description asset preview and profile, overview sidebar webUrl sourceURL overview sidebar createdDate sourceCreatedAt asset profile and properties sidebar lastModifiedDate sourceUpdatedAt asset profile and properties sidebar createdBy sourceCreatedBy asset profile and properties sidebar lastModifiedBy sourceUpdatedBy asset profile and properties sidebar Objects â Atlan maps objects from Salesforce to its SalesforceObject asset type. Source property Atlan property Where in Atlan label name asset preview and profile, overview sidebar name apiName overview sidebar description description asset preview and profile, overview sidebar custom isCustom asset preview and profile, overview sidebar mergable isMergable API only queryable isQueryable API only fieldCount fieldCount asset preview and overview sidebar webUrl sourceURL overview sidebar lastModifiedDate sourceUpdatedAt asset profile and properties sidebar lastModifiedBy sourceUpdatedBy asset profile and properties sidebar Fields â Atlan maps fields from Salesforce to its SalesforceField asset type. Source property Atlan property Where in Atlan label name asset preview and profile, overview sidebar name apiName overview sidebar type dataType asset preview, profile, and filter, overview sidebar description description asset preview and profile, overview sidebar lastModifiedDate sourceUpdatedAt asset profile and properties sidebar lastModifiedBy sourceUpdatedBy asset profile and properties sidebar calculated isCalculated API only calculatedFormula formula overview sidebar defaultValue defaultValue API only caseSensitive isCaseSensitive API only custom isCustom asset preview and profile, overview sidebar encrypted isEncrypted API only nillable isNullable overview sidebar ( Empty values allowed field) polymorphicForeignKey isPolymorphicForeignKey API only order order API only length maxLength properties sidebar precision precision properties sidebar scale numericScale API only unique isUnique API only inlineHelpText inlineHelpText overview sidebar picklistValues picklistValues overview sidebar Reports â Atlan maps reports from Salesforce to its SalesforceReport asset type. Source property Atlan property Where in Atlan id sourceId overview sidebar name name asset preview and profile, overview sidebar reportType reportType overview sidebar detailColumns detailColumns overview sidebar description description asset preview and profile, overview sidebar webUrl sourceURL overview sidebar createdDate sourceCreatedAt asset profile and properties sidebar lastModifiedDate sourceUpdatedAt asset profile and properties sidebar createdBy sourceCreatedBy asset profile and properties sidebar lastModifiedBy sourceUpdatedBy asset profile and properties sidebar Dashboards â Atlan maps dashboards from Salesforce to its SalesforceDashboard asset type. Source property Atlan property Where in Atlan id sourceId overview sidebar name name asset preview and profile, overview sidebar dashboardType dashboardType overview sidebar reportCount reportCount asset preview and profile description description asset preview and profile, overview sidebar webUrl sourceURL overview sidebar Tags: crawl salesforce api Previous Crawl Salesforce Next Preflight checks for Salesforce Organizations Objects Fields Reports Dashboards"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/references/what-does-atlan-crawl-from-sigma",
    "text": "Connect data BI Tools Cloud-based BI Sigma References What does Atlan crawl from Sigma? On this page What does Atlan crawl from Sigma? Atlan crawls and maps the following assets and properties from Sigma. danger Currently, Atlan only represents the assets marked with ð in lineage. For your Sigma workbooks , Atlan also provides asset previews to help with quick discovery and give you the context you need. Workbooks ð â Atlan maps workbooks from Sigma to its SigmaWorkbook asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar url sourceURL overview sidebar Pages ð â Atlan maps pages from Sigma to its SigmaPage asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar Data elements ð â Atlan maps table, pivot table, and visualization elements from Sigma to its SigmaDataElement asset type. Source property Atlan property Where in Atlan displayName name asset profile and overview sidebar type type overview sidebar Data element fields ð â Atlan maps table, pivot table, and visualization element fields from Sigma to its SigmaDataElementField asset type. Source property Atlan property Where in Atlan displayName name asset profile and overview sidebar Datasets â Atlan maps datasets from Sigma to its SigmaDataset asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar url sourceURL overview sidebar Tags: lineage data-lineage impact-analysis Previous Crawl Sigma Next Preflight checks for Sigma Workbooks ð Pages ð Data elements ð Data element fields ð Datasets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/what-does-atlan-crawl-from-snowflake",
    "text": "Connect data Data Warehouses Snowflake References What does Atlan crawl from Snowflake? On this page What does Atlan crawl from Snowflake? Atlan crawls and maps the following assets and properties from Snowflake. Once you've crawled Snowflake , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for Snowflake assets: Streams -  Source type and Stale filters Functions -  Language, Function type, Is secure, and Is external filters Snowflake tags and tag values Lineage â Atlan supports lineage for the following asset types: External Named Stages â Table Pipe â Table External Table Iceberg Table Internal Named Stages â Table Pipe â Table (auto-ingest not recommended) Not supported for External or Iceberg Tables Databases â Atlan maps databases from Snowflake to its Database asset type. Source property Atlan property Where in Atlan DATABASES.DATABASE_NAME name asset profile and overview sidebar DATABASE.COMMENT description asset profile and overview sidebar SCHEMATA (count) schemaCount asset preview and profile DATABASES.DATABASE_OWNER Created (in Snowflake) properties sidebar DATABASES.CREATED sourceCreatedAt properties sidebar DATABASES.LAST_ALTERED sourceUpdatedAt properties sidebar Schemas â Atlan maps schemas from Snowflake to its Schema asset type. Source property Atlan property Where in Atlan SCHEMATA.SCHEMA_NAME name asset profile and overview sidebar SCHEMA.COMMENT description asset profile and overview sidebar TABLES of type %TABLE% (count) tableCount asset preview and profile TABLES of type %VIEW% (count) viewsCount asset preview and profile SCHEMATA.CATALOG_NAME databaseName asset preview and profile SCHEMATA.SCHEMA_OWNER Created (in Snowflake) properties sidebar SCHEMATA.CREATED sourceCreatedAt properties sidebar SCHEMATA.LAST_ALTERED sourceUpdatedAt properties sidebar Tables â Atlan maps tables from Snowflake to its Table asset type. Source property Atlan property Where in Atlan TABLES.TABLE_NAME name asset profile and overview sidebar TABLES.COMMENT description asset profile and overview sidebar COLUMNS (count) columnCount asset preview and profile, overview sidebar TABLES.ROW_COUNT rowCount asset preview and profile, overview sidebar TABLES.BYTES sizeBytes asset filter and overview sidebar TABLES.TABLE_OWNER Created (in Snowflake) properties sidebar TABLES.CREATED sourceCreatedAt properties sidebar TABLES.LAST_ALTERED sourceUpdatedAt properties sidebar For Iceberg tables â In addition to the table metadata above, Atlan supports additional metadata for Iceberg tables crawled from Snowflake. Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. Source property Atlan property Where in Atlan TABLES.IS_ICEBERG tableType asset preview, profile, and filter, and overview sidebar ICEBERG_TABLES.CATALOG_NAME icebergCatalogName overview sidebar ICEBERG_TABLES.ICEBERG_TABLE_TYPE icebergTableType overview sidebar CATALOG_INTEGRATION.CATALOG_SOURCE icebergCatalogSource overview sidebar ICEBERG_TABLES.CATALOG_TABLE_NAME icebergCatalogTableName overview sidebar ICEBERG_TABLES.CATALOG_NAMESPACE icebergCatalogTableNamespace overview sidebar ICEBERG_TABLES.EXTERNAL_VOLUME_NAME tableExternalVolumeName overview sidebar ICEBERG_TABLES.BASE_LOCATION icebergTableBaseLocation overview sidebar TABLES.RETENTION_TIME tableRetentionTime overview sidebar Views â Atlan maps views from Snowflake to its View asset type. Source property Atlan property Where in Atlan TABLES.TABLE_NAME name asset profile and overview sidebar TABLES.COMMENT description asset profile and overview sidebar COLUMNS (count) columnCount asset preview and profile, overview sidebar VIEWS.VIEW_DEFINITION definition asset profile and overview sidebar TABLES.TABLE_OWNER Created (in Snowflake) properties sidebar TABLES.CREATED sourceCreatedAt properties sidebar TABLES.LAST_ALTERED sourceUpdatedAt properties sidebar Materialized views â Atlan maps materialized views from Snowflake to its MaterialisedView asset type. Source property Atlan property Where in Atlan TABLES.TABLE_NAME name asset profile and overview sidebar TABLES.COMMENT description asset profile and overview sidebar COLUMNS (count) columnCount asset preview and profile, overview sidebar TABLES.ROW_COUNT rowCount asset preview and profile, overview sidebar TABLES.BYTES sizeBytes asset filter and overview sidebar VIEWS.VIEW_DEFINITION definition asset profile and overview sidebar TABLES.TABLE_OWNER Created (in Snowflake) properties sidebar TABLES.CREATED sourceCreatedAt properties sidebar TABLES.LAST_ALTERED sourceUpdatedAt properties sidebar External tables â Atlan maps external tables from Snowflake to its Table asset type. Source property Atlan property Where in Atlan TABLES.TABLE_NAME name asset profile and overview sidebar TABLES.COMMENT description asset profile and overview sidebar COLUMNS (count) columnCount asset preview and profile, overview sidebar TABLES.ROW_COUNT rowCount asset preview and profile, overview sidebar TABLES.BYTES sizeBytes asset filter and overview sidebar EXTERNAL_TABLES.LOCATION externalLocation overview sidebar STAGES.STAGE_REGION externalLocationRegion API only EXTERNAL_TABLES.FILE_FORMAT_TYPE externalLocationFormat overview sidebar TABLES.TABLE_OWNER Created (in Snowflake) properties sidebar TABLES.CREATED sourceCreatedAt properties sidebar TABLES.LAST_ALTERED sourceUpdatedAt properties sidebar Columns â Atlan maps columns from Snowflake to its Column asset type. Source property Atlan property Where in Atlan COLUMNS.COLUMN_NAME name asset profile and overview sidebar COLUMNS.COMMENT description asset profile and overview sidebar COLUMNS.ORDINAL_POSITION order API only COLUMNS.DATA_TYPE dataType asset filter, preview, and profile, overview sidebar PRIMARY KEY isPrimary asset preview and filter, overview sidebar COLUMNS.IS_NULLABLE isNullable overview sidebar COLUMNS.CHARACTER_MAXIMUM_LENGTH maxLength properties sidebar COLUMNS.NUMERIC_PRECISION precision properties sidebar Stages â Atlan maps stages from Snowflake to its Stage asset type. Source property Atlan property Where in Atlan Stages.STAGE_NAME name asset profile and overview sidebar Stages.COMMENT description asset profile and overview sidebar Stages.STAGE_SCHEMA schemaName asset profile and overview sidebar Stages.STAGE_CATALOG databaseName asset profile and overview sidebar Stages.STAGE_URL snowflakeStageExternalLocation asset profile and overview sidebar Stages.STAGE_REGION snowflakeStageExternalLocationRegion asset profile and overview sidebar Stages.STAGE_TYPE snowflakeStageType asset profile and overview sidebar Stages.STAGE_OWNER sourceOwners overview sidebar Stages.CREATED sourceCreatedAt properties sidebar Stages.LAST_ALTERED sourceUpdatedAt properties sidebar Stages.STORAGE_INTEGRATION snowflakeStageStorageIntegration asset profile and overview sidebar Streams â Atlan maps streams from Snowflake to its Stream asset type. Source property Atlan property Where in Atlan STREAMS.NAME name asset profile and overview sidebar STREAMS.COMMENT description asset profile and overview sidebar STREAMS.OWNER sourceOwners asset preview and profile, properties sidebar STREAMS.DATABASE_NAME databaseName asset preview and profile STREAMS.SCHEMA_NAME schemaName asset preview and profile STREAMS.SOURCE_TYPE snowflakeStreamSourceType asset filter and overview sidebar STREAMS.STALE snowflakeStreamIsStale asset filter and overview sidebar STREAMS.MODE snowflakeStreamMode overview sidebar STREAMS.STALE_AFTER snowflakeStreamStaleAfter overview sidebar Pipes â Atlan maps pipes from Snowflake to its Pipe asset type. Source property Atlan property Where in Atlan PIPES.PIPE_NAME name asset profile and overview sidebar PIPES.COMMENT description asset profile and overview sidebar PIPES.DEFINITION definition asset profile and overview sidebar PIPES.PIPE_OWNER sourceOwners asset preview and profile, properties sidebar PIPES.PIPE_CATALOG databaseName asset preview and profile PIPES.PIPE_SCHEMA schemaName asset preview and profile PIPES.IS_AUTOINGEST_ENABLED snowflakePipeIsAutoIngestEnabled overview sidebar PIPES.NOTIFICATION_CHANNEL_NAME snowflakePipeNotificationChannelName overview sidebar User-defined functions â Atlan maps user-defined functions (UDFs) from Snowflake to its Function asset type. Atlan currently does not support lineage for user-defined functions from Snowflake. Source property Atlan property Where in Atlan NAME name asset profile and overview sidebar FUNCTION_DEFINITION functionDefinition overview sidebar COMMENT description asset profile and overview sidebar FUNCTION_CATALOG databaseName asset preview and profile FUNCTION_SCHEMA schemaName asset preview and profile FUNCTION_OWNER Created (in Snowflake) properties sidebar CREATED sourceCreatedAt properties sidebar LAST_ALTERED sourceUpdatedAt properties sidebar FUNCTION_LANGUAGE functionLanguage overview sidebar DATA_TYPE functionReturnType API only IS_SECURE functionIsSecure asset filter and properties sidebar IS_EXTERNAL functionIsExternal asset filter and properties sidebar IS_MEMOIZABLE functionIsMemoizable API only ARGUMENT_SIGNATURE functionArguments API only Dynamic tables â Atlan maps dynamic tables from Snowflake to its DynamicTable asset type. Source property Atlan property Where in Atlan TABLES.TABLE_NAME name asset profile and overview sidebar TABLES.COMMENT description asset profile and overview sidebar COLUMNS (count) columnCount asset preview and profile, overview sidebar TABLES.DEFINITION definition asset profile and overview sidebar TABLES.ROW_COUNT rowCount asset preview and profile, overview sidebar TABLES.BYTES sizeBytes asset filter and overview sidebar TABLES.TABLE_OWNER Created (in Snowflake) properties sidebar TABLES.CREATED sourceCreatedAt properties sidebar TABLES.LAST_ALTERED sourceUpdatedAt properties sidebar Tags: connectors data crawl Previous Multiple tag values and concatenation Next Preflight checks for Snowflake Lineage Databases Schemas Tables Views Materialized views External tables Columns Stages Streams Pipes User-defined functions Dynamic tables"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-from-tableau",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau References What does Atlan crawl from Tableau? On this page What does Atlan crawl from Tableau? Atlan crawls and maps the following assets and properties from Tableau. Once you've crawled Tableau , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Projects   -  filter Tableau assets by projects, including nested projects Data sources   -  Is published filter For your Tableau worksheets and dashboards , Atlan also provides asset previews to help with quick discovery and give you the context you need. warning You may need to disable clickjack protection for Tableau asset previews to load. Lineage â info Did you know? Lineage to dashboards may appear incomplete or missing if worksheets are not crawled. Additionally, Tableau assets that haven't been refreshed since May 27, 2025, won't display the new column-level lineage (CLL) or updated lineage paths. Atlan supports lineage for the following: Asset Lineage - Datasource to Dashboard, Datasource to Worksheet, Datasource to Workbook Column Level Lineage - Supported for Datasource to Worksheet and Worksheet to Dashboard Sites â Atlan maps sites from Tableau to its TableauSite asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar Projects â Atlan maps projects from Tableau to its TableauProject asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar owner [sourceOwner](/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity) asset profile and overview sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar hierarchy projectHierarchy asset preview and profile, overview sidebar topLevelProject isTopLevelProject API only Flows â warning Due to limitations at source, Atlan won't be able to crawl Tableau flows if you use the JWT bearer authentication method. Atlan maps flows from Tableau to its TableauFlow asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar owner [sourceOwner](/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity) asset profile and overview sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Metrics â warning Tableau has retired metrics methods in API 3.22 for Tableau Cloud and Tableau Server version 2024.2. If you're using Tableau API version 3.22 or higher, metadata for metrics is unavailable in Atlan. Atlan maps metrics from Tableau to its TableauMetric asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Workbooks â Atlan maps workbooks from Tableau to its TableauWorkbook asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar webpageUrl sourceURL overview sidebar owner [sourceOwner](/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity) asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Worksheets â Atlan maps worksheets from Tableau to its TableauWorksheet asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar source_url sourceURL overview sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Dashboards â Atlan maps dashboards from Tableau to its TableauDashboard asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar createdAt sourceCreatedAt asset profile and properties sidebar updatedAt sourceUpdatedAt asset profile and properties sidebar source_url sourceURL overview sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Data sources â Atlan maps data sources (embedded and published) from Tableau to its TableauDatasource asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar owner sourceOwner (for published data sources only) asset profile and overview sidebar isPublished isPublished asset preview and overview sidebar hasExtracts hasExtracts API only upstreamTables upstreamTables API only upstreamDatabases upstreamDatabases API only isCertified certificateStatus ( VERIFIED ) asset preview and filter, overview sidebar certifier certifier API only certificationNote certificationStatusMessage API only certifierDisplayName certificateUpdatedBy asset preview and overview sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Data source fields â Atlan maps data source fields and column fields from Tableau to its TableauDatasourceField asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar upstreamTables upstreamTables API only upstreamColumns upstreamColumns API only dataCategory tableauDatasourceFieldDataCategory API only role tableauDatasourceFieldRole API only dataType tableauDatasourceFieldDataType API only formula tableauDatasourceFieldFormula API only binSize tableauDatasourceFieldBinSize API only __typename datasourceFieldType API only project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Custom SQL â Atlan parses custom SQL queries used in Tableau data sources to extract lineage information. This process identifies the relationships between data assets based on the SQL logic defined within Tableau. Source property Atlan property Where in Atlan downstreamDatasources TableauDatasource Used to define downstream impact of lineage query CustomSQLQuery Used to form lineage from source Calculated fields â Atlan maps calculated fields from Tableau to its TableauCalculatedField asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar dataCategory dataCategory API only role role API only dataType tableauDataType asset preview, filter, and profile, overview sidebar formula formula overview sidebar project_extra (hierarchy) projectHierarchy asset preview and profile, overview sidebar Lineage â Atlan calculates lineage for Tableau as follows: Source object Tableau object Tableau object (downstream) Table Published data source Published data source Table Published data source Embedded data source Table Embedded data source Column Data source field Calculated field Column Data source field Data source field info Lineage is currently not supported for Tableau flows and metrics . Tags: connectors data crawl Previous Crawl on-premises Tableau Next Preflight checks for Tableau Lineage Sites Projects Flows Metrics Workbooks Worksheets Dashboards Data sources Data source fields Custom SQL Calculated fields Lineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/references/what-does-atlan-crawl-from-thoughtspot",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot References What does Atlan crawl from ThoughtSpot? On this page What does Atlan crawl from ThoughtSpot? Once you've crawled ThoughtSpot , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for all ThoughtSpot assets: Tags and chart type filters Atlan supports lineage for the following ThoughtSpot assets: Answers -  upstream lineage to tables, views, or worksheets from multiple sources (if applicable), no downstream lineage Visualizations -  upstream lineage to tables, views, or worksheets from multiple sources (if applicable) Liveboards -  upstream lineage to visualizations Tables -  upstream lineage to source tables, and column-level lineage between ThoughtSpot tables and worksheets Views -  upstream lineage to ThoughtSpot tables or worksheets, and column-level lineage between ThoughtSpot views and worksheets Worksheets -  upstream lineage to ThoughtSpot tables or views from multiple sources (if applicable) Atlan crawls and maps the following assets and properties from ThoughtSpot. danger Currently, Atlan only represents the assets marked with ð in lineage. Answers ð â Atlan maps answers from ThoughtSpot to its ThoughtSpotAnswer asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_header.created sourceCreatedAt asset profile and properties sidebar metadata_header.modified sourceUpdatedAt asset profile and properties sidebar metadata_header.authorDisplayName sourceCreatedBy asset profile and properties sidebar question.text thoughtspotQuestionText properties sidebar metadata_header.tags assetTags asset filter and overview sidebar visualisations.chart_type thoughtspotChartType asset filter and profile, overview sidebar Visualizations ð â Atlan maps visualizations from ThoughtSpot to its ThoughtspotDashlet asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_header.created sourceCreatedAt asset profile and properties sidebar metadata_header.modified sourceUpdatedAt asset profile and properties sidebar metadata_header.authorDisplayName sourceCreatedBy asset profile and properties sidebar question.text thoughtspotQuestionText properties sidebar visualisations.chart_type thoughtspotChartType asset filter and profile, overview sidebar metadata_header.name thoughtspotLiveboardName API only Liveboards ð â Atlan maps Liveboards from ThoughtSpot to its ThoughtspotLiveboard asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_header.created sourceCreatedAt asset profile and properties sidebar metadata_header.modified sourceUpdatedAt asset profile and properties sidebar metadata_header.authorDisplayName sourceCreatedBy asset profile and properties sidebar metadata_header.tags assetTags asset filter and overview sidebar Tables ð â Atlan maps tables from ThoughtSpot to its ThoughtspotTable asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_header.created sourceCreatedAt asset profile and properties sidebar metadata_header.modified sourceUpdatedAt asset profile and properties sidebar metadata_header.authorDisplayName sourceCreatedBy asset profile and properties sidebar length(metadata_detail.relationships[]) thoughtspotJoinCount asset preview and profile length(metadata_detail.columns[]) thoughtspotColumnCount asset preview and profile metadata_header.tags assetTags asset filter and overview sidebar Views ð â Atlan maps views from ThoughtSpot to its ThoughtspotView asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_header.created sourceCreatedAt asset profile and properties sidebar metadata_header.modified sourceUpdatedAt asset profile and properties sidebar metadata_header.authorDisplayName sourceCreatedBy asset profile and properties sidebar length(metadata_detail.relationships[]) thoughtspotJoinCount asset preview and profile length(metadata_detail.columns[]) thoughtspotColumnCount asset preview and profile metadata_header.tags assetTags asset filter and overview sidebar Worksheets ð â Atlan maps worksheets from ThoughtSpot to its ThoughtspotWorksheet asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_header.created sourceCreatedAt asset profile and properties sidebar metadata_header.modified sourceUpdatedAt asset profile and properties sidebar metadata_header.authorDisplayName sourceCreatedBy asset profile and properties sidebar calculated using unique source and destination table pairs from join paths thoughtspotJoinCount asset preview and profile length(metadata_detail.columns[]) thoughtspotColumnCount asset preview and profile metadata_header.tags assetTags asset filter and overview sidebar Columns ð â Atlan maps columns from ThoughtSpot to its ThoughtspotColumn asset type. Source property Atlan property Where in Atlan metadata_header.name name asset profile and overview sidebar metadata_header.description description asset profile and overview sidebar metadata_detail.dataType thoughtspotColumnDataType asset preview and filter, overview sidebar metadata_detail.type thoughtspotColumnType asset preview and filter, overview sidebar Tags: connectors crawl Previous Crawl on-premises ThoughtSpot Next Troubleshooting ThoughtSpot connectivity Answers ð Visualizations ð Liveboards ð Tables ð Views ð Worksheets ð Columns ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/references/what-does-atlan-crawl-from-microstrategy",
    "text": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy References What does Atlan crawl from MicroStrategy? On this page What does Atlan crawl from MicroStrategy? Atlan crawls and maps the following assets and properties from MicroStrategy. danger Currently Atlan only represents the assets marked with ð in lineage. Once you've crawled MicroStrategy , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Projects , attributes , facts , metrics , cubes , reports , documents , dossiers , and visualizations -  Is Certified filter Cubes , reports , and visualizations -  Type filter Projects â Atlan maps projects from MicroStrategy to its MicroStrategyProject asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner owners overview sidebar Attributes â Atlan maps attributes from MicroStrategy to its MicroStrategyAttribute asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar forms forms overview sidebar Facts â Atlan maps facts from MicroStrategy to its MicroStrategyFact asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar expressions expressions overview sidebar Metrics â Atlan maps metrics from MicroStrategy to its MicroStrategyMetric asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar expression expression overview sidebar Cubes ð â Atlan maps cubes from MicroStrategy to its MicroStrategyCube asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar subtype type asset filter and properties sidebar sqlStatement query properties sidebar Reports ð â Atlan maps reports from MicroStrategy to its MicroStrategyReport asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar subtype type asset filter and properties sidebar Documents ð â Atlan maps documents from MicroStrategy to its MicroStrategyDocument asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar Dossiers ð â Atlan maps dossiers from MicroStrategy to its MicroStrategyDossier asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created_at createdAt properties sidebar updated_at updatedAt properties sidebar owner source owner overview sidebar certifiedInfo isCertified asset filter and properties sidebar certifiedInfo certifiedBy properties sidebar certifiedInfo certifiedAt properties sidebar ancestors location properties sidebar chapter chapterNames overview sidebar Visualizations â Atlan maps dossier visualizations from MicroStrategy to its MicroStrategyVisualization asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar visualizationType visualizationType asset filter and properties sidebar Tags: connectors crawl Previous Crawl MicroStrategy Next Preflight checks for MicroStrategy Projects Attributes Facts Metrics Cubes ð Reports ð Documents ð Dossiers ð Visualizations"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-data-consumers",
    "text": "Get Started Quick Start Guides Data consumers On this page Data consumers Discovery â We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms, tags, and definitions you've added to your data. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Discovery allows you to toggle the type of data asset you are looking for. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Insights â Atlan was built by data teams for data teams, so we know that querying your data is crucial to answering important business questions. SQL is the most common way that data teams query their data, so we wanted to make it easy for Atlan users to seamlessly query and share SQL work with their team. Atlan's Insights workspace allows teams to find the data they need, query it, and save and manage all SQL queries in one place. You can write SQL queries from scratch or use the Visual Query Builder to write SQL scripts quicker. Did you know? The Visual Query Builder is a great way to start building complex queries without fluency in SQL. Think of it as \"fill in the blank\" but for queries! Some of our favorite features about Insights: Customize the SQL editor's look and feel through preferences Examine a data asset's lineage alongside your SQL editor, so you can effortlessly understand your data asset in relation to other assets Save, organize, and share your SQL queries Tags: get-started quick-start Previous Administrators Next Contributors Discovery Insights"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-contributors",
    "text": "Get Started Quick Start Guides Contributors On this page Contributors Asset profile â The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. What's in an asset profile? Summary : The numbers of columns and rows, certification status (e.g. verified, draft, etc.), owner of the asset, and more Column Preview : An overview of column names and definitions Sample Data : A snapshot of what the data looks like (with anything sensitive hidden, according to your policies) Readme : An in-depth description of the asset that should provide all the context, knowledge, and links needed to fully understand the asset Lineage : Visualization of how your data asset relates to other assets, which helps you figure out upstream and downstream dependencies Glossary â The Atlan glossary is a rich tool for defining and organizing your data terminology to improve transparency and share knowledge. No need to ask around about what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one (searchable!) place. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as... Owners of your data, so you know who to ask for clarification Certificate status, to easily understand if the data is still in progress or ready to be used Linked Assets that are relevant to the term, so you can explore other helpful material Did you know? The glossary helps power Atlan's powerful search tool, so tagging and defining assets are critical to helping your team find what they need. Discovery â We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms, tags, and definitions you've added to your data. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Discovery allows you to toggle the type of data asset you are looking for. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Insights â Atlan was built by data teams for data teams, so we know that querying your data is crucial to answering important business questions. SQL is the most common way that data teams query their data, so we wanted to make it easy for Atlan users to seamlessly query and share SQL work with their team. Atlan's Insights workspace allows teams to find the data they need, query it, and save and manage all SQL queries in one place. You can write SQL queries from scratch or use the Visual Query Builder to write SQL scripts quicker. Did you know? The Visual Query Builder is a great way to start building complex queries without fluency in SQL. Think of it as \"fill in the blank\" but for queries! Some of our favorite features about Insights: Customize the SQL editor's look and feel through preferences Examine a data asset's lineage alongside your SQL editor, so you can effortlessly understand your data asset in relation to other assets Save, organize, and share your SQL queries Tags: get-started quick-start Previous Data consumers Next API authentication Asset profile Glossary Discovery Insights"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/api-authentication",
    "text": "Get Started Quick Start Guides Developers API authentication On this page API authentication Who can do this? You will need to be an admin user to create a bearer token. However, you can share the token with anyone to give them programmatic access. To create a bearer token: From the left menu of any screen, click Admin . Under Workspace , click API tokens . In the upper right of the API tokens table, click the Generate API token button and enter the following details: For Name , enter a name for your API token   -  for example, the system or application that will use this token. (Optional) For Description , enter a description for your API token   -  for example, its intended use. You can also add or change the description later. (Optional) For Personas , select any asset-level permissions you want to give to the token. You can also add these later. (Optional) For Collections , select any query collections you want to provide access to the token. You can also add these later. (Optional) If you would like the token to be temporary, for Expiry , choose the time after which the token should automatically become invalid. At the bottom right, click the Save button. (Optional) If and when you no longer need your API token, on an active token's row, click the trash icon to delete your API token and then click the Delete button to confirm deletion. danger Remember to copy or download the token now   -  this is your only opportunity to do so. (If you've already forgotten, just delete the API token and create a new one.) Use the bearer token â You must authenticate all requests to Atlan's APIs. You can authenticate your requests by sending the following header: Authorization: Bearer <token> So, for example, if the API token you copied had the value eyJhbGciOi... , you would use the header: Authorization: Bearer eyJhbGciOi... danger Note that the value of the Authorization header is the combination of the word Bearer , a space, and then the token's value. The token copied from Atlan does not include this Bearer prefix. Token permissions â By default, each API token will have the permissions of an admin user , without connection admin privileges. This means the token is able to: Call administrative API endpoints. For example, to create users and groups. Call governance API endpoints. For example, to create governance objects like tags, custom metadata, personas and purposes. danger The API token will only be able to access connections (and assets within them) that the token itself created. Even connections with All Admins set as connection admins will not be accessible by the token, without a persona assigned to the token. To provide access to any connections and assets, you need to add one or more personas to the token that have access to that connection's assets. Once personas are assigned to the token, the token will be able to: Create, read, update, delete, and search glossaries (and their content) that are accessible by those assigned personas. Create, read, update, delete, and search any assets that are accessible by those personas. Tags: api rest-api graphql Previous Contributors Next Custom solutions Use the bearer token Token permissions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/invite-new-users",
    "text": "Configure Atlan Access control Get started Invite new users On this page Invite new users Who can do this? You will need to be an admin user in Atlan to invite users. Did you know? Usernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. When logging into Atlan for the first time, ensure that you configure your username as per your preference. Without SSO â To invite new users to Atlan, without SSO: From the left menu of any screen, click Admin . Under Workspace click Users . Click theÂ Invite Users button. UnderÂ Invite users to Default enter one or more email addresses of the users to invite. danger Atlan does not allow the use of disposable email addresses. You will receive an error indicating this if you attempt to send an invitation to one. (Optional) To the right of each email, clickÂ Member to change the role of the user . Click theÂ Send Invite button. Your user(s) will now receive an email with a link to sign up on Atlan! ð Note that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users. With SSO â Did you know? When SSO is enforced , you will not be able to invite users in Atlan   -  they can only be invited through the SSO provider. Any users mapped to the SAML app can log into Atlan via SSO. A user profile will be generated for them automatically, if one does not already exist. Admins can also assign default user roles for SSO to give appropriate permissions to users as soon as they log into Atlan. Tags: atlan documentation Previous Access Control Next Create groups Without SSO With SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/manage-users",
    "text": "Configure Atlan Access control Manage users and groups Manage users On this page Manage users Who can do this? You will need to be an admin user in Atlan to manage other users. As an admin user in Atlan, you can: Change a user's role Temporarily disable or permanently remove a user Reactivate a disabled user Change a user's role â To change a user's role: From the left menu of any screen, click Admin . Under Workspace , click Users . Under the Role column, on a given user's row, click the role name. Under Change Role , select the new role to give the user, and then click Change . That's it, the user's role has now been changed! ð Deactivate a user â To deactivate a user, you can either disable or remove them from Atlan. For example, you can disable a user to temporarily suspend their access to Atlan and reactivate it at a future date. If a user leaves the organization, you can remove their access. Atlan recommends that you proceed with caution if you want remove a user. While a disabled user can be reactivated , removing a user is a permanent and irreversible action. If a removed user needs to be restored to Atlan later on, you will need to add them as a new user. You can remove users irrespective of whether you're using basic authentication, SSO, or SCIM provisioning. Note that if you're using SCIM provisioning, disabling or reactivating users in Atlan is not allowed . If a SCIM-provisioned user is unassigned from the Atlan app in the identity provider, that user will be disabled in Atlan as well. Only then will you have the option to remove that user permanently. To deactivate a user: From the left menu of any screen, click Admin . Under Workspace , click Users . Scroll the table all the way to the right, if necessary. On an active user's row, click the 3-dot menu button and then from the dropdown: To temporarily remove a user, click Disable user and then click Disable . To permanently remove a user, click Remove user . In the Remove user dialog, for Transfer ownership , select an existing user to transfer any and all assets and workflows owned by the user you want to remove. Depending on the volume of assets to be transferred, the user removal process may take 15 minutes to a few hours to complete. You will be notified via email upon completion. Check the I understand this action can't be reversed and want to permanently remove this user box. Click Remove user . That's it, the user is now deactivated and will no longer be able to access Atlan. ð¢ Deactivated user accounts do not count towards the total number of Atlan licenses procured by your organization. danger If the user you want to disable is a connection admin , you will need to ensure that other users can manage the connection before disabling the account. You can modify a connection to add more connection admins. Reactivate a user â You can only reactivate a disabled user's account. To reactivate a user: From the left menu of any screen, click Admin . Under Workspace , click Users . Scroll the table all the way to the right, if necessary. On a deactivated user's row, click the 3-dot menu button. Click Enable user , and then Enable . That's it, the user can once again access Atlan! ð Did you know? Atlan has a retention policy of 60 days for user login events. If a user has not logged into Atlan for more than 60 days, the Last Active column will display - for the user. Tags: atlan documentation Previous Create purpose Next Add users to groups Change a user's role Deactivate a user Reactivate a user"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso",
    "text": "Configure Atlan Integrations Identity Management SSO On this page SSO Integration Overview: Connect Atlan with SSO to enable secure authentication, streamline user access, and enhance identity management. Get started â How to enable Azure AD for SSO How to enable Google for SSO How to enable JumpCloud for SSO How to enable Okta for SSO How to enable OneLogin for SSO How to enable SAML 2.0 for SSO Guides â Authenticate SSO credentials to query data Authenticate SSO credentials to view sample data How to enable associated terms How to limit SSO automatically creating users when they log in How to set default user roles for SSO References â SSO integration with PingFederate using SAML Troubleshooting â Troubleshooting SSO Troubleshooting connector-specific SSO authentication Why do I get a 404 error when using PingFederate SSO? Why do I get an authentication error when logging in via Okta for the first time? Why do I get an error while logging in via Google dashboard? Unable to log into Atlan via SSO due to an internal error from Microsoft Defender FAQ â Can Atlan integrate with multiple Azure AD tenants within a single instance? Can we use a Microsoft SSO login? What type of user provisioning does Atlan support for SSO integrations? When does Atlan become a personal data processor or subprocessor? Why did my users not receive an invite email from Atlan? Tags: integrations identity management sso Previous Troubleshooting SCIM provisioning Next How to enable Azure AD for SSO Get started Guides References Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/configure-scim-provisioning",
    "text": "Configure Atlan Integrations Identity Management SCIM Configure SCIM provisioning Configure SCIM provisioning You can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning. SCIM provisioning works in combination with your single sign-on (SSO) setup. Setting up SCIM enables you to manage all your users from one central location. Atlan currently supports SCIM provisioning for the following SSO providers: Azure AD Okta For more questions about SCIM provisioning, head over to Troubleshooting SCIM provisioning . Tags: integration setup Previous SCIM Integration Next How to enable Azure AD for SCIM provisioning"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp",
    "text": "Configure Atlan Integrations Communication SMTP and Announcements Configure SMTP Configure SMTP Who can do this? You will need to be an admin user in Atlan to configure SMTP. The default SMTP setup is preconfigured by Atlan. You should only update this configuration if you want to set up custom SMTP. Atlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and scheduled queries . We provide an embedded SMTP server to do this, out-of-the-box. If desired, you can override this embedded SMTP se rver with your own. To override the embedded SMTP server: From the left menu of any screen, clickÂ Admin . UnderÂ Workspace , clickÂ SMTP . Fill in the configuration of your SMTP server, at least: ForÂ Host the fully-qualified hostname of the SMTP server. ForÂ From Email the email address that should be used to send emails from the server. ForÂ Username the username required by your SMTP server. At the bottom of the page, click theÂ Test SMTP Config button to test your configuration. This will attempt to send an email to your profile's email address. Once you successfully receive the test email, at the bottom of the page clickÂ Save . Tags: setup configuration Previous SMTP and Announcements Integration Next Create announcements"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/concepts/what-are-personas",
    "text": "Configure Atlan Access control Concepts What are personas? On this page What are personas? Personas define policies to control which users can (or cannot) take certain actions on specific assets. They address a combination of two main objectives: Curating the assets that are relevant to a team of users Controlling the actions users can take on those assets (querying data, updating metadata, etc) Did you know? Think of personas as a way of curating assets for a group of users. Team-based personalization â Personas curate the assets that are discoverable by users, and the detailed metadata shown. In this way, users can focus on only those assets (and metadata) relevant to their role in the organization. This reduces distractions and \"noise\" for users. For example, a team of marketers may only work with a few data sets and dashboards. Rather than flooding the marketers with details about every asset in your company, you want to focus their attention. Broad-brush access control â Combined with the personalization are broad-brush access control policies. When defining the policies in a persona, you not only select the assets but also what actions users can take on those assets. For example, your team of marketers should be able to describe their dashboards. But perhaps they should only be able to see the shared data sets that feed those dashboards, not change their descriptions or tags. You can define the persona so that they can read and write the dashboards, but only read the data sets. Tags: atlan documentation Previous Automatically assign roles Next What are purposes? Team-based personalization Broad-brush access control"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/concepts/what-are-purposes",
    "text": "Configure Atlan Access control Concepts What are purposes? On this page What are purposes? Purposes provide ways to interact with tagged assets. They address two main objectives: Grouping assets together in ways they may be used by many teams   -  for example, by project or domain Controlling access to very granular, typically sensitive data When defining a purpose , you choose its tags. Atlan then considers all assets with at least one of those tags as part of the purpose. Did you know? Think of purposes as a way of further protecting particularly sensitive data. Even if a user can see data in a table, you may not want them to see one or two sensitive columns within that table. Asset curation by domain â One way you can use purposes is to curate assets. In this approach, the purpose's tag tends to be a domain. For example, this could be a project or an area of your organization's business. Through the purpose, you can grant permissions to assets with that tag. Did you know? With purposes, anyÂ future assets given a tag will gain the same permissions   -  no policy changes needed. Granular data protection â The other way you can use purposes is to enforce granular data protection. In this approach, the purpose's tag tends to be some level of information sensitivity. For example, this could be personally-identifiable information (PII) or confidential internal financial metrics. These sensitivity tags will tend to be against granular data assets   -  often columns. Personas tend to control permissions at a broader level, for example entire data sources, databases or schemas. Through these more granular tags, purposes give you more fine-grained control. And you can layer this on top of the permissions granted by personas . For example, you might grant permission to preview and query a database to a group of users through a persona . But you don't want those users to be able to see any PII data   -  specific columns   -  wherever they appear in the database. There could be hundreds of these columns, scattered across thousands of tables. By tagging the columns, you can restrict access to them through a single policy in a purpose. This way you don't need to maintain many separate per-column policies through a persona . Tags: atlan documentation Previous What are personas? Next What are groups? Asset curation by domain Granular data protection"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/automate-data-governance",
    "text": "Build governance Stewardship Get Started Automate data governance On this page Automate data governance Who can do this? You must be an admin user in Atlan to enable , create , and manage governance workflows. Anyone with access to Atlan   -  admin, member, or guest user   -  can use the inbox . You can streamline your data governance requirements in Atlan with governance workflows and manage alerts, approvals, and tasks using the inbox . Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. For example, instead of allowing your users to directly query data or update the certification status of an asset, you can specify assets that require advanced controls and create governance workflows to govern them. These workflows will run in the background, ensure that all required approvals are in place, and only then approve users with appropriate permissions to perform any action. You can use governance workflows to ensure: Risk mitigation -  determine how data is used and shared in your organization with automated access policies. Data security -  manage requests for data access and processing to only allow access to authorized individuals or teams. Metadata change management -  monitor and audit metadata changes to align with established organizational standards. New entity creation -  manage and audit documentation of business context such as glossaries and tags to align with established organizational standards. Policy compliance -  set up repeatable processes and approval flows for your data assets in Atlan to adhere to regulatory requirements   -  currently only applicable if you have also enabled the policy center module . Workflow properties â A common set of properties are applicable to all governance workflows in Atlan: Only an admin user can create, update, or delete governance workflows. Out-of-the-box workflow templates. Predefined steps based on workflow selection. Must be associated with an asset type or action. Set up auto-approval rules for users, groups, or owners based on metadata attributes and policies. Activity logs for all workflows available by default. Visibility into the transition states of a workflow. Overlapping workflows   -  governance workflows provide you with the flexibility of creating workflows per team or business domain on the same set of assets instead of creating one complex workflow to cover all your use cases. Atlan will handle all the complexities, only allowing approvals to go through once all approval conditions have been met. Workflow templates â You can choose from the following workflow templates to govern your assets and manage access: Change management â This template allows you to control changes to metadata within your organization's data management and governance framework. Use cases include requests to: Add, update, and remove descriptions manually and using Atlan AI Add, update, and remove certificates Add, update, and remove an alias Link and remove terms from asset profile Add, update, and remove owners Attach, update, and remove tags Add, update, and remove custom metadata Add, update, and remove domains Add, update, and remove READMEs Add, update, and remove announcements Update and archive glossaries, categories, and terms Move terms and categories Change management workflows will override any permissions assigned through user roles or access policies . For example, even for users with edit access , metadata update requests will go through change management workflows. If there are no change management workflows in place, then users with edit access will be able to update metadata while users without edit access will only be able to suggest changes to metadata . New entity creation â This template allows you to control the creation and publication of new entities in Atlan. The new entity creation workflow will override existing glossary policies and user role permissions to create new entities. Creation of the following entities is currently supported for the new entity creation workflow: Glossaries Categories Terms Tags Data products :Â Â Creation of a new data product Change of a data product's status from Sunset , Archived , or Draft to Published Whether you are an admin or a member user in Atlan, the existence of a new entity creation workflow means you will need to submit a request for creating new entities. Guest users are neither allowed to directly create nor suggest the creation of glossaries, categories, terms, and tags. Access management â This template allows you to automate the process of requesting, approving, and revoking access to data assets in Atlan. It includes the combination of a self-service approach as well as mandating human intervention for approval. You can also revoke data access in Atlan or other data sources. For data sources other than Atlan, you can configure additional actions to revoke data access in the data source. Use cases include requests to query data or view sample data for the following supported asset types   -  tables, views, and materialized views. Grant access in Atlan   -  allow requesters to request data access for querying data in Insights and previewing sample data within Atlan only. Raise Jira ticket to grant or revoke data access on source   -  allow requesters to request or revoke data access for any tool. Atlan will create a support ticket in Jira Cloud for your team to grant or revoke data access and display the status of your request in Atlan. You will need to: Integrate Jira Cloud and Atlan . Link your individual Jira Cloud account to Atlan . Install or register a webhook . Create an access management workflow to enable or revoke access everywhere using Jira. Add a Jira project and issue type and specify an issue status while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in Jira. Raise ServiceNow request to grant or revoke data access on source   -  allow requesters to request or revoke data access for any tool. Atlan will create a request in the Atlan Data Access catalog for your team in ServiceNow to grant or revoke data access and display the status of your request in Atlan. You will need to: Integrate ServiceNow and Atlan . Link your individual ServiceNow account to Atlan . Create a data access approval workflow to enable or revoke access everywhere using ServiceNow. Specify the request state(s) for approval while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in ServiceNow. Trigger a webhook   -  allow requesters to request or revoke data access for any tool. Atlan will trigger a webhook to a URL of your choice for your team to grant or revoke data access. For URL , enter the URL for where you want to receive events, including details on requester, approver, and asset, and then validate the URL. danger Atlan will send a sample payload to test if the webhook URL is correct. You must respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. Copy the Secret Key and store it in a secure location to verify data access approval or revocation requests from Atlan. Policy approval â You must enable the policy center module to use the policy approval workflow template. This template allows you to automate approvals for your data governance policies in Atlan. Automated policy approval workflows can help you streamline the approval process, facilitate compliance with regulatory standards, and simplify data governance for your organization. Use cases include requests to: Create new policies Revise existing policies Enable governance workflows and inbox â Who can do this? You must be an admin user in Atlan to enable the governance workflows and inbox module for your organization. To enable governance workflows and inbox for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, under Governance center , turn on Governance Workflows and Inbox to govern your assets and manage alerts, approvals, and tasks in Atlan more effectively. If you'd like to disable the Governance Workflows and Inbox module from your organization's Atlan workspace, follow the steps above to turn it off. Once enabled, you can also temporarily disable the module and turn it on again as needed. For any governance workflows you may have created or existing requests , this will not result in any data loss. Interactions with existing access control mechanisms â Once you have turned on governance workflows and inbox, the module will interact with existing access control mechanisms in Atlan as follows: Requests : Atlan will channel requests and approvals through governance workflows and land them in the inbox. New requests   -  once you have enabled governance workflows and inbox, the requests widget will be replaced by an inbox and your member and guest users will not be able to raise any new requests until an admin user has created at least one governance workflow. To enable your member and guest users to raise new requests in Atlan: Create a change management governance workflow . Select all connections present in your Atlan workspace . Skip auto-approval . Select Anyone approves and list the users or groups designated as your Atlan admins . Publish your first governance workflow! Once published, this comprehensive workflow will allow your member and guest users to raise requests. Now you can focus on creating more use-case-driven workflows and consequently removing governed assets from the first workflow until you no longer need it. Existing requests   -  only admin users can take action on existing requests from the requests center . Your member and guest users will only be able to raise new requests on governed assets.Â Personas and purposes : Metadata policies -  your users must have read access to an asset for triggering governance workflows. If an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow/deny permissions in metadata policies. Data policies : No data policy exists   -  if the workflow connection allows querying and previewing sample data but a data policy has not been configured, your users will be able to raise a data access request on governed assets in the connection. Data policy with explicit restrictions   -  if an existing data policy denies querying and previewing sample data and assets are governed by a governance workflow, your users will not be able toÂ raise a data access request on governed assets in the connection. Data policy with explicit grants   -  if an existing data policy allows querying and previewing sample data and assets are governed by a governance workflow, your users will be able to raise a data access request on governed assets in the connection. Glossary policies -  if an asset (glossaries, categories, and terms) is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow/deny permissions in glossary policies. Domain policies -  governance workflows are currently not applicable to domain policies. User roles -  if an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of their role or permissions. For any asset not governed by a governance workflow, default role permissions will apply. Connection admins -  if an asset is governed by a governance workflow, connection admins will have to go through the approval process for governed assets in the connection. Governance workflows will currently not be triggered for the following actions: Add associated terms Add, update, and remove categories for terms from term profile Add, update, and remove resources Add a README to a term using Atlan AI Link and remove terms from term profile Bulk updates through spreadsheet tools Bulk updates using playbooks Bulk updates using Atlan AI Bulk updates through API, SDK, and CLI operations Metadata updates in supported tools using Atlan browser extension Tags: alerts monitoring notifications workflow automation orchestration Previous Stewardship Next Create governance workflows Workflow properties Workflow templates Enable governance workflows and inbox"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/concepts/what-are-asset-profiles",
    "text": "Use data Discovery Concepts What are asset profiles? On this page What are asset profiles? Every asset in Atlan has its own asset profile, which consists of all the information available for that particular asset. After you've discovered an asset, click to open the asset profile. This view gives you all the context you need about the asset. Components of an asset profile â Overview â This section displays important details about the asset: Technical name and alias , if added Number of rows and columns Connections Description of the asset Certification status (verified, draft, or deprecated) Owner of the asset Lineage view Related assets For table profiles, you can also view the Columns tab. This tab allows you to update the metadata for your columns directly from the asset profile. Lineage â Lineage offers a visual representation of the sources and transformations of your asset. You can also download lineage in a CSV file or as an image from here. Related assets â Related assets displays all the assets that have associations with each other beyond a parent-child relationship. For example, a Microsoft Power BI workspace will generally have reports, datasets, dashboards, and dataflows, all of which are related to each other. Related assets are additionally listed in the Relations tab of the asset sidebar. Column preview â Column preview offers a snapshot of all the columns in a data table, including the column name, data type, and description. This will change to Field preview in the asset profile and Fields tab in the asset sidebar for certain asset types   -  including but not limited to data source fields and calculated fields for Tableau data sources, columns for Microsoft Power BI tables, fields for Looker explores and views, and more. You can also export child assets such as columns or fields from the parent asset profile. Sample data â Sample data shows the sample data for an asset. This helps users understand what kind of data is included in the asset and allows them to copy or export this data. Connection admins can also enforce users to validate their credentials before viewing sample data, helping you enforce better governance across your organization. Linked queries â Linked queries displays any saved queries Â auto-linked to the asset when queried or referenced in the SQL query. This helps users quickly find the saved queries for additional context or launch the query in Insights directly from the asset profile. From the Linked queries tab: View all saved queries linked to the asset, along with a total count of such queries. Hover over a linked query to: View the total number of query runs in the last 30 days in a popover. Click the play icon to run the query in Insights. Click the open asset sidebar icon to view details about the query in the sidebar. Click the 3-dot icon to view any additional linked queries. If you do not have access to query data in Atlan, the linked queries will be displayed with a lock icon . README â Readme provides contextual information about the asset. It's a great place to crowdsource all the tribal knowledge and context that different users might have about the data asset. Asset profile header â This section helps you perform quick actions. From the top right of the asset profile: Click the user avatars to view a list of recently visited users, total views on your asset, total number of unique visitors, and total views by user. Use the days filter to filter asset views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button toÂ star your asset and bookmark it for easy access. Expand the Query dropdown to view sample data or query the asset in Insights. Click the clipboard icon to copy the link for your asset. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement or a resource to your asset. Asset sidebar â The sidebar to the right of the asset profile provides high-level information about the asset. Here's what you can view: Overview offers a preview of the key characteristics of the asset, including linked domains and policies . Columns displays a list of columns in an asset, along with a search bar for quick search. Relations shows a list of all the related assets. Usage displaysÂ usage metadata for your Snowflake and Google BigQuery assets. Lineage shows the upstream sources and downstream transformations for the asset. Fact-Dim Relations displays foreign-key relationships between fact and dimension tables. Fact-dimension relationships between assets can currently only be defined and published via API. Activity serves as a changelog for the asset . Resources are links to internal or external URLs that help your team understand the asset. Queries shows all the saved queries for the asset. Requests for an asset can be filtered by their status, such as Pending , Approved , and Rejected . Properties shows the unique identification number of the asset and other essential properties. Integrations show Slack messages and Jira tickets pertaining to the asset. Custom metadata tabs display custom metadata properties of the asset, if enabled. Tags: atlan documentation Previous How to interpret timestamps Next Provide credentials to view sample data Components of an asset profile"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/concepts/what-is-a-glossary",
    "text": "Build governance Glossary Concepts What is a glossary? On this page A Glossary A glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like cost , P&L , and revenue can be used to group and search all financial data assets. Using familiar terminology helps people quickly understand the data and its context. This is a crucial element of data governance since it adds business context to the data initiatives of an organization. In Atlan, glossary terms can be attached to any data asset and leveraged to power quick and easy data discovery. Why do I need a glossary? â In today's diverse data teams, which include people from different backgrounds and use cases, not all of them think about their data in the same way. For example, one team might think that a particular metric is showing an annualized rate, but the actual rate may be calculated quarterly. This could lead to some real confusion down the road. Defining data terms and sharing those definitions across your team can make a huge difference to data users at all levels of the organization. For teams made up of data analysts, data engineers, data scientists, and decision makers, having a shared language is an important step towards ensuring better collaboration. Building a glossary allows your team to define the metrics, columns, and assets with the same meaning for everyone. Highlights of the Atlan glossary â Here's how the Atlan glossary can help your organization: Powers search and makes it easier to discover data assets Encourages the creation, maintenance, and enrichment of business and functional terms due to their direct and visible use in searches Allows crowdsourcing the task of attaching appropriate glossary terms to data assets Supports automated metadata management through auto-glossary suggestions from the Atlan bot Anatomy of the Atlan glossary â Atlan gives users the option to build hierarchical glossaries. A glossary term is the lowest unit that can exist independently inside a glossary. These terms can then be grouped into categories and linked together as related terms. Did you know? This structure allows for glossaries from multiple domains. Let's look at how terms and categories work together to build a glossary. Term â A term is the lowest unit that is unique to each glossary. It describes the content of the data assets in a useful and precise way. It can exist independently, without belonging to any particular category or subcategory. Did you know? Once youâve added terms to your glossaries, you can also link them to your assets . You can then use terms on the Assets page to quickly filter your assets . Category â A category is a way of organizing the terms in a glossary. It can be used to group together similar terms. Subcategories can be added within categories to provide more context in a glossary. Associated terms â Who can do this? You will need your Atlan administrator to enable associated terms -  except related terms. With associated terms, you can define semantic relationships between your terms. These provide additional context for common definitions in your organization.Â Related term â Similar in definition   -  serves the purpose of a \"see also\" section in a dictionary. Client is a related term for Customer . Recommended term â Preferred form of usage for the current term applied. User may be preferred over Customer in the context of your organization. Synonym â Interchangeable in meaning as another term. Glossary and Dictionary , or Client and Customer . Antonym â Opposite in meaning to a particular term. Minimum is an antonym for the term Maximum , or Loss and Profit are antonyms. Translated term â Translated version of the same term in additional languages.Â Cliente is the Spanish term for Customer . Valid values for â Defines values that are considered appropriate for a related term. Red , Green , Blue , and Yellow are valid values for the term Color . Classifies and Classified by â These have a reciprocal relationship that helps provide more context for both terms. Country classifies United States , while United States is classified by Country . Tags: glossary business-terms definitions Previous Link terms to assets Next Glossary update request approval issue Why do I need a glossary? Highlights of the Atlan glossary Anatomy of the Atlan glossary Associated terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-owners",
    "text": "Use data Discovery Asset Management Add owners On this page Add owners Atlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data. The owner is responsible for maintaining the data asset. They are the right person to contact for questions about the data's frequency of update, progress, status, and more. Add owners to your assets â To add or update owners for a data asset, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to view its asset profile. In the Overview menu to the right, click + under Owners to access a list of users. To assign owners, you can either: Click the checkbox next to an individual user's name.Â Toggle to groups in the top right and select an entire group of users.Â Click Save . Your asset profile will now display the asset owners! ð Search by asset owners â You can also filter your assets by asset owners. Here are the steps: In the Filters menu on the Assets page, click Owners .Â Click the checkbox next to an owner name to view their owned assets. Did you know? You can select No Owners in the Owners filter to view assets that currently do not have assigned owners and assign accordingly if needed. Tags: atlan documentation Previous Star assets Next Add descriptions Add owners to your assets Search by asset owners"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-certificates",
    "text": "Use data Discovery Asset Management Add certificates On this page Add certificates How many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data! Wouldn't it be really convenient if the data could answer these questions? Certificates in Atlan can help! The certification tags help users quickly identify whether a data asset is ready to use, a work in progress, or has some issues. You can add the following four certification tags to any data asset: Verified for ready to be used. Draft for work in progress. Deprecated if the asset no longer exists. No certificate if not required or needs documentation down the line. Did you know? Certificates can be used to quickly filter data assets on the Assets page. This helps build trust in your data assets among users. Add certificates to your assets â To add or update the certificate for your data assets, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on the asset to open its asset profile. In the right menu, click + under Certificate and choose the relevant certification option. Write a message to add more context.Â Now your data can proudly display its status for all to see! ð Once you have selected a certification tag for your data asset, you will get a popup that your certificate has been saved. Tags: atlan documentation Previous Add descriptions Next Add a resource Add certificates to your assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/link-terms-to-assets",
    "text": "Build governance Glossary Term Management Link terms to assets On this page Link terms to assets Once you've set up a glossary , you can link terms from your glossary to your data assets in Atlan. Linking glossary terms with your data assets can help you: Provide additional context for your assets to other users in your organization. Create common definitions once and apply them many times to multiple assets. Offer an abstract point for applying tags to be propagated to all linked assets   -  including their downstream and child assets   -  if propagation is enabled . Example â If your data assets include personal information   -  for example, email addresses   -  you can link your assets to an Email Address term to provide context to your users. You can define the term Email Address once in the glossary. You can link the term to all the columns where an individual's email address appears. You can also tag the term as PII -  and all of the linked assets will be tagged as PII . Link terms to your assets â danger You will first need to create a glossary and add terms to it before you can link terms to your assets. To link a term to an asset: From a term â From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to link to your assets. In the term profile, next to Overview , click Linked assets . Click + Link Assets to get started.Â (Optional) In the sidebar on the right, under the search bar, click an asset type to filter your assets   -  for example, Column . In the sidebar on the right, select the asset(s) to which you would like to link the term. At the bottom of the sidebar, click Link asset(s) to confirm your selections. (Optional) Under Linked Assets , next to the search bar, click the export icon to export linked assets for terms to spreadsheets. From an asset â From the left menu on any screen, click Assets . On the Assets page, select the asset to which you would like to link a term. Under Terms in the asset sidebar, click the + sign to add a term to your asset. In the dialog, expand the glossary menu and then click the term you would like to link to your assets. Click Save to confirm your selections. You can now view linked assets for your glossary term! ð Did you know? You can also set up playbooks to automate the task of updating asset metadata, such as terms and more. Unlink terms from your assets â To unlink a term from an asset: From a term â From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to unlink from your assets. In the term profile, next to Overview , click Linked assets . (Optional) Under Linked Assets , next to the search bar, click the export icon to export linked assets for terms to spreadsheets before unlinking them. Under Linked assets , navigate to the asset(s) from which you would like to unlink the term. To the right of the asset name, click the three dots and then click Unlink asset . From an asset â From the left menu on any screen, click Assets . On the Assets page, select the asset from which you would like to unlink a term. Under Terms in the asset sidebar, hover over the term, and in the top right of the term popover, click the unlink button to unlink the term from the asset. Your assets will now be unlinked from the glossary term. Tags: glossary business-terms definitions Previous Bulk upload terms in the glossary Next What is a glossary? Example Link terms to your assets Unlink terms from your assets"
  },
  {
    "url": "https://docs.atlan.com/tags/get-started",
    "text": "3 docs tagged with \"get-started\" View all tags Administrators Learn about administrators. Contributors Learn about contributors. Data consumers Learn about data consumers."
  },
  {
    "url": "https://docs.atlan.com/tags/quick-start",
    "text": "3 docs tagged with \"quick-start\" View all tags Administrators Learn about administrators. Contributors Learn about contributors. Data consumers Learn about data consumers."
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/k3s/install-secure-agent-on-virtual-machine-k3s",
    "text": "Connect data Secure Agent Manage Agent K3s Install on Virtual Machine (K3s) On this page Install on Virtual Machine (K3s) Did you know? Secure Agent installation can be done by a non-root user. Root access is only needed for setting up system prerequisites before installation. This page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying K3s in a rootless execution mode . System requirements â Before installing the Secure Agent, ensure that the virtual machine (VM) meets the following requirements: At least 80GB of available disk space. A Linux-based OS running on an amd64 (x86_64) architecture with systemd enabled. The Secure Agent requires the following ports for internal services. Ensure these ports are open and accessible: Kubernetes API: 6443 Internal K3s proxy: 10443 , 10080 MinIO storage: 9000 , 32075 MinIO console: 9001 , 30614 Traefik ingress: 31037 , 32547 Prerequisites â Before installing the Secure Agent, complete the following setup steps to prepare your Atlan tenant and virtual machine. Configure Atlan tenant â In Atlan, complete the following steps to configure the tenant: Sign in to your tenant as an Atlan admin. From the left menu of any screen, click Admin . Under Workspace click Labs . Navigate to Workflow Center . Enable the Crawl assets using Secure Agent toggle. Configure virtual machine â On the virtual machine, complete the following steps to configure it: Log in as a root user. Create the required directory to configure cgroup delegation with: sudo mkdir -p /etc/systemd/system/ [email protected] Use the below cat command to create the delegation file with required configuration: cat <<EOF | sudo tee /etc/systemd/system/ [email protected] /delegate.conf [Service] Delegate=cpu cpuset io memory pids EOF Use the below command to reload systemd: sudo systemctl daemon-reload && sudo reboot To keep the Secure Agent running after logout, the root user must enable service persistence for the user installing it by running the following command: sudo loginctl enable-linger ``<user_installing_secure_agent>`` Replace <user_installing_secure_agent> with the actual username of the user installing the Secure Agent. Run the following commands to enable IP forwarding so Secure Agent can communicate with other Secure Agent instances and make network requests to the Atlan tenant. IPv4 forwarding: sudo sysctl -w net.ipv4.ip_forward=1 IPv6 forwarding: sudo sysctl -w net.ipv6.conf.all.forwarding=1 To manage containerized workloads, install fuse-overlayfs with: sudo yum install fuse-overlayfs The VM must have access to the source systemâs secret manager to retrieve secrets. For more information, see how to provide access for some popular secret managers listed below: AWS: Configure access for AWS Secrets Manager. Azure: Configure access for Azure Key Vault. GCP: Configure access for GCP Secret Manager. Permissions required â Before installing the Secure Agent, the user must have the following permissions: Create and modify directories in the userâs home directory: ~/.config/systemd/user , ~/bin , ~/.local/bin , and ~/.rancher . Create and write log files. Execute standard Linux commands: mkdir , chmod , tar , and sed . Download Agent packages â Follow these steps to download the necessary packages for setting up the Secure Agent. Did you know? The steps require Internet access to download files. In case the VM has no Internet connectivity, one can download them separately and copy the files to the VM. Create a folder for deployment and navigate to it: mkdir -p atlan-secure-agent && cd atlan-secure-agent Run the following commands to download the required packages: Download the Kubernetes install package , which contains files to run K3s on an air-gapped VM: curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/k3s_offline_package_main.tar Download the Container images package if an image registry isn't available: curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_images_main.tar Download the Secure Agent install package , which contains files for running the Secure Agent: curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_install_config_main.tar.gz Verify that all the files are downloaded. Install Secure Agent â Follow these steps to install and configure the Secure Agent on the virtual machine. Did you know? The installation can be performed by both root (administrative) and non-root (standard) users. Navigate to the deployment folder (if not already): cd atlan-secure-agent Run the following command to extract the Secure Agent install package: tar -xvf atlan_install_config_main.tar.gz The rootless-install folder is extracted from the Secure Agent install package. Run the following command to create an environment file using the env.sample file located in the rootless-install folder: cp ./rootless-install/.env.sample .env Open the .env file and update these variables: VAR_ATLAN_SECURE_AGENT_NAME=prod-atlan-agent-vm VAR_ATLAN_DOMAIN=tenant.atlan.com VAR_ATLAN_TOKEN=<atlan-api-token> VAR_ATLAN_DATA_PATH=</absolute/path/to/atlan-secure-agent> Replace the environment variable values: VAR_ATLAN_SECURE_AGENT_NAME: Specify a meaningful and unique name for the Secure Agent. For example, prod-atlan-agent-vm . VAR_ATLAN_DOMAIN: Enter your Atlan tenant domain. For example, tenant.atlan.com . VAR_ATLAN_TOKEN: Provide the API key (Bearer token). For more information on generating an API key, see Create a bearer token . VAR_ATLAN_DATA_PATH: Specify the path where the atlan-secure-agent directory is located. Run the following command to grant execution permission for the setup script: chmod +x rootless-install/setup.sh The extracted setup.sh file installs the Secure Agent and K3s. Run the following command to execute the installer: ./rootless-install/setup.sh .env While the installation is in progress, you can run the following command to verify the progress: kubectl get pods -A Verify installation â After installing the Secure Agent, verify that it's running correctly. You can check its status through the Atlan UI or by accessing the Agent UI on K3s. Log in as an Atlan admin or a similar role to access your tenant. For example: https://<tenant>.atlan.com . Navigate to the Agent tab. In the Secure Agents list, use the Search for agents box to enter your Secure Agent name. If the agent appears in the list and is marked Active , installation is complete. Troubleshooting â If you encounter issues during installation, follow these steps: Check the logs using the following command for detailed error messages that may indicate the root cause: tail -f logs/k3s.log For K3s rootless mode issues, follow the K3s official documentation for troubleshooting rootless issues. If you continue to face issues, contact Atlan support by creating a ticket . Tags: atlan documentation Previous Secure Agent Next Install on AWS EKS System requirements Prerequisites Permissions required Download Agent packages Install Secure Agent Verify installation Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
    "text": "Connect data Secure Agent Manage Agent AWS EKS Install on AWS EKS On this page Install on AWS EKS This guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster. System requirements â To deploy the Secure Agent on AWS EKS, ensure the following system requirements are met: Configure network access between your Secure Agent and Atlan tenant. For more information, see Whitelisting Secure Agent . You need Kubernetes version 1.19 or higher. You need to install Helm and kubectl on the machine you're using to connect to the AWS EKS cluster. You need at least 1 node for base services with a disk space of 20 GB and instance configuration as below: Environment Minimum instance type Recommended instance type Production t3.large Custom based on workload Non-production t3.large t3.xlarge info ðª Did you know? For optimal autoscaling, scale nodes based on the number of concurrent workflows. Permissions required â Before installing the Secure Agent, make sure the following permissions are in place: Permissions for the Installer â The user, service or system account performing the installation needs access to the EKS cluster and permissions to manage Custom Resource Definitions (CRDs). Ensure the kubeconfig is correctly configured for your target EKS cluster. If needed, use the following command to configure or update your kubeconfig file. aws eks update-kubeconfig --region ``<region>`` --name ``<cluster-name>`` Replace <region> with your AWS region (for example, us-east-1) and <cluster-name> with the name of your EKS cluster. The installer needs permission to create, update, and delete Custom Resource Definitions (CRDs). If not using the cluster-admin role, grant the following: Create a file named agent-crd-permissions.yaml on your machine. Copy the following content into the file: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   Use a descriptive name name: helm-crd-installer-role rules: - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"]   Recommended verbs for Helm CRD management verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   Use a descriptive name name: helm-crd-installer-binding subjects:   *** IMPORTANT: Modify this section based on who is running Helm ***   Choose ONE of the following options and replace placeholders.   Option 1: Bind to a specific User - kind: User name: \"your-kubernetes-username\"   Replace with the installing user's K8s username recognized by the cluster apiGroup: rbac.authorization.k8s.io   Option 2: Bind to a specific Group   - kind: Group     name: \"your-kubernetes-groupname\"   Replace with the installing user's K8s group name     apiGroup: rbac.authorization.k8s.io   Option 3: Bind to a Service Account (e.g., for CI/CD pipelines)   - kind: ServiceAccount     name: \"installer-sa-name\"   Replace with the installer SA's name     namespace: \"installer-sa-namespace\"   Replace with the installer SA's namespace roleRef:   This refers to the ClusterRole created above kind: ClusterRole name: helm-crd-installer-role apiGroup: rbac.authorization.k8s.io Follow the comments in the file to replace the placeholders. In the above file: Resource: customresourcedefinitions - needed for managing CRDs in the cluster. API Group: apiextensions.k8s.io - required to work with CRDs. Verbs: create, get, list, update, delete - necessary for installing, inspecting, updating, and cleaning up CRDs using Helm. ClusterRoleBinding: needed to assign the role to the user or group performing the installation. Once youâve updated the placeholders, use the below kubectl command to apply the configuration: Once youâve updated the placeholders, use the below kubectl command to apply the configuration: kubectl apply -f agent-crd-permissions.yaml Permissions for the Secure Agent Pod (Runtime) â The Secure Agent runs as pods in your EKS cluster and requires permissions to interact with AWS services like S3. These permissions are granted through IAM Roles for Service Accounts (IRSA). Create a new IAM role for the Secure Agent pod. Configure the trust policy to enable the Secure Agentâs Kubernetes service account to assume the role. Make sure the argo-workflow service account exists in the same namespace where you plan to install the agent. For more information, see the AWS documentation on IAM roles for service accounts (IRSA) . Example: Trust policy for the argo-workflow service account: \"Condition\": { \"StringEquals\": { \":sub\": \"system:serviceaccount::argo-workflow\" } } Replace <namespace> with the namespace where you plan to install agent. Create an S3 bucket (or use an existing one), and attach the following permissions to the IAM role used by the Secure Agent: s3:PutObject : Needed to write logs and artifacts s3:GetObject : Needed to read logs and artifacts. s3:ListBucket : Needed by Argo artifact repository for listing objects. Did you know? The Helm chart automatically configures the necessary Kubernetes RBAC for Argo Workflows, which the Secure Agent uses. No additional configuration is required for the Secure Agent pod.. Prerequisites â Before proceeding, complete the following setup steps to prepare your Atlan tenant and AWS EKS cluster. Configure Atlan tenant â In your Atlan tenant: Sign in as an Atlan admin. Go to Admin from the left menu. Under Workspace , click Labs . Navigate to Workflow Center . Enable the Crawl assets using Secure Agent toggle. Configure Secure Agent settings â The agent_config_values.yaml file is used to configure the Secure Agent, Argo Workflows, and storage for the AWS EKS cluster. Follow these instructions on the machine where you're performing the installation. Create a file named agent_config_values.yaml file. Copy the configuration below into the file:   -----------------------------------------------------------------------------------------   Agent core settings   -  Follow the comments to update:   1. Image registry settings - To be updated only if you are using a private image registry   2. Atlan connection settings - To be updated only if you want agent to use the S3 bucket   3. Argo Private repository settings - To be updated only if you are using private repository for Argo workflows   4. Kubernetes Pod Annotation settings - To be updated only if you want to customize the Kubernetes podâs metadata   5. Argo Private repository settings - To be updated only if you are using private repository for Argo workflows   6. S3 storage settings - To be updated with S3 bucket details.   ----------------------------------------------------------------------------------------- agent: enabled: true enableStorageProxy: false ca: crt: \"\"  Provide a base64-encoded string of a JSON object, e.g., {\"client_id\": 123, \"client_secret\": 1243}.  Set this only if you need to include custom headers in API calls made by the agent. restAPIHeaders: \"\" versions: k3s: \"\" k8s: \"\" helm: \"\"   1. Image Registry Settings image:   Only update if you're using a private image registry registry: \"public.ecr.aws\" repository: \"atlanhq\"   Only update if you're using custom images restImageName: \"rest-2\" restImageTag: \"1.0\"   Only update if you're using custom images jdbcImageName: \"jdbc-metadata-extractor-with-jars\" jdbcImageTag: \"1.0\"   Only update if you're using custom images credentialImageName: \"connector-auth\" credentialImageTag: \"1.0\"   Only update if you're using custom images csaScriptsImageName: \"marketplace-csa-scripts\" csaScriptsImageTag: \"1.0\"   Marketplace scripts image details - keep these values as is unless using custom images marketplaceScriptsImageName: \"marketplace-scripts-agent\" marketplaceScriptsImageTag: \"1.0\" pullPolicy: IfNotPresent pullSecrets: []    Add pull secrets if using private registry annotations: {} labels: {} serviceAccountName: \"\" automountServiceAccountToken: true resources: {}   2. Atlan connection settings - Only update if you want to agent to use the S3 bucket atlan: argoToken: \"\" vaultEnvEnabled: false   Set to true only if the agent should store metadata   in your bucket instead of sending it to Atlan via presigned URL. useAgentBucket: false metadataBucket: \"\" persistentVolume: scripts: enabled: false data: enabled: false minio: enabled: false argo-workflows: images: pullPolicy: IfNotPresent pullSecrets: [] crds: install: true keep: true annotations: {} singleNamespace: true workflow: serviceAccount: create: true rbac: create: true controller:   3. Argo Private repository settings - Only update if you are using a private image repository for Argo image:   update the private image repository details registry: quay.io repository: argoproj/workflow-controller tag: \"\" parallelism: 10 resourceRateLimit: limit: 10 burst: 5 rbac: create: true secretWhitelist: [] accessAllSecrets: false writeConfigMaps: false configMap: create: true name: \"\" namespaceParallelism: 10 workflowDefaults:   4. Kubernetes Pod Annotation settings - Only update if you want to customize the Pod metadata.    For example, the annotation might be used by external systems such as proxies, or monitoring tools, and more. spec: podMetadata: annotations: argo.workflow/agent-type: \"atlan-agent-service\" labels: app.kubernetes.io/name: \"atlan-agent\" podGC: strategy: OnPodSuccess serviceAccountName: argo-workflow automountServiceAccountToken: true ttlStrategy: secondsAfterCompletion: 84600 templateDefaults: container: securityContext: allowPrivilegeEscalation: false resources: {} env: - name: CA_CERT valueFrom: configMapKeyRef: name: cert-config key: ca.crt optional: true - name: REST_API_HEADERS valueFrom: configMapKeyRef: name: agent-registry-settings key: restAPIHeaders optional: true serviceAccount: create: true name: workflow-controller workflowNamespaces: - default replicas: 1 revisionHistoryLimit: 10 nodeEvents: enabled: false server: enabled: true   5. Argo Private repository settings - Only update if you are using a private image repository for Argo image: registry: quay.io repository: argoproj/argocli tag: \"\" rbac: create: true serviceAccount: create: true replicas: 1 autoscaling: enabled: false ingress: enabled: false annotations: ingress.kubernetes.io/ssl-redirect: \"false\" resources: {} executor: securityContext: {} resources: {} artifactRepository: archiveLogs: true useStaticCredentials: false   6. S3 bucket settings - needed by the secure agent to store logs and artifacts s3:   S3 bucket name - Update with the bucket name you created in the Permissions required section. bucket: \"atlan--bucket\"   S3 endpoint endpoint: \"s3.us-east-2.amazonaws.com\"   AWS region - Update with the region where you created bucket in the Permissions required section. region: \"us-east-2\"   Artifact path format keyFormat: \"argo-artifacts/{{workflow.namespace}}/{{workflow.name}}/{{pod.name}}\"   Whether to use insecure connections insecure: false   Use AWS SDK credentials (IAM role) useSDKCreds: true In the configuration file, follow the comments to replace the necessary attributes. You may want to update the below configurations if: You are using a private image registry (Image registry settings) You want the agent to use an S3 bucket (Atlan connection settings) You are using a private repository for Argo workflows (Argo Private repository settings) You want to customize the Kubernetes pod's metadata (Kubernetes Pod Annotation settings) You need specific S3 storage configuration (S3 storage settings) Install using Helm chart â Follow these steps to install the Secure Agent and its dependencies into your AWS EKS cluster using Helm charts. Install the Argo Custom Resource Definitions (CRDs) required by the Secure Agent. This step installs only the CRDs. The Secure Agent is installed in the subsequent step using a Helm upgrade. helm install <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent \\ --version 0.1.0 \\ -n <namespace> \\ --create-namespace -f <path/to/agent_config_values.yaml> \\ --set agent.name=\"<secure-agent-name>\" \\ --set agent.atlan.domain=\"<atlan-tenant-domain>\" \\ --set agent.atlan.token=\"<atlan-api-token>\" \\ --set argo-workflows.controller.workflowNamespaces={<namespace>} \\ --set IsUpgrade=false Replace the placeholders: <namespace> : The Kubernetes namespace where you want to deploy the Secure Agent. <path/to/agent_config_values.yaml> : The path to the YAML config file. <secure-agent-name> : Unique name, like agent-us-east-cdw. <helm-app-name> : Unique Helm release name, like atlan-agent-v1. <atlan-tenant-domain> : Your Atlan domain, e.g., mycompany.atlan.com. <atlan-api-token> : Token used for authentication. See Create a bearer token . Use the following kubectl command to associate the IAM role with the service account. This enables the Secure Agent to access the S3 bucket securely using IAM Roles for Service Accounts (IRSA). Make sure the IAM roleâs trust policy enables the argo-workflow service account to assume the role. kubectl annotate serviceaccount argo-workflow \\ -n  \\ eks.amazonaws.com/role-arn=arn:aws:iam:::role/ Replace the placeholders: <namespace : The Kubernetes namespace where you want to deploy the Secure Agent. <AWS_ACCOUNT_ID> : Your AWS Account ID. <YourAgentIAMRoleName> : The IAM role name you created for the Secure Agent using IRSA. Install the Secure Agent by upgrading the Helm release. This step performs the actual Secure Agent installation after CRDs are in place. helm upgrade <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent \\ --version 0.1.0 \\ -n <namespace> \\ --create-namespace -f <path/to/agent_config_values.yaml> \\ --set agent.name=\"<secure-agent-name>\" \\ --set agent.atlan.domain=\"<atlan-tenant-domain>\" \\ --set agent.atlan.token=\"<atlan-api-token>\" \\ --set argo-workflows.controller.workflowNamespaces={<namespace>} \\ --set IsUpgrade=true Replace the placeholders: <namespace> : The Kubernetes namespace where you want to deploy the Secure Agent. <path/to/agent_config_values.yaml> : The path to the YAML config file. <secure-agent-name> : Unique name, like agent-us-east-cdw. <helm-app-name> : Unique Helm release name, like atlan-agent-v1. <atlan-tenant-domain> : Your Atlan domain, e.g., mycompany.atlan.com. <atlan-api-token> : Token used for authentication. See Create a bearer token . While the installation is in progress, you can run the following command to verify the progress: kubectl get pods -n <namespace> Replace <namespace> with the Kubernetes namespace used for deployment. Verify installation â To confirm successful installation: Sign in to your Atlan tenant as an admin. For example, https://<tenant>.atlan.com . Navigate to the Agent tab. Search for your Secure Agent name. If the agent appears in the list and is marked Active , installation is complete. Tags: security access-control permissions Previous Install on Virtual Machine (K3s) Next Configure workflow execution System requirements Permissions required Prerequisites Install using Helm chart Verify installation"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/configure-secure-agent-for-workflow-execution",
    "text": "Connect data Secure Agent Manage Agent Configure workflow execution On this page Configure workflow execution When using Secure Agent for extraction, source system credentials (secrets) required for workflow execution are stored in a Secret Manager. This guide provides steps to set up workflows with Secure Agent and specify the secret details it uses during workflow execution. Before you begin â Before configuring Secure Agent for workflow execution, ensure you have: A registered and active Secure Agent. Access to one of the supported secret stores: AWS Secrets Manager, Azure Key Vault, GCP Secret Manager, environment variable-based secret injection technique, or a custom secret store. Configure secrets retrieval for workflow execution â Follow these steps to configure Secure Agent to retrieve secrets from a secret store required for the workflow execution. This is necessary for secure data access while running your workflows. ðª Did you know? For each field, you can enter either the name of a secret stored in your secret manager or the actual value. Use secret names when using a secret store with Secure Agent, or enter values directly if no secret is required. AWS Azure GCP Environment variables Custom store Secure Agent retrieves the required secrets from AWS Secrets Manager during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section: Secret path in Secret Manager: Provide the Amazon Resource Name (ARN) or the path of the secret that contains the sensitive configuration details required for the connector. These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution. AWS region: Select the region where your AWS Secrets Manager is located. AWS authentication method: Select how you want the Secure Agent to authenticate when executing the workflow. Choose one: IAM (Recommended) : Use this method if the secure agent was configured to use the AWS IAM permissions to access secrets. IAM Assume Role : Use this method if the agent was configured to access secrets via cross-account roles. AWS Assume Role ARN : Provide the IAM Role ARN that grants the Secure Agent permission to retrieve secrets. Access Key & Secret Key : Use this method if the agent was configured to use the AWS Access Key ID and Secret Access Key via environment variables or Kubernetes secrets. Secure Agent retrieves secrets from Azure Key Vault during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section: Secret path in Secret Manager: Provide the URL of the Azure Key Vault secret that contains the sensitive configuration details required for the connector. These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution. Azure authentication method: Select how you want the Secure Agent to authenticate when accessing the Azure Key Vault secret. Choose one: Managed Identity (Recommended) : Use this method if the agent was configured to use an Azure-managed identity assigned to the agent environment for authentication. Service Principal Authentication : Use this method if the agent was configured to authenticate via a Service Principal using Tenant ID, Client ID, and Client Secret. Azure Key Vault Name: Provide the name of your Azure Key Vault that stores your secrets. Secure Agent retrieves secrets from GCP Secret Manager during workflow execution. The secret is uniquely identified by its name in GCP Secret Manager, without requiring additional attributes. Secure Agent retrieves secrets from environment variables during workflow execution. Secure Agent retrieves secrets from Custom Secret Store during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section: Agent Custom configuration: Secure agent needs information for connecting to the custom secret store. Add the configuration details in JSON format to specify the connection settings and the secrets to retrieve during workflow execution. For example, the JSON configuration to initiate a sample custom store may look like below: { \"store_url\" : \"https://custom-secret-store.example.com\" , \"secret_name\" : \"my-custom-secret\" } Next steps â After configuring the Secure Agent, return to your connectorâs setup guide and continue the workflow setup. Tags: integration connectors workflow automation orchestration Previous Install on AWS EKS Next Deployment architecture Before you begin Configure secrets retrieval for workflow execution Next steps"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/references/deployment-architecture",
    "text": "Connect data Secure Agent References Deployment architecture On this page Deployment architecture The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. High-level architecture â This section describes how the Secure Agent is structured and deployed. It explains the core components that enable metadata extraction, job execution, and communication with Atlan. Figure 1: Atlan Secure Agent deployment architecture. Core components â The Secure Agent runs as a Kubernetes-based application within a customer's private cloud or on-premises environment. It consists of several key components that work together to execute metadata extraction tasks. Argo Workflows â An Argo Workflow server is deployed to coordinate all activities and launch Kubernetes workloads. The Secure Agent uses Argo Workflows to orchestrate and manage metadata extraction jobs. Each workflow represents a unit of work, such as extracting metadata from a source system. Agent orchestrator â A scheduled job that runs every five minutes to check for jobs that need to be executed. It connects to the Atlan tenant, retrieves job details, and initiates workflows accordingly. Auxiliary services â Additional services that support agent operations: Health monitoring service sends periodic heartbeats to Atlan to confirm the agent is active. Logging service uploads execution logs to Atlan for monitoring and debugging. Metadata extraction workflows â Connector-specific jobs that extract metadata from source systems. Workflows run in isolated containers, ensuring security and reliability. Data flow â The Secure Agent supports two modes of metadata transfer. Each mode determines how extracted metadata is delivered to Atlan. Bucket relay â Metadata extraction in bucket relay mode stores metadata in enterprise-managed cloud storage before Atlan retrieves it. Figure 2: Data flow in bucket relay mode. The Secure Agent extracts metadata and writes it to a storage bucket in the customerâs cloud environment (such as AWS S3, Azure Blob Storage, or Google Cloud Storage). This is managed by providing the agent write access to cloud storage. Atlan retrieves metadata from the storage bucket and processes it further. This is managed by providing Atlan read access to list and read files in cloud storage. This mode ensures the extracted data remains within the customerâs infrastructure until Atlan explicitly fetches it. Customers can also use this data for auditing. Direct ingestion â In direct ingestion mode, metadata is transferred directly from the Secure Agent to Atlan. Figure 3: Data flow in direct ingestion mode. The Secure Agent uses pre-signed URLs to upload metadata directly to Atlan. Some cloud storage providers that use pre-signed URLs include: AWS reference Azure reference GCP reference A pre-signed URL grants temporary access to upload files without exposing long-term credentials. Each URL has an expiration time, ensuring access is only available for a limited duration. See also â Secure Agent - Security : Details on security mechanisms. Tags: integration connectors security access-control permissions Previous Configure workflow execution Next Security High-level architecture Core components Data flow See also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/references/security",
    "text": "Connect data Secure Agent References Security On this page Security The Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring. Authentication and authorization â The Secure Agent implements security measures for authentication, encryption, and access control. This section details authentication mechanisms, including API key management and secret handling. API key management â The Secure Agent uses API keys for authentication when communicating with Atlan. These keys verify the agentâs identity and define its access scope. Authentication: API keys authenticate the Secure Agent, allowing it to interact securely with Atlan. Each key is associated with a specific tenant and grants access based on permissions. Storage: API keys are stored in enterprise-managed vaults, such as AWS Secrets Manager, Azure Key Vault, or Kubernetes Secrets. The Secure Agent retrieves the key dynamically during operation, eliminating manual configuration. Expiration: API keys can have an expiration period, such as 90-180 days, or be configured based on internal security policies. Rotation: When an API key nears expiration, a new key can be generated and stored in the secret vault. The Secure Agent automatically fetches the latest key from the vault. Revocation: If an API key is compromised, it can be revoked. Once revoked, the Secure Agent retrieves a newly assigned key from the vault without requiring manual intervention. Secret management â The Secure Agent retrieves credentials securely without storing them locally. Enterprise-managed vaults: The Secure Agent integrates with AWS Secrets Manager, Azure Key Vault, and other vaults to securely store credentials, keeping them within the organizationâs security perimeter. Just-in-time access: Credentials, such as database secrets, are retrieved dynamically from enterprise vaults when needed and are never stored locally. No credential transmission: Secrets are never transmitted to or stored on Atlan, ensuring complete isolation of sensitive information. Data security and encryption â The Secure Agent protects metadata using encryption and strict access controls. Compliance with security standards: The Secure Agent aligns with ISO 27001 and SOC 2 security standards, ensuring strong encryption, data protection, and access control measures. Data in transit: All communication between the Secure Agent and Atlan is encrypted using TLS 1.2 over HTTPS. For network-level protections, see Network security . Data at rest: Metadata stored in customer-managed storage or Atlanâs tenant bucket is encrypted using AES-256. Data minimization: Only essential metadata is extracted and transmitted. Customers can configure data filters to exclude specific metadata fields from processing. Retention control: Atlan doesn't require metadata post-ingestion, and customers can delete metadata from their storage buckets based on internal security policies. Container security â The Secure Agent implements security measures to protect container images, ensuring their integrity and mitigating security risks. Container image hosting: Secure Agent container images are hosted on public repositories, such as Docker Hub and Amazon ECR. Organizations can deploy the Secure Agent from a private container registry to meet their compliance and security requirements. Vulnerability scanning: Trivy scans container images for known vulnerabilities, outdated dependencies, misconfigurations, and exposed secrets. Scans are conducted weekly and whenever new changes are checked in. Image signing and verification: Cosign signs container images to ensure authenticity. Image verification includes: Validating the image signature against Sigstore's transparency log. Verifying the signerâs identity through GitHub workflows. Confirming the certificate issued by GitHubâs OpenID Connect (OIDC) provider. License compliance: Trivy scans for software license compliance to ensure proper licensing for all components within the container images. Network security â The Secure Agent operates within a controlled network environment to facilitate secure metadata extraction and communication with Atlan. SSL certificates â The Secure Agent encrypts communications with Atlan, source systems, proxy servers, and secret managers. Encryption in transit: All data communication between the Secure Agent and Atlan is encrypted using TLS 1.2 over HTTPS. Certificate management: If trusted or well-known certificate authorities are used, no additional configuration is needed. The Default Trusted Certificate Authorities store contains certificates from the most common and trusted CAs, which the Secure Agent uses to secure connections. If internal or private certificate authorities are used, the Secure Agent trusts these custom certificate authorities through the infrastructureâs default certificate store. Whitelisting â Configuring network access ensures only trusted communication between the Secure Agent and Atlan. Domain whitelisting: The Secure Agent requires outbound access to Atlan through the domain tenant.atlan.com . Domain-based whitelisting simplifies network configurations while maintaining security. DNS resolution: The Secure Agent relies on standard DNS resolution to reach Atlan domains. Network configurations must allow name resolution for tenant.atlan.com . IP-based whitelisting: If domain-based whitelisting isnât feasible and specific IP ranges must be allowed, refer to the list of required IP ranges to be whitelisted. If you need further assistance, contact Atlan Support . Logging and monitoring â The Secure Agent captures logs for workflow execution, system orchestration, and Kubernetes operations while also providing monitoring capabilities. Types of logs â Workflow logs: Capture job execution details, including start and completion status, connections to source systems and secret managers, metadata extraction results, and authentication status. These logs are sent to Atlan and accessible from the workflow status page. Orchestration logs: Track the Secure Agentâs scheduled operations, including connection attempts to Atlan, retrieval of workflow requests, and workflow submission to the Argo engine. Logs also include error messages and performance metrics. Argo logs: Provide visibility into workflow execution, including job scheduling, resource allocation, state transitions, and error handling. Kubernetes logs: Capture system-level events, such as pod lifecycle changes, container startup and shutdown, resource allocation, network connectivity, and health checks. Monitoring â Health checks: Secure Agent components run periodic health checks to verify connectivity, resource availability, and system integrity. Resource utilization: CPU, memory, and storage usage are monitored to track system load and detect potential bottlenecks. Logs can be viewed in Atlan or integrated with external monitoring systems. Tags: data api authentication Previous Deployment architecture Authentication and authorization Data security and encryption Container security Network security Logging and monitoring"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/set-up-playbooks",
    "text": "Configure Atlan Playbooks Get Started Set up playbooks On this page Set up playbooks warning ð¤ Who can do this? You will need to be an admin user in Atlan to create playbooks. A common question that data teams often face is how to automate metadata at scale. Having started out as a data team ourselves, we know that automating repetitive tasks can help data teams maximize the value they provide to their organization. One way of doing so is through Atlan's playbooks! Playbooks help power metadata automation for your data assets in Atlan. You can create rule-based automations at scale and update metadata in bulk, helping you streamline your workflows. You can update the following asset metadata using playbooks: Certificates Descriptions Owners Terms Tags Domains Custom metadata For example, imagine your organization needs to transfer ownership of several data assets. Instead of your data team manually updating the ownership of each and every asset, you can create a playbook to automate this process and update the metadata of your assets at scale. Playbook recommendations â Before you begin, review some general guidelines on running playbooks in Atlan: Avoid running multiple playbooks simultaneously on the same set of assets. Allow one playbook run to be completed before proceeding with another operation on the same set of assets. Otherwise, you may experience performance issues and inconsistencies. Review and understand the depth of your asset lineage or hierarchy prior to enabling a tag propagation playbook. For assets with complex lineage, tag propagation may take longer to complete than the playbook runtime. You may want to review and judiciously select a list of assets that need to be tagged directly. For their child and/or downstream assets, Atlan recommends that you enable tag propagation . Create a playbook â To create a playbook in Atlan: From the left menu in Atlan, you can either: Click Assets to navigate to the assets page. From the Filters menu on the left or the tabs along the top, apply any asset filters . Next to the search bar, click the 3-dot icon and then click Create playbook to create a playbook for the filtered assets   -  this option is only visible to admin users. Click Governance to navigate to the governance center. Under the Governance heading of the Governance center , click Playbooks . Click Create New to get started. In the Create new playbook dialog box, enter the following details: For Name , enter a name for the task to be accomplished   -  for example, Update ownership . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) (Optional) For Description , enter a description. (Optional) Select an icon for your playbook. Click Create to save your playbook. Set up rules as filters â To set up rules as filters for your playbook: In the Build Rules page of your playbook, click Filters . For name, add a name to your filter. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select a relevant option: For this example, we'll click Connection and then select a Snowflake connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Domains to filter by specific domains or subdomains to bulk update all the assets included in those data domains or subdomains. Click Products to filter for data products by specific data domains or subdomains. Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. Select Belongs to or Doesn't belong to to filter data products by specific data domains or subdomains . For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View all for a preview. (Optional) To remove a playbook filter, to the right of any filter, click the three horizontal dots and then click Delete . (Optional) To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. Select the actions â To select the actions to be performed based on your rules: In the Build Rules page of your playbook, click Actions . For Select Action , select the relevant metadata option to update: Click Certificate to update the certification status of assets to Verified , Draft , Deprecated , or No certificate . Click Description to update the description of your assets. Click Owners to add, remove, or replace asset owners . In this example, we'll update the ownership of the assets. Click Terms to add terms to your assets or remove or replace them from linked assets . Click Tags to add tags to your assets or remove or replace them from tagged or propagated assets. Note that if there are multiple tag actions to be performed, Atlan will execute them in the following order: ADD , REMOVE , and then REPLACE . Click Domain to add your assets to a specific domain or subdomain or remove them from an existing linked domain or subdomain . Click any custom metadata structure and then select a custom metadata property to update or unlink it from your assets. For Select operator , select the relevant option. The operators will vary depending on the selected action. For Values , select the relevant option(s). The values will vary depending on the selected actions. (Optional) To add more actions, click Add Action . Did you know? You can control tag propagation when adding tags as an action in playbooks. Tag propagation is disabled by default. If you enable tag propagation, you will also be able to configure how tags are propagated . Run the playbook â If you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it. To run the playbook: You can either: To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. danger If you're scheduling multiple playbooks, Atlan recommends spacing out the schedules as much as possible to minimize any overlap between the playbook workflow runs. For more about workflows in general, see workflow recommendations . Click Complete to run the playbook. In the resulting screen, click Go to profile to view your playbook profile. Once your playbook has completed its run, you will see the metadata updated for your assets! ð Did you know? If you have any questions about setting up playbooks, head over here . Tags: atlan documentation Previous Playbooks Next Manage playbooks Playbook recommendations Create a playbook Set up rules as filters Select the actions Run the playbook"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/manage-playbooks",
    "text": "Configure Atlan Playbooks Management Manage playbooks On this page Manage playbooks Once you've created a playbook , you can monitor, modify, or delete it at any time. You can also enable notifications to monitor your playbook runs directly in Slack or Microsoft Teams. Monitor a playbook â To monitor your playbooks runs: When running a playbook immediately, you will be redirected to the monitoring page within 5 seconds. At any other moment: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, select the playbook you'd like to view. In the Overview section of your playbook, you'll be able to monitor: A summary of the rules, actions, and updated assets. An activity log for recent runs and updates over time. Did you know? The activity log in the playbooks manager can help you keep track of playbook runs, ranging from 24 hours to 30 days. Select any of the entries to navigate to the corresponding playbook. Modify a playbook â To modify an existing playbook: From the left menu of any screen in Atlan, click Governance. Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, click the playbook you'd like to modify. On your playbook page: To edit the name and description of your playbook, hover over your playbook and click Edit . To modify the rules of your playbook, click Rules to make your changes and then click Update to save them. (Optional) To add a new rule to an existing playbook, in the left menu for playbook rules, click + Add new Rule .Â To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. To modify the schedule for your playbook, in the upper right of your screen: Click Run Now to run it immediately. Click the pencil icon to modify or remove the schedule. Delete a playbook â To delete an existing playbook: From the left menu of any screen in Atlan, click Governance. Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, hover over the playbook you'd like to delete and click Delete Playbook . Click Delete to confirm. Enable playbook notifications â You can set up Slack or Microsoft Teams alerts for your playbook runs in Atlan. This can help you monitor your playbooks directly in Slack or Microsoft Teams. You can also choose to receive alerts for failed playbook runs only. Before you can enable notifications for playbooks, you will need to either: Integrate Slack and Atlan Integrate Microsoft Teams and Atlan To enable notifications for playbook runs: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . In the upper-right of the playbooks manager, under Activity , click the bell icon. In the Enable notifications popup: Click Setup now to integrate Slack or Microsoft Teams . If you have already integrated Slack or Microsoft Teams, click Enable . In the notifications setup dialog, configure the following: For Notifications channel , you can either: If you have already configured a Slack or Microsoft Teams channel to receive workflow alerts, that channel will be preselected. You can use the same channel to receive both workflow and playbook run alerts and skip to the next step. If you have not configured a workflow alerts channel or want to add a different one, enter the channel name to receive notifications for playbook runs. This channel will be displayed as the Playbooks alert channel Â in yourÂ Slack or Microsoft Teams integration. To select the type of notifications you want to receive, you can either: Click Both success and failure alerts to receive notifications for both successful and failed playbook runs. Click Failure alerts only to limit notifications to failed playbook runs only. Click Save to save your notification preferences. (Optional) To disable notifications, from the notifications setup dialog, remove the playbook alerts channel configured for Slack or Microsoft Teams . You will now receive Slack or Microsoft Teams notifications for all your playbook runs in Atlan! ð The Atlan bot will share playbook run alerts, including details like run status, start time, run time, trigger type, last three runs, and more. Tags: atlan documentation Previous Set up playbooks Next Automate data profiling Monitor a playbook Modify a playbook Delete a playbook Enable playbook notifications"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/troubleshooting/troubleshooting-playbooks",
    "text": "Configure Atlan Playbooks Troubleshooting Troubleshooting playbooks On this page Troubleshooting playbooks warning ð¤ Who can do this? You will need to be an admin user in Atlan to create playbooks. Here are a few things to know about setting up playbooks : What are the known limitations of the domain action? â Following are the known issues or limitations when using the domain action : Atlan currently does not support adding glossaries, categories, and terms to domains. If you do not have read permission on the assets you want to add to a domain, those assets will be removed from the playbook workflow during processing.Â If you do not have update permission on the assets you want to add to a domain, the playbook workflow will fail. However, some assets may still be linked to the domain before the failure occurs. What type of infrastructure costs can I expect to incur? â Atlan uses Elasticsearch to run playbooks, so expect infrastructure costs to be minimal and not a determining factor for utilizing playbooks. What is the maximum number of playbooks that can be run? â We recommend building no more than a maximum of 20 rules per playbook. However, the total number of playbooks that can be run is still to be determined. From a technical standpoint, playbooks leverage the workflow infrastructure, which means there are no hard limits. Depending on the number of playbooks that need to be run, the infrastructure will have to be scaled accordingly.Â Do I also need to have update permissions for playbooks? â Yes. You need to have the permission to update assets in Atlan in order to run playbooks for updating them.Â If you do not have the permission to update an asset, you will be unable to update it using playbooks. Additionally, Atlan uses the permissions of the playbook creator in determining the assets to be updated and not that of the user who runs the playbook. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Can I automate requests for updates through playbooks? â No, Atlan currently does not support automating asset update requests through playbooks. Is there a way to undo updates made through playbooks? â Currently, there is no button to undo asset updates. However, you can modify your existing playbooks . You can either turn off the filters or add new rules to reverse the updates. Is there a way to view or download a report of updated assets from previous playbook runs? â Currently, no. You can monitor your existing playbooks to view a high-level summary of asset updates from previous playbook runs. Observability of results is on the roadmap. Can I get email notifications for playbook run successes or failures? â Currently, no. However, you can set up Slack or Microsoft Teams alerts for your playbook runs in Atlan. How to handle an offload node status is not supported error? â If you encounter an offload node status is not supported error message, the playbook workflow may have exceededÂ the EtcD size limit. Playbooks use Argo workflow templates, which are stored as Kubernetes resources. This creates a limit to their size. To handle this error, Atlan recommends the following: Reduce the number of rules in your playbook Optimize filters for asset selection Tags: atlan documentation Previous Automate data profiling"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/automate-data-profiling",
    "text": "Configure Atlan Playbooks Management Automate data profiling On this page Automate data profiling â Available via the Data Quality Studio package warning ð¤ Who can do this? You need to be an admin user in Atlan to create profiling playbooks. Monitoring and improving data quality is critical to building trust in your data assets. Atlan solves for this with profiling playbooks! Profiling playbooks help power data observability for your assets in Atlan. You can create profiling playbooks to scan your assets at scale, identify any issues or inconsistencies, and improve the data quality of your assets. Supported sources â Atlan currently supports column profiling for the following connectors: Amazon Athena Amazon Redshift Databricks Google BigQuery Microsoft SQL Server MySQL PostgreSQL Snowflake Trino Create a profiling playbook â To create a profiling playbook: In the left menu in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . To the right of the Create New button, click the downward arrow and then select Profiling Playbook . In the Create new profiling playbook dialog, enter the following details: For Name , enter a name for the task to be accomplished   -  for example, Tables scan . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) For Connection , select a supported connection from the dropdown menu   -  in this example, we'll select a Google BigQuery connection development . (Optional) For Description , enter a description for your playbook. (Optional) Select an icon for your playbook. Click Create to save your playbook. Set up rules as filters â Did you know? The assets to be scanned are pre-filled based on your selected connection. To set up rules as filters for your profiling playbook: In the Build Rules page of your profiling playbook, click Filters . For the name field, add a name to your filter   -  for example, Profiling action . To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select the relevant option. For this example, we'll select Name listed under Properties . (Optional) To further refine your asset selection: Click Connection to select a specific connection.Â Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View all for a preview. Confirm profiling actions â danger Column profiling is currently only supported for number and text data types. The profiled column assets will be populated with preconfigured metrics. To select the actions to be performed based on your rules: The default profiling actions to be performed include: Base metrics : Distinct count   -  number of rows that contain distinct values, relative to the column. Missing count   -  number of rows that do not contain specific values. Numeric metrics : Minimum and maximum values   -  smallest and greatest values in a numeric column. Average   -  calculated average of values in a numeric column. Standard deviation   -  calculated standard deviation of values in a numeric column. Variance   -  calculated variance of values in a numeric column. String metrics : Average length   -  average length of string values in a column. Minimum and maximum length   -  minimum and maximum length of string values in a column. Click Next to proceed to the next step. In the Optimize your Profiling query popup, the following message will appear: This Profiling playbook will query x rows across y assets. To avoid significant computing costs, review your applied filters before proceeding . Click Review filters to review your existing filters or click Continue anyway to proceed. Note that Atlan is working to support sampling functionality in the future. Run the playbook â If you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it. To run the playbook: You can either: To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. Click Complete to confirm your selections. In the resulting screen, click Go to profile to view your playbook profile. Once your playbook run is completed, you will see the data profile updated for your assets! ð View profiled assets â To view the profiled assets for your playbook: In the Overview page of your playbook, to the right of Profiling action , click the total count of profiled assets. In the sidebar to the right, profiled assets will be indicated with a bar graph icon. Click any profiled asset to proceed to viewing profiling data. From the table sidebar, click the Column tab to view column assets and then select any of the profiled columns. In the column sidebar to the right, click Profile to view profiling data for the selected column asset. Did you know? Once you've created a profiling playbook, you can monitor, modify, or delete it at any time. Tags: connectors data crawl Previous Manage playbooks Next Troubleshooting playbooks Supported sources Create a profiling playbook Set up rules as filters Confirm profiling actions Run the playbook View profiled assets"
  },
  {
    "url": "https://docs.atlan.com/tags/playbooks",
    "text": "One doc tagged with \"playbooks\" View all tags Playbooks Create and manage automated workflows for metadata management and data governance."
  },
  {
    "url": "https://docs.atlan.com/tags/automation",
    "text": "25 docs tagged with \"automation\" View all tags Always On Integrate Atlan with Always On to enable continuous automation and suggestions. Automate data governance You can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Automatically assign roles based on group names Learn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app. Automation Integrations Integrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On. AWS Lambda Integrate Atlan with AWS Lambda to automate workflows and triggers. Browser Extension Integrate Atlan with the Browser Extension to enhance your data catalog experience. Configure workflow execution Learn about configure workflow execution. Connections Integration Integrate Atlan with Connections to create webhooks and automate notifications. Create governance workflows :::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/. Create webhooks If your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or [submit a request](/support/submit-request). Delegate administration The workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows. Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance. Generate lineage between assets App Learn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app. How often does Atlan crawl Snowflake? Learn about how often does atlan crawl snowflake?. Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Manage governance workflows :::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/. Manage requests If your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected. Playbooks Create and manage automated workflows for metadata management and data governance. Requests Requests allow users to suggest changes to assets that they cannot directly change themselves. Revoke data access As an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the [scope of governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows). Send alerts for workflow events Learn how to configure alerts for workflow events in Atlan via email or Google Chat. Stewardship Learn how to implement data stewardship in Atlan through automated workflows, policies, and task management. Troubleshooting Anomalo connectivity Learn about troubleshooting anomalo connectivity. User Role Sync Complete configuration reference for the User Role Sync app properties and settings. Webhooks Integration Integrate Atlan with Webhooks to automate actions and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/workflows",
    "text": "5 docs tagged with \"workflows\" View all tags Alteryx Integrate, catalog, and govern Alteryx assets in Atlan using OpenLineage. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. Playbooks Create and manage automated workflows for metadata management and data governance. Send alerts for workflow events Learn how to configure alerts for workflow events in Atlan via email or Google Chat. Stewardship Learn how to implement data stewardship in Atlan through automated workflows, policies, and task management."
  },
  {
    "url": "https://docs.atlan.com/tags/metadata",
    "text": "12 docs tagged with \"metadata\" View all tags Atlan MCP Overview Learn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup. Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. OpenLineage configuration and facets Learn about openlineage configuration and facets. Playbooks Create and manage automated workflows for metadata management and data governance. Troubleshooting Amazon DynamoDB connectivity Learn about troubleshooting amazon dynamodb connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Tableau connectivity Learn about troubleshooting tableau connectivity. What does Atlan crawl from CrateDB? Complete list of CrateDB assets and metadata properties extracted by Atlan during crawling What does Atlan crawl from Dagster Learn about the Dagster metadata that Atlan captures and visualizes Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/capabilities",
    "text": "10 docs tagged with \"capabilities\" View all tags Atlan AI Integrate and leverage Atlan AI capabilities for enhanced data documentation, and lineage analysis. Data Models Create and manage data models to structure and organize your data assets. Data Products Create and manage data products to organize and govern your data assets by domain. Discovery Find, understand, and use data assets across your organization with powerful search, filtering, and browsing features. Insights Query and analyze your data using Atlan's powerful query builder and SQL capabilities. Lineage Track and visualize data lineage across your data landscape to understand data flow and dependencies. Playbooks Create and manage automated workflows for metadata management and data governance. Reporting Generate comprehensive reports on your data assets, usage, and governance. Requests Request and manage changes to assets that you don't have direct edit access to. Usage and Popularity Track and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/access-archived-assets",
    "text": "Use data Discovery Asset Management Access archived assets On this page Access archived assets To access archived assets in Atlan, complete the f ollowing steps. Archived assets â Did you know? Archived assets still exist in Atlan. They are only \"soft-deleted\" and do not appear in search results (by default). You can only access archived assets through discovery. Assets can be archived when: You lose permissions to an asset at source. The asset name is changed at source. An asset is removed at source or moved to a different schema or database. Delete your assets either by deleting a connection or configuring it in Atlan. Search for archived assets â To search for archived assets: From the left menu of any page, click Assets . Under Filters on the left, expand Properties . At the bottom of the list of properties, click Is Archived and then Yes . Now use any additional filters , and the results will include only archived assets. View details of an archived asset â To view the details of an archived asset: Search for the asset using the steps above. Narrow down your results using the other filters or search details. Click the card for the asset you want to view. Use the right sidebar to view the details of the asset. The icons on the far right of the sidebar allow you to review different aspects of the asset. Deleted assets â Assets that have been (hard-)deleted no longer exist in Atlan. So you cannot find them by searching, or even access them through a direct link. Tags: data asset-profile Previous Search and discover assets Next Add an alias Archived assets Deleted assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/configure-language-settings",
    "text": "Use data Discovery Configuration Configure language settings On this page Configure language settings How does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level. Atlan admins can change the default language for their Atlan workspace from the admin center. Individual users can also set a personal language preference, which overrides the default workspace settings - English or otherwise. Atlan currently supports the following additional languages: French Japanese Portuguese To enable any of the additional supported languages or request ones not listed in this section in your Atlan workspace, Atlan admins must contact Atlan support . Configure workspace language settings â Who can do this? Once Atlan has enabled a preferred language for your organization, you need to be an admin user in Atlan to configure language settings for your Atlan workspace. To configure workspace language settings: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Workspace settings heading of the Labs page, for Default workspace language , click the English language dropdown to set a preferred language. Your users can now use Atlan in their preferred language! ð Configure personal language settings â Who can do this? Once Atlan has enabled a preferred language for your organization, anyone with access to Atlan - admin, member, or guest user - can update language settings for their Atlan instance. To set a personal language preference: In the top right corner of your Atlan instance, click your name, and then from the dropdown, click Language . From the Language dropdown, select a preferred language for your Atlan instance. You can now use Atlan in your preferred language! ð Troubleshooting â If you wish to improve translations, such as for some specific technical term that's not translating correctly, you can directly submit that through Atlan's open source language support . Note that you need to have a Github account to be able to do so. Enter src/locales/default and choose the language you wish to update translations for. Submit a pull request to edit any existing translations or add new terms. Alternatively you can submit a request to Atlan support . Tags: atlan documentation Previous Add a resource Next How to use the filters menu Configure workspace language settings Configure personal language settings Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/concepts/how-to-interpret-timestamps",
    "text": "Use data Discovery Concepts How to interpret timestamps On this page How to interpret timestamps Atlan displays timestamps for assets in local timezones based on the location of your browser. The date and time display includes a combination of the following components: Relative time   -  for example, 2 hours ago , 1 day ago , 3 months ago Absolute time   -  for example, Mar 14, 2024, 9:40:01 AM Atlan displays the following timestamps in the asset sidebar: Overview tab â Usage -  timestamp for the number of read queries on an asset at source within a specific date range, as fetched from the miner run. This also includes an absolute time value for Last usage data updated (in Atlan) . Only applicable to data sources for which Atlan supports mining query history. Last queried -  timestamp for the latest read query on an assetÂ at source as fetched from the miner run or in Atlan within a specific date range. Only applicable to data sources for which Atlan supports mining query history. dbt run status -  status and timestamp for the last run of the dbt job updating an asset in dbt, as fetched from a dbt crawler run. Usage tab â Row update frequency -  timestamps for recent row updates on an asset at source within a specific date range, as fetched from the miner run. Only applicable to data sources for which Atlan supports mining query history. Up to five recent row updates will be displayed, if available. Properties tab â Last updated (in Atlan) -  timestamp for when any metadata attribute of the asset was last updated in Atlan. For example, when you linked a term or added a certificate to an asset in Atlan. Last synced with source Â   -  timestamp for when a workflow run last checked for this asset at source . This timestamp also includes a link to the connection workflow. Created (in Atlan) -  timestamp for when this asset was first created and published in Atlan during a crawler run. Last updated (on source) -  timestamp for when any metadata for the asset was last altered at source , as fetched from the crawler run. Created at (on source) -  timestamp for when the asset was first created at source , as fetched from the crawler run. Frequently asked questions â Why are metrics missing after miner runs? â If you notice a time lag from when your miner workflows last ran, note that Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. (Read more about miner logic here .) This may also depend on when your miner workflow was scheduled to run, which you can modify at any time. Why do some timestamps have variable time ranges? â Miners have a configurable property that governs the window of time for which metrics are reported. If a miner has been failing consistently, Atlan may reduce this window from 30 to only 14 days for reporting metrics. This is applicable to all date and time properties populated by miners. Is last updated in Atlan the same as last synced with source? â No, Last updated (in Atlan) records the time when any metadata is updated on the asset in Atlan while Last synced with source records the time when a workflow ran successfully updating the asset with changes from source, if any. If no metadata updates were made on the asset in Atlan before the next scheduled workflow run,Â Last synced with source Â may be considered as the more current timestamp reflecting any or no changes on the asset as fetched from source. Why are there discrepancies in time for some miner-related timestamps? â For timestamps related to miner runs: If no one has queried the asset at source, the timestamp for Last queried may be older than the date range recorded for Usage . If no one has used the asset at source for the duration of time that query history was mined, Row update frequency may not display any time value. Even when a miner run fails, it partially publishes assets, resulting in inconsistencies. For example, the date range in the Last queried tooltip may be for a successful miner run but the absolute time may be a different value if the asset had been partially published. Tags: atlan documentation Previous How do I use the filters menu? Next What are asset profiles? Overview tab Usage tab Properties tab Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/faq",
    "text": "Use data Discovery FAQ Discovery FAQs On this page Discovery FAQs How does Atlan handle archived or deleted assets? â If an asset is removed from a workflow or a user loses permissions to an asset, the asset will be archived in Atlan. The asset will be unarchived with all the metadata intact if it is included in the next workflow run or users permissions are restored. In Atlan, an asset's typename and qualifiedName pair serves as a distinctive identifier. The qualifiedName is a string that has been concatenated and contains the asset's source, host, and hierarchy. The related asset in Atlan will not change unless a modification is made, such as changing the schema or table name. Is it possible to search for fields across all data sources? â If you know exactly what you're looking for, Atlan suggests using exact match search. Wrap the search terms within double quotation marks \" \" when typing it in the search bar. For example, \"instacart_total_users\" . For contextual search , the Assets page provides a number of filters to help you narrow down your search results. Filter data assets by various properties, asset types, and so on. The search results in Atlan also provide a quick count of all the resulting data assets, organized by asset type. These counts will alter in real time as you apply filters. What is included in the \"All assets\" view? â The All assets view includes all assets in your Atlan data estate. If your Atlan admin has turned off View \"All assets\" in Assets Discovery from Labs in the admin center , you may be unable to see this view. In that case, you'll only be able to view the assets curated for the persona(s) or purpose(s) you belong to in Atlan. Admin users will still have full access to all assets, even when this default behavior is turned off. Can I search by a value to find assets with that value? â Atlan allows you to search and discover metadata, not data. As a constantly updated data catalog of all your data assets and metadata, Atlan allows you to identify and access your data assets as well as the tribal knowledge and business context associated with them. The powerful, intelligent search returns relevant search results. Why do I not see more results when searching with the search bar (or Cmd + K)? â The search bar is meant to be a quick search option. It works best when you know the name of the asset. As such, it only loads 20 search results at a time. You can also quickly access your recently visited starred assets from the search bar. For more in-depth discovery, search from the Assets page: More results Additional filters What is the timezone for data display? â For asset properties such as Last updated (in Atlan) , Last synced with source , or Created (in Atlan) , the timestamps are displayed in local timezones based on the user's browser location. To learn more, see How to interpret timestamps . Asset Profile FAQs â Can we replace or rename existing assets? â The expected behavior will vary based on the supported connector and asset type. If the asset name is reflected in the qualifiedName of the asset and you rename the asset, Atlan will create a net-new asset. This is because the qualifiedName determines asset uniqueness in Atlan. Consider the following examples: The name of a Snowflake table is part of its qualifiedName . Any changes to the table name in Snowflake will result in a new qualifiedName , and thus the creation of a new asset in Atlan. Conversely, the qualifiedName of Microsoft Power BI reports are based on the UUIDs of the assets in Microsoft Power BI   -  no names are embedded. In this case, renaming a Microsoft Power BI report does not change its UUID in Microsoft Power BI. This means that the qualifiedName of that report in Atlan will remain unchanged. Atlan will simply update the existing asset. However, this may not be the case where a Microsoft Power BI table is concerned, as the table name is included in the qualifiedName of the asset. If your use case is to enable quick discovery in Atlan, consider adding a business-friendly alias to the asset. What is an activity log? â The activity log in the asset sidebar provides a changelog for your data assets. Having a record of all the changes made to an asset can help build trust in your data assets and promote transparency across your organization. View the activity log of an asset â To view the activity log of an asset: From the left menu of any screen in Atlan, click Assets . Click on an asset to view its asset profile. From the asset sidebar to the right, click the Activity tab to view the activity log. If an asset was updated from Google Sheets or Microsoft Excel , an Updated using Google Sheets or Updated using Microsoft Excel stamp will appear, respectively. (Optional) Click the Google Sheets or Microsoft Excel link to view the source spreadsheet. (Optional) To filter the activity log by a specific type of activity, under Activity , click the dropdown arrow and then: Click Alias to view alias activity by user and timestamp of update. Click Description to view any changes in the description of an asset. Click Starred to view starred activity by user and timestamp. Click Announcement to view any changes to the announcement on an asset. Click Terms to view any updates on linked terms . Click Certificate to view any changes in the certification status of an asset. Click Owners to view any changes to the ownership of an asset. Click Tag Added or Tag Removed to view any changes for tags directly added to an asset. Click Tag Added (Propagation) or Tag Removed (Propagation) to view any changes for tags propagated to an asset. Click Column Added , Column Deleted , or Column Updated to view column changes. Click Readme Added or Readme Updated to view when a README was created for an asset and which user created or updated it. (Optional) Click the Filter by column menu to filter your activity logs by specific columns. You can now view all the changes that were made to an asset in the activity log! ð Did you know? Activity logs for metadata changes are persisted throughout the lifecycle of the Atlan instance for your organization. Why do dbt descriptions keep getting deleted? â If you notice that the descriptions from a Snowflake table, for example, get deleted when dbt is your source of truth, it is likely that you have the data source scheduled to run after the dbt run. Atlan recommends following the order of operations as documented in How to order workflows . You can also use dbt's persist_docs feature to ensure that your metadata persists through workflow runs . What is the timeframe for recently verified assets? â Atlan displays up to 20 most recently verified assets at a time in the Recently verified assets section on your Atlan homepage. You can scroll down further and click Load more to view more recently verified assets. This is not based on any particular timeline. In fact, this list may include assets that were updated as long as a week ago, if no new assets were verified more recently. What signals Atlan to auto-add the deprecated certifications in Looker? â These certifications are sourced from Looker. Once the Looker crawler has run successfully and detected that an asset was deleted, Atlan will attached a Deprecated certificate to the asset because it no longer exists at source. For example: If tiles are deleted, the API response generated is titled Look Deleted . In this case, Atlan will add the deprecated certificate. If Atlan crawls any models that do not have a project associated with them (indicated by a missing project_name key in the model API response), Atlan adds a deprecated certificate to the asset. Is it possible to add PDFs to an asset README? â Atlan currently does not support embedding a PDF file in an asset README. Alternatively, PDF files can be linked as a resource . How does version control work for description changes in source tools vs. Atlan? â Considering that users may update metadata in Atlan and the source tool, Atlan manages descriptions in two fields -  populating either or both depending on where it was created or updated. This is the format Atlan follows: Source descriptions are stored in the description field Any description added or updated in Atlan is stored in the userDescription field Separating them into two fields ensures that the connection package does not override the descriptions entered by users in Atlan every time the workflow runs and updates the asset. This way, the description field in Atlan becomes the source of truth. There is one exception though. If the description field has not been edited at all in Atlan and the connection package only ever brings the descriptions from the source, the workflow will keep updating the description field with what is available at source   -  only source edits will come through. Once the description field is edited in Atlan, the userDescription field takes over. If you would like to restore the original source description, simply clear the description added in Atlan and it will automatically revert to the source description. As a best practice, we recommend all subsequent edits to the description be done in Atlan. This is valid across all connector workflows. Can Atlan track schema changes? â Atlan tracks schema changes for SQL sources, which can be viewed in the activity log of an asset. However, Atlan is not a schema registry or a data modeling tool such as dbt. Are different fonts supported for READMEs and descriptions? â Atlan currently does not support customizing the following for asset descriptions , READMEs , and README templates : Fonts Font sizes Font colors Can Atlan handle assets with a large number of rows and columns? â You can safely catalog your assets in Atlan, regardless of the number of rows or columns. Since Atlan only extracts metadata, data volume is not a consideration. Note that if an asset has a large number of columns, the column preview in the asset profile may take some time to load. Why do I not see any tables or columns under database, only schema? â If you're viewing related assets for databases from the Relations tab in the asset sidebar, Atlan will only display schemas. You can click the schema asset and then open the Relations tab of the schema to view tables and views. To find tables and views contained within a database, Atlan also recommends using the database asset type filter for quick discovery. Is the sample data preview cached? â When previewing sample data on any asset, Atlan retrieves the sample data from source each time. No data is cached in Atlan. Delete an asset â Have one or more assets in Atlan that you don't want to be there? You have several options for removing them. Specific assets â To remove targeted, specific assets use one of our SDKs or our open API . Set of related assets â To remove a set of related assets, such as an entire schema: Modify the connector's configuration with a filter to exclude the asset(s). Re-run the connector's workflow with the new configuration. Did you know? This will archive the schema and its assets. An entire connection's assets â To remove an entire connection and all its assets, see How to delete a connection . If an asset is deleted via API, will workflows recreate the asset on the next run? â If you delete an asset at source, Atlan will archive that asset in the next crawler run. Atlan recommends that you manage additions or deletions through source workflows. Considering that Atlan workflows run differential crawls, any changes to your assets will be reflected in subsequent crawler runs. This helps deliver faster runtimes and improves workflow performance. If you delete an asset using APIs, there can be two scenarios: Archived: Assets are soft-deleted, moving to an Archived state. The next workflow run will restore the asset only if any changes are made to any of its source metadata properties   -  for example, a new column was added. You can also restore archived assets . Hard-deleted: The asset is completely removed from the metastore. The next workflow run creates a new asset if the previously deleted asset is still present or reintroduced in the source system. For example, this can happen if the asset was never actually removed from the source or if it was deleted and later recreated. However, the newly created asset doesn't retain any of its original Atlan metadata. You can't restore hard-deleted assets. Atlan recommends the deletion of assets to be managed by source workflows. The delete and restore endpoints are generally meant for assets created via APIs. Add a README â A README is an essential part of every code repository. The better the README, the more collaborators will want to work on your code. The same holds true for your data assets. Each data asset should have its own README, which provides a description of its characteristics and other critical information. Atlan allows users to add a README for every data asset, using an intuitive, rich text editor. You can document the tribal knowledge associated with each data asset in a README and reduce dependencies on your team members. The README appears right below each data table in the asset profile, displaying the data and the metadata on the same page and bridging the gap between the two. Atlan currently supports the following file types for asset READMEs: Google Docs Google Sheets Google Slides Miro boards FigJam boards Lucidchart dbdiagram ERD Lab Microsoft Word Microsoft Excel Microsoft PowerPoint Google Data Studio Google Looker Studio Canva Add a README â A README can be added to different types of data assets in Atlan, including BI dashboards, widgets, columns, databases, and schemas. The character limit for READMEs is 100,000 characters. A portion of this limit is used to ensure compatibility with rich text formatting, slightly reducing the available character limit. To add a README to an asset, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to view its asset profile. In the Readme section of the asset profile, click +Add . You can either: Click Blank Page to create a new README from scratch. Click Use to select an existing template as a starting point. Enter your knowledge into the README. Type / to use the text editor options to format your text, embed links, and more: Click Heading 1 to add a title or main heading. Click Heading 2 to add subheadings and create sections. Click Heading 3 to create subcategories within sections. Click Bulleted List to create a bulleted list. Click Numbered List to create an ordered list. Click Checklist to create a checklist of items. Click Formula to add formulae from a list of supported functions . Click Code to add a code snippet. Click Image to embed or upload an image. Click Quote to add block quotations. Click Mention to tag another user in your Atlan workspace. Click Table to create a table. Click Google Docs to paste a Google Doc link and embed your online documents. Click Google Sheets to paste a Google Sheets link and embed your online spreadsheets. Click Google Slides to paste a Google Slides link and embed your online presentations. Click Miro Board to paste a Miro board link and embed your boards. Click FigJam to paste a FigJam link and embed your boards. Click Lucidchart to paste a Lucidchart link and embed your documents or models. danger To embed a Lucidchart document or model, you will need to activate the embed code. Activating an embed code will disable password protection on published documents and make them accessible publicly. Click DBDiagram to paste a dbdiagram link and embed your database diagrams. Click ERD Lab to paste an ERD Lab link and embed your entity relationship diagrams. Click Microsoft Word to paste a Microsoft Word link and embed your online documents. Click Microsoft Excel to paste a Microsoft Excel link and embed your online spreadsheets. Click Microsoft PowerPoint to paste a Microsoft PowerPoint link and embed your online presentations. Click Google Data Studio or Google Looker Studio to paste a Google Data Studio or Google Looker Studio link and embed your reports and dashboards. Click Canva to paste a Canva link and embed your Canva graphics and presentations. Click Save . Your README is ready to be shared! ð Although it may take some time to create, a README is a critical step for documenting any data asset and making it trustworthy. Use README shortcuts â Atlan supports keyboard and markdown shortcuts to supercharge your README documentation. Keyboard shortcuts â In the table below, Mod stands for modifier key   - Command for Mac and Ctrl for Windows. Shortcut Action Mod + Shift + B block quote Mod + B bold Mod + I italics Mod + Shift + 8 bulleted list Mod + E code Mod + Alt + C code block Enter thrice, â exit code block Mod + Alt + 1 heading 1 Mod + Alt + 2 heading 2 Mod + Alt + 3 heading 3 Mod + Alt + 4 heading 4 Mod + Alt + 5 heading 5 Mod + Alt + 1 heading 6 Enter add new list item Shift + Tab uplift list item Tab sink list item Mod + Shift + 7 ordered list Mod + Alt + 0 set paragraph Mod + Shift + X strikethrough Mod + U underline Mod + Shift + 9 toggle task list Mod + Shift + L left align text Mod + Shift + E center align text Mod + Shift + R right align text Mod + Shift + J justify text Mod + Shift + H highlight text Within a table: Â Tab go to next cell Shift + Tab go to previous cell Backspace delete table when all cells are selected Mod - Backspace delete table when all cells are selected Delete delete table when all cells are selected Mod - Delete delete table when all cells are selected Markdown shortcuts â Markdown shortcuts are triggered by pressing space after the shortcut   -  except for bold and italics. For example, to add a block quote, type > and then tap the spacebar. Shortcut Action > block quote ** (text) ** , __ bold * (text) * , _ italics - bulleted list ` code ``` , ~~~ code block   heading 1    heading 2     heading 3      heading 4       heading 5        heading 6 --- , -- , ___ horizontal divider any numeric digit ordered list ~~ strikethrough [] unchecked task [x] checked task == highlight Does Atlan support asset previews? â Yes, Atlan provides asset previews for supported tools to help with quick discovery and give you the context you need. Typically, the What does Atlan crawl from (connector name)? documentation will indicate whether asset previews are supported for a specific connector. For example, Atlan supports asset previews for: Tableau worksheets and dashboards Microsoft Power BI reports Google BigQuery tables, views, and materialized views Sigma workbooks Can I search for assets by README, description, or other metadata? â If the keywords you're searching by is present in the asset name or description , only then will the asset appear in your search results. You can also use a variety of filters to narrow down your search. Note that asset READMEs are currently not searchable in Atlan. This is because Atlan stores them as a relation to a data asset rather than as a direct metadata attribute. Can I add the Google Sheets extension for everyone in my organization? â Yes! To install the Google Sheets Atlan extension at the workspace level, follow the instructions in this guide . You will need to be an administrator or have access to the admin console of your organization's Google account for this setup. Once installed, users in your organization can connect Atlan with Google Sheets to start using the extension. Can I embed presentations or docs from SharePoint in a README? â Yes, you can embed links to your Microsoft Word, Excel, and PowerPoint files stored in SharePoint or OneDrive in READMEs. Refer to How to add a README to learn more. Why can hard deleted assets be immediately restored after deletion? â The API is eventually consistent. This means that there is a short window of time, up to a few minutes, during which you can use the restore API endpoint to restore the asset that was hard deleted   -  even if it no longer appears on the UI. How can I make external documents and spreadsheets appear as assets in Atlan? â Atlan supports cataloging files through APIs. The supported file types include DOC, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files. Will Atlan send a notification if there's a new table added to my schema? â You can configure any of the following options to receive notifications on asset additions: Create a webhook Use the asset change notification custom package Tags: discovery faq search assets data faq-discovery Previous Provide credentials to view sample data How does Atlan handle archived or deleted assets? Is it possible to search for fields across all data sources? What is included in the \"All assets\" view? Can I search by a value to find assets with that value? Why do I not see more results when searching with the search bar (or Cmd + K)? What is the timezone for data display? Asset Profile FAQs Can we replace or rename existing assets? What is an activity log? Why do dbt descriptions keep getting deleted? What is the timeframe for recently verified assets? What signals Atlan to auto-add the deprecated certifications in Looker? Is it possible to add PDFs to an asset README? How does version control work for description changes in source tools vs. Atlan? Can Atlan track schema changes? Are different fonts supported for READMEs and descriptions? Can Atlan handle assets with a large number of rows and columns? Why do I not see any tables or columns under database, only schema? Is the sample data preview cached? Delete an asset If an asset is deleted via API, will workflows recreate the asset on the next run? Add a README Does Atlan support asset previews? Can I search for assets by README, description, or other metadata? Can I add the Google Sheets extension for everyone in my organization? Can I embed presentations or docs from SharePoint in a README? Why can hard deleted assets be immediately restored after deletion? How can I make external documents and spreadsheets appear as assets in Atlan? Will Atlan send a notification if there's a new table added to my schema?"
  },
  {
    "url": "https://docs.atlan.com/tags/discovery",
    "text": "9 docs tagged with \"discovery\" View all tags Discovery Find, understand, and use data assets across your organization with powerful search, filtering, and browsing features. Discovery FAQs Frequently asked questions about Atlan's Discovery capabilities. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. OpenLineage configuration and facets Learn about openlineage configuration and facets. Troubleshooting Amazon DynamoDB connectivity Learn about troubleshooting amazon dynamodb connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Tableau connectivity Learn about troubleshooting tableau connectivity. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/search",
    "text": "2 docs tagged with \"search\" View all tags Discovery Find, understand, and use data assets across your organization with powerful search, filtering, and browsing features. Discovery FAQs Frequently asked questions about Atlan's Discovery capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/browse",
    "text": "One doc tagged with \"browse\" View all tags Discovery Find, understand, and use data assets across your organization with powerful search, filtering, and browsing features."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts/how-tos/create-data-contracts",
    "text": "Build governance Contracts Get Started Create data contracts On this page Create data contracts Private Preview A data contract is an agreement between a data producer and consumer that specifies requirements for generating and using high-quality, reliable data. As a powerful tool for data management, data contracts can help you standardize contractual obligations between data producers and consumers, organize your assets with embeddable contract metadata, and enforce them with data quality rules. In Atlan, you can directly add a data contract to supported assets and provide helpful context to your downstream users. For a data contract to help build trust in your assets, it should be: Templatized and easily comprehensible   -  use Atlan's YAML contract template to create standardized contracts and push to Atlan. Version-controlled   -  continuously validate and monitor your data contracts either in runtime or real-time. Embeddable   -  embed the contract as metadata for a supported asset. Enforceable   -  enforce your contracts with data quality rules. Extensible   -  identify new specifications, generate new versions, and then compare and contrast them. Did you know? You can create webhooks for data contracts and receive notifications for when a contract is added or updated to a URL of your choice. Supported asset types â You can create data contracts for the following asset types: Tables Views Materialized views Output port assets of data products Supported asset metadata â Atlan maps the following asset metadata properties to it contract properties: Metadata property Contract property name dataset typeName type userDescription or description description ownerUsers owner.users ownerGroups owners.groups certificateStatus certification.status certificateStatusMessage certification.message announcementType announcement.type announcementTitle announcement.title announcementMessage announcement.description meaningNames terms classificationDef.displayName tags.name classifications.propagate tags.propagate classifications.restrict_propagation_through_lineage tags.restrict_propagation_through_lineage classifications.restrict_propagation_through_hierarchy tags.restrict_propagation_through_hierarchy column.name columns.name column.userDescription or column.description columns.description column.dataType columns.data_type column.isPrimary columns.primary !column.isNullable columns.required column.precision columns.precision column.numericScale columns.scale tags on column columns.tags column.meaningNames columns.terms custom metadata (CM) custom_metadata.<CM> Add a data contract to an asset â Who can do this? Any non-guest user with edit access to an asset's metadata can create, deploy, and manage data contracts. This only includes admin and member users in Atlan. To add a data contract to an asset, you can either: Create a contract directly in Atlan from the Contracts tab of the asset profile. You can create and maintain data contracts as easily as editing a word document. Use Atlan CLI to import an existing contract from your local machine to Atlan directly or through a CI/CD pipeline. Atlan CLI is a command-line tool that you can download directly from Atlan to your local machine to create and push data contracts to Atlan. Once you have published the contract, you can also sync metadata from a contract to the governed asset in Atlan. Once created, you will be able to monitor and manage your data contracts in Atlan. To add a data contract: From the left menu of any screen in Atlan, click Assets . (Optional) From the Filters menu on the left, click Properties and then click Has contract . Click No to filter for assets without a contract. From the Assets page, select an asset to open the asset sidebar. In the left Overview sidebar, click Add contract . In the Contract tab of the asset profile, you can either: Click Create contract to create a draft contract directly in Atlan based on asset metadata. Click Import contract to use Atlan CLI to import an existing contract from your local environment to Atlan. You will first need to install and connect Atlan CLI and then push the contract to Atlan. Refer to our developer documentation to complete the steps. (Optional) Click the Edit button to edit the contract. Congrats on adding a data contract in Atlan! ð View a data contract â To view a data contract: From the left menu of any screen in Atlan, click Assets . From the Assets page, select an asset to open the asset sidebar. From the left Overview sidebar, click View contract to navigate to the Contract Â tab in the asset profile. (Optional) In the Contract tab, you can view the contract specifications for your asset in a YAML format. You can also: Click the Document icon to open a read-only, simplified view of your contract. Next to Published version , click the version dropdown to view the latest version of the contract. Select an older version and then click CompareÂ with published version to compare them side by side. Click the Edit button to edit the contract. Click the clipboard icon to copy the YAML code. Under Timeline , view a timeline for the evolution of your contract. Under Summary , view details of who last updated your contract and when. Tags: atlan documentation Previous Contracts Next Add contract impact analysis in GitHub Supported asset types Supported asset metadata Add a data contract to an asset View a data contract"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts/how-tos/add-contract-impact-analysis-in-github",
    "text": "Build governance Contracts Impact Analysis Add contract impact analysis in GitHub On this page Add contract impact analysis in GitHub Private Preview Impact analysis helps you identify how modifications to your data contracts might impact downstream processes, data quality, and overall business operations. This can help you analyze proposed changes and mitigate potential risks before implementation. If you have ever changed a data contract only to find out later that it broke a downstream table or dashboard, Atlan provides a GitHub Action to help you out. This action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request. Prerequisites â Before running the action, you will need to create an Atlan API token . You will also need to assign a persona to the API token and add a metadata policy that provides the requisite permissions on assets for the Atlan action to work. For example, you can add the following permissions: Asset, such as a table   - Read only Any downstream connections, such as Microsoft Power BI   - Read only You will need to configure the default GITHUB_TOKEN permissions. Grant Read and write permissions to the GITHUB_TOKEN in your repository to allow the atlan-action to seamlessly add or update comments on pull requests. Refer to GitHub documentation to learn more. Configure the action â To set up the Atlan action in GitHub: Create repository secrets in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token. Add the GitHub Action to your workflow: Create a workflow file in your repository   - .github/workflows/atlan-action.yml . Add the following code to your workflow file: name : Atlan action on : pull_request : types : [ opened , edited , synchronize , reopened , closed ] jobs : get-downstream-impact : name : Get Downstream Assets runs-on : ubuntu - latest steps : - name : Checkout uses : actions/checkout@v4 - name : Run Action uses : atlanhq/atlan - action@v1 with : GITHUB_TOKEN : $ { { secrets.GITHUB_TOKEN } } ATLAN_INSTANCE_URL : $ { { secrets.ATLAN_INSTANCE_URL } } ATLAN_API_TOKEN : $ { { secrets.ATLAN_API_TOKEN } } ATLAN_CONFIG : .atlan/config.yaml Test the action â After you've completed the configuration above, create a pull request with a changed Atlan data contract file to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request: The GitHub workflow will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. The comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms. View impacted assets directly in Atlan. Inputs â Name Description Required GITHUB_TOKEN For writing comments on PRs to print downstream assets true ATLAN_INSTANCE_URL For making API requests to the user's tenant true ATLAN_API_TOKEN For authenticating API requests to the user's tenant true ATLAN_CONFIG For impact analysis of Atlan data contracts , if included in a GitHub PR true Tags: data api Previous Create data contracts Prerequisites Configure the action Test the action Inputs"
  },
  {
    "url": "https://docs.atlan.com/tags/contracts",
    "text": "One doc tagged with \"contracts\" View all tags Contracts Learn how to manage data contracts and agreements in Atlan to ensure data quality and compliance."
  },
  {
    "url": "https://docs.atlan.com/tags/agreements",
    "text": "One doc tagged with \"agreements\" View all tags Contracts Learn how to manage data contracts and agreements in Atlan to ensure data quality and compliance."
  },
  {
    "url": "https://docs.atlan.com/tags/data-quality",
    "text": "20 docs tagged with \"data quality\" View all tags Anomalo Integrate, catalog, and govern Anomalo assets in Atlan. Configure alerts Set up real-time notifications for data quality rule failures via Slack or Microsoft Teams. Contracts Learn how to manage data contracts and agreements in Atlan to ensure data quality and compliance. Data quality permissions Reference for data quality permission scopes and configuration in Atlan. Databricks Data Quality Studio Set up and configure Databricks for data quality monitoring through Atlan. Enable auto re-attachment of rules Learn how to enable automatic re-attachment of data quality rules to Snowflake tables and views. Enable data quality on connection Enable and configure data quality for your Databricks connection in Atlan. Enable data quality on connection Enable and configure data quality for your Snowflake connection in Atlan. Monte Carlo Integrate, catalog, and govern Monte Carlo assets in Atlan. Operations Atlan crawls and manages the following data quality operations and results from Snowflake. Roles and permissions Explanation of Snowflake's security model and role requirements for data quality operations. Rules and dimensions Reference for available data quality rules and classification dimensions in Snowflake data quality. Set up Databricks Configure Databricks to enable data quality monitoring through Atlan. Set up Snowflake Configure Snowflake to enable data quality monitoring through Atlan. Setup and configuration Common questions about Databricks data quality setup and configuration. Snowflake Data Quality Studio Set up and configure Snowflake for data quality monitoring through Atlan. Soda Integrate, catalog, and govern Soda assets in Atlan. Upgrade to Snowflake data quality studio Update existing Snowflake data quality integration to the latest version What's auto re-attachment Understand automatic re-attachment of data quality rules to assets that are dropped and recreated. What's Data Quality Studio Understand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/governance",
    "text": "18 docs tagged with \"governance\" View all tags Access Control Learn how to manage user permissions and access to data assets in Atlan for security and compliance. Automatically assign roles based on group names Learn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app. Contracts Learn how to manage data contracts and agreements in Atlan to ensure data quality and compliance. Custom Metadata Learn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information. Data Products Create and manage data products to organize and govern your data assets by domain. Databricks Data Quality Studio Set up and configure Databricks for data quality monitoring through Atlan. Domains Learn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way. Glossary Learn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization. Requests Request and manage changes to assets that you don't have direct edit access to. Set up Databricks Configure Databricks to enable data quality monitoring through Atlan. Set up Snowflake Configure Snowflake to enable data quality monitoring through Atlan. Snowflake Data Quality Studio Set up and configure Snowflake for data quality monitoring through Atlan. Stewardship Learn how to implement data stewardship in Atlan through automated workflows, policies, and task management. Tags Learn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance. Upgrade to Snowflake data quality studio Update existing Snowflake data quality integration to the latest version User Role Sync Complete configuration reference for the User Role Sync app properties and settings. Users and groups Learn how to manage users and groups in Atlan to control access and organize your data team. What's Data Quality Studio Understand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/integrations",
    "text": "14 docs tagged with \"integrations\" View all tags Atlan MCP Overview Learn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup. Automation Integrations Integrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On. Collaboration Integrations Integrate Atlan with collaboration tools like Microsoft Teams and Slack. Communication Integrations Integrate Atlan with communication tools like SMTP and Announcements. Connections Integration Integrate Atlan with Connections to create webhooks and automate notifications. Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance. Export Assets :::warning Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to [enable asset export](enable-asset-export). Identity Management Integrations Integrate Atlan with identity management tools like SCIM and SSO. Integrations Learn how to integrate Atlan with project management, communication, collaboration, automation, and identity management tools. Project Management Integrations Integrate Atlan with project management tools like Jira and ServiceNow. SCIM Integration Integrate Atlan with SCIM to automate user provisioning. SMTP and Announcements Integration Integrate Atlan with SMTP to send announcements and notifications. SSO Integration Integrate Atlan with SSO to enable secure authentication and access control. Webhooks Integration Integrate Atlan with Webhooks to automate actions and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/setup",
    "text": "54 docs tagged with \"setup\" View all tags Configure SCIM provisioning You can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning. Configure SMTP Atlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and [scheduled queries](/product/capabilities/insights/how-tos/schedule-a-query). We provide an embedded SMTP server to do this, out-of-the-box. Configure Snowflake data metric functions Configure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Crawl Amazon DynamoDB Once you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB. Crawl Amazon Redshift Once you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift. Crawl Apache Kafka Learn about crawl apache kafka. Crawl AWS Glue Once you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue. Crawl Confluent Kafka Learn about crawl confluent kafka. Crawl Databricks To crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl dbt Once you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan. Crawl Domo Once you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo. Crawl Google BigQuery Once you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery. Crawl Looker Once you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker. Crawl Matillion Once you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion. Crawl Metabase Once you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase. Crawl Mode Once you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode. Crawl MongoDB Once you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB. Crawl Monte Carlo Once you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo. Crawl Oracle Once you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle create-user-in-oracle), you can establish a connection between Atlan and Oracle. Crawl Qlik Sense Cloud Once you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud. Crawl Redash Once you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash. Crawl Salesforce Once you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce. Crawl SAP ECC To crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl SAP HANA Once you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA. Crawl SAP S/4HANA To crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl Sigma Once you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma. Crawl Sisense Once you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense. Crawl Soda Once you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda. Crawl Teradata Once you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata. Crawl ThoughtSpot Once you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot. Create an AWS Lambda trigger Once you have configured the [AWS Lambda permissions](/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda), you can run an AWS Lambda function. Enable data quality on connection Enable and configure data quality for your Databricks connection in Atlan. Enable data quality on connection Enable and configure data quality for your Snowflake connection in Atlan. Integrations Learn how to integrate Atlan with project management, communication, collaboration, automation, and identity management tools. Manage dbt tags Atlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags. Mine Google BigQuery Once you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage. Mine Snowflake Once you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage. Mine Teradata Once you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage. Set up Claude with Remote MCP Learn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access. Set up CrateDB Configure authentication and connection settings for CrateDB connector Set up cross-workspace extraction Configure a single service principal to crawl metadata from all workspaces within a Databricks metastore using system tables Set up Cursor with Remote MCP Learn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access. Set up Databricks Configure Databricks to enable data quality monitoring through Atlan. Set up Informatica CDI Configure authentication and user permissions for Informatica Cloud Data Integration connector Set up Local MCP Server The Atlan Model Context Protocol (MCP) server enables seamless interaction between external tools and Atlan services. It defines the data structures and conventions used to prepare and transmit context for model execution, making it easier to integrate Atlan into your existing workflows. Set up Matillion Configure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance. Set up Microsoft Copilot Studio with Remote MCP Learn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication. Set up n8n with Remote MCP Learn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows. Set up SAP ECC Set up user accounts and permissions required for SAP ECC metadata extraction in Atlan. Set up SAP S/4HANA Set up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan. Set up Snowflake Configure Snowflake to enable data quality monitoring through Atlan. Set up Windsurf with Remote MCP Learn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication. Troubleshooting Atlan browser extension Can I add the browser extension for everyone in my organization? Troubleshooting SSO Can I change the username of a provisioned user in Atlan?"
  },
  {
    "url": "https://docs.atlan.com/cdn-cgi/l/email-protection",
    "text": "Please enable cookies. Email Protection You are unable to access this email address atlan.com The website from which you got to this page is protected by Cloudflare. Email addresses on that page have been hidden in order to keep them from being accessed by malicious bots. You must enable Javascript in your browser in order to decode the e-mail address . If you have a website and are interested in protecting it in a similar way, you can sign up for Cloudflare . How does Cloudflare protect email addresses on website from spammers? Can I sign up for Cloudflare? Cloudflare Ray ID: 97d02f310cb3ff5b • Your IP: Click to reveal 35.192.169.128 • Performance & security by Cloudflare"
  },
  {
    "url": "https://docs.atlan.com/tags/support",
    "text": "2 docs tagged with \"support\" View all tags Customer support Learn about customer support at Atlan. Frequently Asked Questions Find answers to common questions about using Atlan, organized by topic area for quick resolution."
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/data-and-metadata-persistence",
    "text": "Get Started Core Concepts Data and metadata persistence On this page Data and metadata persistence Atlan is a fully virtualized solution that does not involve moving data from existing storage layers. Atlan crawls metadata from upstream data sources and stores it in a secure VPC (virtual private cloud). Atlan pushes any queries to existing processing layers. For example, directly to your database, warehouse, or a processing layer such as Athena or Presto on top of blob storage. So the data itself stays put   -  Atlan does not move or store it. Not sure on the difference between data and metadata ? Try our helpful primer . Data previews and queries â Atlan gives users the ability to see sample data previews for a data asset as well as the results for any queries run on Atlan. In both cases, Atlan pushes the request upstream to the data source, and shows a 100-row sample of the result to Atlan users. Atlan does not cache any of this data. So each time a user previews or queries data, it is re-queried from the source. Every time a user runs a query, Atlan streams query results in batches directly from your data source. Since the data is streamed in real-time from the data source, there is no need to persist the query results in Atlan's cache or storage layer. This ensures that the data displayed is always up to date and accurate, eliminating the need for storing intermediate query results. Metadata storage â Atlan stores the metadata it collects and creates in applications and databases within the VPC. This includes: asset metadata user data Asset metadata â Atlan stores asset metadata, including lineage, in: Apache Atlas, a graph database layer that stores entity relationships and attributes Elasticsearch, to optimize search on the product Cassandra, as the persistence back-end User data â Atlan stores data on users, roles, and groups in its own PostgreSQL database. Keycloak uses this information for access and identity management. Atlan hashes all sensitive fields like passwords and stores them securely. Any user data transmitted over the internet is SSL-encrypted over HTTPS. Tags: upstream-dependencies data-sources Previous Authentication and authorization Next Encryption and key management Data previews and queries Metadata storage"
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/encryption-and-key-management",
    "text": "Get Started Core Concepts Encryption and key management On this page Encryption and key management Atlan has adopted global industry standards in security practices and solutions. Amazon S3 server-side encryption secures the S3 bucket launched by Atlan. Atlan uses AES-256 as the SSE algorithm in the S3 bucket. All the EBS (Elastic Block Storage) launched by Atlan is encrypted. Atlan uses encrypted storage classes to provision persistent volumes to the microservices running inside the Kubernetes cluster. Key and credential management â Atlan uses HashiCorp Vault to manage the following: Keys   -  Vault manages encryption keys to encrypt sensitive data at rest and in transit. SecretsÂ   -  Vault encrypts and securely stores secrets such as API keys, tokens, and credentials. Passwords   -  passwords are hashed and stored encrypted. Data in transit â Atlan uses standard encryption to protect data in transit. Atlan uses hypertext transfer protocol secure (HTTPS) for secure communication when data is in transit. This protocol is encrypted using Transport Layer Security (TLS). Two-factor authentication (2FA) is also supported for accessing resources. Data at rest â Data-At-Rest Encryption (DARE) is the encryption of data stored in different storage components and not moving through networks. Cloud storage â Atlan encrypts the data at rest in different cloud resources like volumes and cloud storage using cloud provider-managed keys. Amazon S3   -  Atlan uses server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data at rest in Amazon S3. This encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode (AES-GCM) to encrypt all uploaded objects. Azure Blob Storage   -  Atlan uses Microsoft-managed keys to encrypt the data at rest in Azure Blob Storage. This encryption uses 256-bit AES encryption and is FIPS 140-2 compliant. Google Cloud Storage   -  Atlan uses Google-managed keys to encrypt the data at rest in Google Cloud Storage. This encryption uses AES-256 using Galois/Counter Mode (GCM) to encrypt all uploaded objects. Volumes â Volumes are used by the StatefulSet running in the tenants. These volumes are encrypted at rest by the cloud provider-managed keys. Amazon Web Services (AWS)   -  Atlan uses the default Amazon Elastic Block Store (EBS) encryption for encrypting the data at rest in all the volumes. Amazon EBS encrypts volume with a data key using industry-standard AES-256 data encryption. Microsoft Azure   -  Atlan uses Azure Storage encryption , which uses server-side encryption (SSE), for encrypting the data at rest in all the volumes. Data in Azure managed disks is encrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant. Google Cloud Platform (GCP)   -  Atlan uses Google-managed encryption to encrypt the data at rest in all the volumes. This encryption uses the Advanced Encryption Standard (AES) algorithm, AES-256. Over the internet â Communication between the client and Atlan public endpoints is always conducted over hypertext transfer protocol secure (HTTPS). HTTPS is encrypted in order to increase the security of data transfer. Any user data transmitted over the internet is SSL-encrypted over HTTPS. Tags: security access-control permissions Previous Data and metadata persistence Next High availability and disaster recovery (HA/DR) Key and credential management Data in transit Data at rest"
  },
  {
    "url": "https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr",
    "text": "Get Started Core Concepts High availability and disaster recovery (HA/DR) On this page High availability and disaster recovery (HA/DR) High availability (HA) â Atlan uses Amazon Elastic Kubernetes Service (EKS) for high availability (HA). Amazon EKS runs and scales the Kubernetes control plane across multiple AWS Availability Zones to ensure high availability. It automatically scales control plane instances based on load, detects and replaces unhealthy control plane instances, and patches the control plane. The benefits of using this concept are: Scalability and reliability help the system remain stable Promotes self-healing to ensure that containers are running in a healthy state Handles node failures gracefully Auto-scaling enables automated cluster creation Application HA â Atlan ensures application HA through the following: Multiple replicas for both stateless and stateful applications Load balancing with services Rolling updates to maintain the availability threshold Pod-to-node distribution to ensure that critical application pods are running on dedicated nodes Usage of inter-Availability Zone (AZ) data transfers to avoid single-zone failure impact Disaster recovery (DR) â Atlan follows industry best practices for disaster recovery. Atlan uses Argo Workflows for orchestration to successfully implement a disaster recovery strategy and reduce production downtime, so that business impact is minimized in the event of an outage. If a disaster is detected, the Disaster Assessment Team   -  comprising key stakeholders from IT, platform, operations, and support   -  will be promptly notified through Atlanâs established communication channels. The team will conduct a thorough evaluation to determine the extent of the damage and prioritize remediation based on an internal list of critical services and applications. In case of a disaster, a tenant will be recreated, and the following actions performed to restore the tenant: Onboard a new tenant with no data and a different domain. Use Argo Workflows to restore the data from last backup. Scale down the previous tenant. Change the domain of the new tenant to that of the previous one. Update the Cloudflare record with the load balancer of the newly onboarded tenant. All the aforementioned action items are automated. The entire process of restoring all the components of a tenant from backup takes around 3-4 hours. In case of data loss for any particular component, it can also be recovered from the last backup. Here are a few parameters that help reduce downtime and expedite the process of disaster recovery: Infrastructure â Single-tenant SaaS is the default deployment option for most Atlan users. In this model, Atlan manages the infrastructure needs and ensures that all instances are spread across multiple Availability Zones (AZ) in each AWS Region where the user instance is deployed. Availability Zones are multiple, isolated locations within a single AWS Region. Multi-AZ deployments provide enhanced availability for instances within a single AWS Region. With multi-AZ, your data is synchronously replicated to standby in a different Availability Zone. danger Atlan currently does not support multi-region deployment. Atlan service overview â The diagram below illustrates the relationships and communication flows between each service.Â The bottom-most layer shows the services that are entirely independent, such as Cassandra, Postgres, and more. Most of the other services depend on these to function.Â Backups and restore â Atlan runs an automated daily backup of each tenant. By default, the backup is scheduled at 3:00 AM UTC, configurable as per the requirement of an organization. The backup of each tenant is stored in its respective cloud storage. The backups are encrypted at rest by the default cloud provider key. This key uses the Advanced Encryption Standard (AES) 256 algorithm. Since Atlan uses the cloud provider key, the key is rotated by the cloud provider. Atlan controls access to the cloud storage where the backup is stored, and only provides access in case of troubleshooting an issue. Each backup process captures a full backup of all the data, with no incremental backups being performed. Atlan also monitors the backup to ensure that backups are not skipped. Alerts are generated in case a backup run fails for the support team to examine the issue. The lifecycle policy for backups in the cloud provider is set to 15 days, which means Atlan will retain backups for all the components for 15 days. Backups of the following components are taken on a daily basis: Argo Workflows Elasticsearch Cassandra Redis Postgres Atlan can restore a single component in case of data corruption for any single point of failure, such as a metastore and its components like Elasticsearch and Cassandra. It is also possible to do a full-cluster restore in case of an unintended operation or a data loss or corruption event. Did you know? Argo Workflows powers all the backup and restore packages in Atlan. It includes a retry mechanism in case of any errors while completing the steps in the workflow. It also sends alerts in case of entire package failure as part of observability. Migration â Atlan has an easy process to migrate the application to other AWS Regions. In case of total region outage and the need for migrating an instance to another region or account, this migration activity will be performed via Atlanâs backup and restore packages. RTO, RPO, and retention â Greater RTOs and RPOs as well as system recovery are crucial for ensuring that multiple mission-critical applications are quickly restored. It is now possible to minimize the impact of a disruption and perform a recovery within a few hours of an outage. Atlan carries out a daily backup of all critical services once every 24 hours, so in a worst case scenario provides an RPO of 24 hours. For all critical applications, RTO is less than 3 hours. Atlan retains daily backups for 15 days. Post-recovery validation â The following post-recovery actions are performed: Post restoration, Atlan conducts data integrity checks to ensure that the restored data is accurate and complete. Atlan performs system tests to confirm that all components of the tenant are functioning correctly after restoration. Tags: lineage data-lineage impact-analysis Previous Encryption and key management Next Cloud logging and monitoring High availability (HA) Disaster recovery (DR) RTO, RPO, and retention Post-recovery validation"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-azure-ad-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Get Started How to enable Azure AD for SSO On this page Enable  Azure AD for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your Azure AD administrator to carry out the tasks below in Azure AD. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate Azure AD SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) â To choose Azure AD as your SSO provider, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Azure AD and then clickÂ Configure . UnderÂ Service provider metadata , copy theÂ Identifier (Entity ID) , Reply URL (Assert Consumer Service URL) , and Logout Url . Set up SAML app (in Azure AD) â To set up a SAML app, within Azure's portal : From the menu on the left, open Azure Active Directory . UnderÂ Azure Active Directory | Overview click theÂ Add button and thenÂ Enterprise application . UnderÂ Browser Azure AD Gallery click theÂ Create your own application button: ForÂ What's the name of your app? enter a name, such asÂ Atlan . ForÂ What are you looking to do with your application? selectÂ Integrate any other application you don't find in the gallery (Non-gallery) . At the bottom of theÂ Create your own application dialog, click the Create button. Wait for the application details to be shown   -  this can take around 1 minute. UnderÂ Getting Started and within the Set up single sign on tile, click the Get started link. UnderÂ Select a single sign-on method click theÂ SAML tile. In the upper-right of the Basic SAML Configuration card, click the Edit button and enter: ForÂ Identifier (Entity ID) click Add identifier and enter the value you copied from Atlan above. ForÂ Reply URL (Assertion Consumer Service URL) click Add reply URL (twice) and enter the two values you copied from Atlan above. The longer URL should be enabled under the Default column. ForÂ Logout Url (Optional) enter the value you copied from Atlan above. At the top of the page, underÂ Basic SAML Configuration , click theÂ Save button. Close theÂ Basic SAML Configuration dialog by clicking X in the upper-right. In the upper-right of the Attributes & Claims card, click the Edit button: Navigate to the Additional claims section, click each of the following claims to modify their Name exactly as suggested below and remove the Namespace value: email   - > user.mail firstName   - > user.givenname lastName   - > user.surname (Optional) username   - > ExtractMailPrefix(user.mail) info ðª Did you know? For users assigned to Atlan through SSO, the username will be populated from the username mapping. Otherwise, the username will be the email prefix by default, which users can update while registering on Atlan for the first time. To configure group claims:Â From the options along the top, click + Add a group claim . In the popover, under Which groups associated with the user should be returned in the claim? , select Groups assigned to the application . From the Source attribute dropdown, select Cloud-only group display names (Preview) . If you have a hybrid setup , select sAMAccountName instead and then check the Emit group name for cloud-only groups checkbox. danger Please ensure that the Cloud-only group display names attribute contains the actual group display names. If not, you will need to update the source attribute with the relevant one that contains group display names. Click Advanced options to expand the dropdown menu: Check the Customize the name of the group claim box. For Name , enter memberOf . This is required if you want to retain group membership in Atlan. Click Save and close the popover by clickingÂ X in the upper-right. Download Azure AD's metadata file (in Azure AD) â To download Azure AD's metadata file, within the same Azure AD app's SAML-based Sign-on page: Within theÂ SAML Signing Certificate card, to the right ofÂ Federation Metadata XML , click theÂ Download link. Within theÂ Set up <application> card, copy theÂ Logout URL . Assign users or groups to the app (in Azure AD) â To assign users or groups to the app, within the Azure AD application's page: UnderÂ Manage, click Users and groups . At the top of the table, click theÂ Add user/group button. In the resulting Add Assignment dialog, under the Users or Groups Â heading, click the None Selected link. In the resulting Users or Groups Â dialog, search for users or groups to add and click to select them. When finished, at the bottom of theÂ Users or Groups dialog, click the Select button. At the bottom of theÂ Add Assignment dialog, click theÂ Assign button. Upload Azure AD's metadata file (in Atlan) â To complete the configuration of Azure AD SSO, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Azure AD and then clickÂ Configure . To the right ofÂ Identity provider metadata click the Import from XML button. Select the XML file downloaded from Azure AD above. UnderÂ Single Logout Service URL , enter the logout URL copied from Azure AD above. At the bottom of the screen, clickÂ Save . Congratulations   -  you have successfully set up Azure AD SSO in Atlan! ð Did you know? By default, users can now log into Atlan with either Azure AD SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your Azure AD administrator to provision access to users through the Azure portal and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings â danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from Azure AD to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan   -  irrespective of whether or not you enable SCIM . To automatically assign Azure AD users to Atlan groups based on their Azure AD groups, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in Azure AD to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each Azure AD group with access to Atlan. Click theÂ Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð Did you know? You can configure SCIM provisioning in Azure AD to manage your users and groups in Atlan. Plus, once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous SSO Integration Next How to enable Google for SSO Choose SSO provider (in Atlan) Set up SAML app (in Azure AD) Download Azure AD's metadata file (in Azure AD) Assign users or groups to the app (in Azure AD) Upload Azure AD's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-google-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Get Started How to enable Google for SSO On this page Enable  Google for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your Google domain administrator to carry out the tasks below in the Google Admin Center. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate Google SSO for Atlan, complete the fo llowing steps. Choose SSO provider (in Atlan) â To choose Google as your SSO provider, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Google and then clickÂ Configure . UnderÂ Service provider metadata , copy theÂ ACS URL andÂ Entity ID . Set up SAML app (in Google Admin Center) â To set up a SAML app, within Google Admin Center : From the menu on the left, expandÂ Apps and then click onÂ Web and mobile apps . At the top of the table, click theÂ Add app link and then clickÂ Add custom SAML app . Enter a name for your app, such asÂ Atlan and then click the Continue Â button. UnderÂ Option 1: Download IdP metadata click theÂ Download metadata button, save the file, and then click the Continue button. Under Service provider details , enter your Atlan SAML settings: ForÂ ACS URL , enter the value you copied from Atlan above. ForÂ Entity ID , enter the value you copied from Atlan above. Click theÂ Continue button. UnderÂ Attributes , define the following mappings from Google Directory attributes on the left to App attributes on the right: Primary email   - > email First name   - > firstName Last name   - > lastName (Optional) To configure group mapping in Atlan , under Group membership (optional) , enter the following details: For Google Groups , select all the Google groups you want to map to Atlan. You can select up to 75 groups in total. For App attribute , enter memberOf . This is required if you want to retain group membership in Atlan. At the end of the form, click the Finish button. Assign users to the app (in Google Admin Center) â To assign users to the app, within Google Admin Center : From the app page, expand User access . UnderÂ Service status change toÂ ON for everyone Â and then clickÂ Save . Upload Google's metadata file (in Atlan) â To complete the configuration of Google SSO, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Google and then clickÂ Configure . To the right ofÂ Identity provider metadata click the Import from XML button. Select the GoogleIDPMetadata.xml file downloaded from Google above. At the bottom of the screen, clickÂ Save . Congratulations   -  you have successfully set up Google SSO in Atlan! ð Did you know? By default, users can now log into Atlan with either Google SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your Google domain administrator to provision access to users through the Google Admin Center and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings â danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from Google to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign Google users to Atlan groups based on their Google groups, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under theÂ SSO Groups column, type the name of the corresponding group in Google to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each Google group with access to Atlan. Click theÂ Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable Azure AD for SSO Next How to enable JumpCloud for SSO Choose SSO provider (in Atlan) Set up SAML app (in Google Admin Center) Assign users to the app (in Google Admin Center) Upload Google's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-jumpcloud-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Get Started How to enable JumpCloud for SSO On this page Enable  JumpCloud for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your JumpCloud administrator to carry out the tasks below in JumpCloud. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate JumpCloud SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) â To choose JumpCloud as your SSO provider, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Jumpcloud and then clickÂ Configure . UnderÂ Service provider metadata , copy the IdP Entity ID , SP Entity ID , and ACS URL . Set up SAML app (in JumpCloud) â To set up a SAML app, within JumpCloud Console : From the menu on the left, under User Authentication click SSO . To the left of the search box, click the large circular + icon. At the bottom of the page, click theÂ Custom SAML App button. Under theÂ General Info tab, forÂ Display Label enter a name such asÂ Atlan . Change to the SSO tab and enter your Atlan SAML settings: ForÂ IdP Entity ID enter the value you copied from Atlan above. ForÂ SP Entity ID enter the value you copied from Atlan above. ForÂ ACS URL enter the value you copied from Atlan above. BelowÂ Signature Algorithm ensureÂ Sign Assertion is enabled. Scroll to the bottom of theÂ SSO tab and underÂ User Attribute Mapping click theÂ add attribute button. Define the following mappings fromÂ Service Provider Attribute Name on the left toÂ JumpCloud Attribute Name on the right: email   - > email firstName   - > firstname lastName   - > lastname group   - > group (you may need to selectÂ Custom User or Group Attribute from the JumpCloud Attribute Name drop-down, and then type in group ) username   - > username Under theÂ Group Attributes heading, enable theÂ include group attribute box and enter the value memberOf . This is required if you want to retain group membership in Atlan. Change to theÂ User Groups tab and check the box for each user group you want to be enabled for SSO. Below the form, click theÂ activate button and when prompted click the continue button. Download JumpCloud metadata file (in JumpCloud) â To download the JumpCloud metadata file, within JumpCloud Console : From the SSO app page, click your Atlan SSO application to open it. Change to the SSO tab and under JumpCloud Metadata click the Export Metadata button. Upload JumpCloud's metadata file (in Atlan) â To complete the configuration of JumpCloud SSO, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Jumpcloud and then clickÂ Configure . To the right ofÂ Identity provider metadata click the Import from XML button. Select the JumpCloud-saml2-metadata.xml file downloaded from JumpCloud above. At the bottom of the screen, clickÂ Save . Congratulations   -  you have successfully set up JumpCloud SSO in Atlan! ð Did you know? By default, users can now log into Atlan with either JumpCloud SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your JumpCloud administrator to provision access to users through JumpCloud and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings â danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from JumpCloud to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign JumpCloud users to Atlan groups based on their JumpCloud groups, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in JumpCloud to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each JumpCloud group with access to Atlan. Click theÂ Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable Google for SSO Next How to enable Okta for SSO Choose SSO provider (in Atlan) Set up SAML app (in JumpCloud) Download JumpCloud metadata file (in JumpCloud) Upload JumpCloud's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-okta-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Get Started How to enable Okta for SSO On this page Enable  Okta for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your Okta administrator to carry out the tasks below in Okta. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate Okta SSO for Atlan, complete the foll owing steps. Choose SSO provider (in Atlan) â To choose Okta as your SSO provider, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Okta and then clickÂ Configure . UnderÂ Service provider metadata , copy theÂ Single sign on URL Â andÂ Audience URI (SP Entity ID) . Set up SAML app (in Okta) â To set up a SAML app, within Okta's administration console: From the menu on the left, expandÂ Applications and then click onÂ Applications . At the top of the table, click theÂ Create App Integration button. In theÂ Create a new app integration dialog, selectÂ SAML 2.0 and then click Next . UnderÂ General Settings enter: ForÂ App name , enter a name for the application, such asÂ Atlan . Click theÂ Next button. UnderÂ SAML Settings - General enter: ForÂ Single sign on URL enter the value you copied from the field of the same name in Atlan above. EnsureÂ Use this for Recipient URL and Destination URL is enabled. ForÂ Audience URI (SP Entity ID) enter the value you copied from the field of the same name in Atlan above. UnderÂ Attribute Statements (optional) define the following mappings from Name (Name format) on the left to Value on the right: firstName (Basic)   - > user.firstName lastName (Basic)   - > user.lastName email (Basic)   - > user.email group (Basic)   - > user.group info ðª Did you know? For users assigned to Atlan through SSO, the username will be populated from the username mapping. Otherwise, the username will be the email prefix by default, which users can update while registering on Atlan for the first time. UnderÂ Group Attribute Statements (optional for SSO login, required for group sync) define the following mappings fromÂ Name (Name format) on the left to Filter on the right: memberOf (Unspecified)   - > Matches regex [\\s\\S]+ -  for examples of how to filter groups with regex in Okta, refer to Okta documentation . This is required if you want to retain group membership in Atlan. While this step is optional for basic SSO authentication, you must configure the memberOf attribute if you want to sync Okta groups to Atlan and use group-based access control At the bottom of the form, click the Next button. UnderÂ Help Okta Support understand how you configured this application select I'm an Okta customer adding an internal app and forÂ App type enableÂ This is an internal app that we have created . Click theÂ Finish button. Download Okta's metadata file (in Okta) â To download Okta's metadata file, within the Okta app's page: Open theÂ Sign On tab. Under theÂ SAML Signing Certificates heading, in the table, click theÂ Actions link under theÂ Actions column. From the drop-down, clickÂ View IdP metadata . Save the XML file, if it appears in plain text in your browser. Assign users to the app (in Okta) â To assign users to the app, within the Okta app's page: Open theÂ Assignments tab. At the top of the table, click theÂ Assign button and select Assign to People to add individual users or Assign to Groups to add groups. To the right of each user to whom you want to assign the application, click Assign . To assign the application to a group, you may have to locate it first. For individual users, confirm that the data is correct in the Assign Atlan to People dialog. For groups, complete the fields in the Assign Atlan to Groups dialog if it appears. Click Save and Go Back . Repeat steps 3 to 5 for each user or group to which you want to assign the application. When finished, in the respective dialog box, click Done . Upload Okta's metadata file (in Atlan) â To complete the configuration of Okta SSO, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ Okta and then clickÂ Configure . To the right ofÂ Identity provider metadata click the Import from XML button. Select the XML file saved from Okta above. At the bottom of the screen, clickÂ Save . Congratulations   -  you have successfully set up Okta SSO in Atlan! ð Did you know? By default, users can now log into Atlan with either Okta SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your Okta administrator to provision access to users through Okta and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings â danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from Okta to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan   -  irrespective of whether or not you enable SCIM . To automatically assign Okta users to Atlan groups based on their Okta groups, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under theÂ SSO Groups column, type the name of the corresponding group in Okta to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each Okta group with access to Atlan. Click theÂ Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð Did you know? You can configure SCIM provisioning in Okta to manage your users and groups in Atlan. Plus, once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable JumpCloud for SSO Next How to enable OneLogin for SSO Choose SSO provider (in Atlan) Set up SAML app (in Okta) Download Okta's metadata file (in Okta) Assign users to the app (in Okta) Upload Okta's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-onelogin-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Get Started How to enable OneLogin for SSO On this page Enable  OneLogin for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your OneLogin administrator to carry out the tasks below in OneLogin. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate OneLogin SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) â To choose OneLogin as your SSO provider, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ OneLogin and then clickÂ Configure . UnderÂ Service provider metadata , copy theÂ Audience (EntityID) , Recipient , ACS (Consumer) URL Validator , and ACS (Consumer) URL . Set up SAML application (in OneLogin) â To set up a SAML application, within OneLogin admin console: From the menu along the top, navigate to Applications and then click onÂ Applications . In the upper right, click the Add App button. In the search box, enterÂ saml custom and then click SAML Custom Connector (Advanced) . UnderÂ Display Name enter a name for your app, such asÂ Atlan and then click theÂ Save button. Change to theÂ Configuration tab and underÂ Application details enter your Atlan SAML settings: ForÂ Audience (EntityID) enter the value you copied from Atlan above. ForÂ Recipient enter the value you copied from Atlan above. ForÂ ACS (Consumer) URL Validator enter the value you copied from Atlan above. ForÂ ACS (Consumer) URL enter the value you copied from Atlan above. ForÂ Login URL enter the same value used for the fields above. Change to theÂ SSO tab and change the following: ForÂ SAML Signature Algorithm setÂ SHA-512 . UnderÂ Login Hint ensureÂ Enable login hint is checked. Change to the Parameters tab and use the circular + icon to add mappings for the following: email   - > Email firstName   - > First Name lastName   - > Last Name In the upper right, click the Save button. Download OneLogin's metadata file (in OneLogin) â To download the metadata file for the application, within OneLogin: From the application page, in the upper right navigate to More Actions and click SAML Metadata . (Optional) Map groups to the app (in OneLogin) â To map OneLogin groups to the app, within the OneLogin application: In the top left, click the Users tab, and from the dropdown, select Mappings . Under Mappings , click New Mapping to create a new group mapping for Atlan. In the New Mapping dialog, enter the following details: For Name , enter a meaningful name for your group mapping   -  for example, SSOGroupA . Under Conditions , click the + button and enter the following details: From the attributes dropdown, select Group to map all your OneLogin groups to Atlan.Â From the operators dropdown, select is . From the values dropdown, select the group name. Under Actions , enter the following details: From the Set role dropdown, select Set memberOf . This is required if you want to retain group membership in Atlan. From the Set memberOf to dropdown, enter the group name. Click Save to confirm your selections. Under Mappings , click New Mapping to remove any group mappings if none are selected. In the New Mapping dialog, enter the following details: For Name , enter a meaningful name for your group mapping   -  for example, clearMemberOf . Under Conditions , click the + button and enter the following details: From the attributes dropdown, select Group .Â From the operators dropdown, select is . From the values dropdown, keep the default selection None . Under Actions , enter the following details: From the Set role dropdown, select Set memberOf . From the Set memberOf to dropdown, leave as blank. Click Save to confirm your selections. Under Mappings , click the Reapply All Mappings tab, and in the corresponding screen, click Continue to confirm. In the top left, click the Applications tab, and from the dropdown, click Applications . Under Applications , select your SAML application. From the left menu of SAML Custom Connector (Advanced) , click Parameters . In the upper right of the parameters page, click the + button to add a new parameter. In the New Field dialog, enter the following details: For Field name , enter memberOf . For Flags , check the Include in SAML assertion box. Click Save to proceed to the next step. In the corresponding Edit Field memberOf dialog, from the Value dropdown, select MemberOf . Click Save to confirm your selections. If any of your OneLogin users do not belong to any groups, you can either add them to an existing group or create a new one. Once you have configured group mapping in Atlan , they will be able to log in to Atlan and assigned the same permissions as their OneLogin group. Upload OneLogin's metadata file (in Atlan) â To complete the configuration of OneLogin SSO, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ OneLogin and then clickÂ Configure . To the right ofÂ Identity provider metadata click the Import from XML button. Select the onelogin_metadata_1234567.xml file downloaded from OneLogin above. At the bottom of the screen, clickÂ Save . Congratulations   -  you have successfully set up OneLogin SSO in Atlan! ð Did you know? By default, users can now log into Atlan with either OneLogin SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your OneLogin administrator to provision access to users through OneLogin and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings â danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from OneLogin to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign OneLogin users to Atlan groups based on their OneLogin groups, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in OneLogin to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each OneLogin group with access to Atlan. Click theÂ Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable Okta for SSO Next How to enable SAML 2.0 for SSO Choose SSO provider (in Atlan) Set up SAML application (in OneLogin) Download OneLogin's metadata file (in OneLogin) (Optional) Map groups to the app (in OneLogin) Upload OneLogin's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-saml-2-0-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Get Started How to enable SAML 2.0 for SSO On this page Enable  SAML 2.0 for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your SAML 2.0 administrator to carry out the tasks below in your custom IdP. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate SAML 2.0 SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) â To choose SAML 2.0 as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select SAML 2.0 and then click Configure . For Alias , type in an alias for the SAML 2.0 connection and then click Next . Under Service provider metadata , copy the Atlan SAML Assertion URL and Atlan Audience URI (SP Entity ID) . Set up SAML app (in custom IdP) â If you have PingFederate as your IdP, refer to SSO integration with PingFederate using SAML for the SAML assertion URL to use. To set up a SAML app within your custom IdP: Create a new SAML application in your IdP with the name Atlan . For Entity/Issuer ID , enter the Atlan Audience URI (SP Entity ID) value you copied from above. For Assertion Consumer Service (ACS) URL , enter the Atlan SAML Assertion URL value you copied from above. Add the required users and groups to the application. Configure the IdP to return the following attributes in the SAML response: firstName lastName email memberOf (listing the user's group memberships, which will be required for group mapping in Atlan) Save the SAML metadata XML file for the SSO URL and X.509 public certificate file of the IdP. danger The SSO URL must be accessible from Atlan via an internet connection. Configure IdP details (in Atlan) â To complete the configuration of SAML 2.0 SSO, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . UnderÂ Choose SAML provider , selectÂ SAML 2.0 and then clickÂ Configure . For Alias , type in an alias for the SAML 2.0 connection and then click Next . To the right ofÂ Identity provider metadata, click the Import from XML button. Select the XML file saved from the IdP above. For Attribute Mapper , modify the IdP attribute names for email, first name, and last name if these will be different in the IdP SAML response. (Optional) For Customize , under Sign in button text , type any custom message you'd like your users to see on the Atlan login screen.Â At the bottom of the screen, click Save . Congratulations   -  you have successfully set up SSO for your custom IdP in Atlan! ð Did you know? By default, users can now log into Atlan with either SAML 2.0 SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your SAML 2.0 administrator to provision access to users through your custom IdP and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings â danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from your custom IdP to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign SSO users to Atlan groups based on their custom IdP groups, within Atlan: From the left menu on any screen, navigate toÂ Admin . Under theÂ Workspace heading, clickÂ SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in your custom IdP to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each custom IdP group with access to Atlan. Click theÂ Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! ð Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable OneLogin for SSO Next Authenticate SSO credentials to query data Choose SSO provider (in Atlan) Set up SAML app (in custom IdP) Configure IdP details (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/enable-azure-ad-for-scim-provisioning",
    "text": "Configure Atlan Integrations Identity Management SCIM How to enable Azure AD for SCIM provisioning On this page Enable  Azure AD for SCIM provisioning You can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with System for Cross-domain Identity Management (SCIM). To enable Azure AD for SCIM provisioning, complete the following steps. Did you know? For any questions about SCIM provisioning, head over here . Prerequisites â Azure AD SSO must be enabled for Atlan . Azure AD users or groups must be assigned to Atlan . Group mapping must be configured , only required if syncing mapped groups from Azure AD to Atlan. For any new groups created in Azure AD, you will first need to create corresponding groups in Atlan and then map the groups to sync them through SCIM provisioning. Retrieve SCIM token in Atlan â Who can do this? You will need your Atlan admin to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Azure AD administrator. Â You will need to generate a SCIM token in Atlan for authentication in Azure AD . To retrieve the SCIM token, within Atlan: From the left menu on any screen, click Admin . Under the Workspace heading, click SSO . On the Single Sign on page for Azure AD, under Overview , navigate to Automate Provisioning with SCIM and toggle it on. Under SCIM token , click the + Generate token button to create a SCIM token. In the SCIM token generated dialog, click the Copy button to copy the SCIM token and store it in a secure location. danger The SCIM token will only be displayed once after it has been generated, you cannot retrieve it later. Enable SCIM provisioning in Azure AD â Who can do this? You will need your Azure AD administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Atlan admin . You can enable SCIM provisioning in Azure AD to automatically sync your users and groups to Atlan. Configure SCIM provisioning in Azure AD â To configure SCIM provisioning, within Azure AD: Log in to your Azure portal and search for and select Azure Active Directory . From the left menu under Manage , select Enterprise applications . From the All applications page, select the SAML application you created to configure SSO in Atlan. In the left menu of your application page,Â under Manage , click Provisioning . From the Provisioning mode dropdown, click Automatic . Under Admin credentials , enter the following details: For Tenant URL , enter your Atlan tenant URL in the following format   - https://<your-tenant-dns>/api/service/scim . For Secret Token , enter the SCIM token you copied in Atlan. Click the Test connection button to confirm connectivity to Atlan. When successful, in the top right, click Save to save the configuration. In the Mappings section, verify that Provision Azure Active Directory Groups and Provision Azure Active Directory Users are enabled. Under Mappings : Click Provision Azure Active Directory Groups , and under Attribute Mappings , define the following mappings from Azure AD on the left to Atlan on the right: displayName - > displayName -  Note that this field is currently unsupported in Atlan. objectId - > externalId members - > members Click Provision Azure Active Directory Users , and under Attribute Mappings , define the following mappings from Azure AD on the left to Atlan on the right: mailNickname - > userName -  If the username is not mapped, the default username will be the UserPrincipalName (UPN). Switch([IsSoftDeleted], , \"False\", \"True\", \"True\", \"False\") - > active displayName - > displayName mail - > emails[type eq \"work\"].value givenName - > name.givenName surname - > name.familyName objectId - > externalId To save any changes, click Save . Provision users and groups â danger You will need to assign users or groups to Atlan from Azure AD before you can provision them. You will also need to configure group mapping Â to sync mapped groups from Azure AD to Atlan. For any new groups created in Azure AD, you will first need to map the groups in Atlan to sync them through SCIM provisioning. After you have enabled SCIM provisioning and assigned users and groups to Atlan in Azure AD, you can provision them to Atlan. In Azure AD, users and groups can be provisioned in two ways   - provisioning cycle and on-demand provisioning . Note the following: The username and email address of new and existing users cannot be changed once users have been provisioned to Atlan. If provisioning any users that already exist in Atlan, ensure that their Azure AD credentials match the existing credentials in Atlan for provisioning to be successful. To provision users and groups, within Azure AD: Log in to your Azure portal and search for and select Azure Active Directory . From the left menu under Manage , select Enterprise applications . From the All applications page, select the SAML application you created to configure SSO in Atlan. In the left menu of your application page,Â under Manage , click Provisioning and select a provisioning method: To enable provisioning cycle , in the upper left of the Overview page, click Start provisioning and toggle the Provisioning Status to On . To enable on-demand provisioning , from the left menu, click Provision on demand . To provision users or groups on demand: For Select a user or group , search for and select a user or group. At the bottom of the screen, click Provision . Repeat the steps for every user or group you want to provision. Once you have enabled SCIM provisioning, Azure AD will automatically provision and update user accounts in Atlan. However, the sync typically happens every 40 minutes . So, it may take up to 40 minutes for user provisioning to be completed in Atlan. Did you know? There are known limitations to on-demand provisioning in Azure AD. Tags: atlan documentation Previous Configure SCIM provisioning Next How to enable Okta for SCIM provisioning Prerequisites Retrieve SCIM token in Atlan Enable SCIM provisioning in Azure AD"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/enable-okta-for-scim-provisioning",
    "text": "Configure Atlan Integrations Identity Management SCIM How to enable Okta for SCIM provisioning On this page Enable Okta for SCIM provisioning You can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM). To enable Okta for SCIM provisioning, complete the following steps. Did you know? For any questions about SCIM provisioning, head over here . Prerequisites â Okta SSO must be enabled for Atlan . Okta users must be assigned to Atlan . Group mapping must be configured , only required if syncing mapped groups from Okta to Atlan. For any new groups created in Okta, you will first need to create corresponding groups in Atlan and then map the groups to sync them through SCIM provisioning. Retrieve SCIM token in Atlan â Who can do this? You will need your Atlan admin to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Okta administrator. Â You will need to generate a SCIM token in Atlan for authentication in Okta . To retrieve the SCIM token, from within Atlan: From the left menu on any screen, click Admin . Under the Workspace heading, click SSO . On the Single Sign on page for Okta, under Overview , navigate to Automate Provisioning with SCIM and toggle it on. Under SCIM token , click the + Generate token button to create a SCIM token. In the SCIM token generated dialog, click the Copy button to copy the SCIM token and store it in a secure location. danger The SCIM token will only be displayed once after it has been generated, you cannot retrieve it later. Enable SCIM provisioning in Okta â Who can do this? You will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Atlan admin . You can enable SCIM provisioning in Okta to automatically sync your users and groups to Atlan. Configure SCIM provisioning in Okta â To configure SCIM provisioning, from within Okta: Log in to your Okta admin console. From the menu on the left, expand the Applications menu and then click Applications . Under Applications , select the SAML application you created to configure SSO in Atlan. From the tabs along the top of your application page, click the General tab and then click Edit . Under App Settings , for Provisioning , click SCIM and then click Save to confirm. From the tabs along the top of your application page, click the Provisioning tab and then click Edit . For SCIM connection , enter the following details: For SCIM connector baseÂ URL , enter your Atlan tenant URL in the following format   - https://<your-tenant-dns>/api/service/scim/ . For Unique identifier field for users , enter userName as the field name of the unique identifier for your users on your SCIM server. For Supported provisioning actions , click to enable the following provisioning actions: Import New Users and Profile Updates -  this allows Okta to import new users and user profile updates to Atlan. Push New Users -  this allows user information to flow from Okta to Atlan. Push Profile Updates -  this allows profile information to flow from Okta to Atlan. Push Groups -  this allows group information to flow from Okta to Atlan. Import Groups -  this allows Okta to import new groups and group profile updates to Atlan. For Authentication Mode , click the dropdown and then select HTTP Header . To authenticate using HTTP Header , you will need to provide a bearer token that will provide authorization against Atlan. For Authorization , in the Token field, enter the SCIM token you copied in Atlan. Click the Test Connector Configuration Â button to confirm connectivity to Atlan. Once successful, at the bottom of the form, click Save to save the configuration. Under the left Settings menu of the Provisioning tab, two new tabs will appear   - To App and To Okta . Click To App to configure settings for SCIM provisioning to Atlan. For Provisioning to App page, click Edit and then click to enable the following: Create Users -  assigns a new Atlan account to each user managed by Okta. Okta does not create a new account if it detects that the username specified in Okta already exists in Atlan. The user's Okta username is assigned by default. Update User Attributes -  updates the user profiles of users assigned to Atlan. Profile changes made in Atlan will be overwritten with their respective Okta profile values. Deactivate Users -  automatically deactivates user accounts when they are unassigned in Okta or their Okta accounts are deactivated. Okta will also reactivate the Atlan account if the app integration is reassigned to a user in Okta. Â Click Save to save the configuration. Map Okta user attributes to Atlan â danger You will need to assign users to Atlan from Okta before you can provision them. After you have enabled SCIM provisioning and assigned users to Atlan in Okta, you can provision them to Atlan. Note the following: The username and email address of new and existing users cannot be changed once users have been provisioned to Atlan. If provisioning any users that already exist in Atlan, ensure that their Okta credentials match the existing credentials in Atlan for provisioning to be successful. To provision users to Atlan, from within Okta: Log in to your Okta admin console. From the menu on the left, expand the Directory menu and then click Profile Editor . On the Profile Editor page, in the left menu under Users , click Apps and select the SAML application you created to configure SSO in Atlan. On your application page, under Attributes , click Mappings . In the User Profile Mappings dialog box, click Okta User to App . In the Okta User to App page, userName is already set by Atlan. Define the following mappings from Okta on the left to Atlan on the right: user.firstName - > givenName user.lastName - > familyName user.email - > email Click Save to save your selections. Once saved, at the bottom of the dialog, click Apply updates now . (Optional) Navigate to the Provisioning tab of the SAML application you created to configure SSO in Atlan to confirm the attribute mappings. Enable group push in Okta to Atlan â danger You will need to configure group mapping in Atlan before you can enable group push from Okta to Atlan. To enable group push to Atlan, from within Okta: Log in to your Okta admin console. From the menu on the left, expand the Applications menu and then click Applications . Under Applications , select the SAML application you created to configure SSO in Atlan. From the tabs along the top of your application page, click the Push Groups tab and then click Edit .Â Under Push Groups to App , click the settings icon. FromÂ the Group Push Settings dialog, click Rename app groups to match group name in Okta and then click Save to rename groups in Atlan when linking groups. Under Push Groups to App , click the Push Groups button and then select Find groups by name to push your Okta groups to Atlan: For Push groups by name , in the Enter a group to push... field, enter the name of an Okta group you want to push to Atlan. To the right of your selected Okta group, under Match result & push action , click the Create Group dropdown and then select Link Group . Click Save to save your selections. (Optional) Repeat steps 1 to 3 to push additional Okta groups to Atlan. Tags: integration connectors Previous How to enable Azure AD for SCIM provisioning Next Troubleshooting SCIM provisioning Prerequisites Retrieve SCIM token in Atlan Enable SCIM provisioning in Okta"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles",
    "text": "Configure Atlan Access control Concepts What are user roles? On this page What are user roles? Overview â All users in Atlan need to be assigned one of the following predefined roles. danger User roles play a relatively small part in determining access to metadata and data. For more details on all the possible access control mechanisms, see How do I control access to metadata and data? Admin â AnÂ admin user can manage Atlan: Set up integrations with external collaboration tools Set up data connections and run workflows Manage users, groups, tags, and access policies Maintain extensions to the metadata Turn experimental features on and off In addition, theÂ admin user can do everything theÂ member user can do. There are two optional sub roles within the admin to delegate adminitration for workflows or governance without full platform level admin access. These workflow and governance sub roles can be enabled by admins. Member â AÂ member user can discover, maintain, and query assets: Find and view metadata for assets Update metadata for specific assets (via personas and policies)   -  for example, attach tags Suggest metadata updates for all other assets Approve or reject suggested metadata changes (via personas and policies) Preview sample data and query data in specific assets Guest â AÂ guest user can only discover assets: Find and view metadata for assets Suggest updates to metadata for any assets (if enabled from the admin center ) Never update metadata for any assets Preview sample data or query data in specific assets (via personas and policies) Detailed permissions â To understand the table of permissions, note the following: The permission to manage allows a user to create, read, update, and delete objects. â   -  capability included. â   -  capability will be a paid addition, reach out to your customer success manager for more information. Basic metadata   -  read asset name, description, certificates, and more. Permission to act may be limited . Permission Admin Member Guest Manage tags â Â Â Manage custom metadata and options â Â Â Manage users and groups â Â Â Manage access (personas, purposes, policies) â Â Â Edit the organization's profile â Â Â Create API tokens â Â Â Set up SSO â Â Â Create workflows â Â Â Approve or reject suggested metadata changes â â Â Manage glossaries â Â Â Manage categories and terms â â Â Bulk upload terms (via glossary policies) â â Â Preview sample data â â â Suggest changes to metadata â â â Edit metadata (via personas and policies) â â Â View basic metadata for assets â â â Create Jira issues on assets â â â Share assets on Slack or Teams â â â Insights Included Add-on Add-on Create and run new queries â â â Create collections, folders, and saved queries â â Â View and run saved queries â â â Schedule queries â â Â Tags: integration connectors Previous What are groups? Next What are the sidebar tabs? Overview Detailed permissions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/concepts/what-are-groups",
    "text": "Configure Atlan Access control Concepts What are groups? What are groups? Groups provide a way to organize many users together. You can use them in the same way as individual users, but at a more abstract level. They help you scale your governance of data and metadata in Atlan as your teams grow. For example, you can use groups to: Isolate the administration of different data sources Assign asset ownership Maintain access policies across a team of users Did you know? Creating groups comes in handy when a new person joins the team. You can add the new person to one or two groups instead of adding them to many objects and policies. Tags: atlan documentation Previous What are purposes? Next What are user roles?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/concepts/what-are-tags",
    "text": "Build governance Tags Concepts What are tags? Tags Atlan allows admin users to create tags for classifying data assets. You can use these tags in several ways: To identify important characteristics of data assets For grouping different assets together To apply granular access policies against those groupings Did you know? As the volume of your data and its consumption increases, tags help you maintain data security. For example, you can use tags to identify assets according to: Data protection guidelines such as GDPR and CCPA compliance,Â for example: Right to be forgotten (RTBF) as a tag for GDPR compliance Right to know or right to delete for CCPA regulations Drive data cleanup for user data management at source Industry information security practices, such as: CIA (Confidentiality, Integrity, and Availability) ratings of data assets ISO 2700x data classification schemes   -  for example, Public, Internal, Confidential, and Restricted Governmental schemes for classified information such as Top Secret, Secret, and Confidential Department-specific requirements   -  for example, HR, Finance, Sales, and Engineering Personal identification information   -  for example, PII PII-tagged data can also be hashed, redacted, or nullified in Atlan For details on tagging assets, see How to attach a tag . For details on managing tags from supported sources in Atlan, see: How to manage Databricks tags How to manage dbt tags How to manage Google BigQuery tags How to manage Snowflake tags Tags: atlan documentation Previous Remove a tag"
  },
  {
    "url": "https://docs.atlan.com/get-started/references/atlan-s-open-api",
    "text": "Get Started Quick Start Guides Developers Atlan's open API Atlan's open API Like you, we hate being locked into closed, black-box platforms. That's why we built Atlan's core platform on leading open-source projects and made every action API-driven. Did you know? Everything visible on Atlan is powered by APIs. Our mission is to help you activate your metadata , to help your team do its best work. So we are gradually opening up these APIs for everyone. For details of what's available, see our dedicated developer portal . To get started, create an API token from within Atlan. Tags: api rest-api graphql Previous Software development kits (SDKs) Next Authentication and authorization"
  },
  {
    "url": "https://docs.atlan.com/platform/how-tos/generate-har-files-and-console-logs",
    "text": "Get Started Administration Generate HAR files and console logs On this page Generate HAR files and console logs Atlan is built on REST APIs , so you can see the requests being sent by the UI to the API gateway through your browser's developer console. To help Atlan troubleshoot issues, you may be asked to create and send a HAR file and browser console logs: Console or network logs frequently provide critical error details that are required to determine the underlying cause of the issue or bug that you are experiencing. HAR files include all the network traffic from when you started recording, including sensitive information like passwords and private keys. To avoid including such information in a HAR file, Atlan recommends using a text editor to manually edit the file and remove any sensitive content before sending it to Atlan support. Generate in Google Chrome â Launch Google Chrome and navigate to the relevant webpage in Chrome. In the upper-right corner of your screen, click the three vertical dots.Â From the Chrome menu, click More Tools and then click Developer Tools . In the left menu, click on the Network tab and then select Fetch/XHR as your filtering option. Under the Network tab, click Preserve log .Â A red circle will appear on the left to show that you have started recording the network log. If you see a black circle, click on it to turn it red and start recording. To allow Google Chrome to record the interaction between the browser and website, refresh the page. Confirm if you can view new entries in the console. Next to the red circle icon, click the circle slash icon to clear logs. Replicate the issue that you experienced in the browser. For example, if it's a particular click that triggers an error, perform this action so that the error is recorded in the console. Once the page has loaded, navigate to the Console tabÂ and right-click in the console box. Select Save as... and enter a name for the file. Return to the Network tab and right-click the element that triggered an error. This is typically marked in red in the console. Click Save as HAR with content to save the HAR file. (Recommended) To remove any sensitive information from the HAR file: Open the HAR file in a text editor of your choice. Search for all instances of passwords and replace the values with a placeholder value such as ***** . For example, in the following sample password content, you can replace <YourPrivateKey> and <YourPrivateKeyPassword> Â with placeholder values: \"headersSize\": -1, \"bodySize\": 3762, \"postData\": { \"mimeType\": \"application/json\", \"text\": \"{\\\"host\\\":\\\"<YourHostName>\\\",\\\"port\\\":<port>,\\\"authType\\\":\\\"keypair\\\",\\\"username\\\":\\\"PRD_SDCSHOP_TU_ETL_CATALOG\\\",\\\"password\\\":\\\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\\\<YourPrivateKey>\\\\n-----END ENCRYPTED PRIVATE KEY-----\\\",\\\"extra\\\":{\\\"role\\\":\\\"GLOBAL_TALEND_CATALOG_DBADMIN\\\",\\\"warehouse\\\":\\\"PRD_SDCSHOP_ETL\\\",\\\"private_key_password\\\":\\\"<YourPrivateKeyPassword>\\\"},\\\"connectorConfigName\\\":\\\"atlan-connectors-snowflake\\\",\\\"query\\\":\\\"show atlan schemas\\\",\\\"schemaExcludePattern\\\":[\\\"INFORMATION_SCHEMA\\\"]}\" } } Save the HAR file. Upload the HAR file and browser console log to your Atlan support ticket. Tags: api rest-api graphql Previous Cloud logging and monitoring Next Tenant access management Generate in Google Chrome"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-access-management",
    "text": "Get Started Administration Tenant access management On this page Tenant access management For any Atlan tenant, there are three types of access required for troubleshooting an issue: access to cloud resources access to vCluster or dedicated Kubernetes (K8s) cluster access to product Access to cloud resources â Identify need for access   -  access to the cloud resources of a tenant is generally required to debug an issue related to infrastructure components. For any tenant, cloud resources include cloud storage, secrets, Kubernetes cluster, network rules, and more. Request access   -  if an Atlan engineer troubleshooting an issue requires access to any cloud resource, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification. Approval process   -  the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control. Access granting   -  once approved, the IT team at Atlan will grant access to the resource for a specified period of time. Access revocation   -  after the specified period of time is over or troubleshooting has been completed, the IT team will revoke access to avoid any unauthorized or prolonged access. Access to cluster â Identify need for access   -  access to vCluster or dedicated Kubernetes (K8s) cluster is required to troubleshoot an issue related to Kubernetes resources. Management of Kubernetes clusters   -  Atlan uses Loft for managing K8s clusters and vclusters. All K8s clusters and vclusters are added to and managed from Loft. Request access   -  if an Atlan engineer troubleshooting an issue requires access to a vcluster or dedicated cluster, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification. Approval process   -  the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control. Access granting   -  once approved, the IT team at Atlan will grant access to the resource for a specified period of time. Access revocation   -  after the specified period of time is over or troubleshooting has been completed, the IT team will revoke access to avoid any unauthorized or prolonged access. Access to product â Identify need for access   -  access to the product may be required to troubleshoot an issue with product features or connector workflows. Creation of support user   -  Atlan creates a support user named atlansupport while creating a tenant. The credentials for this user is stored with the IT team. Security of credentials   -  the credentials for this user are securely stored in 1Password , and managed by the IT team. Access to these credentials is tightly controlled, requiring explicit permission from the IT team to access them, and support user passwords are reset every 90 days. Request access   -  if an Atlan engineer troubleshooting an issue requires access to the product, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification. Approval process   -  the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control. Access granting   -  once approved, the IT team at Atlan will share the password of the Atlan support user for a specified period of time. Monitoring and logging   -  all actions performed using the atlansupport account are monitored and logged in Keycloak , with logs retained for 60 days. Tags: atlan documentation Previous Generate HAR files and console logs Next Tenant logs Access to cloud resources Access to cluster Access to product"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-logs",
    "text": "Get Started Administration Tenant logs On this page Tenant logs Atlan can help you understand the events that occur in your tenants, including user and administrative actions. Learn more about logging and retention as follows: Tenant logs â Note the following: Load balancer logs for Azure and GCP tenants are currently not enabled. AuditSearch and SearchLog records are persisted forever in Elasticsearch. The 30-day retention period pertains to application logs written to logging Elasticsearch. An example of block storage mentioned below is Amazon Elastic Block Store (EBS) for AWS. Production tenants â Log types Retention Storage AWS Azure GCP Active tenant overall backup 15 days Object storage â â â Offboarded tenant overall backup AWS   -  30 days, Azure and GCP   -  15 days Object storage â â â Load balancer logs 30 days Object storage â â â Audit   - user events 60 days PostgreSQL â â â Audit   - admin events Unlimited PostgreSQL â â â Application logs 30 days Elasticsearch and object storage â â â Workflow logs 90 days ClickHouse â â â Workflow artifacts 180 days Object storage â â â Application metrics 60 days VictoriaMetrics (block storage) â â â Proof of value (POV) tenants â Log types Retention Storage AWS Azure GCP Active tenant overall backup 15 days Object storage â â â Offboarded tenant overall backup AWS   -  3 days, Azure and GCP   -  15 days Object storage â â â Load balancer logs 30 days Object storage â â â Audit   - user events 60 days PostgreSQL â â â Audit   - admin events Unlimited PostgreSQL â â â Application logs 30 days Elasticsearch and object storage â â â Workflow artifacts 180 days Object storage â â â Application metrics 60 days VictoriaMetrics (block storage) â â â Atlan logs â Service Type Logging pipeline Destination Heracles application Fluent Bit S3 Argo application, server Argo, Fluent Bit S3 Atlas application, audit, perf Fluent Bit S3 Numaflow application Fluent Bit S3 Kube events application Fluent Bit S3 Wisdom application, audit Fluent Bit S3 Chronos application Fluent Bit S3 Redis application Fluent Bit S3 Kong application, audit Fluent Bit, PostgreSQL, Keycloak REST API S3 Keycloak application Fluent Bit S3 Elasticsearch application Fluent Bit S3 Cassandra application Fluent Bit S3 Heka application Fluent Bit S3 Pgpool application, server Fluent Bit S3 Kafka events Fluent Bit S3 Cloud storage lifecycle â The cloud storage created for each tenant has its own lifecycle. The lifecycle policy is attached to paths in the cloud storage. The lifecycle policy applied to a production tenant is as follows: Amazon Web Services (AWS) â Lifecycle policy Path Action DeleteClusterLogsAfter30Days logs/ Expires DeleteArgoArtifactsAfter180Days argo-artifacts/ Transitions to S3 Glacier Flexible Retrieval, then expires DeleteArgoBackupAfter15Days backup/argo/ Expires DeleteAltanScheduleQuery argo-artifacts/default/schedule-query/ Expires DeletePostgresBackupAfter15Days backup/postgres/ Expires DeleteRedisBackupAfter15Days backup/redis/ Expires DeleteCassandraBackupAfter15Days backup/cassandra/ Expires DeletePrometheusBackupAfter15Days backup/prometheus/ Expires DeleteALBLogsAfter30Days AWSLogs/ Expires Microsoft Azure â Lifecycle policy Path Action DeleteClusterLogsAfter30Days logs/ Delete DeleteArgoArtifactsAfter180Days argo-artifacts/ Moves to archive after 90 days and delete after 180 days DeleteArgoBackupAfter15Days backup/argo/ Delete DeletePostgresBackupAfter15Days backup/postgres/ Delete DeleteRedisBackupAfter15Days backup/redis/ Delete DeleteCassandraBackupAfter15Days backup/cassandra/ Delete DeleteAltanScheduleQuery argo-artifacts/default/schedule-query/ Delete if blobs not modified in 1 day DeleteSparkEventLogsAfter15Days spark-event-logs/ Delete Google Cloud Platform (GCP) â Lifecycle policy Path Action DeleteClusterLogsAfter15Days logs/ Delete DeleteArgoArtifactsAfter180Days argo-artifacts/ Archive DeleteArgoArtifactsAfter270Days argo-artifacts/ Delete DeleteArgoBackupAfter3Days backup/argo/ Delete DeletePostgresBackupAfter3Days backup/postgres/ Delete DeletePrometheusBackupAfter3Days backup/prometheus/ Delete DeleteRedisBackupAfter3Days backup/redis/ Delete DeleteScheduleQueryAfter1Day argo-artifacts/default/schedule-query/ Delete DeleteSparkEventLogsAfter15Days spark-event-logs/ Delete DeleteCassandraBackupAfter3Days backup/cassandra/ Delete Tags: lineage data-lineage impact-analysis Previous Tenant access management Next Tenant monitoring Tenant logs Atlan logs Cloud storage lifecycle"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-monitoring",
    "text": "Get Started Administration Tenant monitoring Tenant monitoring Atlan monitors tenants from a centralized observability platform. The following essential metrics are monitored: Infrastructure metrics such as CPU utilization and memory usage Application performance metrics Availability and uptime metrics Latencies The setup for monitoring tenants consists of Zenduty, Grafana, VictoriaMetrics server, vmagent, and Prometheus exporters such as Node Exporter and Blackbox Exporter. The architecture for the tenant monitoring system is as follows: Tenant monitoring in Atlan includes the following processes: All the metrics of a tenant are collected in the centralized monitoring stack that runs in the Atlan control plane. These metrics can be visualized in Grafana , which also runs in the Atlan control plane. Atlan associates have access to Grafana over Atlan VPN and can use available dashboards to monitor tenants. Based on the alerting rules, alerts are triggered for the tenant and managed through Zenduty , an incident alerting tool. Tags: atlan documentation Previous Tenant logs Next Tenant offboarding"
  },
  {
    "url": "https://docs.atlan.com/platform/references/tenant-offboarding",
    "text": "Get Started Administration Tenant offboarding On this page Tenant offboarding An Atlan tenant consists of multiple infrastructure resources, such as EC2 nodes, S3 buckets, ALB, EKS, and IAM roles, Loft vcluster, and more. Offboarding an Atlan vcluster tenant involves removing all the infrastructure resources associated with that particular tenant or instance.Â While most of the resources can be deleted immediately without any data loss, we handle the cleanup of storage resources like S3 buckets with care. S3 buckets associated with a tenant are not immediately removed   -  they are retained for 30 days after a tenant is offboarded. This ensures that there is a disaster recovery mechanism in place if a tenant is offboarded by mistake or faces any unrecoverable errors. Tenant decommissioning â You can raise a support ticket for tenant decommissioning. The customer success team at Atlan will notify the infrastructure support team responsible for executing tenant offboarding. These requests are usually resolved within the first 48 hours to ensure that no unnecessary costs are incurred for infra resources. Frequently asked questions â Where is customer data stored? â Any customer-specific data in Atlan   -  for example, query logs, assets, service logs, and more   -  are stored in EBS volumes and S3 buckets. How long does customer data persist after tenant offboarding? â EBS volumes are immediately deleted during offboarding. However, S3 buckets are retained for a period of 30 days. Tags: atlan documentation Previous Tenant monitoring Next Infrastructure security Tenant decommissioning Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/tags/monitoring",
    "text": "9 docs tagged with \"monitoring\" View all tags Automate data governance You can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring. Link your Microsoft Teams account To get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users. Manage tasks :::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can use the inbox. Security monitoring Learn about security monitoring. Supported sources Learn about supported sources. Troubleshooting Fivetran connectivity Learn about troubleshooting fivetran connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity. What's Data Quality Studio Understand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/logs",
    "text": "One doc tagged with \"logs\" View all tags Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/compliance",
    "text": "One doc tagged with \"compliance\" View all tags Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/siem",
    "text": "One doc tagged with \"siem\" View all tags Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/opentelemetry",
    "text": "One doc tagged with \"opentelemetry\" View all tags Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/tags/otlp",
    "text": "One doc tagged with \"otlp\" View all tags Cloud logging and monitoring Learn about Atlan's Cloud logging and monitoring exported in OpenTelemetry Protocol (OTLP) format for SIEM integration and security monitoring."
  },
  {
    "url": "https://docs.atlan.com/platform/references/security-monitoring",
    "text": "Get Started Security & Compliance Security monitoring On this page Security monitoring Atlan has built-in monitoring systems that help users manage the behind-the-scenes infrastructure. These ensure adherence to the highest standards of security. Full visibility into infrastructure performance â We provide admins complete visibility of CPU, memory, and storage metrics through Grafana . These are industry-standard dashboards. Slack alerts and notifications â For proactive alerting, Slack notifications can also be enabled across Atlanâs infrastructure. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Tags: dashboards visualization analytics alerts monitoring notifications security access-control permissions Previous How are resources isolated? Next Compliance standards and assessments Full visibility into infrastructure performance Slack alerts and notifications"
  },
  {
    "url": "https://docs.atlan.com/platform/references/compliance-standards-and-assessments",
    "text": "Get Started Security & Compliance Compliance standards and assessments Compliance standards and assessments Atlan adheres to various industry standards and regulations to ensure the security, privacy, and integrity of the platform. This entails conducting both external audits and internal assessments to continuously improve compliance standards. Following is an overview of Atlan's key compliance certifications and internal assessment practices: Compliance Description Status Frequency ISO 27001 The Information Security Management System (ISMS) standard ensures data confidentiality, integrity, and availability. Certified Annual ISO 27701 The Privacy Information Management System (PIMS) standard manages PII and ensures compliance with privacy regulations like GDPR and CCPA. Certified Annual SOC 2 Type II The SOC (System and Organization Controls) 2 Type II report attests to the security, availability, confidentiality, and privacy controls for service organizations. Certified Annual GDPR The General Data Protection Regulation (GDPR) is an EU regulation that ensures the protection of personal data by enforcing strict privacy and security measures, along with giving individuals control over their data. Atlan adheres to GDPR through ongoing compliance, including breach notifications, data subject rights, and consent management. Certified Annual EU-U.S. Data Privacy Framework The Data Privacy Framework outlines policies and controls that govern how Atlan handles personal information to ensure data protection and compliance with privacy regulations like GDPR. Compliant Annual HIPAA HIPAA, or the Health Insurance Portability and Accountability Act, safeguards protected health information (PHI). Certified Annual VAPT assessments Annual third-party Vulnerability Assessment and Penetration Testing (VAPT) assessments help identify and mitigate potential vulnerabilities within the Atlan platform. Ongoing Annual Tags: security access-control permissions Previous Security monitoring Next Incident response plan"
  },
  {
    "url": "https://docs.atlan.com/tags/integration",
    "text": "123 docs tagged with \"integration\" View all tags Add custom metadata <div style={{position: \"relative\", paddingBottom: \"calc(66.33333333333333% + 41px)\", height: 0}}> <iframe src=\"https://demo.arcade.software/1dT1bPneM5fp1O71lb. Add descriptions You can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information. Administration and Configuration Complete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization. AI and Automation Features Guide to Atlan's AI capabilities and automation features for enhanced data governance and productivity. Always On Integrate Atlan with Always On to enable continuous automation and suggestions. Atlan browser extension security Learn about atlan browser extension security. AWS Lambda Integrate Atlan with AWS Lambda to automate workflows and triggers. Browser Extension Integrate Atlan with the Browser Extension to enhance your data catalog experience. Bulk enrich metadata Atlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan. Can Atlan integrate with Airflow to generate lineage? Atlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage). Can I add Atlan's browser extension for everyone in my organization? Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension). Can I be notified if there is a change in downstream dashboards or a schema drift? You can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events. Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Refer to our [troubleshooting Jira documentation](/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira) to learn more. Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case. Can I create backups of glossaries? Atlan also allows you to [export your glossary assets](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) to spreadsheets and keep a record of your contextual information. Can I query any DW/DL? You can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature. Can site renaming affect the Jira integration? Learn about can site renaming affect the jira integration?. Can the Hive crawler connect to an independent Hive metastore? Learn about can the hive crawler connect to an independent hive metastore?. Can we use a Microsoft SSO login? Learn about can we use a microsoft sso login?. Configure SCIM provisioning You can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning. Configure Snowflake data metric functions Configure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Configure workflow execution Learn about configure workflow execution. Connectors and capabilities Learn about connectors and capabilities. Create an AWS Lambda trigger Once you have configured the [AWS Lambda permissions](/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda), you can run an AWS Lambda function. Create announcements Adding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions. Create README templates Admin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and [enrich their assets with READMEs](/product/integrations). They will also be able to see a rich preview of each template before adding the relevant documentation. Custom solutions Learn about custom solutions. Data Connections and Integration Complete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues. Delete a connection Learn about delete a connection. Deployment architecture The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Download impacted assets in Microsoft Excel Once you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Enable  Azure AD for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  Google for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  JumpCloud for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  Okta for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  OneLogin for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  SAML 2.0 for SSO SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso). Enable  Snowflake OAuth Atlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware. Enable  SSO for Amazon Redshift You will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift). Enable  SSO for Google BigQuery Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan. Enable Okta for SCIM provisioning You can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM). ETL tools connectors Overview and entry point for all ETL tools connectors in Atlan. How are product updates deployed? Learn about how are product updates deployed?. How do I send messages or search assets from Slack? Sending messages and searching assets from Slack are disabled. Refer to [Troubleshooting Slack](/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack) to learn more. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. Infrastructure security Learn about infrastructure security. Integrate Anomalo Once you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo. Integrate Atlan with Google Sheets The Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan. Integrate Atlan with Microsoft Excel The Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel. Integrate Jira Cloud You must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work. Integrate Jira Data Center You will need to [configure an incoming link](https://confluence.atlassian.com/adminjiraserver/configure-an-incoming-link-1115659067.html) with an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider. Integrate Microsoft Teams Once you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams. Integrate ServiceNow If your Atlan admin has [enabled the governance workflows and inbox module](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) in your Atlan workspace, you can create a ServiceNow integration to allow your users to [grant or revoke data access](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) for governed assets in Atlan or any other data source. Integrate Slack To integrate Slack and Atlan, follow these steps. Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Jira Integrate Atlan with Jira to automate ticket creation and link your Jira account. Link your account To [export assets to and bulk enrich metadata from](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) a supported spreadsheet tool,. Link your Jira account To create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that [set up the Jira integration](/product/integrations/project-management/jira/how-tos/integrate-jira-cloud), but not for other users. Link your Microsoft Teams account To get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users. Link your ServiceNow account To request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that [set up the ServiceNow integration](/product/integrations/project-management/servicenow/how-tos/integrate-servicenow), but not for other users. Link your Slack account To see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that [set up the Slack integration](/product/integrations/collaboration/slack/how-tos/integrate-slack), but not for other users. Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Manage custom metadata structures :::warning Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones. Manage requests If your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected. Microsoft Teams Integrate Atlan with Microsoft Teams to enable collaboration and notifications. Monitor connectivity Atlan runs its crawlers through an orchestrated set of automated tasks. Okta first-time login authentication error Learn about why do i get an authentication error when logging in via okta for the first time?. OpenLineage configuration and facets Learn about openlineage configuration and facets. PingFederate SSO 404 error If you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion. Provide credentials to query data Learn about provide credentials to query data. Provide credentials to view sample data Learn about provide credentials to view sample data. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Report on assets Learn about report on assets. Report on automations You can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions. Schedule a query You must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients. ServiceNow Integrate Atlan with ServiceNow to automate ticket creation and link your ServiceNow account. Set default user roles for SSO :::warning Who can do this? You will need to be an admin user and [configure SSO](/product/integrations/identity-management/sso) with a provider first. Set up a private network link to Hive Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Set up a private network link to Trino :::warning Who can do this? You will need your AWS administrator involved - you may not have access to run these tasks yourself. Set up Anomalo Atlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata. Set up Fivetran Learn about set up fivetran. Set up MongoDB Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password]( create-database-user-in-mongodb) to fetch metadata. Set up Monte Carlo :::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups). Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Set up Salesforce Learn about setting up Salesforce authentication for Atlan. Slack Integrate Atlan with Slack to enable collaboration and notifications. SSO integration with PingFederate using SAML To use both IdP- and SP-initiated SSO, add both the URLs mentioned above. Supported sources Learn about supported sources. Troubleshooting Atlan browser extension Can I add the browser extension for everyone in my organization? Troubleshooting AWS Glue connectivity Learn about troubleshooting aws glue connectivity. Troubleshooting connector-specific SSO authentication Learn about troubleshooting connector-specific sso authentication. Troubleshooting Jira What fields are supported when creating tickets or requesting access? Troubleshooting Metabase connectivity Learn about troubleshooting metabase connectivity. Troubleshooting Microsoft Teams Why do I get an error while adding Atlan to Microsoft Teams? Troubleshooting Mode connectivity Learn about troubleshooting mode connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity. Troubleshooting SCIM provisioning Learn about troubleshooting scim provisioning. Troubleshooting ServiceNow Why is the security\\_admin role required to complete the ServiceNow integration? Troubleshooting Sisense connectivity Learn about troubleshooting sisense connectivity. Troubleshooting Slack What do the colors in Slack notifications for modified assets mean? Troubleshooting spreadsheets Why do I need admin consent for exporting assets to Microsoft Excel? Troubleshooting SSO Can I change the username of a provisioned user in Atlan? Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. update column metadata in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets. Update column metadata in Microsoft Excel Once you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel. Use the filters menu You can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you. View event logs Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. What are user roles? Learn about what are user roles?. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Confluent Schema Registry? Atlan crawls and maps the following assets and properties from Confluent Schema Registry. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase. What does Atlan crawl from Microsoft SQL Server? Atlan crawls and maps the following assets and properties from Microsoft SQL Server. What does Atlan crawl from Mode? Atlan crawls and maps the following assets and properties from Mode. What is included in the Jira integration? With two of your most important workspaces connected, you can save time and improve the way you track issues for your data. What is included in the Microsoft Teams integration? With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team. What is included in the Slack integration? Learn about the features and capabilities of the Slack integration with Atlan. What is the crawler logic for a deprecated asset? Learn about what is the crawler logic for a deprecated asset?. What type of user provisioning does Atlan support for SSO integrations? Atlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:. What's the difference between connecting to Athena and Glue? Learn about what's the difference between connecting to athena and glue?. Why did my users not receive an invite email from Atlan? If you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:. Why do I get an error message when I click on Atlan's browser extension? Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension). Why is Atlan's browser extension not loading? Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension)."
  },
  {
    "url": "https://docs.atlan.com/faq/basic-platform-usage",
    "text": "Get Started FAQs Basic Platform Usage On this page Basic Platform Usage Essential information about using Atlan's core features, from browser requirements to data querying and asset management. Which browsers does Atlan support? â Atlan supports browsers for which all functionalities can be validated. Atlan recommends using Google Chrome for an optimal user experience. What's the availability of Atlan? â Atlan software is constantly available (24x7). You'll be notified in advance if Atlan experiences any downtime or outage. See also High availability and disaster recovery (HA/DR) . How does Atlan access the data for queries? â Atlan pushes all queries to the source. Each query generated in Atlan is a pushdown SQL query into the underlying source database. This data is neither stored nor cached anywhere in Atlan. It's encrypted in transit using AWS Key Management Service (AWS KMS). See also Encryption and key management . In addition, Atlan applies access policies to the results before displaying them. Why replace my current IDE with Atlan? â Atlan's Insights feature is far and beyond any other IDE: It provides a quick preview of the tables and data. Scheduled queries - you can create a collection of saved queries and schedule the output to be delivered to users via email. Visual Query Builder - querying for people who aren't proficient in SQL. Parameterization - add variables to your SQL queries that can be edited by users during runtime. Can I use Python or R to query data? â Atlan currently supports using Python to query data. Refer to the developer documentation to learn more. Support for R is unavailable. Can I switch off the query feature for data security concerns? â You can block all users from querying data across all data assets. You'll need to be an admin user to toggle the Insights option off from the admin workspace. What types of files can be received into Atlan? â Atlan supports cataloging files through APIs. The supported file types include document, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files. Can I export all the data from Atlan? â Atlan enables you to export all your assets or a filtered subset of assets and asset metadata to Google Sheets and Microsoft Excel spreadsheets. Learn more here . How do I update my profile picture in Atlan? â To update your profile picture in Atlan: In the top right of your instance, click your name, and then from the dropdown, click Profile . In the Profile sidebar, click the edit icon and then click Change avatar to upload a new image. (Optional) You can add or update other relevant details in your Atlan profile - for example, primary role, skills, and even your Slack member ID. Click Save to save your changes. Why does an emoji appear as one color in the menu and another on Atlan UI? â Emojis may appear differently across different browsers and operating systems. This is because emojis are rendered by the browser or operating system, and each has its own way of displaying them. For a consistent experience, consider using Google Chrome. Why is the count of assets in the connection dropdown different than the asset pill? â When you select a supported source from the Connector dropdown, the All assets tab displays an exact count of assets for each connection. If you haven't selected a connector, then the All assets tab displays a rounded count of all assets in your Atlan instance. Disabling all assets view â Here are a few things to know about restricting asset visibility : What's the default view for users with multiple personas or purposes? â The default view depends on the alphabetical order of the personas or purposes that the member and guest users belong to. Personas are first sorted alphabetically, and if the user doesn't belong to any persona, then purposes are sorted next to determine the default view on the Assets page. Can access to glossaries be restricted? â Yes, an Atlan admin can disable all glossaries view . Can users view linked assets they don't have access to for announcements? â Member and guest users are able to view the names of linked assets in the announcements on the homepage. However, if they open the asset profile of a restricted asset, they'll encounter a Not found page. Tags: atlan documentation faq-platform Previous Getting Started and Onboarding Next Support and Technical Help"
  },
  {
    "url": "https://docs.atlan.com/faq/support-and-technical-help",
    "text": "Get Started FAQs Support and Technical Help On this page Support and Technical Help Complete guide to getting support, understanding API limits, and accessing technical assistance for Atlan. What's the availability of support? â Atlan support provides full coverage during business hours across most time zones. See also Customer support . Is there a limit on the number of API requests that can be performed? â Most automated operations that need to process many records follow one of these patterns: Search-based extract End-to-end bulk update These patterns minimise the total number of API calls because each request extracts or updates many records, so you're unlikely to hit rate limits. If you make raw API requests directly, different endpoints have different limits. As a rule of thumb, assume about 1,000 requests per minute per host (your Atlan domain). Whenever possible, adopt the patterns mentioned earlier to avoid these limits. What's the recommended path for pagination of asset searches with scroll API? â Refer to Paging search results in the developer documentation for the recommended scroll API implementation. Is Atlan compatible with data quality tools? â Yes. Atlan integrates with several data-quality and observability platforms: Native integrations such as Monte Carlo , Soda , and Anomalo Ability to surface quality scores and alerts in asset profiles REST APIs to ingest additional quality metadata from any tool you choose For the latest list, see supported sources . Tags: api rest-api graphql faq-support Previous Basic Platform Usage"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-platform",
    "text": "2 docs tagged with \"faq-platform\" View all tags Basic Platform Usage Essential information about using Atlan's core features, from browser requirements to data querying and asset management. Getting Started and Onboarding Everything you need to know when starting with Atlan - from trials and demos to deployment options and implementation requirements."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/use-the-filters-menu",
    "text": "Use data Discovery Configuration How to use the filters menu On this page Use the filters menu You can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you. Once you have added filters, you can also: Bookmark your search results with applied filters for quick access. Copy the browser URL with applied filters and share it with other users in your organization   -  they may need to log into Atlan first to view the search results. Export filtered assets to spreadsheets and enrich asset metadata in bulk. Did you know? You can resize the filters panel. Hover over the edge of the panel and then click and drag the slider arrow to resize it. Atlan will remember your preferred size for the duration of your session. The panel will be returned to the default size when you log in again. Use the following filters to help with asset disco very. Start by either clicking Assets in the left panel or the search bar from any screen in Atlan: Source â You can use source-specific filters to curate a list of relevant assets to search from. To filter by a specific source: In the Filters menu on the left, click Source . Click Choose connection to filter by a supported connector . (Optional) Select an existing connection for a selected connector: Click All Databases to filter by databases for a selected connection. Click All Schemas to filter by schemas for a selected connection. Asset type â You can filter your search results by specific types of assets. The asset type filter also includes a quick count of all the resulting assets grouped by type. You can select multiple asset types to group search results by asset types. For example, if you would only like to view column assets: Under the search bar on the Assets page, click the Asset type dropdown. From the Asset type dropdown, click the Column tab to only view column assets. Only column assets will be displayed in the results, with the tab showing a total count of the column assets. (Optional) In the Filters menu on the left, under Asset Type Filters , click Column to add a type-specific property filter to further refine your search: Click Parent asset type to filter columns by a parent asset type   -  tables and views. Click Parent asset name to filter columns by the name of a table or view, or set a matching condition such as pattern match. Click Data type to filter columns by data types. Click any of the column keys to filter by column keys. (Optional) If an asset type you're filtering for does not match the search keywords but there are other asset types that match, click Check other matches to view those assets or click Clear search to clear your search and start over. Domains â Domains provide a logical way of mapping and organizing assets within a specific domain or business entity. You can filter assets by a single domain, multiple domains, or no domains. To filter assets by domains: In the Filters menu on the left, click Domains to expand the menu. Under Domains , to filter assets by domains: Check the boxes to select one or more domains or subdomains to filter your assets. Click No domains to filter assets not mapped to any domain. Metadata filters â Certificate â You can filter your asset search based on the certificate attached to the data assets   - Verified , Draft , Deprecated , and No certificate . For example, if you would only like to view verified assets: In the Filters menu on the left, click Certificate to expand the menu. Under Certificate , click Verified to only view verified assets in your search results.Â Owners â You can filter your asset search by selecting one or more owners . You can also toggle between individual users and groups to filter results based on a group of users. Or, select No Owners in the owners filter to view assets that currently do not have any owners and assign them if needed. For example, if you would like to filter assets by a group of owners: In the Filters menu on the left, click Owners to expand the menu. Under Owners , click the group icon . Click the group name by which you want to filter your assets. Tags â You can filter your assets by user-generated tags , such as public , PII , and more. You can also select No Tag in the tags filter to view assets that currently do not have any tags and add them if needed. For example, if you would like to filter assets for a data compliance check: In the Filters menu on the left, click Tags to expand the menu. Under Tags , click the relevant option   -  for example, PII . (Optional) Filter for assets by tags imported from supported sources: Select a synced Snowflake tag to view tagged Snowflake assets only. To filter by Snowflake tag values, next to the tag name, click the rightward arrow to open the tag value menu. In the Filter by tag value dialog, click the Select tag value dropdown and then select a tag value to filter assets. You can either search by predefinedÂ allowed values or tag values. Select a synced dbt tag to view tagged dbt Cloud or dbt Core assets only. Select a synced Databricks tag to view tagged Databricks assets only. You can also filter by Databricks tag values. Terms â You can filter your asset search by terms from your glossaries . You can also select No Terms in the terms filter to view assets that currently do not have any linked terms and add them if needed. For example, if you would like find assets linked to a specific term: In the Filters menu on the left, click Terms to expand the menu. Under Terms , click the relevant term   -  for example, Marketing Analysis -  to discover assets linked to that term. (Optional) Next to the search bar in the Terms filter, click the Advanced options icon to set a matching condition. Click the operators dropdown, and then: Click Or to filter assets that match any selected term(s). Click And to filter assets that match all selected terms. Click None of to filter assets that do not match any of the selected term(s). Click Not empty to filter assets that have one or more linked terms . Click Empty to filter assets without any linked terms . Properties â Properties offer a variety of filters to narrow down your asset search. You can filter your assets by common asset properties, such as name, description, last updated, and more.Â To search by common asset properties: In the Filters menu on the left, click Properties Â to expand the menu. From the Properties menu, you can: Click Title to search by the technical name or alias of an asset. From the dialog, set your preferred matching condition (see Description filter). Click Description to search by the description of an asset in Atlan or from the source.Â To set a matching condition, from the Description Â dialog, click the operators dropdown to: Click Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Click Starts With or Ends With to filter assets using the starting or ending sequence of values. Click Contains to find assets with specified values contained within the property. Click Pattern to filter assets using supported Elastic DSL regular expressions . Click Is Empty or Is Not Empty to filter assets with or without null values. Click Starred assets to filter for all your starred assets . Click Has lineage to filter for assets with or without data lineage . Click Has readme to filter for assets with or without README files . Click Has resources to filter for assets with or without resources . Click Announcement to find assets with a specific announcement type. From the Announcement dialog, click the dropdown menu to select Information , Issue , Warning , or No announcement . Click Unique identifier to find assets with a unique ID. From the dialog, select your preferred matching condition and type the relevant information. Click Qualified name to filter by a unique name for the asset. From the dialog, select your preferred matching condition and type the relevant information (see Description filter). Click Last updated (in source) or Last updated (in Atlan) to find assets by when they were last updated and where. From the dialog, set the condition to Before or After and then select a date. Click Created (in source) or Created (in Atlan) to find assets by when they were created and where. From the dialog, set the condition to Before or After and then select a date. Click Created by (Atlan) or Last updated by (Atlan) to filter assets by a user who created or last updated the asset in Atlan. From the dialog, from the Select user dropdown, select the user name. Click Is archived to view archived assets in the search results. (Optional) Click the + sign to add more filtering options   -  currently only available with the Title , Description , Unique identifier , Qualified name , Last updated (in source and Atlan), and Created (in source and Atlan) filters. dbt â danger The dbt filters will only appear in the filters menu if dbt assets have been crawled . The dbt filters allow you to filter your dbt Cloud and dbt Core assets to find the most relevant results. For example, if you need to find assets from a specific project in dbt: In the Filters menu on the left, click dbt to expand the menu. Under dbt , click Project name to filter by a specific project. In the Project name dialog: Select the relevant matching condition. For Type , type the project name to filter your assets   -  for example, food-beverage . (Optional) Click the + sign to add more filtering options. (Note: This may not be available for all the dbt filters). Usage â The usage filters allow you to filter your assets by usage metadata. For example, you can: Filter assets with zero queries and archive them. Find costly assets to better optimize your operations. Discover recently updated assets and follow up on the updates.Â Custom metadata filters â danger You first need to set up custom metadata properties and toggle on the Show in filter slider during setup. When you add custom metadata in Atlan, you can also choose to set custom metadata properties as filters to help with quicker asset discovery. For example, if you would like to filter your assets by custom user roles metadata: In the Filters menu on the left, click the custom metadata filter   -  for example, Stewards . Under the custom metadata filter, select the custom metadata property   -  such as Data Steward . In the property dialog, select the matching condition and user to filter your assets. (Optional) For custom metadata option properties only: You can either: For custom metadata properties with five or fewer options, click the operators dropdown. For custom metadata properties with more than five options, next to the search bar, click the Advanced options icon and then click the operators dropdown. From the operators dropdown, you can: Click Or to filter assets that match any selected value(s). Click And to filter assets that match all selected values   -  only supported if multiple values are allowed for custom metadata options. Click None of to filter assets that do not match any of the selected value(s)   -  only supported if multiple values are allowed for custom metadata options. Click Not empty to filter assets that have one or more assigned values for the selected property. Click Empty to filter assets without any assigned values for the selected property. Asset type filters â You can choose from two different sets of filters   -  type-specific or connector-specific. Type-specific filters â If you're filtering by a specific asset type , you can select type-specific property filters to further refine your search. For example, if you're filtering by: Tables or views   -  you can filter these asset types by a specific row or column count. Columns   -  you can filter column assets by parent asset type and name, data type, or column keys . Process   -  you can filter process assets by the SQL query. Query   -  you can filter saved queries by visual queries . Connector-specific filters â Connector-specific filters will appear in the filters menu only if there are crawled assets for a supported source and asset-type filters specific to the connector are applied. Connector-specific filters are currently supported for the following sources: Anomalo Apache Kafka , Confluent Kafka , Aiven Kafka , Redpanda Kafka , and Amazon MSK Apache Airflow/OpenLineage , Amazon MWAA , Astronomer , Google Cloud Composer , and Apache Spark Google BigQuery Matillion Microsoft Azure Cosmos DB Microsoft Azure Event Hubs Microsoft Power BI MicroStrategy MongoDB Monte Carlo Redash Salesforce Sisense Snowflake Soda Tableau ThoughtSpot Qlik Sense Cloud and Qlik Sense Enterprise on Windows Google Cloud Storage buckets and objects Microsoft Azure Data Lake Storage accounts, containers, and objects Preset datasets, dashboards, and workspaces API paths Files -  supported file types include DOC, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files To use a connector-specific filter: From the Assets page, click the Asset type filter, and then from the dropdown, select an asset type from a supported connector   -  in this example, we'll select Soda Checks . In the Filters menu on the left, click the Soda filter to expand the menu. From the Soda filter menu, filter your Soda checks by check status, owner, or last scanned at date. (Optional) For properties that allow selecting multiple values, you can set your preferred matching condition. Click the operators dropdown and then: Click Or to filter assets that match any selected value(s). Click And to filter assets that match all selected values. Click None of to filter assets that do not match any selected value(s). Click Not empty to filter assets that have one or more assigned values for the selected property. Click Empty to filter assets without any assigned values for the selected property. Tags: data integration Previous Configure language settings Next Add custom metadata Source Asset type Domains Metadata filters Custom metadata filters Asset type filters"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-descriptions",
    "text": "Use data Discovery Asset Management Add descriptions On this page Add descriptions You can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a README . Doing so will enrich your data asset with the relevant contextual information. Did you know? The description editor in Atlan also supports Markdown syntax. You can crawl descriptions in Markdown at source, view them in the asset sidebar and profile, and make edits in Markdown directly in Atlan. Add descriptions to your assets â To add or update a description for your data asset, follow these steps: From the left menu on any screen, click Assets . On the Assets page, click on an asset to view its Overview in the right menu. Under Description , click on the text box to add a description. Once you've added new text or updated the existing one, click anywhere to automatically save your changes. Your asset description is now live! ð You can also add or edit asset descriptions directly from the asset profile. Did you know? The size limit for description values is 32766 bytes. Depending on the types of characters used, the character limit for descriptions can range from 8191 to 32766 characters. Add column descriptions â You can also add a description for a single column rather than an entire data asset. To add column descriptions, follow these steps: On the Assets page, click on an asset to view its asset profile. Under Column Preview in the asset profile, navigate to a column and click +Add under Description . Once you've entered the text, click anywhere to save it. Search using asset descriptions â You can also filter your assets by the keywords in your asset descriptions. Here are the steps: In the Filters menu on the Assets page, click Properties . Next, click Description from the dropdown menu. In the Description popup display, enter a keyword in the text box. Click on the downward arrow and then select the preferred matching condition. To add multiple description filters, click + in the Description popup. The Assets page will now display a list of assets filtered by your description text. You can click Clear All in the Filters menu at any time to remove all your filters. danger If you're using integration code or custom packages to update asset descriptions, there may be additional nuances to consider since these can override either (or both) description attributes: description and userDescription . Tags: data integration crawl asset-profile Previous Add owners Next Add certificates Add descriptions to your assets Add column descriptions Search using asset descriptions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/star-assets",
    "text": "Use data Discovery Asset Management Star assets On this page Star assets Who can do this? Anyone with access to Atlan   -  admin, member, or guest user   -  can star assets. Atlan allows you to star your most used assets and bookmark them for quick and easy access. Once you have starred your assets, you can: Access your starred assets from anywhere in Atlan Get a personalized view of your homepage with starred assets Filter by your starred assets Sort your assets by star count View starred activity in the activity log Set up Slack or Microsoft Teams notifications for metadata updates Star an asset â Navigate to the left menu of any screen in Atlan and click Assets to begin: From asset sidebar â To star an asset from the asset sidebar: From the Assets page, click an asset you want to star. Navigate to the the upper right of the Overview sidebar and click the star button to star the asset and add it to your list of starred assets. From asset profile â To star an asset from the asset profile: From the Assets page, right-click an asset you want to star and select Open profile . Navigate to the top right of the asset profile and click the star button to star the asset and add it to your list of starred assets. Click the star button again to remove the asset from your list of starred assets. Did you know? Only you have the power to edit stars from your starred assets. However, for assets you've starred, other users can view your username while hovering over the star button and from the activity log. View starred assets â Once you have starred your assets, you can use the Starred assets widget to view them from anywhere in Atlan. The starred assets widget will show you a total count and complete list of your starred assets. To view starred assets: From the top right of any screen in Atlan, click the star icon. This will also show you a total count of your starred assets. From the Starred assets popup, you can view starred assets sorted by most recently starred or use the search bar to search for specific starred assets. Click Open in Assets to view all your starred assets. The corresponding Assets page will only show all your starred assets with the Starred assets filter applied. To further refine your search, add more filters . (Optional) Next to the search bar on the Assets page, click the sort button. From the Star count sorting menu, click Most starred to view most starred assets or Fewest starred to view least starred assets.Â Click any starred asset to view more details: In the top right of the Overview sidebar, hover over the star button to view total star count and users who have starred it in a popover. From the sidebar tabs on the right, click the Activity tab to view starred activity by user and timestamp. Enable metadata update alerts â You can set up Slack or Microsoft Teams notifications for metadata updates on all your starred assets in Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts directly delivered to your Slack or Microsoft Teams account, you can stay informed about the latest changes to your starred assets. Before you can set up notifications for starred assets, you will need to: Slack: Integrate Slack and Atlan Link your Slack account to Atlan Microsoft Teams: Integrate Microsoft Teams and Atlan : For any existing Microsoft Teams integrations prior to February 22, 2024 only, you will need to update the Atlan app in your Microsoft Teams workspace to use this feature. For all new Microsoft Teams integrations from February 22, 2024 onward, no additional configuration required. Link your Microsoft Teams account to Atlan To enable notifications for starred assets: From the top right of any screen in Atlan, click the star icon. From the bottom left of the Starred assets popup, click Enable notifications . To link your Slack or Microsoft Teams account: If you have already linked your Slack account , skip this step. To link your Slack account to Atlan , in the Receive notifications for starred assets dialog, click Slack , and from the corresponding screen, click Allow to continue. If you have already linked your Microsoft Teams account , skip this step. To link your Microsoft Teams account to Atlan , in the Receive notifications for starred assets dialog, click Teams , and from the corresponding screen, click Allow to continue. In the notifications setup dialog, for Notify me about , you can either: Click All updates to receive notifications for all the changes listed in Custom updates made to your starred assets. Click Custom updates to limit notifications to specific types of metadata updates: Click Name to receive an alert when the name of an asset is updated. Click Description to receive an alert when a description is added to, updated, or removed from an asset. Click Announcement to receive an alert when an announcement is added to, updated, or removed from an asset. Click Certificate to receive an alert when a certificate is added to, updated, or removed from an asset. Click Owners to receive an alert when an owner is assigned to or removed from an asset. Click Readme to receive an alert when a README is added to, updated, or removed from an asset. Click Terms to receive an alert when a term is linked to or unlinked from an asset . Click Tags Â to receive an alert when a tag is attached to or removed from an asset. Click Propagated Tags to receive an alert when a tag is propagated to or removed from an asset. Â Click Save to save your notification preferences. (Optional) To edit notification settings, from the bottom left of the Starred assets popup, click Enabled . In the notifications setup dialog, you can further customize your notifications. (Optional) To remove notifications, from the bottom left of the Starred assets popup, click Enabled . In the notifications setup dialog, click Disable notifications to reset your notification settings. You will now receive Slack or Microsoft Teams notifications for changes made to all your starred assets in Atlan! ð Tags: data asset-profile Previous Add an alias Next Add owners Star an asset View starred assets Enable metadata update alerts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-an-alias",
    "text": "Use data Discovery Asset Management Add an alias On this page Add an alias Who can do this? Any non-guest user with edit access to an asset's metadata can add an alias. This only includes admin and member users. An alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use Atlan AI to do the same, if Atlan AI is enabled in your Atlan workspace . This can help you improve the readability of your asset names while providing useful context to your users. Atlan recommends adding an alias that's unique to each asset, in a one-to-one relationship. To relate your assets, you can instead attach tags to group them by use case or link terms to group them conceptually. Atlan currently supports adding an alias to everything EXCEPT the following asset types: Database Schema Connection Process, including ConnectionProcess BIProcess ColumnProcess Query Example â For an asset with a technical name like FCT_SUPPLIER_TRANSACTIONS , you can add Supplier Transaction Records as an alias. Once you have added an alias, you can: Search for assets either with their technical names or aliases, Atlan will match on the most relevant title. Use the Title property filter to filter for assets either by their technical names or aliases. Set asset name display preferences for personas , choosing whether the technical name or alias should be displayed prominently in search and discovery. View aliases in search results , asset preview, asset profile , asset sidebar , and lineage graph . Add an alias â To add an alias to your asset: From the left menu of any screen in Atlan, click Assets . On the Assets page, click any asset with a technical name. To add an alias, you can either: Open the asset profile, and to the right of the asset name, click the pencil icon. Navigate to the Overview sidebar, and to the right of the asset name, click the pencil icon. In the Add an alias dialog, you can either: In the Type a business-friendly name field, enter an alias for your asset. If Atlan AI is enabled , under Atlan AI Suggested , click an Atlan AI-suggested alias for your asset. (Optional) Click the refresh icon to regenerate Atlan AI suggestions and compare different sets of suggestions. Click Add to save your preferred alias for the asset. (Optional) Once you have added an alias, you can: Hover over the asset name to view both the technical name and alias in a popup. From the asset sidebar tabs on the right, click the Activity tab to view alias activity by user and timestamp of update. To edit your alias, click the pencil icon to make any changes. From the filters menu on the left, click Properties and then click Title to filter assets by the technical name or alias. Your asset will now display an alias! ð You can also set asset name display preferences to technical name or alias for your personas. If no preference has been specified and an alias is available, then Atlan will display the alias over the technical name on an asset by default. Tags: atlan documentation Previous Access archived assets Next Star assets Example Add an alias"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/supported-sources",
    "text": "Connect data Connectivity Framework Connector Framework References Supported sources On this page Supported sources At Atlan, we have two core philosophies on integrations: Simple setup with out-of-the-box connectors . In three steps , you'll have your sources connected, with activity logs and automated Slack or Microsoft Teams alerts for easy monitoring. Extensible through Open APIs . We've built Atlan on an Open API architecture, so every action on Atlan can be driven by APIs. This means you can bring in any data product you want, from any source. Out-of-the-box connectors â For more information on supported capabilities of Atlan's current integrations, see Connectors and capabilities . Data sources â Amazon Athena Amazon Redshift AWS Glue (including S3) Databricks Google BigQuery Hive Microsoft Azure Synapse Analytics Microsoft SQL Server MySQL Oracle PostgreSQL PrestoSQL Redash Salesforce SAP ECC SAP HANA Snowflake Teradata Trino NoSQL data sources â Amazon DynamoDB Microsoft Azure Cosmos DB MongoDB Business intelligence tools â Amazon QuickSight Domo IBM Cognos Analytics Looker Metabase Microsoft Power BI MicroStrategy Mode Qlik Sense Cloud Qlik Sense Enterprise on Windows Sigma Sisense Tableau ThoughtSpot Data movement tools â dbt Cloud dbt Core Fivetran Matillion Microsoft Azure Data Factory Data quality tools â Anomalo Monte Carlo Soda Event buses â Aiven Kafka Amazon MSK Apache Kafka Confluent Kafka Microsoft Azure Event Hubs Redpanda Kafka Schema registry â Confluent Schema Registry Orchestration tools â Airflow/OpenLineage Amazon MWAA/OpenLineage Astronomer/OpenLineage Google Cloud Composer/OpenLineage Alteryx Data processing tools â Apache Spark/OpenLineage Didn't find a tool in your stack? â You can use Atlan's Open APIs to bring in metadata from any source in the modern data stack. For example: Data sources â AWS S3 buckets and objects Google Cloud Storage buckets and objects Microsoft Azure Data Lake Storage containers and objects NetSuite Vertica APIs Files Business intelligence tools â Google Data Studio Preset Tags: integration connectors alerts monitoring notifications faq troubleshooting api rest-api graphql Previous OpenLineage configuration and facets Next Can I connect to any source with an ODBC/JDBC driver? Out-of-the-box connectors Didn't find a tool in your stack?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains/how-tos/manage-domains",
    "text": "Build governance Domains Get Started Manage domains On this page Manage domains â Available via the Data Marketplace package Who can do this? You must be an admin user in Atlan to create and manage domains. Any non-guest users must be granted the update metadata permission to be able to add assets to a domain. Domain policies currently don't have any impact outside the products module . Domains provide a logical way of mapping and organizing assets within a specific domain or business entity. For example, you can create domains for the following: Functions such as finance, sales, and human resources Business units or brands for different products and services Geographic regions of operation Environments such as development and production Most importantly, domains help promote shared ownership and domain-level governance in your organization. To create a domain, complete the following steps. Add a domain â To add a domain: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Domains . If you have enabled the products module , refer to How to create data domains instead. On the Domains page, under All domains , click Create domain . For Overview , enter basic details for your domain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your domain. For Name , enter a meaningful name for your domain   -  for example, Product Operations . (Optional) Click the domain icon to change the icon for your domain. (Optional) For Description , enter a description for your domain. For Owners , assign additional users or groups as domain owner(s). In the top right of the screen, click the Create button to complete setup. Congrats on creating a domain in Atlan! ð Your users can now add assets to your domain . (Optional) Add a subdomain â danger You will first need to create a domain before you can add subdomains. Subdomains help you logically segment your domains according to business needs. To add a subdomain: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Domains . If you have enabled the products module , refer to How to create data domains instead. OnÂ the Domains page, under All domains , select a domain or subdomain to add a subdomain. From the upper right of your domain page, click the + Add button and then click New sub-domain . For Overview , enter basic details for your subdomain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your subdomain. For Name , enter a meaningful name for your subdomain   -  for example, Analytics . (Optional) Click the domain icon to change the icon for your subdomain. (Optional) For Description , enter a description for your subdomain. For Owners , assign additional users or groups as subdomain owner(s). In the top right of the screen, click the Create button to complete setup. Congrats on creating a subdomain in Atlan! ð Manage a domain â The domain profile includes essential details about the domain. You can also curate what your domain users will be able to view. To manage a domain: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Domains . If you have enabled the products module , refer to How to create data domains instead. OnÂ the Domains page, under All domains , hover over a domain or subdomain to: View domain owners in the Owners column. Click + Add personas to add a persona for governing assets within a domain or subdomain. danger Any non-guest users must be granted the update metadata permission to be able to add assets to a domain. Click the star button to star your domain and bookmark it for easy access. For subdomains only, to the right of the subdomain name, click the 3-dot icon and then: Click Move to to move a subdomain to a different parent domain. In the Move to dialog, select a relevant parent domain within the same or a different domain and then click Move to confirm the changes. Click Convert to domain to convert a subdomain into a parent domain. In the Convert to domain dialog, click Convert to domain to confirm your changes. Click a domain or subdomain to navigate to the domain or subdomain profile, respectively. On your domain page, the Overview tab displays important details about the domain. (Optional) From the top right, click the + Add button and then click New sub-domain to add data subdomains. Under Summary , view a total count of assets in your domain and the domain description: (Optional) Click + Add stakeholder to add stakeholders . (Optional) Click the Description field to update the description. (Optional) For Owners , click the pencil icon to add or remove owners . (Optional) If custom metadata properties are available, you can add custom metadata to your domain. (Optional) Click + Add resource to add a resource to your domain. Under Readme , click + Add to add a README to your data domain or use Atlan AI for documentation . Â From the top right of the domain profile: Click the user avatars to view a list of recently visited users, total views on your domain, total number of unique visitors, and total views by user. Use the days filter to filter domain views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your domain and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to configure the following: Click Add announcement to add an announcement to your domain. Click Add a resource to add resources to your domain. Click Archive to archive your domain   -  ensure that your domain is empty before you archive it. Change to the Assets tab to view assetsÂ within your domain. Change to the Statistics tab to monitor domain usage . If you have enabled the products module , change to the Lineage tab to view business lineage for your domain . Did you know? You can set up playbooks to bulk add assets to your domains and subdomains or remove them. Tags: atlan documentation Previous Domains Next How to organize assets Add a domain (Optional) Add a subdomain Manage a domain"
  },
  {
    "url": "https://docs.atlan.com/tags/asset-profile",
    "text": "5 docs tagged with \"asset-profile\" View all tags Access archived assets Learn about access archived assets. Add custom metadata <div style={{position: \"relative\", paddingBottom: \"calc(66.33333333333333% + 41px)\", height: 0}}> <iframe src=\"https://demo.arcade.software/1dT1bPneM5fp1O71lb. Add descriptions You can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information. Search and discover assets Atlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context. Star assets :::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can star assets."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights",
    "text": "Use data Insights On this page Insights â Available via the Insights package Overview: Use Atlan's Insights to query and analyze your data assets. Leverage the visual query builder or write SQL queries to explore your data, create visualizations, and share insights with your team. Get started â How to query data Guides â Query building â How to make a query interactive : Create interactive queries with parameters. How to query without shared credentials : Query data without sharing credentials. How to provide credentials to query data : Configure credentials for querying. Query management â How to save and share queries : Save and collaborate on queries with your team. How to schedule a query : Set up automated query execution. Concepts â Query builder actions : Learn about the different actions available in the query builder. References â Tips and tricks : Handy shortcuts and advanced capabilities for Insights. Troubleshooting â Troubleshooting exporting large query results : Resolve issues when exporting large query result sets. Troubleshooting bring your own credentials : Fix credential-related problems when running queries. FAQ â Why do I only see tables from the same schema to join from in a visual query : Understand schema-based join limitations. Can I turn off sample data preview for the entire organization : Learn about sample data preview settings. Are there any limits on concurrent queries : Understand query concurrency limits. Can I query any data warehouse or data lake : Learn about supported data sources. How to monitor for runaway queries : Track and manage query performance. Can we restrict who can query our data warehouse : Understand query access controls. How can I identify an insights query in my database access log : Track query execution in logs. How to use parameterized queries : Create dynamic queries with parameters. What controls the frequency of queries : Understand query scheduling and execution. Tags: insights query analysis capabilities Next How to query data Get started Guides Concepts References Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/query-without-shared-credentials",
    "text": "Use data Insights Credentials How to query without shared credentials On this page query without shared credentials Who can do this? You will need to beÂ a connection admin in Atlan to enable bring your own credentials (BYOC) on a specific connection. Don't want to use a single shared service account to access data? With bring your own credentials (BYOC), users need to provide their own credentials before they can query data. Each user's permissions in the data store itself are then applied to each query. This is helpful for organizations that have already invested in defining granular controls in their data stores. With BYOC, you can reuse those controls without any extra work! Atlan currently supports the following connectors for BYOC: Amazon Athena Amazon Redshift Databricks MySQL PostgreSQL Presto Snowflake Trino Did you know? To query data using SSO credentials instead, refer to Authenticate SSO credentials to query data . Enable BYOC on a connection â To enable BYOC on a connection: From the left menu of any screen, click Assets . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select the connection for which you want to enable BYOC. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type ,Â click Basic credentials to enforce user credentials for querying data . UnderÂ Display sample data , for Source preview ,Â click Basic credentials to enforce user credentials for viewing sample data . (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Tags: atlan documentation Previous Schedule a query Next Provide credentials to query data Enable BYOC on a connection"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/concepts/what-are-the-query-builder-actions",
    "text": "Use data Insights Concepts What are the query builder actions? On this page What are the query builder actions? When using the Visual Query Builder you can use any of the following actions in your query. To illustrate their use, imagine the following sample tables: Data for examples â Expenses table â CATEGORY_ID MEDIUM SPEND 1 Facebook 447000 1 Google 94500 1 LinkedIn 12300 2 Sales 700000 2 Marketing 400000 3 Salesforce 250000 3 Hubspot 75000 Category table â ID NAME 1 Advertisement 2 Salary 3 Software 4 Other Actions â Join data â Join data to combine the contents of several assets. To join data you'll need to select: The type of join (see table below). The left asset and the column data to match with the right asset's column. The right asset and the column data to match with the left asset's column. Type of join Explanation Inner Join returns only matching records from both the left table and the right table. Left Join returns all records from the left table, and only matching records from the right table. Right Join returns all records from the right table, and only matching records from the left table. Outer Join returns all records from both tables. For example, to get a set of results that shows the category of expenses along with the spend: Select Inner Join or Left Join as the join type Select Expenses for the left table, and CATEGORY_ID as its column Select Category for the right table, and ID as its column The resulting table would be: CATEGORY_ID MEDIUM SPEND ID NAME 1 Facebook 447000 1 Advertisement 1 Google 94500 1 Advertisement 1 LinkedIn 12300 1 Advertisement 2 Sales 700000 2 Salary 2 Marketing 400000 2 Salary 3 Salesforce 250000 3 Software 3 Hubspot 75000 3 Software Did you know? You can join more than 2 assets by clicking the Add another link at the bottom of the Join data tile. Each additional join will add objects to the list of assets that can be used on the left of the join. Filter â Filter data to return only some of the rows. To filter data you'll need to select: The column data by which to narrow the rows. The operation used to compare that column's data. The value to compare against. For example, to retrieve only the advertising spend: Select NAME for the column Select Equal to for the operation Enter Advertisement for the value The resulting table would be: CATEGORY_ID MEDIUM SPEND ID NAME 1 Facebook 447000 1 Advertisement 1 Google 94500 1 Advertisement 1 LinkedIn 12300 1 Advertisement Group â Group data to combine rows together into buckets. To group data you'll need to select the column by which to bucket the data. For example, to bucket the data by category: Select NAME If you remove the filter step above, the resulting table would be: NAME Advertisement Salary Software Did you know? It may look like you've over-simplified the data now. If your query stopped here, that would be true. But think of this action as preparation for other actions, like aggregating. Aggregate â Aggregate data to calculate a metric from many rows of data. For example, to calculate a metric against each of the buckets created by grouping. To aggregate data you'll need to select: The column on which to run the calculation The calculation to run: Count : calculate the total number of rows Unique Count : calculate the number of unique values for this column across the rows Sum : add all the values of the rows together Max : find the highest value in the rows Min : find the lowest value in the rows Average : add all the values of the rows together, then divide by the number of rows For example, to calculate the total spending by category: Select SPEND for the column Select Sum for the calculation The resulting table would be: NAME sum_SPEND Advertisement 862800 Salary 1100000 Software 325000 Sort â Sort data to return the rows in a defined order. To sort data you'll need to select: The column by which to order the results The direction in which to order them: ASC : return the lowest value as the first row and highest value last DESC : return the highest value as the first row and lowest value last For example, to return the category with the highest spending: Select sum_SPEND for the column Select DESC for the order The resulting table would be: NAME sum_SPEND Salary 1100000 Advertisement 862800 Software 325000 Tags: atlan documentation Previous Provide credentials to query data Next Insights tips and tricks Data for examples Actions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/references/tips-and-tricks",
    "text": "Use data Insights References Insights tips and tricks On this page Insights tips and tricks At Atlan, we are committed to powering your user experience. Here are a few tips and tricks to help you get the most out of Insights for you and your team! Export large query results via email â Who can do this? You will need your Atlan administrator to enable scheduled queries . Once enabled, you need to be a connection admin to increase the query limit to more than 100,000 rows. Atlan currently has an upper limit of 30 million rows. Reach out to your data success manager if youâd like to increase the limit for your organization. Previously, users could only scan and see query results for up to 100,000 rows by default. To help you scan more rows and export those results, you can now export queries for more than 100,000 rows directly to your email inbox. To export query results for more than 100,000 rows to your email inbox: In the query editor in Insights , type a limit for more than 100,000. (Note: Be sure to uncheck the Limit to 100 rows box in the query editor.) Click Run to run the query. The query results set will only display 100,000 rows. In the yellow bar above the query results set, click Send results via email . You will need to save the query to get the results via email. In the Save Query dialog box: Select the relevant Query collection to save your query. If you haven't created one, you will get a prompt to create a query collection during this step.Â For Query name , type a name for your query. (Optional) Add a description, certification status, and term.Â Click Save Query to receive the query results in your email inbox. (Optional) To add an announcement to alert others in your team, click Add Announcement .Â The query results will be delivered to your email inbox in a CSV file! ð If your saved query consists of multiple queries, you'll receive the results for each query in a separate CSV file. Did you know? Once the query run is successful, you will also be able to download the query results as a CSV file directly in Atlan. If you donât wish to run the query, you can abort the query at any time. If you have any questions, head over here . Open asset sidebar from Insights â To get all the context you need before querying an asset, you can open the asset sidebar directly from Insights . To open the asset sidebar: In the Explorer left panel in Insights , hover over a table and click on the Open sidebar icon. (The name may vary depending on the asset type, such as Open table sidebar for table assets). You will now be able to view the asset sidebar for the asset you'd like to query! ð Search and sort query results â Search query results â With the search feature enabled for query results, you can use the search bar to type a search term and filter your results by that specific term.Â To search your query results after running a query: In the query results set in Insights , click the Search results bar. In the Search results bar, type your search term, such as Tuesday , to see the query results for that term. Sort query results â If you'd like to sort your query results, use the sort feature to order your query results.Â To sort your query results after running a query: In the query results set in Insights , click on a column name , such as user id , to sort your results in an ascending or descending order. Did you know? Atlan currently supports text, number, and Boolean data types for sorting query results. Add inferred data types â danger Atlan currently supports inferred data types only for Databricks Â and Snowflake assets. If your column data is stored in a standard data type   -  for example, STRING -  you can add an inferred data type to the column. This will help you query the data using the Visual Query Builder .Â For example, if your column contains dates stored in a STRING format, you can infer this column as a DATE data type in Atlan. To add an inferred data type: danger You will need to select an acceptable inferred data type as per your data source. From a query â From the left menu of any screen, click Insights . At the top of the screen, to the right of the Untitled tab, click the + button and select New visual query . Under Select from , choose the table you want to query. In the column selector to the right, select the column to which you want to add an inferred data type   -  for this example, we'll choose a column with aÂ STRING data type, stringDateColumn . Hover over your selected column, and in the top right of the metadata popover, click the Open sidebar icon. At the top of the column sidebar, click the three dots icon and then click Add inferred datatype . For Add Inferred datatype , type a secondary data type   -  for this example, we'll add a DATE data type. Click Add to add your inferred data type.Â (Optional) To query the column with an inferred data type: In the query editor, click the blue circular + button and then select Filter as the action. For WHERE , select the column with an inferred data type. Under the selected column, click the inferred data type option to query the column in Atlan. From an asset â From the left menu of any screen, click Assets . On the Assets page, click a column asset with a STRING data type to open the asset sidebar   -  for example, stringDateColumn . At the top of the column sidebar, click the three dots icon and then click Add inferred datatype . For Add Inferred datatype , type a secondary data type   -  for this example, we'll add a DATE data type. Click Add to add your inferred data type. The inferred data type for your selected column will now be displayed in the asset sidebar! ð To remove the inferred data type from a column asset, click the pencil icon. In the dialog, click x to remove the data type and then click Remove to confirm removal.Â View querying costs â To help you gain the most value-driven insights in Atlan, you will receive a cost nudge before querying your Google BigQuery views and materialized views . This will inform you about the precise bytes that will be spent during the execution of the query, helping you decide if you would still like to run the query. For all subsequent runs of the same query, Atlan will use cached query results from Google BigQuery   -  note that the query text must be the same as the original query. Google BigQuery table assets are already cost-optimized for querying. To view querying costs for a Google BigQuery view: Under the Explorer tab in Insights , hover over a Google BigQuery view and click the play icon. In the cost preview dialog, click Cancel to cancel running the query based on the costs shown or click Run to proceed with running the query. Did you know? You will also receive a cost nudge before viewing Google BigQuery sample data previews . Tags: atlan documentation Previous What are the query builder actions? Next Are there any limits on concurrent queries? Export large query results via email Open asset sidebar from Insights Search and sort query results Add inferred data types View querying costs"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/concurrent-queries-limit",
    "text": "Use data Insights FAQ Are there any limits on concurrent queries? Are there any limits on concurrent queries? Atlan allows up to 8 concurrent queries per user.Â If a user submits more than 8 queries simultaneously, any additional queries will be placed in a waiting state. There is currently no limit on the total number of concurrent queries across multiple users in a tenant. Atlan is benchmarked to perform optimally with up to 200 users querying simultaneously. For scheduled queries , Atlan sets a limit of 50 concurrent queries to ensure stability and optimal resource consumption. Tags: atlan documentation faq-insights Previous Insights tips and tricks Next Can I query any DW/DL?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/troubleshooting/troubleshooting-bring-your-own-credentials",
    "text": "Use data Insights Troubleshooting Troubleshooting bring your own credentials On this page Troubleshooting bring your own credentials Once your connection admins have configured bring your own credentials (BYOC) in Atlan, users will need to provide their own credentials before they can query data or preview sample data . On that note, here are a few things to keep in mind: How do I distinguish between credentials for BYOC and the crawler? â If user credentials are enforced: There will be an admin credential setup for the crawler, which is part of the workflow configuration.Â There will also be a separate user credential setup required for each user who wants to query the connection via Insights or view sample data.Â The crawler's credentials will be used for the miner as well   -  such as for reading DDL commands and query history at source. What credentials do I need for sample data preview? â Whether you need to provide credentials for sample data preview will depend on the connection settings. If default credentials are enabled, you'll be able to view sample data without having to provide your own credentials. If user credentials are enforced, you'll be prompted to provide your credentials for viewing sample data. Can I use admin or user credentials for both workflow and BYOC setup? â Both types of credentials can be used for configuring a crawler or enabling user credentials, as long as they are valid and have the required permissions. What type of credentials should be used for the workflow setup? â The credentials used must be associated with an account that has many permissions. However, this can be somewhat limited for certain sources. For instance, if using the account usage method for setting up a Snowflake workflow instead of the information schema method. Tags: atlan documentation Previous Why do I only see tables from the same schema to join from in a visual query? Next Troubleshooting exporting large query results"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/schedule-a-query",
    "text": "Use data Insights Query Management Schedule a query On this page Schedule a query Who can do this? Before scheduling a query, you will need your Atlan administrator to enable scheduled queries . You might want to schedule a query to run repeatedly. For example: If you want to fetch data for the same query at different times. If you want to share data with business teams on a consistent basis, for example, weekly. If a data analyst is out of office but wants the data to be shared with users periodically. danger You must save your query before you can schedule it. Your SMTP configuration must also be in a working state to send results to recipients. Schedule a query â To schedule a query: Open Insights . In the upper left, click the papers-in-a-box icon to open your query collections. Choose the query collection where the query you want to schedule is saved. Hover over the saved query you want to schedule, click the three dots to its right, and then click Schedule . In the New Schedule Query dialog, enter the scheduling details:Â For Name , enter a meaningful name for the scheduled query. For Runs every , choose how often the query should run: For Hour and Day , you can go down to the minute for hourly and daily runs. For Week , you can select multiple days for a weekly run. For Month , you can select multiple dates for a monthly run. For Custom cron , write your own custom cron. For Timezone , select a relevant option or keep the default one. For Share Results , select the user(s) to whom you want to send the results from each run. (Optional) If there are any existing schedules for your query, from the top right, click the existing schedule link to view more details in a sidebar. At the bottom right of the dialog, click Done . (Optional) Under Query successfully scheduled , click the Run Now link if you want to test the scheduled query. At the bottom right of the dialog, click Finish . Your query will now run on the defined schedule and the results will be sent out automatically! ð Note that the download link for the results is only valid for 6 hours.Â Did you know? If a scheduled query fails, you'll receive an email notification with the name of the query, so you can troubleshoot and get it running in no time. Change a scheduled query â To change the schedule for a query: Open Insights . In the upper left, click the repeating-arrow icon to open your scheduled queries. Hover over the scheduled query you want to change, click the three dots to its right, and then: (Optional)Â Click Run now to run the scheduled query immediately. (Optional) Click Pause to pause the scheduled query. Click Edit to update the scheduled query. In the Update Schedule Query dialog box, make any desired changes to the schedule or recipients. At the bottom right of the dialog box, click Update to save the changes. (Optional) Under Query successfully updated , click the Run Now link if you want to test the scheduled query. At the bottom right of the dialog, click Finish . Your query will now run on the changed schedule and the results will be sent out automatically! ð Remove a scheduled query â To remove a scheduled query: Open Insights . In the upper left, click the repeating-arrow icon to open your scheduled queries. Hover over the scheduled query you want to stop, click the three dots to its right, and then click Delete . In the Delete Schedule dialog box, click Delete . Your query will no longer run automatically. Tags: data integration configuration Previous Make a query interactive Next How to query without shared credentials Schedule a query Change a scheduled query Remove a scheduled query"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/make-a-query-interactive",
    "text": "Use data Insights Query Management Make a query interactive On this page Make a query interactive If you want to share a query with others, but limit how they can change the query, you can make it interactive . Others can then only change the value(s) that are interactive in your query   -  not the query itself. Who can do this? The owner of the query, or anyone with edit access to the query can make it interactive. Once interactive, anyone with read-only access to the query can also edit only the interactive part. SQL queries â To make a SQL query interactive: Open the query in Insights. Highlight the value you want to be interactive. On the bar separating the query from the results, click the curly braces icon and then click the Add variable button. Click the settings icon to the right of the Enter a string text box to define the custom variable: For Name , enter a meaningful name for the custom variable. For Type , change the data type of the custom variable, if necessary. For Default value , enter the value to use for the custom variable if other users do not change it. At the bottom of the Edit dialog for the custom variable, click Save . In the upper right of the query editor, click Run to confirm the query still operates as expected. That's it, your query is now interactive! ð Did you know? You can select from a wide range of data types for your custom variables   -  string, number, date, date and timestamp ranges, and multi-value options. VQB queries â To make a Visual Query Builder (VQB) query interactive: Open the query in Insights. Find the value you want to be interactive. In the right of the box for that value, click the lightning bolt icon. In the upper right of the query editor, click Run to confirm the query still operates as expected. That's it, your query is now interactive! ð Tags: atlan documentation Previous Save and share queries Next Schedule a query SQL queries VQB queries"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/how-tos/integrate-slack",
    "text": "Configure Atlan Integrations Collaboration Slack How-tos How to integrate Slack On this page Integrate Slack Who can do this? You will need to be an admin in Atlan to configure the Slack integration. You will also need inputs and approval from an administrator of your Slack workspace. To integrate Slack and Atlan, follow these steps. Retrieve Slack tokens â To retrieve Slack's integration tokens: Access your Slack apps console at: https://api.slack.com/apps At the bottom of the page, in theÂ Your App Configuration Tokens box click theÂ Generate Token button. In theÂ Generate Your App Configuration Token dialog, from the Workspace drop-down choose your Slack workspace and then click theÂ Generate button. From theÂ Your App Configuration Tokens box: UnderÂ Access Token click theÂ Copy button and save this temporarily. UnderÂ Refresh Token click theÂ Copy button and save this temporarily. danger These tokens will usually expire after 12 hours, so will need to be used the same day they are generated. Connect Atlan to Slack â To connect Atlan to Slack, from within Atlan: From the left menu, clickÂ Admin . UnderÂ Workspace , clickÂ Integrations . In theÂ Slack tile, click theÂ Connect button. Enter the tokens copied above: ForÂ Access token enter the access token value. ForÂ Refresh token enter the refresh token value. ClickÂ Next to continue. UnderÂ Install the Atlan app in your Slack workspace click theÂ Install now button. At the bottom of the resulting Slack popup, click the Allow button. (If you want more details on what each permission does, see What does Atlan do with each Slack permission? ) (Optional) Request permission from your Slack admin â If you are not a workspace administrator in Slack, you will be prompted to request permission to install. To request permission to install the integration: UnderÂ Add a message for your App Managers enter an explanation for installing the app. At the bottom of the form, click theÂ Submit button. Contact your Slack workspace administrator and ask them to approve the Atlan app. Once approved, you'll get an alert in Slack from Slackbot. Once you receive the Slackbot alert, return to theÂ Integrations menu in Atlan (in the Admin Center) and click theÂ Add to Slack button. Atlan is now connected to Slack! ð Configure integration from Atlan to Slack â To configure the Slack integration from Atlan, from the Integrations sub-menu: Expand the Slack tile. Under theÂ Configurations tab, enter channels to use in your Slack workspace. Enter the channel name or provide a link to the channel without a   , and press tab or enter after each channel to add multiple channels. (Optional) For Channels , add any channels that users should be able to post to from within Slack or get notified on glossary updates . (Optional) For Announcements channel , enter the name of a single channel that can be used to view announcements on assets in Atlan. (Optional) For Workflows alert channel , enter the name of a single channel that can be used to view alerts for workflow activities in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. (Optional) For Playbooks alert channel , enter the name of a single channel that can be used to view alerts for playbook runs in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. (Optional) ForÂ Query output share channels , add any channels where users should be able to share query output. (Optional) For Request notifications , toggle on the slider to receive Slack notifications when requests are raised in Atlan and approve or reject them directly from Slack. At the bottom of the tile, click theÂ Update button. Users can now post to Slack without leaving Atlan! ð Did you know? Channels need to be public for Atlan to be able to post to them. If you try to integrate a private channel you will see an error for that channel when you try to update. Configure integration from Slack to Atlan â To configure the Atlan integration from Slack, from within Slack: Open each channel you want Slack users to be able to query Atlan from within. At the top of the channel, click the name of the channel. Change to theÂ Integrations tab. In theÂ Apps tile, click theÂ Add an app button. Find the Atlan app under the In your workspace heading and click the Add button next to it. (Optional) To add an icon to the Atlan app in Slack, from the Apps page, click the Atlan app. On the Display Information page, under App icon & Preview , click + Add App Icon and upload the Atlan icon for the app.Â Users can now search for assets in Atlan without leaving Slack! ð Did you know? You can even add the Atlan app to private channels. Tags: integration api configuration Previous Slack Next Link your Slack account Retrieve Slack tokens Connect Atlan to Slack Configure integration from Atlan to Slack Configure integration from Slack to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams",
    "text": "Configure Atlan Integrations Collaboration Microsoft Teams How-tos How to integrate Microsoft Teams On this page Integrate Microsoft Teams Who can do this? You will need to be an admin in Atlan to configure the Microsoft Teams integration. You will also need inputs and approval from the Application Administrator and Teams Administrator of your Microsoft Teams workspace. To integrate Microsoft Teams and Atlan, follow the se steps. Retrieve the team link â To retrieve the team link, from within Microsoft Teams: In the left menu in Microsoft Teams , click Teams . Under Teams , navigate to your team, and to the right of the team name, click the three dots . From the dropdown menu, click Get link to team . From the Get a link to the team dialog, click Copy to copy the team link and save it in a temporary location. Connect Atlan to Microsoft Teams â Once you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams. Atlan requires the following delegated permissions for the Microsoft Teams integration: offline_access -  allows the Atlan app to access resources on behalf of the users, even when users are not currently using the app. User.Read -  allows users to sign in to the app, and allows the app to read the profile of signed-in users. AppCatalog.Submit -  allows the app to submit application packages to the catalog and cancel submissions that are pending review on behalf of the signed-in user. ChannelMessage.Read.All -  allows the app to read a channel's messages in Microsoft Teams, on behalf of the signed-in user (only required if you have admin access to publish the app in Microsoft Teams). AppCatalog.ReadWrite.All -  allows the app to create, read, update, and delete apps in the app catalogs (only required if you have admin access to publish the app in Microsoft Teams). TeamsAppInstallation.ReadWriteSelfForUser -  allows a Teams app to read, install, upgrade, and uninstall itself for the signed-in user. From within Atlan: From the left menu on any screen, click Admin . Under Workspace , click Integrations . In the Microsoft Teams tile, click Connect . In the Add to Microsoft Teams dialog, for Team link , paste the team link you copied from Microsoft Teams above. Click Next to continue. For Publish the Atlan app : If you have Application Administrator access to Microsoft Teams: Click Publish now to publish the Atlan app in your Microsoft Teams workspace. In the Permissions requested popover, check the box for Consent on behalf of your organization to automatically grant access to the application for all users and then click Accept to publish the Atlan app. If you do not have admin access to Microsoft Teams: Click Request to publish . In the Permissions requested popover, click Accept to publish the Atlan app. Contact your Microsoft Teams administrator to approve and publish the Atlan app in your Microsoft Teams workspace. Did you know? If your global administrator has enabled the admin consent workflow , you will be prompted to request admin approval while attempting to install the Atlan app in your Microsoft Teams workspace. Reach out to your admin to approve the admin consent request from the Microsoft Entra admin center. Additionally, if there is any expiry date set for such requests, ensure that the request is approved within that period of time, otherwise you will need to request approval again. Add Atlan to the Microsoft Teams directory â danger You must allow 24 hours to elapse after setting up the Microsoft Teams integration in Atlan and before adding the Atlan app to your Microsoft Teams directory. This is required for the Atlan app to be activated in Microsoft Teams. To learn more, refer to Microsoft Teams documentation . Once the Atlan app has been published in your Microsoft Teams workspace, you will need your Teams Administrator to add Atlan to your Microsoft Teams directory. From within Microsoft Teams: Log in to the Microsoft Teams admin center with Teams Administrator access. In the left menu of Microsoft Teams , click Teams apps and then click Manage apps . On the Manage apps page, use the search bar to search for the Atlan app. To the left of the app name, click the circle to select the Atlan app. From the tabs along the top of the All apps section, click Add to a team . In the Add to a team form, configure the following: Click the search bar and enter the name of the team to which you want to add the Atlan app. Hover over the team name and then click Add . Click Apply to confirm your selection(s). Atlan is now connected to Microsoft Teams! ð Configure integration from Atlan to Microsoft Teams â Now that Atlan is connected to Microsoft Teams, you can configure the Microsoft Teams integration from Atlan. From the Integrations sub-menu: Expand the Microsoft Teams tile. Under the Configurations tab, configure the following: For any existing Microsoft Teams integrations prior to February 22, 2024 only, you will need to update the Atlan app in your Microsoft Teams workspace to use new features like enabling notifications for starred assets : If you have Application Administrator access to Microsoft Teams: Click Install now to update the Atlan app in your Microsoft Teams workspace. In the Permissions requested popover, click Accept to update the Atlan app. If you do not have admin access to Microsoft Teams: Click Request update . Contact your Microsoft Teams administrator to update the Atlan app in your Microsoft Teams workspace. Once updated, click Check status in Atlan to view the status of your requested update. info ðª Did you know? For all new Microsoft Teams integrations from February 22, 2024 onward, no additional configuration required. For Channels , enter channels to use in your Microsoft Teams workspace to allow users to post to from within Microsoft Teams or get notified about glossary updates . Enter the channel name without a   , and press tab after each channel if you want to enter multiple channels. (Optional) For Announcements channel , enter the name of a single channel that can be used to view announcements on assets in Atlan. (Optional) For Workflows alert channel , enter the name of a single channel that can be used to see alerts for workflow activities in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. (Optional) For Playbooks alert channel , enter the name of a single channel that can be used to see alerts for playbook runs in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. At the bottom of the tile, click the Update button. Users can now post to Microsoft Teams without leaving Atlan! ð Did you know? Channels need to be standard for Atlan to be able to post to them. If you try to integrate a private or shared channel, you will see an error for that channel when you try to update. Tags: integration connectors Previous Microsoft Teams Next Link your Microsoft Teams account Retrieve the team link Connect Atlan to Microsoft Teams Add Atlan to the Microsoft Teams directory Configure integration from Atlan to Microsoft Teams"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/concepts/what-are-requests",
    "text": "Use data Requests Concepts What are requests? On this page Requests Did you know? Atlan supports governance workflows ! Once you have enabled governance workflows and inbox , Atlan will channel requests and approvals for your governed assets through governance workflows and land them in your inbox . Who can do this? Any user without edit access to an asset's metadata can request changes to an asset. Requests allow users to suggest changes to assets that they cannot directly change themselves. Let's break that down a little bit: Admin users can directly change assets where they have permission, and suggest changes to all other assets. Member users can directly change assets where they have permission, and suggest changes to all other assets. Guest users can only suggest changes to an asset if the admin has enabled requests for guests . Examples â The key point to understand is the statement \"where they have permission\". As described in How do I control access to metadata and data? there are many options in Atlan to control these permissions. So let's look at a few examples: Connection admin permissions â In the simplest case, connection admins : Have direct edit access to all assets in a connection Both admin users and member users can be connection admins Connection admins will generally not be able to suggest changes to assets in their connection, because they can make changes to them directly. (The only exception to this is described below   -  when the edit permission granted to connection admins by default is overridden by a policy that explicitly denies edit permissions.) Policy-based permissions â You can grant more granular permissions through access policies : Control the subset of assets in a connection that can be directly changed (or not) Control the specific characteristics of assets that can be directly changed (or not) For example, in a metadata policy you could specify that: For any assets (tables, columns) in a certain schema, a certain group of users can only add or remove terms (no other changes). In this example, any admin users or member users in that group will still be able to suggest other changes to the assets in that schema. (For example, changes to the description, owners, certification, or tags.) Did you know? Remember that access is denied by default . This means if an admin user or member user is not a connection admin, and is not part of any policy giving them permission to change metadata, then by default the user will only be able to suggest changes to metadata. Overridden permissions â Keep in mind, though, that explicit restrictions always take priority . This means even when one policy (or being a connection admin) would otherwise give permission to change assets directly, it is possible to override that permission. For example, in a metadata policy you could specify that: For any assets (tables, columns) in a certain database, all updates are explicitly denied to a certain group of users. Even if a user does have permission to directly update an asset from some other policy, this explicit deny policy will take priority. In this case, the user will only be able to suggest changes to the assets in that database. It does not matter if they are an admin user or member user, or even a connection admin. Tags: workflow automation orchestration Previous Manage requests Examples"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/slack-integration",
    "text": "Configure Atlan Integrations Collaboration Slack FAQ What is included in the Slack integration? On this page What is included in the Slack integration? With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team. Once you integrate Slack with Atlan you can do all of the following â all without leaving Atlan! Share data assets on Slack Link important Slack threads to Atlan assets Start Slack conversations Set up Slack notifications for announcements , governance workflows , glossary updates , starred assets , and playbook runs When users share assets on Slack (like assets, glossaries and saved queries) Atlan provides an embedded preview and context for the asset directly in Slack. info ðª Did you know? To use all of the features outlined, each user needs to connect their individual Slack account to their Atlan profile. Ask questions about assets, without leaving Atlan â To ask questions about assets on Slack, without leaving Atlan: For any asset, in the asset sidebar, click the Slack icon. In the Share on Slack dialog: For Channel , choose the Slack channel where you want to post your question. For Message , enter the question you want to ask on the Slack channel. (Optional) Type @ to select another user from your Atlan workspace and tag them in your message. (Optional) For Add as a resource , toggle the slider on to add the message as a resource to your asset. When you add the message as a resource, anyone viewing the asset in Atlan will be able to see previous discussions in Slack about that asset. Click Share to post on Slack. Your message is now on Slack and linked to the asset for future reference! ð Add existing Slack discussions to assets â warning ð¤ Who can do this? Any non-guest user with edit access to an asset's metadata can add existing Slack discussions to assets. Ever discussed a data asset in Slack? It's important to bring that context back to your asset. From within Slack â To add existing discussions in Slack into Atlan, from Slack: On any discussion in Slack, open the context menu for actions and click Send to Atlan . Find the asset you want to link the discussion to: For Title enter a brief summary of the discussion. For Asset Types select the type of asset you want to link the discussion to. For Asset start typing in the asset you want to link the discussion to. Slack will run a search against Atlan and give you a list of options â select your asset from the list. At the bottom right of the dialog, click the Submit button. Your existing discussion on Slack is now linked to the asset for future reference. And Atlan has even updated the discussion thread with a link to the asset for everyone's convenience within Slack! ð From within Atlan â To add existing discussions in Slack into Atlan, from Atlan: From the asset, on the right of the screen, click the Slack sidebar icon: If there are existing discussions on the asset, to the right of Slack Conversations , click the Add link. If there are no existing discussions on the asset, click the Add Slack Thread button. In the Add Slack Thread dialog, under the Link heading, paste the link to the Slack discussion. Under the Title heading, write a brief description of the discussion. This will only be displayed to users who have not linked their Slack account . At the bottom of the dialog, click the Add button. Your existing discussion on Slack is now linked to the asset for future reference! ð Start direct chats from Atlan user profiles â You can only start a direct chat with Atlan users who have linked their Slack account to their Atlan profile. Once linked, the Say Hi Slack icon will appear next to the user's name. To start a direct chat with another Atlan user: Hover over any username. In the resulting dialog, click the Say Hi Slack icon next to the user's name. You are now in a direct chat with that user in Slack! ð Enable alerts for glossary updates in Atlan â warning ð¤ Who can do this? You will need to be an admin user in Atlan to enable Slack notifications for glossary updates. Once enabled, anyone in the selected Slack channel will be able to view these alerts directly in Slack. You can set up Slack notifications for updates made to your glossaries in Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts sent to a Slack channel of your choice , you'll be able to stay informed about the latest changes to your glossaries. Slack notifications can only be set up at the glossary level. Even if configured from a specific term or category profile, the notification settings will be applicable to the entire glossary. To enable Slack notifications for glossary updates, from Atlan: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of the glossary for which you want to set up change notifications. From the top right of the glossary profile, click the bell icon to set up notifications for glossary updates. In the notifications setup dialog, click Slack and then enter the following details: For Notification channel , click the dropdown to select a Slack channel where you want to receive notifications from the channel(s) configured in your Slack integration in Atlan . For Notify me about , you can either: Click All updates to receive notifications for all the changes listed in Custom updates made to your glossary. Click Custom updates to limit notifications to specific types of updates: Click + New term or category to receive an alert when a new term or category is added to the glossary. Click Name to receive an alert when the name of the glossary or that of a nested category or term is updated. Click Description to receive an alert when a description is added to, updated, or removed from the glossary or a nested category or term. Click Announcement to receive an alert when an announcement is added to, updated, or removed from the glossary or a nested category or term. Click Certificate to receive an alert when a certificate is added to, updated, or removed from the glossary or a nested category or term. Click Owners to receive an alert when an owner is assigned to or removed from the glossary or a nested category or term. Click Readme to receive an alert when a README is added to, updated, or removed from the glossary or a nested category or term. Click Tags (for terms) to receive an alert when a tag is attached to or removed from a term within the glossary. Click Categories updated (for terms) to receive an alert when a term is assigned to a different category within the same glossary. (Notifications for moving terms across glossaries are currently not supported.) Click Remove term to receive an alert when a term is removed from the glossary. (Notifications for removal of categories are currently not supported.) Click Save to save your notification preferences. (Optional) To edit notification settings, click the bell icon. In the notifications setup dialog, you can change the channel or further customize your notifications. (Optional) To remove notifications, click the bell icon. In the notifications setup dialog, click the Enabled (by username) dropdown. Click Remove notification to reset your notification settings. You will now receive Slack notifications for changes made to your glossary! ð If the Slack channel you selected for sending notifications is removed from your Slack integration in Atlan, glossary alerts will continue to be sent to that channel. In that case, you can either remove the notifications or select a different channel. info ðª Did you know? You can also set up Slack notifications for metadata updates on all your starred assets in Atlan. Query Atlan from within Slack â Find terms â To search for terms in Atlan from within Slack: Enter /search-term <search words> . Review the results (only visible to you). Send any result(s) you want to the channel. You've now shared terms with others without ever leaving Slack! ð Find queries â To search for queries in Atlan from within Slack: Enter /search-query <search words> . Review the results (only visible to you). Send any result(s) you want to the channel. You've now shared queries with others without ever leaving Slack! ð Provide context for any asset link â Other times you may have a link for an asset already. You can do more than just share the link â Atlan's bot will expand the context of that link directly in Slack. To provide detail for an asset in Slack: Paste an asset link into Slack, along with any other context in your message. The Atlan bot will expand the link with a card of contextual metadata. The channel can now understand the basics about the asset without ever leaving Slack! ð info ðª Did you know? Once your organization's Slack account is integrated with Atlan, your users will be able to receive Slack notifications on their requests . View workflow alerts in Slack â You can configure the Slack integration to receive alerts for workflow activities from Atlan. This can help you monitor your workflows directly in Slack. You can also choose to receive failure alerts only. To view workflow alerts in Slack: Open the Slack channel you configured to view workflow notifications. The Atlan bot will share workflow alerts â including details like run status, start time, run time, trigger type, and last three runs. (Optional) Click View on Atlan to open the workflow link in Atlan. You can now inspect crawled assets or troubleshoot in case of any failed workflows. info ðª Did you know? Atlan also supports workflow alerts for all custom packages . If you have set up any custom packages on your Atlan instance, you will be able to monitor your workflows directly in Slack. Take action on requests in Slack â You can configure the Slack integration to receive notifications for metadata update requests raised in Atlan, and approve or reject them directly from Slack. To take action on requests in Slack: From the left Apps menu in Slack, click the Atlan app. If request notifications are enabled , the app will display requests raised in Atlan â including asset name and basic information, and the type of metadata update requested. To take action on a request: Click Reject to reject the update. Click Accept to accept the update. The status of the request will be updated in Slack to inform your team that no further action is required. Tags: slack integration collaboration data faq faq-integrations Previous Slack permissions explained Next Bulk enrich metadata Ask questions about assets, without leaving Atlan Add existing Slack discussions to assets From within Slack From within Atlan Start direct chats from Atlan user profiles Enable alerts for glossary updates in Atlan Query Atlan from within Slack Find terms Find queries Provide context for any asset link View workflow alerts in Slack Take action on requests in Slack"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/faq/microsoft-teams-integration",
    "text": "Configure Atlan Integrations Collaboration Microsoft Teams FAQ What is included in the Microsoft Teams integration? On this page What is included in the Microsoft Teams integration? With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team. Once you've integrated Microsoft Teams with Atlan , you can do all of the following   -  all without leaving Atlan! Share data assets on Microsoft Teams Link important Microsoft Teams threads to Atlan assets Set up Microsoft Teams notifications for announcements , glossary updates , starred assets , and playbook runs When users share assets on Microsoft Teams   -  including asset links, glossaries, and saved queries   -  Atlan will provide an embedded preview and context for the asset directly in Microsoft Teams. Share assets without leaving Atlan â To share assets on Microsoft Teams, without leaving Atlan: From any asset in Atlan, in the asset sidebar, click the Share on Teams icon. In the Share on Teams dialog: For Channel , choose the Microsoft Teams channel where you want to post. For Message , enter the message you want to share for the asset on the channel. (Note that tagging another user from your Atlan workspace in the message is currently not supported.) (Optional) For Add as a resource , toggle the slider on to add the message as a resource to your asset. When you add the message as a resource, anyone viewing the asset in Atlan will be able to see previous discussions in Microsoft Teams about that asset. Click Share Â to post on Microsoft Teams. Your message is now on Microsoft Teams and linked to the asset for future reference! ð Add existing Microsoft Teams discussions to assets â Who can do this? Any non-guest user with edit access to an asset's metadata can add existing Microsoft Teams discussions to assets. If you've ever discussed a data asset in Microsoft Teams, it could be helpful to bring that context back to your asset. To add existing discussions from Microsoft Teams, within Atlan: From the asset, on the right of the screen, click theÂ Teams sidebar icon: If there are existing discussions on the asset, to the right of Teams conversations , click the Add link. If there are no existing discussions on the asset, click the + Add Teams thread button. In the Add Teams thread dialog, under the Link heading, paste the link to the Microsoft Teams thread. Under the Title heading, write a brief description of the thread. At the bottom of the dialog, click the Add button. Your existing discussion on Microsoft Teams is now linked to the asset for future reference! ð Enable alerts for glossary updates in Atlan â warning ð¤ Who can do this? You will need to be an admin user in Atlan to enable Microsoft Teams notifications for glossary updates. Once enabled, anyone in the selected channel will be able to view these alerts directly in Microsoft Teams. You can set up Microsoft Teams notifications for updates made to your glossaries in Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts sent to a Microsoft Teams channel of your choice , you'll be able to stay informed about the latest changes to your glossaries. Microsoft Teams notifications can only be set up at the glossary level. Even if configured from a specific term or category profile, the notification settings will be applicable to the entire glossary. To enable Microsoft Teams notifications for glossary updates, from Atlan: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of the glossary for which you want to set up change notifications. From the top right of the glossary profile, click the bell icon to set up notifications for glossary updates. In the notifications setup dialog, click Teams and then enter the following details: For Notification channel , click the dropdown to select a Microsoft Teams channel where you want to receive notifications from the channel(s) configured in your Microsoft Teams integration in Atlan .Â For Notify me about , you can either: Click All updates to receive notifications for all the changes listed in Custom updates made to your glossary. Click Custom updates to limit notifications to specific types of updates: Click + New term or category to receive an alert when a new term or category is added to the glossary. Click Name to receive an alert when the name of the glossary or that of a nested category or term is updated. Click Description to receive an alert when a description is added to, updated, or removed from the glossary or a nested category or term. Click Announcement to receive an alert when an announcement is added to, updated, or removed from the glossary or a nested category or term. Click Certificate to receive an alert when a certificate is added to, updated, or removed from the glossary or a nested category or term. Click Owners to receive an alert when an owner is assigned toÂ or removed from the glossary or a nested category or term. Click Readme to receive an alert when a README Â is added to, updated, or removed from the glossary or a nested category or term. Click Tags (for terms) to receive an alert when a tag is attached to or removed from a term within the glossary. Click Categories updated (for terms) to receive an alert when a term is assigned to a different category within the same glossary. (Notifications for moving terms across glossaries are currently not supported.) Click Remove term to receive an alert when a term is removed from the glossary. (Notifications for removal of categories are currently not supported.) Â Click Save to save your notification preferences. (Optional) To edit notification settings, click the bell icon. In the notifications setup dialog, you can change the channel or further customize your notifications. (Optional) To remove notifications, click the bell icon. In the notifications setup dialog, click the Enabled (by username) dropdown. Click Remove notification to reset your notification settings. You will now receive Microsoft Teams notifications for changes made to your glossary! ð If the Microsoft Teams channel you selected for sending notifications is removed from your Microsoft Teams integration in Atlan, glossary alerts will continue to be sent to that channel. In that case, you can either remove the notifications or select a different channel. Did you know? You can also set up Microsoft Teams notifications for metadata updates on all your starred assets in Atlan. Expand any asset link in Microsoft Teams â If you share an asset link in Microsoft Teams, Atlan's bot will expand that link directly in the Microsoft Teams channel and provide an embedded preview of the asset. To provide detail for an asset in Microsoft Teams: Paste an asset link into Microsoft Teams, along with any other context in your message. The Atlan bot will expand the link with a card of contextual metadata. (Optional) Click View in Atlan to open the asset link in Atlan. The channel can now understand basic information about the asset without leaving Microsoft Teams! ð View workflow alerts in Microsoft Teams â You can configure the Microsoft Teams integration to receive alerts for workflow activities from Atlan. This can help you monitor your workflows directly in Microsoft Teams. You can also choose to receive failure alerts only. To view workflow alerts in Microsoft Teams: Open the Microsoft Teams channel you configured to view workflow notifications. The Atlan bot will share workflow alerts   -  including details like run status, start time, run time, trigger type, and last three runs. (Optional) Click View in Atlan to open the workflow link in Atlan. You can now inspect crawled assets or troubleshoot in case of any failed workflows. Did you know? Atlan also supports workflow alerts for all custom packages . If you have set up any custom packages on your Atlan instance, you will be able to monitor your workflows directly in Microsoft Teams. Tags: data integration faq faq-integrations Previous Troubleshooting Microsoft Teams Next Slack Share assets without leaving Atlan Add existing Microsoft Teams discussions to assets Enable alerts for glossary updates in Atlan Expand any asset link in Microsoft Teams View workflow alerts in Microsoft Teams"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo",
    "text": "Connect data Data Quality & Observability Monte Carlo On this page Monte Carlo Overview: Catalog Monte Carlo monitors, incidents, and rules in Atlan. Gain visibility into data quality metrics, alerts, and governance for your data assets. Get started â Follow these steps to connect and catalog Monte Carlo assets in Atlan: Set up the connector Crawl Monte Carlo assets References â What does Atlan crawl from Monte Carlo : Learn about the Monte Carlo assets and metadata that Atlan discovers and catalogs. Preflight checks for Monte Carlo : Verify prerequisites before setting up the Monte Carlo connector. Tags: monte carlo connector observability data quality connectivity Next Set up Monte Carlo Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo",
    "text": "Connect data Data Quality & Observability Monte Carlo Crawl Monte Carlo Assets Crawl Monte Carlo On this page Crawl Monte Carlo Once you have configured the Monte Carlo permissions , you can establish a connection between Atlan and Monte Carlo. To crawl metadata from Monte Carlo, review the order of operations and then complete the following steps. Select the source â To select Monte Carlo as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Monte Carlo . In the right panel, click Setup Workflow . Provide your credentials â To enter your Monte Carlo credentials: For Authentication , API Key Authentication is the default selection. For API Key ID , enter the API key ID you copied . For API Secret , enter the API secret you copied . Click the Test Authentication button to confirm connectivity to Monte Carlo. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Monte Carlo connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Monte Carlo crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click the Include filter . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click the Exclude filter . (This will default to no assets, if none specified.) ToÂ enable crawling assets with specific incident statuses , click Include Incident Statuses and select the relevant option(s). To include unresolved incidents by default, we recommend selecting the No Status and Acknowledged filters. (This will default to all incident statuses, if none are specified.) For Incidents and Alerts time range , specify a date range for which you want to crawl alerts and incidents from Monte Carlo. The default date range is set to the last 30 days, you can either keep the default selection or change to the last 14 or 45 days. (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the enrichment: To map Monte Carlo metadata enrichment to assets from specific connections only, for Include Connections , specify the connections in Atlan, or leave it blank to include all connections. If you have specified any connections, Atlan will map monitors, alerts, and incidents only to the assets included in those connections. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Monte Carlo crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Monte Carlo Next What does Atlan crawl from Monte Carlo? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/references/what-does-atlan-crawl-from-monte-carlo",
    "text": "Connect data Data Quality & Observability Monte Carlo References What does Atlan crawl from Monte Carlo? On this page What does Atlan crawl from Monte Carlo? Private Preview Atlan supports both automated and custom monitors as native assets for search and discovery. Atlan also supports crawling incidents for all types of monitors . Once you've crawled Monte Carlo , you can use connector-specific filters Â to search for your Monte Carlo assets as well as assets from other supported sources to which Monte Carlo monitors have been applied   -  for example, Snowflake tables. The following filters are currently supported for Monte Carlo assets: Monitor type   -  filter monitors by type of monitor Monitor status   -  filter monitors by monitor status Incident count   -  filter monitors by the total count of incidents Last synced in Atlan   -  filter monitors by timestamp for last updated in Atlan The following Monte Carlo filters are currently available for supported SQL assets: Monitor status   -  filter SQL assets associated with any monitors by monitor status Monitor type   -  filter SQL assets associated with any monitors by type of monitor Alert priority   -  filter SQL assets by priority level of alerts, ranging from 1 to 4 Alert type   -  filter SQL assets by specific types of alerts Incident severity   -  filter SQL assets by severity level of incidents, ranging from 1 to 4 Alert subtype   -  filter SQL assets by alert subtypes Alert status   -  filter SQL assets by specific alert statuses Alert owner   -  filter SQL assets by alert owners Last synced in Atlan   -  filter SQL assets by timestamp for when any associated monitors and incidents were updated in Atlan Atlan crawls and maps the following assets and properties from Monte Carlo. Monitors â Atlan maps automated and custom monitors from Monte Carlo to its MCMonitor asset type. Source property Atlan property name name Audience mcLabels uuid mcMonitorId monitorStatus mcMonitorStatus monitorType mcMonitorType warehouseName mcMonitorWarehouse scheduleType mcMonitorScheduleType namespace mcMonitorNamespace ruleType mcMonitorRuleType ruleSql mcMonitorRuleCustomSql scheduleConfig mcMonitorRuleScheduleConfig ruleComparisons mcMonitorRuleComparisons monitorUrl sourceUrl breachRate mcMonitorBreachRate scheduleConfig mcMonitorRuleScheduleConfigHumanized alertCondition mcMonitorAlertCondition priority mcMonitorPriority isOotbMonitor mcMonitorIsOotb Alerts and incidents â Atlan maps alerts and incidents from Monte Carlo to its MCIncident asset type. Alerts inherit priority levels from custom monitors. If an alert is confirmed as an issue or requires resolution, Monte Carlo enables you to mark it as an incident and apply a severity level. Atlan will display the status of your assets at source. Refer to Monte Carlo documentation to learn more. Source property Atlan property name name tableLinked mcAssetReferences uuid mcIncidentId incidentType mcIncidentType incidentSubTypes mcIncidentSubTypes severity mcIncidentSeverity feedback mcIncidentState warehouse.name mcIncidentWarehouse incidentOwner sourceOwners incidentUrl sourceUrl priority mcIncidentPriority Tags: connectors crawl Previous Crawl Monte Carlo Next Preflight checks for Monte Carlo Monitors Alerts and incidents"
  },
  {
    "url": "https://docs.atlan.com/tags/api",
    "text": "72 docs tagged with \"api\" View all tags Add contract impact analysis in GitHub Add contract impact analysis in GitHub <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Add impact analysis in GitHub Learn about add impact analysis in github. Add impact analysis in GitLab Learn about add impact analysis in gitlab. Administration and Configuration Complete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization. AI and Automation Features Guide to Atlan's AI capabilities and automation features for enhanced data governance and productivity. API authentication Learn about api authentication. Atlan's open API Learn about atlan's open api. Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case. Connectors and capabilities Learn about connectors and capabilities. Crawl Fivetran Learn about crawl fivetran. extract lineage and usage from Databricks Once you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal. Generate HAR files and console logs Atlan is built on [REST APIs](https://apidocs.atlan.com/), so you can see the requests being sent by the UI to the API gateway through your browser's developer console. How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Integrate Anomalo Once you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo. Integrate Apache Spark/OpenLineage Atlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Slack To integrate Slack and Atlan, follow these steps. Interpret usage metrics Atlan currently supports usage and popularity metrics for the following connectors: Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Preflight checks for Amazon QuickSight The [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission. Preflight checks for Anomalo This check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid. Preflight checks for Databricks Before [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co. Preflight checks for Domo Atlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo. Preflight checks for Fivetran Learn about preflight checks for fivetran. Preflight checks for Google BigQuery Each request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication service-accounts). Preflight checks for Looker First, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management). Preflight checks for MicroStrategy First, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html /Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions. Preflight checks for Monte Carlo Before [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c. Preflight checks for Redash Before [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti. Preflight checks for Sigma First, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission. Preflight checks for Sisense Atlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6 /folders) to check if it's responding with a response status code 200. Preflight checks for Tableau The [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm server_info) REST API is used to fetch the `restApiVersion` value. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Security The Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring. Set up an AWS private network link to Databricks For all details, see [Databricks documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html). Set up Anomalo Atlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata. Set up Confluent Kafka Atlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata. Set up Confluent Schema Registry :::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself. Set up Fivetran Learn about set up fivetran. Set up IBM Cognos Analytics :::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself. Set up Looker :::warning Who can do this? You will probably need your Looker administrator to run these commands - you may not have access yourself. Set up Microsoft Azure Data Factory Atlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata. Set up Monte Carlo :::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups). Set up Qlik Sense Cloud :::warning Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps - you may not have access yourself. Set up Qlik Sense Enterprise on Windows :::warning Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps - you may not have access yourself. Set up Redash :::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself. Set up Sigma :::warning Who can do this? You will probably need your Sigma administrator to complete these steps - you may not have access yourself. Set up Sisense Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Set up Soda :::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -. Set up Tableau :::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself. Software development kits (SDKs) Learn about software development kits (sdks). Support and Technical Help Complete guide to getting support, understanding API limits, and accessing technical assistance for Atlan. Supported sources Learn about supported sources. Tags and Metadata Management Complete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization. Troubleshooting Databricks connectivity Learn about troubleshooting databricks connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Matillion connectivity Learn about troubleshooting matillion connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting Sigma connectivity Learn about troubleshooting sigma connectivity. Troubleshooting Trino connectivity Learn about troubleshooting trino connectivity. View query logs You can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization. What does Atlan crawl from Databricks? Atlan crawls and maps the following assets and properties from Databricks. What does Atlan crawl from Fivetran? Learn about what does atlan crawl from fivetran?. What does Atlan crawl from Microsoft SQL Server? Atlan crawls and maps the following assets and properties from Microsoft SQL Server. What does Atlan crawl from Salesforce? Atlan only performs GET requests on these five endpoints:. What lineage does Atlan extract from Matillion? Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools. What lineage does Atlan extract from Microsoft Azure Data Factory? Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01). What lineage does Atlan extract from Microsoft Azure Synapse Analytics? Learn about what lineage does atlan extract from microsoft azure synapse analytics?. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage",
    "text": "Use data Lineage On this page Lineage Overview: Data lineage in Atlan helps you understand how data moves across your data landscape. Use lineage to perform root cause analysis, impact analysis, and automate metadata propagation. Get started â How to view lineage Guides â Lineage Visualization â How to download and export lineage : Export lineage information for external use. Concepts â What is lineage : Understand the fundamentals of data lineage. What is column-level lineage : Learn about column-level lineage tracking. What are processes : Understand how processes are represented in lineage. What are partial assets : Learn about partial asset representation in lineage. Tags: lineage data-flow dependencies capabilities Next How to view lineage Get started Guides Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/view-lineage",
    "text": "Use data Lineage Get Started How to view lineage On this page view lineage The lineage graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps. View the lineage graph â To view the lineage graph: From the left menu of any screen in Atlan, click Assets . (Optional) In the Filters menu on the left, expand the Properties menu and then click Has lineage to filter for assets with data lineage. Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph. On the lineage graph, the home icon indicates the base asset: Click the + button to the left of the base asset to view upstream assets. Click the + button to the right of the base asset to view downstream assets. (Optional) Hover over any asset to view the metadata popover for more context on the selected asset   -  including asset type, database and schema names, owners , usage and popularity metrics , announcements , and Monte Carlo and Soda data quality details for supported assets. From the popover, you can also: Click the View impact button to view impacted assets . Click the Download impact button to download the lineage report for the asset. Click the outward-facing arrow icon to open lineage in a new tab. (Optional) Hover over the + button to the right of any asset and then click the Expand all Â button to view assets further upstream or downstream horizontally. (Optional) Click any circular process button to view more details about the lineage process in the sidebar. (Optional) Click Show all to view more upstream or downstream assets vertically. If there are more than 100 assets, a popup will appear asking you to scroll to the bottom of the list and click Show all again to view all dependencies. (Optional) For any asset on the lineage graph, click the view columns menu to expand the columns view: The default view shows 10 columns. Click Show more columns to view the full list of columns. Use the search bar to search for specific columns. View a total count of columns that have lineage. Next to the search bar, click the sort icon to sort columns in an alphabetical or reverse alphabetical order, ascending or descending order, or by last updated in Atlan. Hover over any column name and then click the curvedÂ downward arrow to view impacted assets or click the downward arrow to download the lineage report directly from the column. Select a column, and then from the column sidebar, click the Lineage tab. From the Lineage tab: Click View impact to view and download impacted assets directly from the sidebar. Click the eye icon to customize your view of column-level lineage in the sidebar: For Depth , select the level of depth up to Max Depth for column-level lineage. Turn on Show process nodes Â to view lineage processes in the sidebar. For Show hierarchy , keep the default view of child assets or toggle it off. For any asset on the lineage graph, from the sidebar,Â click the Columns tab. From the Columns tab: You can view a list of columns for the selected asset. Click Show columns with lineage to only view columns with lineage: For any column with lineage in the sidebar, click the play button to locate that asset on the lineage graph. (Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow. (Optional) From the top right of the lineage graph: Click the Find in canvas search bar to search for any specific assets on the lineage graph. Click the downward arrow to view more options: Click View impact to view impacted assets . Click Download impact to download the lineage report for the asset. Click Download lineage as image to download an image of the lineage graph. Click the eye icon to set preferences for the lineage graph: For Additional metadata , show or hide the following context for your assets   -  schema name, database name, asset type, announcements , data quality, or popularity metrics . For Order columns and fields , order assets on the lineage graph alphabetically, in an ascending or descending order, or by last updated in Atlan. For Line arrows , show or hide the arrows that indicate data flows on the lineage graph. For Pin metadata sidebar , toggle on the slider to pin the sidebar open on the lineage graph. Click the question mark icon to share feedback or view documentation. (Optional) From the bottom right of the lineage graph: Toggle the Visibility slider to customize the visibility of assets not included in your selected lineage path. Click the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base asset. Click the minimap icon to view an abridged version of the lineage graph. Click the information icon to view the map legend. Click the fullscreen icon to expand the lineage view to fullscreen mode. Click the minus or plus icons to zoom out or zoom in on the lineage graph, respectively. Did you know? IfÂ the products module is enabled in your Atlan workspace, you can also create data products directly from the lineage graph . Tags: lineage data-lineage impact-analysis Previous Lineage Next Download and export lineage View the lineage graph"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/download-and-export-lineage",
    "text": "Use data Lineage Manage lineage Download and export lineage On this page Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to view , download , and export your impacted assets and share it with others in your organization. The impact report includes upstream and downstream assets, along with the following attributes: Status -  export status and total count of assets exported, only for export option Name -  asset name and link Type -  asset type Business Name (Alias) -  business-oriented alias of assets, if any Connector -  name of supported source Database Â   -  name of the database, if applicable Schema Â   -  name of the schema, if applicable Table -  name of the parent table or view, only for column assets Lineage depth -  starts at 1 and increases with each lineage level Description - asset description , if any Owner Users and Owner Groups - asset owners , if any Certification Status and Certification Message - certification status of asset , if any Tags and Propagated Tags - tags directly attached or propagated to an asset, if any Terms -  glossary terms for linked assets Announcement Type , Announcement Title , and Announcement Message - announcements on assets , if any Source URL -  only supported for dbt Cloud and Core, Matillion, Microsoft Power BI, Mode, Monte Carlo, Salesforce, Sigma, Sisense, Soda, and Tableau assets Usage - number of queries by number of users , only for supported sources Popularity - popularity indicator , only for supported sources Last refresh on source -  metadata last altered at source Workbook , Data source , and Source owner -  only applicable to Tableau assets Qualified Name -  fully qualified name of the asset Source Asset GUID -  globally unique identifier of the source asset GUID -  globally unique identifier of the exported asset Immediate Upstream and Immediate Downstream -  one level directly upstream and downstream, respectively, of the dependent asset. View impacted assets â The impacted assets view includes asset metadata, BI source URLs, and Atlan URLs for both upstream and downstream assets. To view impacted assets: In the right menu from any screen, click Assets . Click an asset to navigate to its asset profile. In the asset profile, click Lineage . In the top right of the lineage graph, click the downward arrow and then click View impact to view impacted assets. Note that if the total number of impacted assets exceeds 200, you will only be able to download lineage. In the impacted assets report: To view downstream assets for impact analysis , click Downstream . To view upstream sources for root cause analysis , click Upstream . (Optional) To view the source URL for BI assets, click the source URL option   -  for example, View in Looker . (Optional) To navigate to the impacted asset in Atlan, click the Atlan URL option. (Optional) Click Impact Report to download the impacted assets report in a CSV file or export to Google Sheets or Microsoft Excel . Click Close to return to the lineage graph. Download lineage â You can download lineage for your data assets in a CSV file or as an image. Download lineage in a report â You can download a lineage report for your assets in a CSV format for further analysis. When you share the CSV file with team members, they will also be provided with the BI source URLs and asset links in Atlan. As long as they have access to the asset in the BI tool and Atlan, they'll be able to open both directly from the CSV file. To download the lineage report: In the right menu from any screen, click Assets . Click on an asset to navigate to its asset profile. In the asset profile, click Lineage . In the top right of the lineage graph, click the downward arrow and then click CSV File to download the lineage report for the asset.Â Once the report has been fetched, click Download report to download it as a CSV file. You will now be able to view the upstream and downstream lineage of your assets in a CSV file! ð danger Clicking the CSV File button may not instantly result in a link to the CSV file. When you click the button, it triggers a workflow to scan assets with lineage, retrieve a list of those assets via the API, and then generate a CSV file with the lineage report. For high-volume impact reports, the workflow can take a few minutes to run before the link to the CSV file is ready. The progress bar under the button displays the status of the workflow and will turn green once all the tasks in the workflow have been executed. Download lineage as an image â Atlan offers you the option to download lineage as a high-resolution PNG file, providing you with the visual clarity you need for your presentations. To download lineage as an image: In the right menu from any screen, click Assets . Click on an asset to navigate to its asset profile. In the asset profile, click Lineage . In the top right of the lineage graph, click the downward arrow and then click As an image Â to download an image of the lineage graph. You will now be able to view a high-resolution image of your asset lineage! ð Export lineage to spreadsheets â Atlan enables you to export lineage to spreadsheets. This can help you assess the downstream impact of any changes made to an upstream asset for impact analysis . Atlan currently supports exporting impacted assets to: Google Sheets Microsoft Excel online Once your Atlan admin has integrated a supported tool, you will be able to export impacted assets to spreadsheets. Your existing permissions and access policies in Atlan will determine whether you can export the impact report, but at a minimum you'll require read permission on the assets you want to export. Did you know? Atlan currently limits the total number of assets you can export to 20,000 rows each for both upstream and downstream assets. Reach out to your customer success manager if you'd like to increase the limit for your organization. Enable lineage export â The export to Google Sheets and Microsoft Excel icons and buttons will only be visible if your Atlan admin has integrated a supported tool. If you cannot see the export icon or button, reach out to your Atlan admin to integrate Google Sheets or Microsoft Excel. To integrate a supported tool, your Atlan admin must follow the steps in Enable asset export . Export impacted assets â Who can do this? Once an Atlan admin has integrated a supported tool, any admin, member, or guest user in Atlan with read permission on assets can export the impact report to spreadsheets. danger Atlan recommends that you avoid exporting assets during workflow runs. Exporting assets while you have workflows running in the background may lead to duplicate assets on the spreadsheet. Atlan allows you to export your impacted assets to spreadsheets and view asset metadata in bulk. At an individual user level, only one export is allowed for each supported tool at a time, the rest will be auto-queued for execution. There are currently no limitations at a tenant level. You can export lineage from the following areas: Overview and Lineage tabs in sidebar for assets with lineage Lineage graph for asset-level lineage Impacted assets view in lineage graph Column assets with lineage from lineage graph To export lineage: From the left menu of any screen in Atlan, click Assets . To export lineage, in the Assets page, you can: Select an asset, and then from the asset sidebar: In the Overview tab, click the 3-dot icon and then click Impact report . Click the Lineage tab and then click the export icon. Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph: From the top right of the lineage graph, click the downward arrow. For any asset with column-level lineage on the lineage graph, click the view columns menu and hover over a column name to view export options. To export lineage to a spreadsheet tool: Click Google Sheets to export impacted assets to a Google Sheets spreadsheet. Click Microsoft Excel to export impacted assets to a Microsoft Excel workbook. A sign-in dialog will appear and you will be redirected to sign in with your Google or Microsoft account. From the corresponding screen, click Allow to connect to Google Sheets or Microsoft Excel. If you're already signed in, skip this step. To track the progress of the export, navigate to the spreadsheet. The Queued status will change to Success once impacted assets have been exported. On the spreadsheet, you can view metadata for your impacted assets. Atlan exports the impact report to two individual sheets within the main spreadsheet, one each for Upstream and Downstream . For any metadata attribute not applicable to a particular asset, the column will display an empty value. (Optional) To view your asset export history, navigate to the Assets page. Next to the search bar on the Assets page, click the 3-dot icon and then click Export . From the Export dialog, expand the History dropdown to view your last 10 exports. Note that only you can currently view your own export history. That's it, you've successfully exported your impacted assets from Atlan! ð danger Atlan currently does not support updating asset metadata in the spreadsheet and syncing impacted assets back to Atlan. Tags: lineage data-lineage impact-analysis downstream-impact dependencies upstream-dependencies data-sources Previous How to view lineage Next Generate lineage between assets App View impacted assets Download lineage Export lineage to spreadsheets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-is-column-level-lineage",
    "text": "Use data Lineage Concepts What is column-level lineage? Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. For example, instead of stating that the upstream table STG CUSTOMERS is dependent on the downstream table RAW CUSTOMERS , you can point to the specific column that has an impact on the downstream asset. Knowing the impact on the downstream asset is essential for impact analysis. However, in order to make more granular decisions, users must understand how the impact occurs. Column-level lineage reveals exactly what is impacted and how. To learn more about data lineage, read this guide . Tags: lineage data-lineage impact-analysis downstream-impact dependencies upstream-dependencies data-sources Previous Generate lineage between assets App Next What is lineage?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-are-partial-assets",
    "text": "Use data Lineage Concepts What are partial assets? On this page What are partial assets? If a supported data source has not been crawled, mined, or cataloged via APIs but is present in lineage, Atlan will create partial assets to provide a complete representation of data lineage. For example, if your orchestration pipelines reference any assets as a source or target that are yet to be cataloged in Atlan, these will be represented as partial assets on the lineage graph. Key characteristics of partial assets: Displayed with a dotted border on the lineage graph. Available in the visual representation of lineage in Atlan - lineage or pipeline view. You can search for and view a partial asset profile and lineage. You can not enrich metadata for partial assets. You can view metadata for partial assets in the sidebar. You can view and download the impact report for partial assets. Column-level lineage for partial assets is supported. Supported sources â Atlan currently supports the creation of partial assets for the following tools: Airflow/OpenLineage Amazon MWAA/OpenLineage Astronomer/OpenLineage Google Cloud Composer/OpenLineage Apache Spark/OpenLineage Fivetran View partial assets â There are multiple ways to view partial assets : Like any asset in Atlan â From the left menu of any screen, click Assets . Search for asset by name, or use Atlan discovery filter to find partial assets. Click an asset to open its profile. From pipeline lineage or lineage graph â From the top left of an asset profile, click the Lineage tab to view lineage or the Pipeline tab for orchestration assets. A partial asset is represented with a dotted border on the lineage graph. Select a partial asset to view more details. (Optional) Open the asset sidebar to view metadata, lineage, and properties of the partial asset, and navigate to the profile. Hover over the partial asset to view a metadata popover: You can view the following details   -  asset name and type, connection, database, and schema names, and owner, if assigned. Click the View impact button to view impacted assets . Click the Download impact button to download the lineage report for the asset. Click the view columns menu to view column assets. Hover over any column name and then click the curvedÂ downward arrow to view impacted assets or click the straight downward arrow to download the impact report directly from the column. Convert partial assets to catalogued asstes â To convert a partial asset into a fully published asset, complete the following steps in any order: If the sourcer is supported, run the crawler for the supported data source of the partial asset. Run the orchestration pipeline that initially cataloged the partial asset. Tags: atlan documentation Previous What is lineage? Next What are processes? Supported sources View partial assets Convert partial assets to catalogued asstes"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/concepts/what-are-processes",
    "text": "Use data Lineage Concepts What are processes? On this page What are processes? Processes represent the movement and transformation of assets in Atlan. On the lineage graph , processes are the circular buttons with icons denoting the sources. Processes help you understand the transformations taking place from one asset to another and whether to take action. By default, process assets are only visible on the lineage graph and sidebar. If enabled by an admin user, you will be able to search , filter , and discover process assets and track metrics in the reporting center . View processes â From lineage graph and sidebar â To view processes on the lineage graph and sidebar: From the left menu of any screen, click Assets . Click an asset to open its asset profile. From the top left of the asset profile, click the Lineage tab. Click any circular process button to view more details in the sidebar. (Optional) For Overview in the sidebar, under Query , click the full screen icon to expand the SQL code snippet. (Optional) In the right sidebar, click Lineage to view the input and output for the process. From asset preview â Who can do this? You will need your Atlan admin to enable discovery of process assets . To view process assets in asset preview: From the left menu of any screen, click Assets . Under the search bar on the Assets page, slide through the asset type tabs or click the three dots icon to select the Process tab. Click any process asset to view details in the sidebar. (Optional) For Overview in the sidebar, under Query , click the full screen icon to expand the SQL code snippet. (Optional) In the right sidebar, click Lineage to view the input and output for the process. (Optional) From the left filters menu , click Process and then click Query to filter process assets by the SQL query. From reporting center â Who can do this? You must be an admin user in Atlan to view the reporting center . To track process assets from the reporting center : From the left menu in Atlan, click Reporting and then click Assets . From the Assets dashboard, for the All Asset Types filter, select Process . (Optional) To further refine your search , click More filters . (Optional) Once the process assets are displayed, click any data point to view processes on the Assets page. Tags: lineage data-lineage impact-analysis Previous What are partial assets? Next How can Atlan generate upstream lineage from the data warehouse layer? View processes"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/references/how-can-atlan-generate-upstream-lineage-from-the-data-warehouse-layer",
    "text": "Use data Lineage References How can Atlan generate upstream lineage from the data warehouse layer? How can Atlan generate upstream lineage from the data warehouse layer? Atlan has 2 API endpoints to generate lineage: lineage between assets column-level lineage Atlan generates upstream lineage in the following ways: Custom jobs: Constraint   -  these jobs do not have an interface for Atlan to extract lineage. For example, custom Python jobs that move data from RDBMS to S3 to Snowflake. Recommended path   -  use the lineage API to push source-to-target mappings to Atlan. Data integration tools:Â Constraint   -  these tools have historically not had metadata APIs for Atlan to extract lineage. Recommended path   -  Atlan has released support for data integration tools that have released metadata APIs, such as Fivetran . Orchestration tools:Â Constraint   -  you need to run DAGs or jobs to catalog assets and lineage in Atlan, unlike other workflows that catalog assets after a workflow run. Recommended path   -  use the lineage API to either create lineage directly or using OpenLineage. Tags: lineage data-lineage impact-analysis upstream-dependencies data-sources api rest-api graphql Previous What are processes? Next Lineage Generator (no transformations)"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/airflow-lineage-integration",
    "text": "Use data Lineage FAQ Can Atlan integrate with Airflow to generate lineage? Can Atlan integrate with Airflow to generate lineage? Atlan currently supports native integration with Apache Airflow/OpenLineage . Atlan also supports integrating with the following Apache Airflow distributions: Amazon MWAA Astronomer Google Cloud Composer Tags: connectors integration faq faq-lineage Previous Source asset type Next Can Atlan read a dump of SQL statements to create lineage?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/troubleshooting/troubleshooting-lineage",
    "text": "Use data Lineage Troubleshooting Troubleshooting lineage On this page Troubleshooting lineage So you've crawled your source, and mined the queries, but lineage is missing. Why? Where to look first? â Views â Check the SQL attribute of the view data asset   -  this must have SQL in it for view lineage to appear. The crawler workflows populate the SQL attribute. If it's empty on the view asset, the crawler is the suspect. Tables â The miner workflows populate table lineage. If it's missing, the miner is the suspect. Check the SQL picked up by the miner (for example, in S3 ). If the miner picks up the necessary SQL but lineage is not produced, check if any of the assets involved are missing. Data stores to BI assets â For Atlan to link these assets, the upstream assets (data stores) must first exist. If they are only created after the downstream assets, lineage will stay unlinked. Or if some of the assets are missing, lineage may have gaps preventing linkage. Show more menu â Lineage may appear missing if the linked asset is hidden in the Show more menu. Although it will still appear in the list of upstream or downstream assets in the Lineage tab in the side profile, it will not appear visually in the lineage graph. Click Show more columns to see the rest of the assets and their lineage. Miner logic â When setting up the miner for the first time, you will need to provide a start date   -  ranging from the last two days up to past two weeks of query history. If an asset has not been queried during the selected time period, data lineage will be unavailable. For subsequent runs, the miner will fetch query history based on the following logic: START_TIME â¤ CURRENT_DATE - INTERVAL '1 DAY' For example, the miner logic for January 23 will be: Jan 22 5 p.m. â¤ Jan 23 00:00 - 1 day Jan 22 5 p.m. â¤ Jan 22 00:00 The miner will not fetch the data for the previous day (January 22) on the current day (January 23). Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Causes of missing lineage â There are several reasons why lineage may be missing: Workflow ordering â The order of operations you run in Atlan is important. To have lineage across tools, you need to: Crawl data stores first. Mine query logs (and dbt) second. Crawl BI tools last. If you've used a different order, the upstream assets (data stores) may not yet exist when you load the BI metadata. Then you can have lineage within the BI metadata, but not between the BI metadata and the data sources. If that's the case for you, don't worry. Re-run your existing workflows in the order above and Atlan should resolve it. Crawling filters â Another reason lineage may have gaps is that linking assets do not exist, even after re-running the crawler. When crawling a source, you can specify filters on which metadata to include and exclude. If you've excluded metadata needed to link assets into lineage, then end-to-end lineage will have gaps. Check that you have not excluded any of the asset(s) you're expecting to be in lineage. (And remember that using an include filter means that not all metadata is being crawled   -  some is being excluded.) If in doubt, try running your workflows without include or exclude filters. Source permissions â Atlan is not the only place where you can filter metadata. Atlan accesses your sources through credentials you provide. Those credentials have assigned permissions controlling what (meta)data they can access in the source. If those permissions prevent access to some (meta)data, then Atlan cannot crawl that metadata. So if ordering and filter don't fix the problem, check your source permissions. Are they providing access to all the data assets you need for lineage? Different connections, same source â We currently do not resolve lineage across different connections for the same source. You need to crawl (and mine) all assets from a given source through the same connection to generate lineage. danger This one is the most subtle of the causes. The assets may even appear to be in the environment in this case. Check the qualifiedName of the asset matches exactly what lineage expects. Temporary tables â If your data processing tool uses temporary tables, Atlan can still support generating lineage accurately. For example: Table A â temporary table â table B Lineage will be represented as table A â table B in Atlan. In this case, Atlan assumes that tables A and B are present in Atlan. However, if table A is missing, then Atlan will not be able to generate lineage. Cross-connection links â If the combination of database, schema, and table name for an asset is the same across different connections, it is possible that Atlan may create unexpected links for these assets. For example, if your Production environment has the same set of databases, schemas, and tables as your Staging environment and both these source systems are crawled, Atlan may connect BI reports to either of these assets due to the name-match algorithm. Indirect data flow â Atlan currently only processes and visualizes direct data flow on the lineage graph. However, assets can be related through other means such as control flow or conditional statements, in which case there is no data movement between them. Atlan currently neither processes nor visualizes such relationships on the lineage graph. For example, when processing the following query: insert into tgt_tab ( col_x , col_y ) select col_x , case when col_y > 100 then 'High' else 'Low' end from src_tab Atlan will display the following links in the lineage graph: src_tab â tgt_tab (table-level data flow) src_tab.col_x â tgt_tab.col_x (column-level data flow) Note that there is no lineage generated for col_y. This is because the data present in src_tab.col_y does not actually flow or get transferred to tgt_tab.col_y. Lineage persistence â Lineage in Atlan is reflective of the last valid set of transformations performed for a particular target table in the external ( source ) system. Atlan retains these transformations as lineage and does not auto-delete or sunset the process entities ( links ). The exception to this rule is when new information pertaining to the same target table is inferred in the latest job run. In this case, Atlan will replace the previous links with new ones. Tags: data crawl Previous Why is the SQL query visible only in Snowflake process and not in dbt process nodes? Next Why is my Databricks lineage API not working? Where to look first? Causes of missing lineage Lineage persistence"
  },
  {
    "url": "https://docs.atlan.com/faq/tags-and-metadata-management",
    "text": "Configure Atlan Frequently Asked Questions Tags and Metadata Management On this page Tags and Metadata Management Complete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization. What are some examples of tags? â To learn more about examples of tags, see What are tags? Why does tag propagation take time to apply? â When tag propagation is enabled , it automatically triggers a background task in the metastore. This background task is created to reduce the API load and response time. After each background task has been created, the API simply returns a 200 OK response code. Tag propagation is completed once the background tasks have been executed - including tag attachment or removal by propagation. The lifecycle of a background task: When a background task is created for tag propagation, its status changes to PENDING . Multiple tasks may be in the same PENDING state, depending on the number of assets to be propagated. As a task gets picked up, its status changes to IN PROGRESS . As each task is executed, the tag is propagated to an asset. Once the task is completed, its status changes to COMPLETE . As the next task gets picked up, this cycle repeats until tag propagation is completed for all the assets. How are tags propagated for new assets? â Tag propagation is disabled by default in Atlan. If you have enabled tag propagation , tags are automatically propagated to a child or downstream asset created after running a workflow. This means that when a new asset is registered, tag propagation is automatically triggered in the metastore and runs as a background task. For example, if a workflow adds an additional column to a table, a new background task for adding tags is created in the metastore. This new task is executed when all the previous tasks have been completed in the queue. The speed with which these tasks are completed depends on the number of pending tasks and the volume of tags to be added or removed. The same process also applies to tag deletion and updating tags through playbooks . Can I delete a tag? â Tags can be deleted, but this requires careful consideration due to their impact on data governance. For detailed instructions on tag deletion, see the tag deletion guide . Is reverse tag sync supported for column-level tags? â Reverse tag sync capabilities depend on your data source and configuration. For specific data source support and configuration options, consult the connector documentation or contact Atlan support. Tags: data api faq-metadata Previous Security and Compliance Next User Management and Access Control"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/mine-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift Crawl Redshift Assets Mine Amazon Redshift On this page Mine Amazon Redshift Once you have crawled assets from Amazon Redshift , you can mine its query history to construct lineage and retrieve usage and popularity metrics . To mine lineage from Amazon Redshift, review the order of operations and then complete the following steps. Select the miner â To select the Amazon Redshift miner: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the filters along the top, click Miner . From the list of packages, select Redshift Miner and click on Setup Workflow . Configure the miner â To configure the Amazon Redshift miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner extraction method , select Query History . For Start time , choose the earliest date from which to mine query history. danger Amazon Redshift only stores query history for 2-5 days . If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to further configure the miner: For Cross Connection , click Yes to extract lineage across all available data source connections or click No to only extract lineage from the selected Amazon Redshift connection. For Control Config , if Atlan support has provided you with a custom control configuration, select Custom and enter the configuration into theÂ Custom Config box. You can also: Enter {âignore-all-caseâ: true} to enable crawling assets with case-sensitive identifiers. (Optional) For Enable Popularity , click Yes to retrieve usage and popularity metrics for your Amazon Redshift assets from query history: For Excluded Users , type the names of users to be excluded while calculating usage metrics for Amazon Redshift assets. Press enter after each name to add more names. Run the miner â To run the Amazon Redshift miner, after completing the steps above: To check for any permissions or other configuration issues before running the miner, click Preflight checks . You can either: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the miner has completed running, you will see lineage for Amazon Redshift assets that were created in Amazon Redshift between the start time and when the miner ran! ð Tags: connectors data crawl Previous Crawl Amazon Redshift Next What does Atlan crawl from Amazon Redshift? Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/crawl-dbt",
    "text": "Connect data ETL Tools dbt Crawl dbt Assets Crawl dbt On this page Crawl dbt Once you have configured dbt Cloud service token or uploaded your dbt Core project files to cloud storage , you can crawl dbt metadata into Atlan. To enrich metadata in Atlan from dbt, review the order of operations and then complete the following steps. Select the source â To select dbt as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select dbt Assets and then click Setup Workflow . Provide your credentials â dbt core â To enter your dbt Core credentials: For Extraction method , click Object Storage . Enter the details for the object storage location of your project files. Click the Test Authentication button to confirm connectivity to object storage using these details. Once authentication is successful, navigate to the bottom of the screen and click Next . dbt cloud â To enter your dbt Cloud credentials: For Extraction method , clickÂ Cloud . For Host Name , enter the domain name of your dbt Cloud instance, if not the default. Include the https:// . For more information on access URLs, refer to dbt documentation . For Authentication Type , Service Account is the default selection for service account token . Change to PAT to enter a personal access token (PAT) instead. For Token , enter the dbt Cloud token you generated . Click the Test Authentication button to confirm connectivity to dbt Cloud using these details. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the dbt connection configuration: Provide a Connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, no one can manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the dbt crawler, you can further configure it. Did you know? If a project appears in both the include and exclude filters, the exclude filter takes precedence. dbt core â On the Configuration page for dbt Core, you can override the defaults for any of these options: To limit the enrichment to a particular connection with materialized assets, click Connection and select the relevant option. (This defaults to all connections, if none are specified.) To import existing tags from dbt to Atlan , for Import Tags , click Yes . dbt cloud â On the Configuration page for dbt Cloud, you can override the defaults for any of these options: To select the dbt projects and environments you want to exclude from crawling, click Exclude Metadata . (This defaults to no projects, if none are specified.) To select the dbt projects and environments you want to include in crawling, click Include Metadata . (This defaults to all projects, if none are specified.) To limit the enrichment to a particular connection with materialized assets, click Connection and select the relevant option. (This defaults to all connections, if none are specified.) To import existing tags from dbt to Atlan , for Import Tags , click Yes . For Advanced options , click Yes to configure the crawler further: For Enrich Metadata in Materialized Assets , click Yes to enable enrichment for both dbt and materialized assets or No for dbt assets only. Run the crawler â To run the dbt crawler, after completing the previous steps: To check for any permissions or other configuration issues before running the crawler, click Preflight checks You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you can see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up dbt Core Next Manage dbt tags Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/mine-queries-through-s3",
    "text": "Connect data Connectivity Framework Connector Framework How-tos Mine queries through S3 On this page Mine queries through S3 Once you have crawled assets from a supported connector,Â you can mine query history. Supported connectors include the following: Amazon Redshift Google BigQuery Hive Microsoft Azure Synapse Analytics Microsoft SQL Server Snowflake Teradata For each of the supported connectors, Atlan supports mining query history via S3. This is useful when you have files that hold query history beyond what the source itself retains. To mine lineage from these sources from S3, complete the following steps. Structure the query files â To make the query history files available for Atlan, ensure the files: Use a .json extension. Are present in a single S3 bucket and prefix (directory). To structure the contents of the files for Atlan, ensure: Each line is a single JSON value. (The JSON object cannot be pretty-formatted or span multiple lines.) Each SQL query is on its own line. Commas are not used to separate the lines. Did you know? You can also provide a default database and schema, and session IDs in the JSON. If a SQL query has only the name of the table or view it queries, Atlan will use the default database and schema to generate lineage for the query. Including the session ID speeds up lineage processing. If provided, ensure that all queries belonging to the same session are next to each other in the file. Here is an example of what your JSON should look like. (Here it is split across multiple lines to assist reading, but remember it must all be on a single line in the file!) { \"QUERY_TEXT\" : \"insert into NETFLIX_DB.PUBLIC.MOVIES_FILTERED as select m.* from MOVIES m where m.RATING > 5;\" , \"DATABASE_NAME\" : \"NETFLIX_DB\" , \"SCHEMA_NAME\" : \"PUBLIC\" , \"SESSION_ID\" : \"5c2f0a41-5d02-46f1-b9bd-ef80ad571013\" } The name of the keys or properties in the JSON can be configured while setting up the miner package. In the example above, the default database ( DATABASE_NAME ) and schema ( SCHEMA_NAME ) will be used to qualify the query against the table MOVIES as NETFLIX_DB.PUBLIC.MOVIES . Set up the S3 bucket â The query files must be available in an S3 bucket. You can either upload these files to the Atlan deployment bucket or use your own S3 bucket. Option 1: Use the Atlan S3 bucket â To avoid access issues, we recommend uploading the required files to the same S3 bucket as Atlan. Raise a support request to get the details of your Atlan bucket and include the ARN value of the IAM user or IAM role we can provision access to. To configure access, add the following IAM policy to the default EC2 instance role used by the Atlan EKS cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" ] , \"Resource\" : [ \"arn:aws:s3:::<bucket-name>\" , \"arn:aws:s3:::<bucket-name>/<prefix>/*\" ] } ] } Replace <bucket-name> with the bucket where the data is uploaded. Replace <prefix> with the prefix (directory) where all the files have been uploaded. If you instead opt to use your own S3 bucket, you will need to complete the following steps: Option 2: Use your own S3 bucket â danger S3 buckets with VPC endpoints currently do not support cross-region requests . This may result in workflows not picking up objects from your bucket. You'll first need to create a cross-account bucket policy giving Atlan's IAM role access to your bucket. A cross-account bucket policy is required since your Atlan tenant and S3 bucket may not always be deployed in the same AWS account. The permissions required for the S3 bucket include   - GetBucketLocation , ListBucket , and GetObject . To create a cross-account bucket policy: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new policy to allow access by this ARN and update your bucket policy with the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<role-arn>\" } , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" ] , \"Resource\" : [ \"arn:aws:s3:::<bucket-name>\" , \"arn:aws:s3:::<bucket-name>/<prefix>/*\" ] } ] } Replace <role-arn> with the role ARN of Atlan's node instance role. Replace <bucket-name> with the name of the bucket you are creating. Replace <prefix> with the name of the prefix (directory) within that bucket where you will upload the files. Once the new policy has been set up, please notify the support team. Your request should include the S3 bucket name and prefix. This should be done prior to setting up the workflow so that we can create and attach an IAM policy for your bucket to Atlan's IAM role. (Optional) Update KMS policy â If your S3 bucket is encrypted, you will need to update your KMS policy. This will allow Atlan to decrypt the objects in your S3 bucket. Provide the KMS key ARN and KMS key alias ARN to the Atlan support team. The KMS key that you provide must be a customer managed KMS key. (This is because you can only change the key policy for a customer managed KMS key, and not for an AWS managed KMS key. Refer to AWS documentation to learn more.) To whitelist the ARN of Atlan's node instance, update the KMS policy with the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Decrypt Cross Account\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<role-arn>\" } , \"Action\" : [ \"kms:Decrypt\" , \"kms:DescribeKey\" ] , \"Resource\" : \"*\" } ] } Replace <role-arn> with the role ARN of Atlan's node instance role. Select the miner â To select the S3 miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select the miner for your source and click on Setup Workflow . Configure the miner â To configure the S3 miner: For Connection , select the connection to mine. (To select a connection, a crawler must have already run against that source.) For Miner extraction method , select S3 . Enter the details for your files: For Bucket Name , enter the name of your S3 bucket or Atlan's bucket, including s3:// . For Bucket Prefix , enter the S3 prefix (directory) within the bucket where the files are located. (Optional) For Bucket Region , enter the name of the S3 region in which the bucket exists. For SQL Json key , enter the JSON key containing the SQL query value. (In the example above, this was QUERY_TEXT .) For Default Database Json Key , enter the JSON key containing the name of the default database. (In the example above, this was DATABASE_NAME .) For Default Schema Json Key , enter the JSON key containing the name of the default schema. (In the example above, this was SCHEMA_NAME .) For Session ID Json Key , enter the JSON key containing the session ID under which the query ran. (In the example above, this was SESSION_ID .) (Optional) For Control Config , if Atlan support has provided you a custom control configuration, select Custom and enter the configuration into the Custom Config box. You can also: Enter {âignore-all-caseâ: true} to enable crawling assets with case-sensitive identifiers. Run the miner â To run the S3 miner, after completing the steps above: To run the miner once, immediately, at the bottom of the screen click the Run button. To schedule the miner to run hourly, daily, weekly or monthly, at the bottom of the screen click the Schedule & Run button. Once the miner has completed running, you will see lineage for your source's assets created by the queries in S3! ð Frequently asked questions â If I remove queries from S3 and run the miner, does it remove the lineage generated from those queries? â No, we do not remove lineage from older queries that are no longer in the bucket. Does the miner reprocess files in the S3 prefix? â Yes, we process all files present in the S3 prefix and publish any new lineage generated. We recommend removing older files when updating the files in the S3 prefix. I used this approach for initial mining. Can I convert the miner I already set up to do its future mining direct from the source (not S3)? â Yes, just edit the workflow configuration . Alternatively, you can also set up another miner for the same connection. Are the database and schema name parameters always required in the JSON file? â The DATABASE_NAME and SCHEMA_NAME fields can be set to null if that data is already available in the query. These properties are used as a fallback option for when queries are run in the context of a certain schema or database. What SQL statements should be added to the S3 miner JSON file for lineage? â You will need to add DDL and DML statements to the S3 miner JSON file for mining lineage. SELECT is not required since it is a DQL statement. Both UPDATE and DELETE can be based on values from another table, so these statements will be required for generating lineage. Tags: connectors data crawl Previous Connect data sources for Azure-hosted Atlan instances Next How to order workflows Structure the query files Set up the S3 bucket Option 1: Use the Atlan S3 bucket Option 2: Use your own S3 bucket Select the miner Configure the miner Run the miner Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/mine-google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery Crawl BigQuery Assets Mine Google BigQuery On this page Mine Google BigQuery Once you have crawled assets from Google BigQuery , you can mine its query history to construct lineage. To mine lineage from Google BigQuery, review the order of operations and then complete the following steps. Select the miner â To select the Google BigQuery miner: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the filters along the top, click Miner . From the list of packages, select BigQuery Miner and then click Setup Workflow . Configure the miner â To configure the Google BigQuery miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , select Query History . For Start time , choose the earliest date from which to mine query history. info ðª Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. (Optional) By default, the miner fetches data from the US region. To fetch data from another region , for Region , select Custom and then enter the region where your INFORMATION_SCHEMA is hosted under Custom BigQuery Region . Enter the region in the following format region-<REGION> , replacing <REGION> with your specific region   -  for example, europe-north1 . To check for any permissions or other configuration issues before running the miner, click Preflight checks . At the bottom of the screen, click Next to proceed. danger If running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Configure the miner behavior â To configure the Google BigQuery miner behavior: (Optional) For Calculate popularity , change to True to retrieve usage and popularity metrics for your Google BigQuery assets from query history: To select a pricing model for running queries , for Pricing Model , click On Demand to be charged for the number of bytes processed or Flat Rate for the number of slots purchased. For Popularity Window (days) , 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Google BigQuery assets. Press enter after each name to add more names. (Optional) For Control Config , click Custom to configure the following: For Fetch excluded project's QUERY_HISTORY , click Yes to mine query history from databases or projects excluded while crawling metadata from Google BigQuery . If Atlan support has provided you with a custom control configuration,Â enter the configuration into theÂ Custom Config box. You can also: (Optional) Enter {âignore-all-caseâ: true} to enable crawling assets with case-sensitive identifiers. Run the miner â To run the Google BigQuery miner, after completing the steps above: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Google BigQuery assets that were created in Google BigQuery between the start time and when the miner ran! ð Tags: connectors data crawl setup Previous Crawl Google BigQuery Next Manage Google BigQuery tags Select the miner Configure the miner Configure the miner behavior Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/crawl-looker",
    "text": "Connect data BI Tools Cloud-based BI Looker Crawl Looker Assets Crawl Looker On this page Crawl Looker Once you have configured the Looker user permissions , you can establish a connection between Atlan and Looker. To crawl metadata from Looker, review the order of operations and then complete the following steps. Select the source â To select Looker as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Looker Assets and click on Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to Looker and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Looker credentials: For Host Name , enter the full URL for your Looker API host, including the https:// . For Port , keep 443 for Looker instances created after July 7, 2020, or switch to 19999 for older instances. For Client ID , enter the client ID you generated when setting up user permissions . For Client Secret , enter the client secret you generated when setting up user permissions . (Optional) For Field Level Lineage : For Private SSH Key , paste the private SSH key for the key you configured in GitHub . For Passphrase for the private key , enter the passphrase that protects the key, if any. (If the key is not protected by a passphrase, leave this blank.) For SSH Known Hosts , add any value that needs to be hardcoded in the ~/.ssh/known-hosts file before cloning your project Git repositories using SSH. (If not required, leave this blank.) At the bottom of the form, click the Test Authentication button to confirm connectivity to Looker using these details. When successful, at the bottom of the screen click the Next button. Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Looker. This method uses Atlan's looker-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include projects.json , dashboards.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Looker. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Looker data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the Looker connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the Looker crawler, you can further configure it. (These options are only available when using the direct extraction method .) You can override the defaults for any of these options: Looker folders contain saved content, such as dashboards , looks , and tiles : To select the Looker folders you want to include in crawling, click Include Folders . (This will default to all folders, if none are specified.) To select the Looker folders you want to exclude from crawling, click Exclude Folders . (This will default to no folders, if none are specified.) Looker projects contain LookML files, such as models , views , and explores : To select the Looker projects you want to include in crawling, click Include Projects . (This will default to all projects, if none are specified.) To select the Looker projects you want to exclude from crawling, click Exclude Projects . (This will default to no projects, if none are specified.) For Use Field Level Lineage , click True to enable crawling field-level lineage for Looker or click False to disable it. Did you know? If a folder or project appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Looker crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up on-premises Looker access Next Crawl on-premises Looker Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau Crawl Tableau Assets Crawl Tableau On this page Crawl Tableau Once you have configured Tableau , you can establish a connection between Atlan and Tableau. (If you are also using a private network for Tableau, you will need to set that up first , too.) To crawl metadata from Tableau, review the order of operations and then complete the following steps. Select the source â To select Tableau as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Tableau Assets and click on Setup Workflow . Provide credentials â InÂ Direct extraction, Atlan connects to Tableau and crawls metadata directly. InÂ Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Tableau credentials: For Host Name , enter the host name of your Tableau Online or Tableau Server instance (or the private DNS name if your Tableau Server instance uses an SSL certificate ). For Port , enter the port number of your Tableau instance. For Authentication , choose how you would like to connect to Tableau: For Basic authentication, enter the Username and Password you use to log in to Tableau. For Personal Access Token authentication, enter the Personal Access Token Name and Personal Access Token Value you generated _Create_a_personal_access_token). For JWT Bearer authentication, enter your Tableau Server username or Tableau Online email address for Username , and the Client ID , Secret ID , and Secret Value you copied from the connected app in Tableau. (Optional) For SSL , keep the default Enabled to use HTTPS or click Disabled to use HTTP. For Site , enter the name of the site you want to crawl. (If left blank, the default site will be used.) danger If you are using Tableau Online, the site is required for Atlan to authenticate properly. (Optional) For SSL certificate , this is only required if your Tableau Server instance uses a self-signed or an internal CA SSL certificate , paste a supported SSL certificate in the recommended format . At the bottom of the form, click the Test Authentication button to confirm connectivity to Tableau using these details. When successful, at the bottom of the screen click the Next button. Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Tableau. This method uses Atlan's tableau-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: ForÂ Bucket name , enter the name of your S3 bucket. ForÂ Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include dashboards/result-0.json , workbooks/result-0.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Tableau. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Tableau data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the Tableau connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Configure the crawler â Before running the Tableau crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the Tableau projects you want to include in crawling, click Include Projects . (This will default to all assets, if none are specified.) To select the Tableau projects you want to exclude from crawling, click Exclude Projects . (This will default to no assets, if none are specified.) To have the crawler ignore Tableau projects based on a naming convention, specify a regular expression in the Exclude Projects Regex field. To check for any permissions or other configuration issues before running the crawler, click Preflight checks . Did you know? If a project appears in both the include and exclude filters, the exclude filter takes precedence. (The Exclude Projects Regex also takes precedence.) Configure advanced controls â Before running the Tableau crawler, you can also configure advanced controls for the crawler. On the Advanced page, you can override the defaults for any of these options: For Alternate Host URL , enter the protocol and host name to be used for viewing assets directly in Tableau. For Crawl Unpublished Worksheets and Dashboards , click Yes to enable crawling hidden worksheets and dashboards or No to skip crawling them. For Hidden Datasource Fields , click Yes to enable crawling hidden datasource fields or No to skip crawling them. Crawl embedded dashboards: Embedded dashboard here means linking or displaying a dashboard inside another dashboard by providing a link to the dashboard in a Web Page item of the embedding dashboard. Click Yes to enable relationships between different embedded dashboards. Click No to skip creating relationships between embedded dashboards. Run the crawler â To run the Tableau crawler, after completing the steps above: You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up a private network link to Tableau server Next Crawl on-premises Tableau Select the source Provide credentials Configure the connection Configure the crawler Configure advanced controls Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/tags/lineage",
    "text": "76 docs tagged with \"lineage\" View all tags Alteryx Integrate, catalog, and govern Alteryx assets in Atlan using OpenLineage. Amazon MWAA OpenLineage Integrate, catalog, and visualize Amazon MWAA lineage in Atlan. Apache Airflow OpenLineage Integrate, catalog, and visualize Apache Airflow lineage in Atlan. Apache Spark OpenLineage Integrate, catalog, and visualize Apache Spark lineage in Atlan. Astronomer OpenLineage Integrate, catalog, and visualize Astronomer lineage in Atlan. Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Connectors and capabilities Learn about connectors and capabilities. Crawl Dagster assets Create a crawler workflow in Atlan to capture lineage from Dagster assets Create data products You can either create a data product from the products module or lineage graph. Dagster Integrate, catalog, and visualize Dagster lineage in Atlan. Dagster integration Frequently asked questions about Dagster integration with Atlan Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. DataStax Enterprise Catalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data. Does Atlan support field-level lineage for BI tools? Atlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Generate lineage between assets App Learn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app. Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan. High availability and disaster recovery (HA/DR) Learn about high availability and disaster recovery (ha/dr). How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How do you enable data lineage for different data sources? Learn about how do you enable data lineage for different data sources?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. How is the Atlan lineage graph depicted using Power BI measures? Learn about how is the atlan lineage graph depicted using power bi measures?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Lineage [Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:. Lineage Track and visualize data lineage across your data landscape to understand data flow and dependencies. Lineage Generator (no transformations) Learn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior. Monitor data domains The _Statistics_ tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more. OpenLineage configuration and facets Learn about openlineage configuration and facets. Preflight checks for Apache Airflow Learn about preflight checks for apache airflow. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Report on glossaries The [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further. Report on queries The Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs. Report on usage and cost The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. Set up Dagster Configure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assets Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Source asset type Detailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app. Tag propagation Learn about tag propagation. Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector Tenant logs Learn about tenant logs. Teradata Catalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage. Troubleshooting Amazon DynamoDB connectivity Learn about troubleshooting amazon dynamodb connectivity. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting Amazon Redshift connectivity Learn about troubleshooting amazon redshift connectivity. Troubleshooting Apache Airflow/OpenLineage connectivity Learn about troubleshooting apache airflow/openlineage connectivity. Troubleshooting Apache Spark/OpenLineage connectivity Learn about troubleshooting apache spark/openlineage connectivity. Troubleshooting Domo connectivity Learn about troubleshooting domo connectivity. Troubleshooting Fivetran connectivity Learn about troubleshooting fivetran connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Matillion connectivity Learn about troubleshooting matillion connectivity. Troubleshooting Microsoft Azure Cosmos DB connectivity Learn about troubleshooting microsoft azure cosmos db connectivity. Troubleshooting Microsoft Azure Data Factory connectivity Learn about troubleshooting microsoft azure data factory connectivity. Troubleshooting Microsoft Power BI connectivity Learn about troubleshooting microsoft power bi connectivity. Troubleshooting Mode connectivity Learn about troubleshooting mode connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. Troubleshooting on-premises Looker connectivity Learn about troubleshooting on-premises looker connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting Sigma connectivity Learn about troubleshooting sigma connectivity. Troubleshooting Sisense connectivity Learn about troubleshooting sisense connectivity. Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. Troubleshooting usage and popularity metrics Learn about troubleshooting usage and popularity metrics. Use Atlan AI for lineage analysis â Available to customers in Enterprise and Business-Critical platform editions View event logs Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. view lineage The [lineage](/product/capabilities/lineage/concepts/what-is-lineage) graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps. What are processes? Learn about what are processes?. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Dagster Learn about the Dagster metadata that Atlan captures and visualizes What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase. What does Atlan crawl from Sigma? Atlan crawls and maps the following assets and properties from Sigma. What is business lineage? Learn about what is business lineage?. What lineage do you support? Learn about what lineage do you support?. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/data-lineage",
    "text": "60 docs tagged with \"data-lineage\" View all tags Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Connectors and capabilities Learn about connectors and capabilities. Create data products You can either create a data product from the products module or lineage graph. Does Atlan support field-level lineage for BI tools? Atlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Generate lineage between assets App Learn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app. High availability and disaster recovery (HA/DR) Learn about high availability and disaster recovery (ha/dr). How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How do you enable data lineage for different data sources? Learn about how do you enable data lineage for different data sources?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. How is the Atlan lineage graph depicted using Power BI measures? Learn about how is the atlan lineage graph depicted using power bi measures?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Lineage [Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:. Lineage Generator (no transformations) Learn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior. Monitor data domains The _Statistics_ tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more. OpenLineage configuration and facets Learn about openlineage configuration and facets. Preflight checks for Apache Airflow Learn about preflight checks for apache airflow. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Report on glossaries The [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further. Report on queries The Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs. Report on usage and cost The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Source asset type Detailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app. Tag propagation Learn about tag propagation. Tenant logs Learn about tenant logs. Troubleshooting Amazon DynamoDB connectivity Learn about troubleshooting amazon dynamodb connectivity. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting Amazon Redshift connectivity Learn about troubleshooting amazon redshift connectivity. Troubleshooting Apache Airflow/OpenLineage connectivity Learn about troubleshooting apache airflow/openlineage connectivity. Troubleshooting Apache Spark/OpenLineage connectivity Learn about troubleshooting apache spark/openlineage connectivity. Troubleshooting Domo connectivity Learn about troubleshooting domo connectivity. Troubleshooting Fivetran connectivity Learn about troubleshooting fivetran connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Matillion connectivity Learn about troubleshooting matillion connectivity. Troubleshooting Microsoft Azure Cosmos DB connectivity Learn about troubleshooting microsoft azure cosmos db connectivity. Troubleshooting Microsoft Azure Data Factory connectivity Learn about troubleshooting microsoft azure data factory connectivity. Troubleshooting Microsoft Power BI connectivity Learn about troubleshooting microsoft power bi connectivity. Troubleshooting Mode connectivity Learn about troubleshooting mode connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. Troubleshooting on-premises Looker connectivity Learn about troubleshooting on-premises looker connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting Sigma connectivity Learn about troubleshooting sigma connectivity. Troubleshooting Sisense connectivity Learn about troubleshooting sisense connectivity. Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. Troubleshooting usage and popularity metrics Learn about troubleshooting usage and popularity metrics. Use Atlan AI for lineage analysis â Available to customers in Enterprise and Business-Critical platform editions View event logs Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. view lineage The [lineage](/product/capabilities/lineage/concepts/what-is-lineage) graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps. What are processes? Learn about what are processes?. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase. What does Atlan crawl from Sigma? Atlan crawls and maps the following assets and properties from Sigma. What is business lineage? Learn about what is business lineage?. What lineage do you support? Learn about what lineage do you support?. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/impact-analysis",
    "text": "58 docs tagged with \"impact-analysis\" View all tags Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Connectors and capabilities Learn about connectors and capabilities. Create data products You can either create a data product from the products module or lineage graph. Does Atlan support field-level lineage for BI tools? Atlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). High availability and disaster recovery (HA/DR) Learn about high availability and disaster recovery (ha/dr). How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How do you enable data lineage for different data sources? Learn about how do you enable data lineage for different data sources?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. How is the Atlan lineage graph depicted using Power BI measures? Learn about how is the atlan lineage graph depicted using power bi measures?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Lineage [Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:. Lineage Generator (no transformations) Learn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior. Monitor data domains The _Statistics_ tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more. OpenLineage configuration and facets Learn about openlineage configuration and facets. Preflight checks for Apache Airflow Learn about preflight checks for apache airflow. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Report on glossaries The [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further. Report on queries The Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs. Report on usage and cost The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. Set up on-premises Databricks lineage extraction In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. Tag propagation Learn about tag propagation. Tenant logs Learn about tenant logs. Troubleshooting Amazon DynamoDB connectivity Learn about troubleshooting amazon dynamodb connectivity. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting Amazon Redshift connectivity Learn about troubleshooting amazon redshift connectivity. Troubleshooting Apache Airflow/OpenLineage connectivity Learn about troubleshooting apache airflow/openlineage connectivity. Troubleshooting Apache Spark/OpenLineage connectivity Learn about troubleshooting apache spark/openlineage connectivity. Troubleshooting Domo connectivity Learn about troubleshooting domo connectivity. Troubleshooting Fivetran connectivity Learn about troubleshooting fivetran connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Matillion connectivity Learn about troubleshooting matillion connectivity. Troubleshooting Microsoft Azure Cosmos DB connectivity Learn about troubleshooting microsoft azure cosmos db connectivity. Troubleshooting Microsoft Azure Data Factory connectivity Learn about troubleshooting microsoft azure data factory connectivity. Troubleshooting Microsoft Power BI connectivity Learn about troubleshooting microsoft power bi connectivity. Troubleshooting Mode connectivity Learn about troubleshooting mode connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. Troubleshooting on-premises Looker connectivity Learn about troubleshooting on-premises looker connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting Sigma connectivity Learn about troubleshooting sigma connectivity. Troubleshooting Sisense connectivity Learn about troubleshooting sisense connectivity. Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. Troubleshooting usage and popularity metrics Learn about troubleshooting usage and popularity metrics. Use Atlan AI for lineage analysis â Available to customers in Enterprise and Business-Critical platform editions View event logs Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. view lineage The [lineage](/product/capabilities/lineage/concepts/what-is-lineage) graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps. What are processes? Learn about what are processes?. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase. What does Atlan crawl from Sigma? Atlan crawls and maps the following assets and properties from Sigma. What is business lineage? Learn about what is business lineage?. What lineage do you support? Learn about what lineage do you support?. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/faq",
    "text": "33 docs tagged with \"faq\" View all tags Can Atlan integrate with Airflow to generate lineage? Atlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage). Can Atlan read a dump of SQL statements to create lineage? Atlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/. Can I be notified if there is a change in downstream dashboards or a schema drift? You can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events. Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Refer to our [troubleshooting Jira documentation](/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira) to learn more. Can I create backups of glossaries? Atlan also allows you to [export your glossary assets](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) to spreadsheets and keep a record of your contextual information. Can I query any DW/DL? You can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature. Can I turn off sample data preview for the entire organization? Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data. Dagster integration Frequently asked questions about Dagster integration with Atlan Deployment and security Frequently asked questions about Secure Agent 2.0 deployment and security Discovery FAQs Frequently asked questions about Atlan's Discovery capabilities. Does lineage only cover calculated fields for Tableau dashboards? Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f. Frequently Asked Questions Find answers to common questions about using Atlan, organized by topic area for quick resolution. How can I identify an Insights query in my database access log? Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs. How can I use personas to update a term in a glossary? By default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section. How do I send messages or search assets from Slack? Sending messages and searching assets from Slack are disabled. Refer to [Troubleshooting Slack](/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack) to learn more. Lineage [Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:. Permissions and limitations Frequently asked questions about CrateDB connector setup, permissions, and limitations Roles and permissions Explanation of Snowflake's security model and role requirements for data quality operations. Setup and configuration Common questions about Databricks data quality setup and configuration. Supported sources Learn about supported sources. Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector Troubleshooting Anomalo connectivity Learn about troubleshooting anomalo connectivity. Troubleshooting Slack What do the colors in Slack notifications for modified assets mean? What are Power BI processes on the lineage graph? Note that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets. What does Atlan do with each Slack permission? Learn about what does atlan do with each slack permission?. What is included in the Jira integration? With two of your most important workspaces connected, you can save time and improve the way you track issues for your data. What is included in the Microsoft Teams integration? With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team. What is included in the Slack integration? Learn about the features and capabilities of the Slack integration with Atlan. What is the default permission for a glossary? By default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access. What is the difference between Copy Link and Share on Slack or Teams? Learn about what is the difference between copy link and share on slack or teams?. Why do I only see tables from the same schema to join from in a visual query? When [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder. Why is lineage available for table level but not column level? The home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset. Why is the SQL query visible only in Snowflake process and not in dbt process nodes? Atlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/troubleshooting",
    "text": "8 docs tagged with \"troubleshooting\" View all tags Connection issues Resolve common connection and authentication issues when setting up CrateDB connector Deployment and security Frequently asked questions about Secure Agent 2.0 deployment and security Frequently Asked Questions Find answers to common questions about using Atlan, organized by topic area for quick resolution. Lineage [Data lineage](/product/capabilities/lineage/how-tos/view-lineage) captures how data moves across your data landscape. This information is useful to:. Setup and configuration Common questions about Databricks data quality setup and configuration. Supported sources Learn about supported sources. Troubleshooting Anomalo connectivity Learn about troubleshooting anomalo connectivity. Troubleshooting Slack What do the colors in Slack notifications for modified assets mean?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai",
    "text": "Configure Atlan Atlan AI On this page Atlan AI Overview: Catalog and leverage Atlan AI capabilities to enhance your data assets in Atlan. Gain AI-powered documentation, and lineage analysis capabilities for your data estate. Get started â Remote MCP Overview : Learn about Atlan's hosted Remote MCP server for AI agents How to implement the Atlan MCP server : Set up the local MCP server Guides â How to use Atlan AI for documentation : Generate and manage AI-powered descriptions for your data assets. How to use Atlan AI for lineage analysis : Understand lineage transformations using natural language explanations. Concepts â What is Atlan AI : Learn about Atlan AI's capabilities and features. Atlan AI security : Understand how Atlan AI handles data security and privacy. Tags: atlan-ai ai capabilities Next Atlan MCP Get started Guides Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/atlan-mcp-overview",
    "text": "Configure Atlan Atlan AI Atlan MCP On this page Atlan MCP The Model Context Protocol (MCP) is an open standard that enables AI agents to access contextual metadata from external systems. It provides a consistent way for large language models and automation frameworks to retrieve the context they need to generate accurate and reliable results. Atlan MCP is based on this standard and provides a reference implementation through the Atlan MCP server . The server acts as a secure bridge between Atlanâs metadata platform and AI tools such as Claude, Cursor, Windsurf, and Microsoft Copilot Studio. With Atlan MCP, you can search and discover assets, explore lineage, update metadata, create glossaries, and more, all using real-time context from Atlan. Atlan MCP tools â The Atlan MCP server provides a set of tools that enable AI agents to work directly with Atlan metadata. These tools supply real-time context to AI environments, making it easier to search, explore, and update metadata without leaving your workflow. Search assets : Find assets in Atlan using flexible filters such as name, type, tags, and domains. This helps AI agents surface the most relevant assets for a given task. Query by DSL : Retrieve specific assets using Atlan's DSL query language. This enables precise lookups that go beyond basic search filters. Explore lineage : Trace upstream or downstream lineage for a given asset. This provides visibility into data dependencies and impact across your environment. Update assets : Modify metadata attributes, including descriptions, certification status, and README content. This enables AI agents to keep metadata current as part of automated workflows. Glossary : Create glossaries, categories, and terms. This supports standardized business definitions and improves consistency across teams and tools. Deployment options â You can connect to Atlan MCP in two ways: Remote MCP : A hosted, per-tenant MCP server managed by Atlan. Supports both OAuth and API Key authentication. Currently available in private preview . Local MCP : A locally hosted server using Docker or uv. Provides flexibility for development or testing. Tags: Atlan MCP AI metadata integrations Previous Atlan AI Next Remote MCP Atlan MCP tools Deployment options"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
    "text": "Configure Atlan Atlan AI Documentation How to use Atlan AI for documentation On this page Use Atlan AI for documentation â Available to customers in Enterprise and Business-Critical platform editions Who can do this? Before using Atlan AI, your admin user must enable Atlan AI in your Atlan workspace. Atlan AI helps you automate the process of documenting your data assets. You can use Atlan AI to generate meaningful context for your assets and then simply review the content for accuracy and relevance. Accept, reject, or edit any AI-powered suggestions, the choice is yours! You can use Atlan AI to: Document tables and views with AI-generated descriptions Document child assets with AI-generated descriptions Document terms and categories with AI-generated descriptions Document terms with AI-generated READMEs Add an Atlan AI-generated alias to supported assets Did you know? To ensure full transparency, any changes made using Atlan AI will be marked as Updated using Atlan AI in the activity log . Add a description to a table or view â You can use Atlan AI to add descriptions to your assets in Atlan and provide helpful context to your teams. Supported asset types include: Amazon QuickSight analyses, dashboards, and datasets dbt Cloud and dbt Core models, sources, and tests Looker dashboards, explores, looks, models, tiles, and views Microsoft Power BI workspaces, dashboards, datasets, data sources, pages, reports, tables, and tiles Mode charts, queries, and reports Redash Â dashboards, queries, and visualizations Salesforce objects, dashboards, and reports Sigma workbooks, datasets, pages, and data elements Snowflake streams SQL tables, views, databases, and schemas Tableau workbooks, worksheets, dashboards, data sources, and metrics ThoughtSpot answers and liveboards To add a description to a table or view using Atlan AI: From the left menu on any screen, click Assets . (Optional) Under the search bar on the Assets page, click the Table Â tab. (Optional) In the Filters menu on the left, click Properties to expand the menu and select Description to search for assets without a description. Click an asset to view the Overview tab in the sidebar. Under Description , you can either: For assets without a description, navigate to the text box and click use Atlan AI to add an Atlan AI-suggested description. For assets with an existing description, click the text box and then click the Improve using Atlan AI button to replace the existing description with an Atlan AI-suggested one. (Optional) At the bottom of the Atlan AI is writing... Â box, click Enhance now to briefly describe your organization and help Atlan AI make more relevant suggestions   -  this option is only visible to admin users. Once Atlan AI has generated a description, you can: Click anywhere in the text box to edit the text and then click Apply . Click Apply to add the description to your asset. Click Discard to discard the AI-generated description. Click the retry button to generate a new description, compare the two to select the most relevant option, and then click Apply . (Optional) From the sidebar tabs on the right, click the Activity tab to view the changelog   -  including the Updated using Atlan AI stamp, user information, and timestamp for the update. Your AI-suggested table or view description is now live! ð Add a description to a column â You can use Atlan AI to add descriptions to your columns from the following: Overview tab in the sidebar for column assets Column preview section in the asset profile for table and view assets Columns tab in the sidebar for table and view assets Supported asset types include: Amazon QuickSight analysis visuals, dashboard visuals, and dataset fields dbt Cloud and dbt Core columns Looker fields for explores and views Microsoft Power BI columns and measures Salesforce fields Sigma dataset columns and data element fields SQL columns Tableau data source fields and calculated fields ThoughtSpot visualizations In this example, we'll use Atlan AI to add a description to a column from the Column preview section in a table asset profile . To add a description to a column using Atlan AI: From the left menu on any screen, click Assets . (Optional) Under the search bar on the Assets page, click the Table Â tab. Right-click an asset and then select Open profile to view its asset profile.Â From the asset profile, navigate to the Column Preview section and select a column to document. Under Description , click use Atlan AI to add an AI-generated description to the column. (Optional) At the bottom of the Atlan AI is writing... Â box, click Enhance now to briefly describe your organization and help Atlan AI make more relevant suggestions   -  this option is only visible to admin users. Once Atlan AI has generated a description, you can: Click anywhere in the text box to edit the text and then click Apply . Click Apply to add the description to your asset. Click Discard to discard the AI-generated description. Click the retry button to generate a new description, compare the two to select the most relevant option, and then click Apply . (Optional) From the sidebar tabs on the right, click the Activity tab to view the changelog   -  including the Updated using Atlan AI stamp, user information, and timestamp for the update. Your AI-suggested column descriptions are now live! ð Add a description to glossary assets â You can use Atlan AI to add descriptions to your terms and categories in Atlan and provide useful business context for your linked assets . To add a description to a term using Atlan AI: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI. You can either add a description from the term profile or sidebar. Under Description , you can either: For terms without a description, navigate to the text box and click use Atlan AI to add an Atlan AI-suggested description. For terms with an existing description, click the text box and then click the Improve using Atlan AI button to replace the existing description with an Atlan AI-suggested one   -  this option is only available in the sidebar. (Optional) At the bottom of the Atlan AI is writing... box, click Enhance now to briefly describe your organization and help Atlan AI make more relevant suggestions   -  this option is only visible to admin users. Once Atlan AI has generated a description, you can: Click anywhere in the text box to edit the text and then click Apply . Click Apply to add the description to your term. Click Discard to discard the AI-generated description. Click the retry button to generate a new description, compare the two to select the most relevant option, and then click Apply . (Optional) From the sidebar tabs on the right, click the Activity tab to view the changelog   -  including the Updated using Atlan AI stamp, user information, and timestamp for the update. Your AI-suggested term description is now live! ð Add a README to terms â You can use Atlan AI to generate comprehensive READMEs for your terms in Atlan. This provides you with a solid foundation for documenting business context. You can then simply edit the Atlan AI-generated README and customize the format accordingly. To add a README to a term using Atlan AI: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI. In the Readme section of the asset profile, click Use Atlan AI to add an Atlan AI-suggested README to the term. Click anywhere in the text box to edit or format the text and then click Save . Did you know? If there are automated suggestions for asset descriptions, the option to use Atlan AI to document such assets will be unavailable. Automated suggestions are based on user-generated descriptions for similar assets and may be more accurate than Atlan AI-generated descriptions. Tags: atlan documentation Previous Set up Local MCP Server Next How to use Atlan AI for lineage analysis Add a description to a table or view Add a description to a column Add a description to glossary assets Add a README to terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-lineage-analysis",
    "text": "Configure Atlan Atlan AI Lineage Analysis How to use Atlan AI for lineage analysis On this page Use Atlan AI for lineage analysis â Available to customers in Enterprise and Business-Critical platform editions Who can do this? Before using Atlan AI, your admin user must enable Atlan AI in your Atlan workspace. Atlan AI can help you understand lineage transformations using natural language. You can use Atlan AI to create a natural language explanation for assets with SQL attributes and help you better understand the transformation logic. Explain lineage transformations â To use Atlan AI to explain lineage transformations: From the left menu of any screen, click Assets . Select an asset, and from the top right of the asset card, click theÂ View lineage Â icon to open the lineage graph. On the lineage graph, click any circular process button to view more details in the sidebar. From Overview in the sidebar, under Query , click the Atlan AI icon to explain the SQL query. You can now understand lineage transformations using Atlan AI! ð Did you know? The lineage graph in Atlan provides a granular view of the data flows and transformations for your assets, learn more here . Tags: lineage data-lineage impact-analysis Previous How to use Atlan AI for documentation Next Atlan AI security Explain lineage transformations"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/concepts/security",
    "text": "Configure Atlan Atlan AI Concepts Atlan AI security On this page Atlan AI security Did you know? Atlan uses Azure OpenAI Service to power Atlan AI. Atlan does not send any data to the AI service and only uses metadata for supported capabilities. For questions about data security, see below. Learn more about how Atlan AI processes and stores your data: What services does Atlan AI use? â Atlan uses Azure OpenAI Service to power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model. What data does Atlan send to the AI service? â Atlan does not send any data to the AI service. Atlan only sends metadata for supported capabilities. For example: Atlan AI-suggested asset descriptions -  table, view, column, database, or schema name. Atlan AI-suggested term descriptions -  glossary name and description, category name and description, and term name. Atlan AI-suggested lineage explanations -  SQL transformations in lineage with upstream and downstream asset names. Atlan AI-suggested aliases -  table, view, column, database, or schema name. Atlan AI-suggested READMEs for terms -  glossary, category, and term name and description, and any existing READMEs within the same glossary. Does Atlan use any metadata or data to train Atlan AI? â No, Atlan does not use your metadata or data for fine-tuning or training AI models. Is the data processed through Atlan AI encrypted? â Atlan makes HTTPS requests from your tenant, applicable to all supported cloud platforms . The data is encrypted in transit using TLS 1.2, AWS PrivateLink, or Azure virtual network peering. Atlan uses 256-bit Advanced Encryption Standard (AES-256) algorithm to encrypt data at rest. How does Atlan ensure security development of Atlan AI? â Atlan AI follows OWASP Top 10 that includes application security reviews and Static Application Security Testing (SAST) tools . Does Atlan AI comply with any governance or legal frameworks? â While Atlan is HIPAA and GDPR compliant , Atlan AI is currently not. As Atlan AI matures, compliance continues to be our key focus. Does Atlan AI process PII or other sensitive data? â Atlan AI only processes user input and metadata, which typically do not contain PII or sensitive data. However, it is the organization's responsibility to ensure that PII or other sensitive data is not available in the metadata or shared via user input. What is the data retention policy for Atlan AI? â Atlan does not store any data for Atlan AI. This is enforced in the following two ways: Atlan has an exemption from Microsoft to not store any data. Keeping data sensitivity in mind, Atlan has opted out of abuse monitoring and human review from Azure OpenAI Service. Only the metadata generated using Atlan AI is cataloged in Atlan. How does Atlan manage security vulnerabilities for Atlan AI? â Vulnerabilities and incidents are managed in accordance with the existing program and policy . How does Atlan manage the performance and scale for Atlan AI? â We utilize the scalability of our existing cloud infrastructure while relying on Azure OpenAI . Tags: data model Previous How to use Atlan AI for lineage analysis Next What's Atlan AI?"
  },
  {
    "url": "https://docs.atlan.com/tags/atlan-ai",
    "text": "2 docs tagged with \"atlan-ai\" View all tags Atlan AI â Available to customers in Enterprise and Business-Critical platform editions Atlan AI Integrate and leverage Atlan AI capabilities for enhanced data documentation, and lineage analysis."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags",
    "text": "Build governance Tags On this page Tags Overview: Use tags in Atlan to categorize and organize your data assets. Tags provide a flexible way to add metadata and classifications to your assets, enhancing searchability and governance. Get started â Follow these steps to implement tags in Atlan: Create a new tag Guides â Delete a tag Concepts â What are tags Tags: tags classification categorization organization governance atlan Next Create a new tag Get started Guides Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/create-a-new-tag",
    "text": "Build governance Tags Get Started Create a new tag Create a new tag Who can do this? You will need to be an admin user in Atlan to create tags. To create a new tag: From the left menu of any screen, click Governance . Under the Governance heading, click Tags and then add a new tag: If there are no existing tags, click Add tag . If you have any existing tags, under the Tags heading, click the + New button. Enter details for the tag: For Untitled tag , enter a meaningful name for your tag. (Optional) For Add description..., enter a more detailed description of your tag. (Optional) To personalize your tags, click the tag icon. From the upper right of the Icons dialog: Click Icons to change the icon for your tag. Click the gray box to change the color of your tag icon to green, yellow, or red. Click Emoji to add an emoji to your tag. Click Upload Image to upload an image for your tag. The recommended size for image uploads is 24x24 pixels. ClickÂ Create . (Optional) Under Tags , click the funnel icon to filter your tags by source: Click Atlan to filter for tags created in Atlan and not synced to any external sources. Click Snowflake to filter for tags imported from Snowflake . Click dbt to filter for tags imported from dbt . That's it   -  you now have a tag ready for your team to use for tagging assets ! ð For tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar. Did you know? Once you've created a tag, you can also delete it at any time. Tags: atlan documentation Previous Tags Next Delete a tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/delete-a-tag",
    "text": "Build governance Tags Tag Management Delete a tag On this page Delete a tag Who can do this? You will need to be an admin user in Atlan to delete tags. To delete tags , you can either: If a tag is not attached to any assets, you can delete the tag right away. If a tag is attached to assets, you will need to remove the tag from the tagged assets first and then you can delete it. Delete a tag â To delete a tag without linked assets: From the left menu of any screen, click Governance . Under the Governance heading, click Tags . (Optional) Under Tags , click the funnel icon to filter your tags by source: Click Atlan to filter for tags created in Atlan and not synced to any external sources. Click Snowflake to filter for tags imported from Snowflake . Click dbt to filter for tags imported from dbt . Under the left Tags menu, select a tag to delete. (The total count of linked assets will be displayed as zero if there are no linked assets.) From the top right, click the trash can icon to delete the tag. In the Delete tag dialog, click Delete to confirm deletion. Delete a tag with linked assets â danger If a tag is attached to assets, you will need to remove the tag from the tagged assets Â before deleting it. To delete a tag with linked assets: From the left menu of any screen, click Governance . Under the Governance heading, click Tags . (Optional) Under Tags , click the funnel icon to filter your tags by source: Click Atlan to filter for tags created in Atlan and not synced to any external sources. Click Snowflake to filter for tags imported from Snowflake . Click dbt to filter for tags imported from dbt . Under the Tags menu, select a tag to delete. Under the tag name, click the Linked assets tab to navigate to the linked assets. To remove the tag from the linked assets, you can either: Click a linked asset to open the asset sidebar, and then under Tags in the sidebar, click the pencil icon to edit the tag. Uncheck the tag box and then click Save . Repeat the steps for each tagged asset. Create a playbook to remove the tag from tagged assets at scale. Once the tag has been removed from all the linked assets, from the top right, click the trash can icon to delete the tag. In the Delete tag dialog, click Delete to confirm. Tags: atlan documentation Previous Create a new tag Next Attach a tag Delete a tag Delete a tag with linked assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/remove-a-tag",
    "text": "Build governance Tags Tag Management Remove a tag On this page Remove a tag Atlan allows you to remove tags from a tagged asset . To remove a tag, you will first need to identify the origin of the tag: Directly added to an asset Propagated to an asset through tag propagation Remove a direct tag â To remove a direct tag: In the left menu from any screen in Atlan, click Assets . On the Assets page, select a tagged asset   -  in this example, we'll select the ORDERS table with the Marketing Analysis tag. Under Tags in the right menu, hover over the attached tag to view details. In the metadata popover,Â Linked by indicates that the tag was directly added to the asset. Click the pencil icon to edit the tag. In the popup, next to the tag name, uncheck the box to remove the tag   -  this will also remove the tag from all the assets to which it was propagated . Click Save to confirm tag removal. Remove a propagated tag â To remove a propagated tag: In the left menu from any screen in Atlan, click Assets . On the Assets page, select a tagged asset   -  in this example, we'll select the AUTHORS column with the Publications Department tag. Under Tags in the right menu, you can either: Hover over the attached tag to view details. In the metadata popover, Propagated from indicates that the tag was propagated to the asset. From the popover, click the originating asset to reconfigure tag propagation   -  in this example, the Book_rating table. Click the pencil icon and then click the Propagated tab to view information about tags propagated via upstream assets. From the Propagated tab, click the originating asset to reconfigure tag propagation   -  in this example, the Book_rating table. From the corresponding screen, under Tags in the right menu for the originating asset, click the pencil icon to edit the tag. In the popup, next to the tag name, click Edit : To remove the tag from downstream assets only, from the Propagation dialog, click Hierarchy only (no lineage) . To remove the tag from propagated assets only and not the originating asset, from the Propagation dialog, click No propagation . Click Update to save your changes. Click Save to confirm tag removal. Note that it may take some time for the tag to be removed from all the assets it was propagated to.Â Did you know? You can also create playbooks to automate the task of removing tags from tagged assets with propagation enabled or disabled . Once the playbook run is completed, tags will be removed from your selected assets. Tags: atlan documentation Previous Attach a tag Next What are tags? Remove a direct tag Remove a propagated tag"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/manage-dbt-tags",
    "text": "Connect data ETL Tools dbt Manage dbt in Atlan Manage dbt tags On this page Manage dbt tags Did you know? If you have already set up and crawled dbt, you do not need to make any modifications to your dbt Cloud or dbt Core setup. You only need to configure the dbt crawler to import dbt tags. Atlan will then import your existing dbt tags automatically for you. Atlan imports your dbt tags and allows you to update your dbt assets with the imported tags. Once you've crawled dbt : Your dbt assets in Atlan will be automatically enriched with their dbt tags. Imported dbt tags will be mapped to corresponding Atlan tags through case-insensitive name match   -  multiple dbt tags can be matched to a single tag in Atlan.Â You can also attach dbt tags to your dbt assets in Atlan   -  allowing you to categorize your assets at a more granular level. You can filter your assets by dbt tags. Import dbt tags to Atlan â Who can do this? You will need to be an admin user in Atlan to import dbt tags. You will also need to work with your dbt Cloud or dbt Core administrator for additional inputs and approval. Atlan imports existing dbt tags through one-way tag sync. The imported dbt tags are matched to corresponding tags in Atlan through case-insensitive name match and your dbt assets enriched with the tags synced from dbt. To allow Atlan to import and sync dbt tags, you will need to do the following: Create tags or have existing tags in dbt. Set up dbt Cloud or dbt Core to integrate with Atlan. Configure the dbt crawler to import existing tags from dbt to Atlan. For Import Tags , click Yes to import dbt tags or click No to disable it. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags synced from dbt will be available to use for tagging assets ! ð View dbt tags in Atlan â Once you've crawled dbt Cloud or dbt Core , you will be able to view and manage your dbt tags in Atlan. To view synced dbt tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click dbt to filter for tags imported from dbt. In the Overview section, you can view a total count of synced dbt tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, description, total count of linked assets, connection name, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your dbt tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . Tags synced from dbt cannot be renamed. You can now attach dbt tags to your dbt assets in Atlan! ð Tags: connectors crawl setup Previous Crawl dbt Next Enrich Atlan through dbt Import dbt tags to Atlan View dbt tags in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/manage-google-bigquery-tags",
    "text": "Connect data Data Warehouses Google BigQuery Manage BigQuery in Atlan Manage Google BigQuery tags On this page Manage Google BigQuery tags Atlan imports your Google BigQuery tags and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires Enterprise edition or higher . Once you've crawled Google BigQuery : Your Google BigQuery assets in Atlan will be automatically enriched with their Google BigQuery tags. Imported Google BigQuery tags will be mapped to corresponding Atlan tags through case-insensitive name match   -  multiple Google BigQuery tags can be matched to a single tag in Atlan.Â You can also attach Google BigQuery tags , including tag values and hierarchies, to your Google BigQuery assets in Atlan   -  allowing you to categorize your assets at a more granular level. Atlan supports: Tags -  enrich your Google BigQuery tables, views, and materialized views with tags and tag values. Policy tags Â   -  enrich your Google BigQuery columns with policy tags and tag hierarchies. You can filter your assets by Google BigQuery tags. Atlan currently does not support crawling Dataplex tag templates . Prerequisites â Before you can import tags from Google BigQuery, you will need to do the following: Create tags or have existing tags in Google BigQuery. Grant permissions to import tags from Google BigQuery. Import Google BigQuery tags to Atlan â Who can do this? You will need to be an admin user in Atlan to import Google BigQuery tags. You will also need to work with your Google BigQuery administrator for additional inputs and approval. Atlan imports existing Google BigQuery tags through one-way tag sync. The imported Google BigQuery tags are matched to corresponding tags in Atlan through case-insensitive name match and your Google BigQuery assets enriched with the tags synced from Google BigQuery. To import Google BigQuery tags to Atlan, you can either: Create a new Google BigQuery workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Google BigQuery workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags synced from Google BigQuery will be available to use for tagging assets ! ð View Google BigQuery tags in Atlan â Once you've crawled Google BigQuery , you will be able to view and manage your Google BigQuery tags in Atlan. To view synced Google BigQuery tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click BigQuery to filter for tags imported from Google BigQuery. In the Overview section, you can view a total count of synced Google BigQuery tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, type, and values, description, total count of linked assets, connection name, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Google BigQuery tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . Tags synced from Google BigQuery cannot be renamed. You can now attach Google BigQuery tags to your Google BigQuery assets in Atlan! ð Tags: connectors data crawl Previous Mine Google BigQuery Next What does Atlan crawl from Google BigQuery? Prerequisites Import Google BigQuery tags to Atlan View Google BigQuery tags in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core",
    "text": "Connect data ETL Tools dbt References What does Atlan crawl from dbt Core? On this page What does Atlan crawl from dbt Core? Atlan crawls and maps the following assets and properties from dbt Core. Atlan also supports lineage between the following: dbt models dbt seeds dbt sources SQL tables and views materialized by dbt models, dbt seeds, dbt sources Column-level lineage for these entities Once you've crawled dbt , you can use dbt-specific filters for quick asset discovery: Test status -  filter dbt tests that passed, failed, or have a warning or error Alias -  filter by the name of a dbt model's identifier in the dbt project Unique id -  filter by the unique node identifier of a dbt model Project name -  filter by dbt project name, only supported for dbt Core version 1.6+ Environment name -  filter by dbt environment name Job status -  filter by dbt job status Last job run -  filter by the last run of the dbt job Atlan's dbt crawler also populates custom metadata to further enrich the assets in Atlan. The Atlan dbt-specific property column in the tables below gives the name of the mapped custom metadata property in Atlan. Did you know? Atlan lets you sync your dbt tags and update your dbt assets with the synced tags. You can also map other metadata to Atlan's assets through your dbt models . Tables â Atlan maps tables from dbt Core to its Table asset type. Source property Atlan property Where in Atlan description description asset profile and overview sidebar config (alias) alias asset filter and properties sidebar stats (row_count) rowCount asset profile and filter, overview sidebar stats (bytes) sizeBytes asset filter and overview sidebar stats (last_modified) sourceUpdatedAt asset profile and properties sidebar project (name) assetDbtProjectName asset filter and overview sidebar uniqueId assetDbtUniqueId asset filter and overview sidebar raw_sql or raw_code dbtRawSQL overview sidebar tags assetDbtTags asset filter and overview sidebar packageName assetDbtPackageName asset filter and properties sidebar alias assetDbtAlias asset filter and properties sidebar description description asset profile and overview sidebar created_at sourceCreatedAt asset profile, overview and properties sidebar compiled_sql or compiled_code dbtCompiledSQL overview sidebar freshness_data (criteria) assetDbtSourceFreshnessCriteria overview sidebar Columns â Atlan maps columns from dbt Core to its Column asset type. Source property Atlan property Where in Atlan description description asset profile and overview sidebar meta assetDbtMeta API only tags assetDbtTags asset filter and overview sidebar packageName assetDbtPackageName asset filter and properties sidebar description description asset profile and overview sidebar created_at sourceCreatedAt asset profile, overview and properties sidebar Models â Atlan maps models from dbt Core to its Model asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar executeCompletedAt sourceUpdatedAt asset profile and properties sidebar owner sourceCreatedBy asset profile and properties sidebar status dbtJobRuns.dbtModelRunStatus overview sidebar alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar raw_sql or raw_code dbtRawSQL overview sidebar compiled_sql or compiled_code dbtCompiledSQL overview sidebar stats dbtStats API only config.materialized dbtMaterializationType API only Sources â Atlan maps sources from dbt Core to its DbtSource asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar owner sourceCreatedBy asset profile and properties sidebar alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar tags assetDbtTags asset filter and overview sidebar criteria assetDbtSourceFreshnessCriteria overview sidebar stats dbtStats API only state dbtState API only Tests â warning For dbt Core, upload the run_results.json file to crawl dbt tests. It's recommended to place the file in the same folder as the manifest.json file. Atlan maps tests from dbt Core to its Test asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar name assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar tags assetDbtTags asset filter and overview sidebar status dbtTestStatus asset profile state dbtTestState API only error dbtTestError asset profile and overview sidebar raw_code dbtTestRawCode overview sidebar raw_sql dbtTestRawSQL overview sidebar compiled_code dbtTestCompiledCode API only compiled_sql dbtTestCompiledSQL API only language dbtTestLanguage asset profile and overview sidebar Seeds â Atlan maps models from dbt Core to its Seed asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar executeCompletedAt sourceUpdatedAt asset profile and properties sidebar owner sourceCreatedBy asset profile and properties sidebar status dbtJobRuns.dbtModelRunStatus overview sidebar alias assetDbtAlias asset filter and properties sidebar meta assetDbtMeta API only uniqueId assetDbtUniqueId asset filter and overview sidebar stats dbtSeedStats API only filePath dbtSeedfilePath asset profile and overview sidebar Previous What does Atlan crawl from dbt Cloud? Next Connection issues Tables Columns Models Sources Tests Seeds"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/update-column-metadata-in-google-sheets",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to update column metadata in Google Sheets On this page update column metadata in Google Sheets Once you've connected Atlan with Google Sheets , you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets. Atlan currently supports importing and updating column metadata for the following asset types: Tables Views Materialized views Looker explores Microsoft Power BI tables Salesforce objects Tableau data sources danger You need to be logged into your Atlan instance before you can start importing column metadata for your Atlan assets in Google Sheets. Import column metadata â You can import column metadata for your data assets directly into Google Sheets. To import column metadata for your data assets into Google Sheets: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan .Â From the list of options in the Atlan add-on, click Enrich metadata to view a list of your data assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, click the asset type   -  for example, Tableau Datasources . In the Atlan sidebar on your spreadsheet, select the data asset(s) you want to import. Click Import to import column metadata for your selected assets. The column metadata for your selected assets are now available in Google Sheets! ð Did you know? If any changes are made to your imported columns in Atlan, you'll need to import those columns once again to access the updated version in Google Sheets. Update column metadata â Once you've imported your data assets from Atlan, you can edit the metadata for your selected data asset in Google Sheets. You can make changes to the column metadata once all the columns have been successfully imported. You can only make changes to the metadata in the following columns: Description Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the Column Name , Data Type , Propagated Tags , and Qualified Name columns Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Bulk update tags for columns â danger You cannot make any changes to the metadata in the Propagated Tags column. Navigate to the Tags column to add tags to your column assets in Google Sheets: When adding tags to columns: The tag must already exist in Atlan. If the tag does not exist in Atlan, updates will not sync and you will receive an error message. Tag match is case-sensitive, ensure that the tag is formatted exactly as it exists in Atlan. For example, if the tag in Atlan is formatted as Marketing Analysis , then columns tagged with marketing analysis will not sync. You can add multiple tags in the Tags column   -  separate multiple tags with a comma , . If you are in a region that uses a separator other than a comma, you will need to modify your spreadsheetâs settings to use commas as separators. If you have added tags that exist in Atlan as well as ones that do not, only the existing tags will be synced. The unsupported tags will not sync and you will receive an error message. Tag propagation is disabled by default in Atlan, hence tags will not be propagated. Push your changes to Atlan â Once you've made changes to the column metadata, complete these steps to push your changes: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Push to Atlan . A dialog box will appear once the changes have synced. (Optional) Click Open in Atlan to verify the changes. In Atlan, an Updated using Google Sheets stamp will appear in the activity log for updated assets. (Optional) Click the Google Sheets link to view the source spreadsheet from Atlan. danger If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Google Sheets. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. View asset profiles in Google Sheets â Once you've imported your data assets into Google Sheets, you can also view their asset profiles . Complete these steps: In the menu bar of your Google spreadsheet, click Extensions . In the dropdown menu, click Atlan . Next, click Open asset in sidebar to view the asset profile on your Google spreadsheet. You can also update components of your asset profile directly in Google Sheets. Your changes will sync automatically to Atlan. Did you know? You can download impacted assets for impact analysis in Google Sheets. Tags: connectors data integration crawl Previous Link your account Next How to update column metadata in Microsoft Excel Import column metadata Update column metadata Push your changes to Atlan View asset profiles in Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata",
    "text": "Build governance Custom Metadata On this page Custom Metadata Overview: Create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information. Define custom fields, relationships, and validation rules to capture additional context about your data assets. Get started â Follow these steps to implement custom metadata in Atlan: Add options Guides â Control access to metadata and data Disable data access Add custom metadata badges Manage custom metadata structures References â What happens when users do not have access to metadata Concepts â What is custom metadata FAQ â Can I update input type for existing custom metadata Tags: custom metadata attributes data catalog governance atlan Next Add options Get started Guides References Concepts FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/add-options",
    "text": "Build governance Custom Metadata Get Started Add options On this page Add options Who can do this? You must be an admin user in Atlan to create options for custom metadata properties. Options in Atlan stand for enumerations or enumerated data types. Options allow you to create your own set of predefined and related values. Once you've created your options, you can add them to your custom metadata properties to ensure consistency of usage across the organization. Example â Imagine that you would like to denote values for the data quality level of your metadata in Atlan. To solve for this, you could create an option Data quality and define three indicative values: Bronze -  for freshly crawled metadata Silver -  for asset enrichment in progress Gold Â   -  for well-documented assets Once you've created the option, you can add it as a custom metadata property . Then you can enrich your assets with this additional context for your data teams.Â Create options â To create an option: From the left menu of any screen, click Governance . Under the Metadata heading, click Options . Under the Options heading, click Get started . In the New option dialog, enter the following details: For Name , enter a meaningful name for your option   -  for example, Data quality . For Values , enter a list of values considered valid, separate each value with a semicolon ; - Gold , Silver , and Bronze . Click Create to add your option. You have just created an option! ð To edit the values for your option, click on the pencil icon in the top right to make your changes and then save them. Did you know? Atlan currently only supports deleting options through API . Tags: data crawl Previous Custom Metadata Next Control access to metadata and data? Example Create options"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/disable-data-access",
    "text": "Build governance Custom Metadata Access Management Disable data access On this page Disable data access Who can do this? You will need to be an admin user in Atlan to configure these options. What if you want to block access to data for your users, and only allow them to access metadata? There are different ways to do this in Atlan. From the most wide-reaching to the most granular: Block all querying â To stop all users from querying data, across all data assets: From the left menu of any screen, click Admin . Under Workspace , click Labs . Toggle off the Insights option. (This should also deactivate all sub-options of Insights.) danger Users will still be able to preview sample data, even with Insights turned off. Block by source â When setting up a crawler â To stop all users from accessing data for a source, when setting up the crawler: Set Allow SQL Query to No to stop users from querying any data in the source. Set Allow Data Preview to No to stop users from previewing any data in the source. So to block all access to data for that source, set both options to No . When setting up a connection's credentials â You can configure the credentials for some data sources without data access permissions. If the credentials cannot query data, Atlan will not be able to query or preview data. If Atlan cannot query or preview data, no users in Atlan can query or preview data either. danger This depends on the connector   -  some connectors need a level of data access even to crawl metadata. The specific set up guide for each connector gives you the minimal set of permissions. Block by asset â Who can do this? In addition to being an Admin user, you will need to be a connection admin for the source containing the assets. To stop users from querying or previewing data for specific assets: Define a persona with a data policy that denies access to those assets. Add the users to that persona. danger To ensure Atlan blocks data access for all users (including connection admins) the data policy must explicitly deny query access. A lack of data policy (implicit deny) will not prevent connection admins from querying and previewing data. Block by tag â To stop users from querying or previewing data that has a particular tag : Define a purpose on that tag. Within the purpose, define a data policy that denies query access for those users. (This will also apply to data previews.) Even if only a single column has the tag, Atlan will block querying and previewing of the entire asset. Tags: data crawl Previous Control access to metadata and data? Next Add custom metadata badges Block all querying Block by source Block by asset Block by tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/add-custom-metadata-badges",
    "text": "Build governance Custom Metadata Badge Management Add custom metadata badges On this page Add custom metadata badges Who can do this? You must be an admin user to be able to add badges for custom metadata . Bringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges. Admin users can use badges to highlight custom metadata right in the asset overview, ensuring greater visibility. Custom metadata badges can help users quickly get the context they need for their data assets, for example: If an Airflow DAG was successful If a table's data quality checks failed Atlan currently supports creating badges for custom metadata properties with the following input types   -  text, number, options, and boolean. Note that Atlan currently does not support creating badges for multivalued types. If Allow multiple values is toggled on , you will not be able to create a badge for that custom metadata property. Create custom metadata badges â danger Before creating a new badge, you'll need to have created at least one custom metadata structure . To create a custom metadata badge: From the left menu of any screen, click Governance . Under the Metadata heading, click Badges . Under the Badges heading, click + Create new . In the Create new badge popover, add the following details: For Custom Metadata property , select the property you want to create a badge for   -  in this example, we'll select Last Run Status for Airflow ETL Details as the property.Â For Name , add a name to your badge, such as Airflow Run Status . (Optional) For Description , add a description. Click Create to create your badge. To add options to your custom metadata badge, click +New option . To define the options for your custom metadata badge, add the following details: To set a matching condition for your values, select Equals (=) or Not Equals (!=) . For Enter value , enter a value   -  such as, Successful . Click the grey box to choose a color for your badge. (Optional) Click the eye icon to preview the badge.Â (Optional) Click +New option to add more options to your badge.Â Click Save to save your badge options. Your custom metadata badge is now live! ð Add custom metadata badges to assets â Once you've created your custom metadata badges, you can add them to your assets.Â Did you know? If you edit the custom metadata properties of an asset, the badges will appear automatically. To add a custom metadata badge to an asset: From the left menu of any screen, click Assets .Â On the Assets page, select an asset to add a badge. In the asset sidebar on the right, click the custom metadata tab   -  in this example, we'll select Airflow ETL Details . In the custom metadata panel in the sidebar, click Start editing . Enter the value(s) you want for the custom metadata properties on the asset and click Update . The custom metadata badge will now be displayed in the asset profile and sidebar! ð Tags: atlan documentation Previous Disable data access Next Manage custom metadata structures Create custom metadata badges Add custom metadata badges to assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/manage-custom-metadata-structures",
    "text": "Build governance Custom Metadata Structure Management Manage custom metadata structures On this page Manage custom metadata structures Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones. Before users or integrations can enrich assets with custom metadata, you must first define its structure. Create custom metadata structure â To create a new custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Start adding custom metadata heading, click the + Get started to add a new structure: For Name enter a name for the custom metadata structure. (In our examples , this would be IPR or ETL .) (Optional) To personalize your custom metadata, to the left of the name, click the image icon. From the upper right of the corresponding dialog: Click Icons to add an icon to your custom metadata. Click the gray box to change the color of the icon to green, yellow, or red. Click Emoji to add an emoji to your custom metadata. Click Upload Image to upload an image for your custom metadata. The recommended size for image uploads is 24x24 pixels. (Optional) Add a description of the custom metadata below these. At the bottom right of the dialog, click the Create button. Create properties in the structure â To create custom metadata properties within a custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, select the custom metadata structure you want to change. Click the New property button (no properties yet) or Add property button (to add more properties): For Name , enter a name for one property. (In our examples , this would be one of License type , Provider , Job link , and so on.) For Type , select the type of value you expect users to use for this property: The Text type allows free-form text values. The Integer type allows only whole numbers (no decimals). The Decimal type allows fractional numbers (those with decimal points). The Boolean type allows only a Yes or a No value. The Date type allows both date and time values in the following format   -  day, month, year, hours, minutes, and seconds. The Options type allows you to define your own set of predefined options for values that are valid. The Users type allows only existing Atlan users as values. The Groups type allows only existing Atlan groups as values. The URL type allows only web links. The SQL type allows only SQL code. (Optional) For Description , enter an explanation for how you expect users to use this property. If you chose Options as the type, either: Under Select Options , select an existing set of options to reuse. Click the Create New link to create a new set of options. Under Option name , give the options a name. Under Values , enter the list of values considered valid (separated by ; ). (Optional) Under Assets , you can configure the connections and asset types on which this custom metadata should be visible to: For Connections , select the connection to which you want to limit users to be able to enrich assets with this property. For example, you may want a property to only apply to a specific Snowflake connection. For Applicable asset types , select the kinds of assets you want users to be able to enrich with this property. For example, you may want a property to only apply to SQL assets like tables and views, and not to BI assets. Â (Optional) Under Glossary assets , you can configure the glossaries and glossary asset types on which this custom metadata should be visible to: For Glossaries , select the glossaries to which you want to limit users to be able to enrich assets with this property. For Applicable asset types , select the glossary assets you want users to be able to enrich with this property. For example, you may want a property to only apply to terms within a glossary, and not to categories. (Optional) Under Domain assets , you can configure the data domains, subdomains, and products on which this custom metadata should be visible to: For Domains , select the domains or subdomains to which you want to limit users to be able to enrich with this property. For Applicable asset types , select the domains, subdomains , or products you want users to be able to enrich with this property. For example, you may want a property to only apply to products within a specific subdomain, and not to the parent domain. (Optional) Under Other assets , for Applicable asset types , select assets that neither fall under the rubric of a connection or glossary   -  currently only file assets are supported. (Optional) Under Configurations toggle any extra settings for the property: Allow multiple values controls whether users can enter more than a single value for this property. (Note: this is only available for some types.) Show in filter controls whether users can filter on this property when doing asset discovery. Show in overview controls whether the property will show up in the Overview sidebar tab of assets. (All properties will show in the custom metadata's own tab, but those with this Show in overview enabled will also show in the Overview tab.) That's it, your users can now enrich assets with this custom metadata ! ð Delete properties from a structure â danger Deleting a custom metadata property will remove the values for that property from any assets. To delete custom metadata properties from a custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, select the custom metadata structure you want to change. In the properties table on the right, click the delete icon on the far right of the row containing the property to delete the property. When prompted for confirmation, click the Confirm button. Delete custom metadata structure â You can also delete an entire custom metadata structure. danger Deleting a custom metadata structure will remove all its properties and all its custom metadata values from any assets. You might want to consider using personas to hide the custom metadata , until you confirm it is no longer needed. To delete a custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, select the custom metadata structure you want to delete. In the upper right of the custom metadata structure, click the red delete icon. When prompted for confirmation, click the Delete button. View linked assets â Once users in your organization have enriched their assets with custom metadata , you will be able to view the linked assets right from the governance center. To view assets with custom metadata: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, click Linked Assets to view all the assets linked to the custom metadata. (Optional) Click any asset to open the asset sidebar for more details. Tags: data integration Previous Add custom metadata badges Next What happens when users do not have access to metadata? Create custom metadata structure Create properties in the structure Delete properties from a structure Delete custom metadata structure View linked assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/references/what-happens-when-users-do-not-have-access-to-metadata",
    "text": "Build governance Custom Metadata References What happens when users do not have access to metadata? On this page What happens when users do not have access to metadata? Users can search and discover all assets in Atlan. However, if they have not been granted permissions to act on those assets, their access will be limited. Atlan shows this limited access with a lock icon. The limitations are tied to: Whether or not the user is a connection admin Access policies The limitations are in terms of the actions that you can and cannot perform. A combination of the above two factors will usually determine these limitations. Connection admin â Connection admins manage connectivity to a data source. Even if you are a member user, as a connection admin you'll have full access to the assets from that connection.Â Any user with connection admin status will not see the lock icon for their assets in Atlan. However, there are exceptions   -  an access policy can override a connection admin's default full access. Setting access policies helps you maintain granular control over your assets in Atlan. You can define these access policies by personas and purposes . Access policies â Access policies often supersede the default permissions associated with connection admins and user roles. Access policies either allow or restrict access to certain assets. For example, even as a member user, you'll be able to add tags and terms to an asset if you're part of a persona with a metadata policy that allows this action. Guest users in Atlan can only suggest changes to asset metadata if enabled from the admin center . In fact, access policies can also be used to give users full access to certain assets without making them connection admins. User roles â Although there are default permissions associated with each user role ( admin , member , and guest ), access to assets is entirely dependent on whether the user is a connection admin or part of a persona or purpose. For example, a member user who is neither a connection admin nor part of any persona or purpose will see every single asset in Atlan with a lock icon. Tags: atlan documentation Previous Manage custom metadata structures Next What is custom metadata?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/concepts/what-is-custom-metadata",
    "text": "Build governance Custom Metadata Concepts What is custom metadata? On this page Custom Metadata Atlan provides basic metadata for assets like certification, owners, and descriptions. But since every data team has their own unique needs, Atlan allows you to extend these with your own unique properties. Custom metadata helps users understand and use data through extra context on assets in Atlan. Common, standardized context helps people quickly understand data and its background, especially your organization's business users. With custom metadata, finding and working with data has never been easier. Users can filter data and understand its context through your organization's own perspective! Examples â For example, you could add IPR as a new metadata group to capture intellectual property rights. This could include custom metadata fields like: License type -  to define the type of license under which the asset can be used. Provider -  to define the source of the asset, in cases where the license requires attribution. As another example, you might want to integrate details from a custom-built ETL processing engine. You could create an ETL metadata group, and include fields like: Job link -  to provide a link to the job run in your ETL engine. Last run date -  to show when the job that created or changed this asset last ran. Last run status -  to show whether the last run of the job was successful or not. Highlights of adding custom metadata â With custom metadata, you can: Organize custom fields in groups defined by you. Ensure accuracy of your metadata values by setting restrictions on values. For example, you can define the type of value (date, boolean), predefined options for values, and so on. Maintain standard metadata by telling the users exactly what metadata values to add for each data asset. No confusion means faster data enrichment! Did you know? Custom metadata in Atlan lets you add whatever metadata fields you need. This helps the team keep data clean with perfectly customized metadata provisions. Tags: atlan documentation Previous What happens when users do not have access to metadata? Next Update input type for existing custom metadata Examples Highlights of adding custom metadata"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/faq/change-input-type-custom-metadata",
    "text": "Build governance Custom Metadata FAQ Update input type for existing custom metadata Update input type for existing custom metadata Updating input type from SQL to text for existing custom metadata is currently not supported in Atlan. Tags: atlan documentation Previous What is custom metadata?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products",
    "text": "Configure Atlan Data Products On this page Data Products â Available via the Data Marketplace package Overview: Use Atlan's data products capabilities to organize and govern your data assets by domain. Create curated data products that help your teams discover, understand, and collaborate on data more effectively. Get started â How to create data domains Guides â Domain Management â How to create domain policies : Define access and governance policies. How to monitor data domains : Track domain usage and performance. Product Management â How to create data products : Create and manage your data products. How to add stakeholders : Assign roles and responsibilities. Concepts â What are data products : Learn about data products and their components. What is a product score : Understand how product scores are calculated. What is business lineage : Learn about business lineage in data products. Tags: data-products data-domains governance capabilities Next Create data domains Get started Guides Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-data-domains",
    "text": "Configure Atlan Data Products Get Started Create data domains On this page Create data domains Who can do this? Before you can create a data domain, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to be a domain owner or domain admin in Atlan or have access through domain policies to create and manage data domains. If you do not wish to enable the products module but still want to create domains in Atlan, refer to How to manage domains instead. Data domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization. Data domains in Atlan take true meaning from: Key stakeholders of a specific domain and their roles Data products within that specific domain Create a data domain â To create a data domain: From the left menu of any screen in Atlan, click Products . On the Products page, click Get started . For Overview , enter the basic details for your domain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your domain. For Name , enter a meaningful name for your data domain   -  for example, Customer Service . The character limit for a domain name is 80 characters. (Optional) Click the domain icon to change the icon for your domain. (Optional) For Description , enter a description for your domain. For Owners , assign additional users or groups as domain owner. In the top right of the screen, click the Create button to complete setup. Congrats on creating your data domain in Atlan! ð (Optional) Create a data subdomain â danger You will first need to create a data domain before you can add a data subdomain to it. Data subdomains help you logically segment your data domains according to business needs. To create a data subdomain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. In the upper right of your data domain page, click the + Add button and then click New sub-domain to add a data subdomain. For Overview , enter the basic details for your subdomain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your subdomain. For Name , enter a meaningful name for your subdomain   -  for example, Social Media . The character limit for a subdomain name is 80 characters. (Optional) Click the domain icon to change the icon for your subdomain. (Optional) For Description , enter a description for your subdomain. For Owners , assign additional users or groups as subdomain owner. In the top right of the screen, click the Create button to complete setup. Congrats on creating your data subdomain in Atlan! ð Your data producers can now add data products to your data domain. Update a data domain â The domain profile includes essential details about the data domain. You can also curate what your domain users will be able to view. To update a data domain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. On your data domain page, the Overview tab displays important details about the domain. (Optional) From the top right, click the + Add button and then: Click New sub-domain to add data subdomains. Click New product to add data products. danger Even as a domain owner or admin, you will need to have create, update, and delete permissions through domain policies for a specific domain or subdomain to create and manage data products. Under Summary , view a total count of data products in your domain and domain description: (Optional) Click + Add stakeholder to add stakeholders . (Optional) Click the Description field to update the description. (Optional) For Owners , click the pencil icon to add or remove owners . (Optional) If custom metadata properties are available, you can add custom metadata to your domain. (Optional) Click + Add resource to add a resource to your domain. Under Readme , click + Add to add a README to your data domain or use Atlan AI for documentation . Â From the top right of the data domain profile: Click the user avatars to view a list of recently visited users, total views on your domain, total number of unique visitors, and total views by user. Use the days filter to filter domain views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your domain and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement or a resource to your domain. Switch to the Products tab to view data products within your domain. Switch to the Statistics tab to monitor domain usage . Switch to the Lineage tab to view business lineage for your domain . Move a subdomain or product â You can move subdomains and products within and across domains to better organize your business entity. Move data products to a different subdomain or domain, or create subdomains within the same domain or across your domains in Atlan. You will need the following permissions: Moving a subdomain or product from one domain to another   - Update Domains permission on both the source and target domains. Moving a subdomain or product within the same domain   - Update Domains permission on the domain you want to reorganize. To move an existing subdomain or product: From the left menu of any screen in Atlan, click Products . In the left menu of the Products page, you can either: Drag and drop a subdomain or product into the relevant domain within the same or a different parent domain. In the popup, click Move to confirm the changes. To the right of the subdomain or product name, click the three dots icon and then click Move to . In the Move to dialog, select a relevant parent domain within the same or a different domain and then click Move to confirm the changes. Convert a subdomain to a domain â You can convert subdomains into parent domains. For example, as your organization grows, some small teams may evolve into major departments. In that case, you may want your subdomains in Atlan to better reflect your organizational architecture and convert them to domains. You will need the Update Domains permission on the subdomain and parent domain of the subdomain you want to convert. To convert a subdomain into a domain: From the left menu of any screen in Atlan, click Products . In the left menu of the Products page, to the right of the subdomain you want to convert, click the three dots icon and then click Convert to domain . In the Convert to domain dialog, click Convert to domain to confirm your changes. Archive a data domain â You can archive your data domains and subdomains when they are no longer in use. Note that Atlan does not allow archiving a domain that contains any active subdomains or products. Ensure that your domain content is inactive before you proceed. To archive a data domain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain to archive, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. From the top right of the domain or subdomain profile, click the 3-dot icon and then click Archive . From the Archive domain? dialog, click Archive to archive your data domain. Did you know? To programmatically create, update, and manage data domains using API, refer to our developer documentation . Tags: atlan documentation Previous Data Products Next Add stakeholders Create a data domain (Optional) Create a data subdomain Update a data domain Move a subdomain or product Convert a subdomain to a domain Archive a data domain"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/add-stakeholders",
    "text": "Configure Atlan Data Products Stakeholder Management Add stakeholders On this page Add stakeholders Who can do this? Before you can create a stakeholder, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to have update permissions through domain policies for the specific domain(s) or subdomain(s) to create and manage stakeholders. Stakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams. For example, for the data domain Customer Service , you may want to define a Customer Service Manager stakeholder and assign it to the people who serve that function. This way, your customer support team will know whom to contact for questions or escalate any issues. Once you have created stakeholders or want to use the default options from Atlan: You can add stakeholder information for any and all domains. This will help you provide additional metadata for your domain, but does not help enforce access control currently. For the latter, you will need to create domain policies . You can assign any user as a stakeholder within a specific domain. Each user can be mapped to only one responsibility within a domain. However, users can have multiple stakeholder responsibilities across multiple domains. Create a stakeholder â Atlan provides you with predefined stakeholders for your data domains. You can either use the default options and assign them to your users or create new definitions for stakeholders based on your business needs. To create a stakeholder: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , you can either: If you have enabled the products module , click Domains & products . If you have not enabled the products module,Â click Domains . On the Domains & products or Domains page, change to the Settings tab. Under Stakeholders , you can view the default stakeholders   -  domain owner, data engineer, data product owner, and data architect. To the right of each responsibility, click the right-facing arrow to view the domains it applies to and users assigned to that responsibility. You can neither delete nor make any changes to the default options. To create a new stakeholder, click Add . In the Add responsibility dialog, enter the following: For Name , enter a meaningful name for your stakeholder. Atlan recommends following a naming convention for responsibilities across your domains to help users find stakeholders more easily. For Applies to , click the dropdown to select domains that this responsibility should apply to: To select All domains , you must have update permissions on all data domains in Atlan. To select any one specific domain or multiple domains, you must have update permissions for the specific data domain(s). Add a stakeholder â You can add stakeholders directly from the domain or subdomain profile. To add a stakeholder to your data domain or subdomain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. From the domain or subdomain profile, under Stakeholders , click + Add stakeholder . In the right Stakeholders pane, click + Stakeholders . In the Add Stakeholders dialog, configure the following: For Users or groups , select the users or groups that the stakeholder responsibility should apply to. Click the stakeholder dropdown to select a default stakeholder, a custom one you created, or click Create new to create a new one. Click Add to add the stakeholder to the domain or subdomain. Tags: atlan documentation Previous Create data domains Next Create data products Create a stakeholder Add a stakeholder"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-data-products",
    "text": "Configure Atlan Data Products Product Management Create data products On this page Create data products â Available via the Data Marketplace package Who can do this? Before you can create a data product, you need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You need to have create, update, and delete permissions through domain policies for the specific domain or subdomain to create and manage data products. Data products help your data consumers easily discover and work with data assets. As you get started, here are some questions to consider: What use case(s) are you trying to solve as an organization? How to define a common vocabulary and approach for creating data products and ensuring interoperability across domains? How to design data products to power discovery and drive usage among data consumers? Data products in Atlan can be highly adaptable to the needs of your organization. Create a data product â You can either create a data product from the products module or lineage graph. To create a data product, complete these steps. From the products module: â From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the See all button to view more domains. In the upper right of your data domain or subdomain page, click the + Add button, and then from the dropdown, click New product to add a new data product. From the lineage graph: â From the left menu of any screen in Atlan, click Assets . (Optional) In the Filters menu on the left, expand the Properties menu and then click Has lineage to filter for assets with data lineage. Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph. On the lineage graph, select an asset to create a data product. Atlan will only include assets that are visible on the lineage graph. To include more assets: (Optional) Hover over the + button to the right of any asset and then click the Expand all button to include assets further upstream or downstream horizontally in the data product. (Optional) Click Show all to include assets further upstream or downstream assets vertically in the data product. From the top right of the lineage graph, click the box with plus icon for data products to create a data product from the lineage graph. In the New product via lineage form, configure the following: For Add assets , any assets that are visible on the lineage graph will be automatically included in the data product. Note that process , child, and partial assets are currently not supported for data product creation from the lineage graph. You can either keep all the asset selections or deselect any assets. (Optional) The asset you had selected on the lineage graph will be automatically set as an output port. You can keep that selection, click Output port to remove the current selection, or click Mark as output port on any other assets to set additional output ports. Click Continue to proceed. Provide details â To provide details: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. For Name , enter a meaningful name for your data product   -  for example, Social Media Marketing . The character limit for a product name is 80 characters. For Domain , select a data domain from the dropdown   -  for example, Customer Service . (Optional) For Description , enter a description for your domain. (Optional) For Criticality , select a level of business criticality from the dropdown   -  choose from High , Medium , and Low . (Optional) For Sensitivity , assign a data sensitivity level from the dropdown   -  choose from Public , Internal , and Confidential . (Optional) For Owners , assign additional users or groups as data product owner. (Optional) For Visibility , select who can access and monitor the data product throughout its entire lifecycle: Private to members of this domain -  only members of a specific domain can access the data product. Private to selected members -  only members of a specific domain and other selected users or groups can access the data product. Public -  everyone in the organization can access the data product. If creating a data product from the products module, in the top right of the screen, click the Continue button. If creating a data product from the lineage graph, at the bottom of the form, click the Continue button and skip to the Review the data product section. Add assets â To select assets to include in the data product, you can select via the asset browser or using filters. Add via browser â Click the checkbox to select individual assets to include in your data product. (Optional) Use the search bar to search for assets by the technical name of the asset. (Optional) Filter assets by specific asset types. Click the 3-dot icon to view more asset type filters. (Optional) Click the Show: all dropdown and change to Selected assets to only view your asset selection. (Optional) Click the All filters dropdown, and then from the All filters pane: Click Hierarchy to filter assets by connection, database, and schema. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported tags . Click Terms to filter assets by linked terms . Click Properties to filter assets by common asset properties . In the top right of the screen, click the Continue button. Add via rules â Under Select assets , click Add via filters to add assets to your data product using asset filters. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select a relevant option: Click Connection and then select an existing connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported tags . Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View assets for a preview. In the top right of the screen, click the Continue button. (Optional) Select output ports â Did you know? Output ports determine the relationships between your data products. These relationships are visually represented as business lineage . To select output ports: From the list of assets, select output port(s) to allow your data consumers to consume the data product across domains. These assets will serve as the consumption layer for your data product. For Input ports , Atlan displays a total count of input ports for your data product. These assets are designated as output ports in other data products, and serve as input ports for your data product. Click View assets to view all input port assets. In the top right of the screen, click the Continue button. Review the data product â Once you have reviewed your data product, you can either: Click Save as draft to save your changes in a draft version and publish when ready. Only you and any other users you add as owners to the product will be able to search for, view, and edit your draft products, depending on their permissions . Click Create and publish to publish it immediately. Congrats on creating a data product in Atlan! ð You can also use governance workflows to govern the creation and change in status of data products. Update a data product â Did you know? You can move your data products within and across subdomains or domains to reorganize them as needed. Once you have created a data product, you will also need to monitor and manage it during its entire lifecycle. This helps ensure that the data product stays fresh, up-to-date, and trustworthy. The data product profile in Atlan allows you to curate how your users can use the data product. To update a data product: From the left menu of any screen in Atlan, you can either: Click Products . To select a data product, you can: From the navigation menu on the Products homepage, use the search bar or expand the relevant domain or subdomain. In the Data products section, select a trending or recently viewed data product. The list of Trending products is sorted by the total count of views on each product, with the most viewed product listed at the top. From the top right of any screen in Atlan, click the star icon . From the Starred assets popup, select a starred data product. From the left navigation menu or Products homepage, click My drafts to continue working on your draft products. Products in draft mode are only visible to product owners until these are published. If your Atlan admin has enabled the Show products in asset discovery toggle , click Assets to search for data products from asset discovery . On your data domain page, next to the Overview tab, click the Products tab and select your data product of interest. (Optional) From the top right of the product profile, click the Published dropdown to update the status of your data product: Draft -  data product is only visible to product owners. Published -  data product is published for users to consume. Sunset -  data product is planned for retirement. Archived -  data product is archived and no longer visible to users. To restore an archived data product, click the Restore product button and then click Restore . Atlan will restore the archived data product and it will reappear in product and asset discovery, domain profile, and product lineage. Under Summary , view details about the data domain your data product belongs to, including criticality, sensitivity, and freshness. (Optional) Click the pencil icon to update Criticality to signify business impact: High -  high business impact _Medium   - _ moderate business impact Low -  internal or a non-business impact (Optional) Click the pencil icon to update Sensitivity to assign a data classification: Public -  may be freely accessible Internal -  may only be distributed within the organization Confidential -  may only be limited to a specific domain or team within the organization (Optional) Click Add resource to add a resource to your asset. Under Product Score , view a scorecard for your data product, calculated based on metadata completeness. Under At A Glance , view a total count of linked assets and output ports. Under Readme , click + Add to add a README to your data product or use Atlan AI for documentation . Under Details , you can view and update metadata for your data product   -  including visibility, terms , owners , tags , certificates , and custom metadata . You can also set up playbooks to update product metadata in bulk. Switch to the Assets tab to view linked assets. (Optional) Click the Edit button to add or remove assets from your data product. (Optional) Select an asset to view its asset profile in a sidebar. (Optional) Filter assets by asset types   -  for example, use the Table filter to view table assets only. (Optional) Click Disable as output port to remove an output port or click Set as output port to set an asset as an output port. Switch to the Lineage tab to view business lineage for your data product. Hover over any data product to view the metadata popover for more context. Click the view output ports menu to view output ports for your data product. In the upper right of the lineage graph, click the downward arrow to download an image of the product lineage graph. Switch to the Activity tab to view the activity log for your data product. View details about changes made to the data product and filter for specific types of metadata changes . View top and recent users for your data product. View a list of data producers for your data product. Switch to the Contracts tab to view any linked contracts for the output ports in your data product. From the top right of the data product profile: Click the user avatars to view a list of recently visited users, total views on your asset, total number of unique visitors, and total views by user. Use the days filter to filter asset views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your data product and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement or a resource to your data product. If you have enriched your draft products with terms or tags , your draft products will be visible to other users as linked assets when viewed from the term or tag profile, respectively. However, only a product owner with the requisite permissions to update the product can make any changes to the draft product. Did you know? To programmatically create, update, and manage data products using API, refer to our developer documentation . Tags: lineage data-lineage impact-analysis Previous Add stakeholders Next Create domain policies Create a data product Update a data product"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-domain-policies",
    "text": "Configure Atlan Data Products Domain Management Create domain policies On this page Create domain policies Who can do this? Before you can create a domain policy, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to be a domain admin in Atlan to create and manage domain policies. Domain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more. In Atlan, you can define domain policies for data domains through personas . This framework allows you to ensure that your data products are secure and only accessible to the individuals or teams involved in managing the data. Create a persona â To create a persona: From the left menu of any screen, clickÂ Governance . UnderÂ Access Control , clickÂ Personas . If this is the first persona, click the Get started button. Otherwise click the New persona button. Enter a meaningful name for the persona, (optional) a description, and then click Create . You now have an empty persona. It won't do much until you complete the next steps, too! Add users and groups â To add users and groups to the persona, from within the persona: To the right of theÂ Users and groups box, click theÂ Add button. Select users and / or groups: Under the single-user icon, select users to add to the persona. Under the double-user icon, select groups to add to the persona. Click theÂ Update button to save the users and groups to the persona. Now this persona will be available to those users and groups. It still won't do much, though, without some policies... (Optional) Add rich documentation â To add rich documentation describing the persona: UnderÂ Summary , thenÂ Channels , add any Slack channels relevant to the persona. UnderÂ Resources ,Â add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive   -  anything that has a URL. Under Readme , write a richly-formatted description of the persona. Add a domain policy â To add a domain policy to the persona, from within the persona: Change to theÂ Policies tab. Click theÂ New Policy button and choose the type of policy. ChooseÂ Domain policy . Under Name , briefly describe the policy's intention. UnderÂ Select domains , choose the domain on which to apply the policy   -  for example, Finance . (Optional) For Configure permissions , choose the permissions the policy will grant. By default, all permissions will be granted. To select others: To the right ofÂ Configure permissions , click theÂ Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. You can enable or limit the following: Read : permission to view metadata, resources, and READMEs for data domains Update Domains : permission to update metadata, resources, and READMEs for data domains Create Sub-domains : permission to create new data subdomains within a domain Update Sub-domains : permission to update metadata, resources, and READMEs for data subdomains Create Products : permission to create new data products within a domain Update Products : permission to update metadata, resources, and READMEs for data products Delete Products : permission to delete data products within a domain Update Custom Metadata For Domains : permission to update custom metadata for data domains Update Custom Metadata For Sub-Domains : permission to update custom metadata for data subdomains Update Custom Metadata For Products : permission to update custom metadata for data products At the bottom of the list, click Save . At the bottom of theÂ Domain policy sidebar, clickÂ Save . (Optional) Set preferences â Did you know? You can also personalize the details users will see in the sidebar or filters menu when in a persona. This is great to limit information overload, by showing only what is relevant to a given set of users. To set preferences for the persona: Change to theÂ Preferences tab of the persona. From the left menu, configure the following: To set the default landing page forÂ the persona: Click Navigation to view landing page preferences. For Set default landing page , click the dropdown and then click Products so that your users land on the data products page when they log into Atlan. Â To limit the out-of-the-box tabs that should be visible to the persona: Click Asset sidebar to view asset sidebar preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the asset filters that should be visible to the persona: ClickÂ Asset filters to view asset filter preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the custom metadata that should be visible to the persona: Click Custom metadata to view custom metadata preferences. Click the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the persona. To personalize your data products, click Products and then click the Add cover image button to set a cover image for data products within your selected domain. Tags: atlan documentation Previous Create data products Next Monitor data domains Create a persona Add users and groups (Optional) Add rich documentation Add a domain policy (Optional) Set preferences"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/concepts/what-is-business-lineage",
    "text": "Configure Atlan Data Products Concepts What is business lineage? On this page What is business lineage? Who can do this? Before you can explore business lineage, you will need your Atlan admin to enable the products module in your Atlan workspace . Business lineage is a representation of relationships between your data products in Atlan. This simplified view of product lineage can help you quickly grasp the provenance and relationships of your data products and draw links in the chain of your asset universe. As a map to your data estate, business lineage can help: Data producers understand how their data products are used within the organization. Data consumers gain visibility into the provenance of the data products they use. Both producers and consumers better understand the business relevance and impact of data. View product lineage â danger Atlan currently does not support generating lineage for products with more than 30,000 assets. This is to ensure that the lineage generation workflows are optimized to run efficiently. Output ports determine the relationships between your data products. For example, if your data product includes assets that are designated as output ports in another data product: within the same domain or subdomain, across different subdomains within the same domain, or across entirely different domains, ...this relationship will be visualized as business lineage. The assets designated as output ports in other data products serve as input ports for your data product , automatically linking them based on your asset selection. Note that you cannot set input ports manually. You can, however, designate output ports while creating a data product . Processing lineage between data products involves running cron jobs every hour. For example: For products with <10k assets, lineage appears in 15-20 minutes. For products with >10k assets, lineage may take up to 1 hour to be processed. Supported scenarios â In addition to the scenarios listed above, product lineage will be captured when: Creating products with shared assets: Product 1 â Product 2: If Product 1 is created with Asset 1 as an output port, and Product 2 is consequently created with Asset 1 as a regular asset, lineage will be generated from Product 1 to Product 2 after processing has been completed. Product 2 â Product 1: If Product 1 is created with Asset 1 as a regular asset, and Product 2 is later created with Asset 1 marked as an output port, lineage will be generated from Product 2 to Product 1. Lineage will be generated in either scenario whether products are created via the browse option or rules . Removing or modifying assets: If Asset 1 is removed from Product 2, lineage between Products 1 and 2 will be removed after processing. If Asset 1 is unmarked as an output port in Product 1, lineage will be removed between the two products. If either Product 1 or 2 is archived, lineage will be removed. Dynamic asset listing: For example, if Product 1 is created with Asset 1 (output port) and Product 2 is created with Asset 2 (regular asset) using a tag-based rule (for example, all assets with the PII tag), then adding a PII tag to Asset 1 will generate lineage from Product 1 to Product 2. This is also applicable to data product creation with term-based rules and more. To view product lineage: From the left menu of any screen in Atlan, click Products . To select a data product, you can either: From the left navigation menu on the Products homepage, use the search bar or select the relevant domain and then select a data product. In the Explore products section on the Products homepage, select a recently viewed, starred, or new data product. From the tabs along the top of your product page, click the Lineage tab. On the lineage graph, the home icon indicates the base product. (Optional) Hover over any data product to view a metadata popover for more context   -  including domain name, criticality and sensitivity levels , owner, and product score . You can also open lineage in a new tab. (Optional) For any applicable product, click the view output port menu to view output ports: The default view shows 10 output ports. Click Show more to view the full list of output ports. Hover over an output port in the list and then click the upward arrow to view lineage for that output port asset in a new tab. (Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow. (Optional) From the top right of the lineage graph: Click the Find in canvas search bar to search for any specific products on the lineage graph. Click the downward arrow to download product lineage as an image. Click the eye icon to set preferences for the lineage graph: For Additional metadata , show or hide the following context for your assets   -  domain name, announcements , or sensitivity and criticality levels. For Line arrows , show or hide the arrows that indicate data flows on the lineage graph. Click the question mark icon to share feedback. (Optional) From the bottom right of the lineage graph: Click the minimap icon to view an abridged version of the lineage graph. Click the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base product. Click the fullscreen icon to expand the lineage view to fullscreen mode. Click the minus or plus icons to zoom out or zoom in on the lineage graph, respectively. View domain lineage â Domain lineage is a visual representation of the relationships between your domains and subdomains connected through data products. It can serve as a map to your business domain, helping you understand how data products are used within a specific domain. For example, if there are subdomains that do not contain any products, this comprehensive view will help you target those subdomains for deprecation. Or, if a product is used across multiple subdomains, this may signal the need for more robust measures for change management. When viewing domain lineage, you can view the hierarchy of subdomains and products within a specific domain. To view domain lineage: From the left menu of any screen in Atlan, click Products . To select a data domain , you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. From the tabs along the top of your domain page, click the Lineage tab. On the lineage graph, the home icon indicates the base domain. (Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow. (Optional) From the top right of the lineage graph: Click the Find in canvas search bar to search for any specific products on the lineage graph. Click the downward arrow to download product lineage as an image. Click the plus icon to expand all domain nodes to view data products.Â Hover over a data product in the list and then click the upward arrow to view lineage for that product in a new tab. Click the eye icon to set preferences for the lineage graph. For Additional metadata , show or hide the following context for your assets   -  domain type and announcements . Click the question mark icon to share feedback. (Optional) From the bottom right of the lineage graph: Click the minimap icon to view an abridged version of the lineage graph. Click the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base product. Click the fullscreen icon to expand the lineage view to fullscreen mode. Click the minus or plus icons to zoom out or zoom in on the lineage graph, respectively. Did you know? Archived products and domains are removed from business lineage. However, for any archived asset that may have been designated as an output port in active data products, the lineage links created by that asset will be visible on the lineage graph. Tags: lineage data-lineage impact-analysis Previous Monitor data domains Next What is a product score? View product lineage View domain lineage"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/concepts/what-is-a-product-score",
    "text": "Configure Atlan Data Products Concepts What is a product score? On this page What is a product score? How can Atlan help you build trust in your data products? Product scores! Based on the principles of data as a product, product scores can help you signal the accuracy and completeness of your data products , helping build trust in them. The product scorecard in Atlan enables you to: Identify and prioritize data quality issues Quantify the business impact of data quality Drill down for detailed analysis and take action Share insights across stakeholders The product score is updated daily at 4:00 AM UTC. You can hover over the relative time in the product scorecard to view the timestamp for when any metadata attribute of the product was last updated in Atlan. Did you know? You can also choose to hide the product score from your data products. Contact your Atlan admin to turn off the product score from the admin center. Components of a product score â Atlan calculates and assigns a product score to your data products based on a preset criteria of metadata completeness. Atlan evaluates metadata enrichment on the data product, output ports, or a combination of both. Using a weighted scoring method, values are automatically assigned on the basis of how well your data product satisfies the six principles of data as a product. Atlan scores your data product on a scale of 0-5, with 0 being the lowest and 5 being the highest score. The six principles of data as a product are: Discoverable : The discoverability of a data product is based on the availability of aggregate metadata. Atlan quantifies discoverability in the form of glossary terms linked to the data product and output ports. Understandable : A data product is considered to be understandable when it has defined and curated contextual metadata to help your data consumers understand the product better. Atlan quantifies how understandable a product is in the form of: A description or README added to the data product, or READMEs added to output ports. Addressable : A data product is considered to be addressable when it has well-documented owners and points of contact. Atlan quantifies how addressable a data product is in terms of owners assigned to the data product and its output ports. Secure : A secure data product will clearly signify the sensitivity of data. Atlan quantifies how secure a data product is on the basis of sensitivity classifications on the data product and tags attached to its output ports. Interoperable : Interoperability of a data product is determined on the basis of the visibility and completeness of technical lineage between data assets . Trustworthy : Trustworthiness of a data product is determined on the basis of certificates and data contracts attached to the data product. A product score is not set in stone, and will change depending on the completeness of metadata enrichment. The score can help you understand how to improve your data product to make it more useful for your data consumers. Atlan currently does not allow you to either configure the score or define your own criteria. Scoring rubric â Atlan adheres to the following scoring rubric to assign product scores: Principle 0 1 2 3 4 5 Discoverable No terms linked to product - - - - At least 1 term linked to product Understandable No description on product or output ports AND no README on product Description on product AND â¤ 10% of output ports have descriptions but no READMEs OR No description on product or output ports AND product has a README OR No description or README on product AND 11%-40% output ports have descriptions Description on product AND 11%-40% output ports have descriptions but no READMEs OR Description on product AND < 10% of output ports have descriptions AND product has a README OR No description or README on product AND 41%-70% output ports have descriptions Description on product AND 41%-70% output ports have descriptions but no READMEs OR Description on product AND 11%-40% output ports have descriptions AND product has a README OR No description or README on product AND 91%-90% output ports have descriptions Description on product AND 71%-90% output ports have descriptions but no READMEs OR Description on product AND 41%-70% output ports have descriptions AND product has a README OR No description or README on product AND >90% output ports have descriptions Description on product AND >90% output ports have descriptions but no READMEs OR Description on product AND 71%-90% output ports have descriptions AND product has a README Addressable No owners on product and output ports OR Product does not have an owner AND â¤ 10% of output ports have owners Product has an owner AND â¤ 10% of output ports have owners OR Product does not have an owner AND 11%-40% output ports have owners Product has owners AND 11%-40% output ports have owners OR Product does not have an owner AND 41%-70% output ports have owners Product has owners AND 41%-70% output ports have owners OR Product does not have an owner AND 71%-90% output ports have owners Product has owners AND 71%-90% output ports have owners OR Product does not have an owner AND >90% output ports have owners Product has owners AND >90% output ports have owners Secure Product does not have sensitivity classification AND no tags attached to output ports OR Product does not have sensitivity classification AND â¤ 10% of output ports have tags attached Product has sensitivity classification AND no tags attached to output ports OR Product does not have sensitivity classification AND 11%-40% output ports have tags attached Product has sensitivity classification AND 11%-40% output ports have tags attached OR Product does not have sensitivity classification AND 41%-70% output ports have tags attached Product has sensitivity classification AND 41%-70% output ports have tags attached OR Product does not have sensitivity classification AND 71%-90% output ports have tags attached Product has sensitivity classification AND 71%-90% output ports have tags attached OR Product does not have sensitivity classification AND >90% output ports have tags attached Product has sensitivity classification AND >90% output ports have tags attached Interoperable None of the output ports have technical lineage â¤ 10% of output ports have technical lineage 11%-40% of output ports have technical lineage 41%-70% of output ports have technical lineage 71%-90% of output ports have technical lineage > 90% of output ports have technical lineage Trustworthy Product neither has a certificate nor a contract - - Product does not have a certificate but has a contract OR Product has a certificate but no contract - Product has a certificate and contract Scoring method â To calculate the product score, Atlan uses a weighted scoring method: Assign scores for each principle   -  Atlan assigns a score based on each of the principles to determine a product score within the range of 0-5. Set weights for each principle   -  Atlan determines the weight of each principle based on the completion rate of metadata enrichment. Calculate weighted scores   -  the score for each principle is multiplied by its weight. For example, if Trustworthy has a score of 4 on a scale of 5 and a weight of 30%, the weighted score would be 4 * 0.3 = 1.2 . Sum up the weighted scores   -  Atlan adds up the weighted scores for each principle to arrive at a total score, which is displayed on the data product. Interpret the score   -  you can use the total weighted score to evaluate the data product's overall alignment with the principles of data as a product. A higher score will indicate closer alignment with the principles and metadata completion. Tags: atlan documentation Previous What is business lineage? Next What are data products? Components of a product score Scoring rubric Scoring method"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/how-tos/create-a-persona",
    "text": "Configure Atlan Access control Get started Create persona On this page Create persona Who can do this? You must be an admin user to create personas. To create a persona: From the left menu of any screen, click Governance . Under Access Control , click Personas . If this is the first persona, click the Get started button. Otherwise click the New persona button. Enter a meaningful name for the persona, (optional) a description, and then click Create . You now have an empty persona. It won't do much until you complete the next steps, too! Add users and groups â To add users and groups to the persona, from within the persona: To the right of the Users and groups box, click the Add button. Select users and / or groups: Under the single-user icon, select users to add to the persona. Under the double-user icon, select groups to add to the persona. Click the Update button to save the users and groups to the persona. Now this persona becomes available to those users and groups. It still doesn't do much, though, without some policies... Add rich documentation (optional) â To add rich documentation describing the persona: Under Summary , then Channels , add any Slack channels relevant to the persona. Under Resources , add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive â anything that has a URL. Under Readme , write a richly-formatted description of the persona. Add policies â For the persona to really do anything, you need to define one or more policies. Repeat the following steps for each set of assets and permissions you want to control through the persona. Did you know? The higher level at which you can define the assets, the better. For example, if you create a policy at a database level, all future schemas and tables (in that database) are also covered by the policy. To add policies to the persona, from within the persona: Change to the Policies tab. Click the New Policy button and choose the type of policy. Add a metadata policy â Add a metadata policy to grant or restrict permissions to change metadata. You can also control access to data quality rules through metadata policies. For more information about data quality rules, see data quality rules . Follow these steps to set up a metadata policy: Choose Metadata Policy . Under Name , briefly describe the policy's intention. Under Select a connection , choose the connection on which to apply the policy. (Optional) For Asset selector , choose the assets the policy controls. By default, all assets in the connection are included. To select others: In the All assets box, click the x . Under Asset selector , click the Add link. To search for and select the assets to control with the policy, in the Add Assets dialog: Click Browse to search and select assets from all databases in the connection. Click Search to search from and select individual assets in the connection. Click Custom to search and select assets by their qualified name. Click Save to confirm your selections. (Optional) For Configure permissions choose the permissions the policy grants . By default, all permissions are granted. To select others: To the right of Configure permissions click the Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. At the bottom of the list, click Save . (Optional) For Deny selected permissions choose whether you want to explicitly deny these permissions. danger If enabled, this overrides all grants from any other policies for the same users. At the bottom of the Metadata Policy sidebar, click Save . Did you know? When using the custom asset selector for metadata and data policies, you can add /* after the database name to select all the schemas in that database. Add a data policy â To grant or restrict permissions to query or preview data: Choose Data Policy . Under Name , briefly describe the policy's intention. Under Select a connection , choose the connection on which to apply the policy. (Optional) For Asset selector , choose the assets the policy controls. By default, all assets in the connection are included. To select others: In the All assets box, click the x . Under Asset selector , click the Add link. To search for and select the assets to control with the policy, in the Add Assets dialog: Click Browse to search and select assets from all databases in the connection. Click Search to search from and select individual assets in the connection. Click Custom to search and select assets by their qualified name. Click Save to confirm your selections. (Optional) For Deny Query choose whether you want to explicitly deny the ability to query and preview data on these assets. danger If enabled, this overrides all grants from any other policies for the same users. At the bottom of the Data Policy sidebar, click Save . Add a glossary policy â To grant or restrict permissions to change glossary contents: Choose Glossary Policy . Under Name , briefly describe the policy's intention. Under Select glossary , choose the glossary or glossaries on which to apply the policy. (Optional) For Configure permissions choose the permissions the policy grants . By default, all permissions are granted. To select others: To the right of Configure permissions click the Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. At the bottom of the list, click Save . At the bottom of the Glossary Policy sidebar, click Save . Set preferences (optional) â Did you know? You can also personalize the details users see in the sidebar or filters menu when in a persona. This is great to limit information overload, by showing only what's relevant to a given set of users. To set preferences for the persona: Change to the Preferences tab of the persona. From the left menu, configure the following: To set the default landing page for the persona: Click Navigation to view landing page preferences. For Set default landing page , click the dropdown and then select the page where your users land when they log into Atlan. (See also What will be the default landing page for users with two or more personas? ) Keep Home as the default selection for homepage. Click Assets to direct your users to asset search and discovery. Click Glossary to direct your users to glossaries. Click Insights to direct your users to the query editor. Click Products to direct your users to the homepage for data products. Click Custom to set a custom path of your choice. For Atlan page link , specify a path to the page in Atlan you want your users to land on and make sure it's available to users in the persona. Click Save to save your preference. You must turn off View \"All assets\" in Assets Discovery from Labs in the admin center to make sure that users only land on your preselected page. Complete the steps in How to restrict asset visibility to do so. To limit the asset types that are visible to the persona: Click Asset types to view asset type preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the out-of-the-box tabs that are visible to the persona: Click Asset sidebar to view asset sidebar preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the asset filters that are visible to the persona: Click Asset filters to view asset filter preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To set the default asset name that's visible to the persona: Click Asset name to set display preferences for asset name. Click the Prefer display name over technical name checkbox to display the display name , if added, over the technical name for the persona. Uncheck the box to display the technical name instead. To limit the custom metadata that's visible to the persona: Click Custom metadata to view custom metadata preferences. Click the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the persona. To personalize your data products , click Products and then click the Add cover image button to set a cover image for data products within your selected domain. Tags: atlan documentation Previous Create groups Next Create purpose Add users and groups Add rich documentation (optional) Add policies Set preferences (optional)"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/monitor-data-domains",
    "text": "Configure Atlan Data Products Domain Management Monitor data domains On this page Monitor data domains Who can do this? Before you can monitor domains, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to be a domain owner or admin in Atlan to track domain activity and usage. The Statistics tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more. Summarize data products â The Summary section provides a high-level overview of your domain and data products. To summarize data products: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics tab. The Summary section provides you with a high-level overview of data product metrics: Under Product summary , you can view a total count of data products and output ports as well as information about domains and subdomains. Under Products by status , you can view a total count of products grouped by status: Active -  data products active for users to consume. Sunset -  data products planned for retirement. Archived -  data products archived and no longer available to users. Review enrichment completion â The Activity section displays a total count of data products grouped by type of metadata enrichment. You can also view the total count of products that need to be updated   -  for example, products without a description. To review metadata enrichment for data products: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics Â tab. Under Products by enrichment , navigate to the card you want to view   -  for this example, we'll select the With Description card. (Optional) In the With Description card, click the products remaining button to view all the products without a description in the sidebar. Track product creation over time â The Products created over time graph provides visual insights into your data product creation process. To track product creation over time: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics Â tab. Under Products created over time , view a time graph for product creation within a specific domain. Monitor domain usage â The Usage tabÂ allows you to track and monitor domain usage. You can also view top domain visitors in this section. To track domain usage: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics Â tab. Under Usage , you can either: Track views over time for your data products   -  total views and views by date. View a set of top domain visitors. Tags: lineage data-lineage impact-analysis Previous Create domain policies Next What is business lineage? Summarize data products Review enrichment completion Track product creation over time Monitor domain usage"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/disable-user-activity",
    "text": "Configure Atlan Administration Feature Management Disable user activity On this page Disable user activity Who can do this? You will need to be an admin user in Atlan to disable user activity on asset profiles. You can view recently visited users and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps. Disable asset profile visitors â To disable asset profile visitors for your assets in Atlan: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Assets heading of the Labs page, turn off Asset profile visitors . Your users will not be able to view user activity on your assets in Atlan! ð If you'd like to enable user activity, follow the steps above and then turn it on. Tags: atlan documentation Previous Allow members to view reports Next How to enable associated terms Disable asset profile visitors"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/create-announcements",
    "text": "Configure Atlan Integrations Communication SMTP and Announcements Create announcements On this page Create announcements Adding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions. What type of announcements would you want to share with your team? Here are a few examples: Add an announcements â To add an announcement to an asset: From the left menu of any screen in Atlan, click Assets . On the Assets page, select an asset to add an announcement. You can either: Open the asset profile. From the top right of the Overview tab in the asset profile, click the vertical 3-dot Â icon, and then from the dropdown, click Add announcement . In the Overview tab of the asset sidebar, click the horizontal 3-dot icon, and then from the dropdown, click Add announcement . In the New Announcement dialog, enter the following details: From the top right, click the downward arrow Â and choose from three announcement types: Information , Issue , or Warning . For Add title , enter a title for your announcement. For Description , enter a description for your announcement. (Optional) You can include HTML hyperlinks to direct users to additional information   -  for example, wrap the text with <a href=\"https://my.url.com\">description text</a> . (Optional) You can use Markdown syntax to write a description. Any headings will be rendered in heading 6 ( <h6> ). danger Atlan currently does not support adding images to your announcements. (Optional) To share your announcement on Slack or Teams, you can either: Click the Share buttonÂ to integrate Slack or Microsoft Teams . If you have already integrated Slack or Microsoft Teams , click the checkbox for Share on Slack or Share on Teams , respectively. (Optional) To configure the announcements channel for Slack or Teams: If you have already configured a Slack or Microsoft Teams channel to receive announcement alerts, that channel will be preselected. You can change to a different channel, if available. If you have not configured a channel for announcements, enter the channel name to receive notifications for announcements. This channel will be displayed as the Announcements channel Â in yourÂ Slack or Microsoft Teams integration. Click Add to create your announcement. (Optional) To edit an announcement, from the top right of the announcement box, click the 3-dot icon and then click Edit . (Optional) To delete an announcement, from the top right of the announcement box, click the 3-dot icon and then click Delete . You just created an announcement! ð This announcement will be visible to anyone who views the asset. You can also create similar announcements for other types of data assets, including glossaries, categories, and terms . To create, remove, and manage announcements using API, refer to our developer documentation . Did you know? You can only create one announcement per asset. To add more information to your announcement, you can either edit the existing one to update it or delete the old one and create a new announcement. Tags: data integration Previous Configure SMTP Next Manage system announcements Add an announcements"
  },
  {
    "url": "https://docs.atlan.com/tags",
    "text": "Tags A â access-control 19 administration 1 agreements 1 AI 2 AI agents 1 air-gapped 1 airflow 1 aiven 1 alerts 9 alteryx 2 always on 1 amazon 5 amazon-athena 1 amazon-s3 4 analysis 1 analytics 8 announcements 2 anomalo 1 apache 3 api 72 app 6 architecture 1 assests 1 asset-profile 5 assets 3 astronomer 1 atlan 133 Atlan MCP 8 atlan-ai 2 atlas 1 attributes 1 authentication 53 auto-re-attachment 2 automation 25 aws 4 aws lambda 1 azure 4 B â bigid 4 bigquery 1 browse 1 browser-extension 3 business intelligence 15 business-terms 17 C â calculation-view 1 capabilities 10 cassandra 1 catalog 7 categorization 1 cdi 4 classification 1 Claude 1 cloud 1 cloudera 2 collaboration 4 communication 2 compliance 1 composer 1 concepts 1 configuration 19 confluent 2 connect 2 connections 1 connectivity 64 connector 63 connectors 299 container-images 1 contracts 1 cosmosdb 1 cratedb 6 crawl 227 crawling 1 credentials 1 crm 1 cross-workspace-extraction 1 Cursor 1 custom metadata 1 D â dagster 4 dapr 1 dashboards 6 data 255 data assets 1 data factory 1 data integration 6 data lake 1 data quality 20 data transformation 1 data warehouse 4 data-catalog 6 data-domains 1 data-flow 1 data-lineage 60 data-modeling 1 data-models 1 data-products 1 data-sources 13 data-transfer 1 database 21 databricks 6 datastax 1 dbt 1 definitions 18 dependencies 9 deployment 4 dimensions 1 discovery 9 docker 1 documentation 117 domains 1 domo 1 downstream-impact 8 dynamodb 1 E â ecc 1 embedded 1 enrichment 1 erd 1 erp 6 etl 7 etl-tools 10 event hubs 1 F â faq 33 faq-administration 2 faq-automation 2 faq-connections 6 faq-connectors 1 faq-discovery 1 faq-governance 6 faq-insights 9 faq-integrations 17 faq-lineage 16 faq-metadata 1 faq-platform 2 faq-security 1 faq-support 1 firewall 1 fivetran 1 G â gcp 2 gcs 3 get-started 3 glossary 18 glue 1 google 2 google-gcs 3 governance 18 graphql 25 groups 1 guides 1 H â helm 1 help 1 hive 1 hosted 1 how-to 1 hybrid bi 5 I â ibm cognos 1 identity management 3 impact-analysis 58 impala 2 informatica 6 insights 1 integration 123 integrations 14 inventory-reports 1 J â jira 2 K â kafka 5 kubernetes 1 L â lambda 1 lineage 76 logic 1 logs 1 looker 1 M â matillion 2 messaging 6 metabase 1 metadata 12 metadata-extractor 1 metrics 2 microsoft 4 Microsoft Copilot Studio 1 microsoft teams 1 microstrategy 1 migration 1 mode 1 model 13 mongodb 1 monitoring 9 monte carlo 1 msk 1 multiple-concatenation 1 mwaa 1 mysql 1 N â n8n 1 network 1 nosql 3 notifications 9 O â oauth 1 observability 3 offline 1 on-premises 1 openlineage 6 opentelemetry 1 operations 1 oracle 1 orchestration 12 organization 2 otlp 1 P â parsing 1 permissions 20 playbooks 1 policies 1 popularity 1 postgresql 1 power bi 1 preflight-checks 1 prestosql 1 privacy 4 project management 3 properties 2 Q â qlik sense 2 query 1 query history 1 quick-start 3 quicksight 1 R â redash 1 redpanda 1 redshift 1 reference 8 relational 4 releases 1 remote 6 reporting 1 requests 1 rest-api 25 roles 2 rules 2 S â s3 4 s4hana 1 salesforce 10 sap 2 sap-ecc 1 sap-hana 1 sap-s4hana 1 schema 6 schema registry 1 schema-drift 5 schema-monitoring 5 scim 2 scopes 1 search 2 secrets 1 secure-agent 10 security 24 servicenow 2 setup 54 siem 1 sigma 1 sisense 1 slack 5 smtp 2 snowflake 12 soda 1 spark 1 spreadsheets 1 sql 4 sql server 1 sso 2 stewardship 1 storage 8 support 2 synapse 1 T â tableau 2 tags 1 tasks 1 teams 1 teradata 1 terminology 1 thoughtspot 1 tokens 1 transformations 2 trino 1 troubleshooting 8 U â upstream-dependencies 13 usage 1 user groups 2 users 1 V â visualization 6 W â warehouse 1 webhooks 4 Windsurf 1 workflow 13 workflows 5"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control",
    "text": "Configure Atlan Access control On this page Access Control Overview: Manage user permissions and access to data assets in Atlan for security and compliance. Control who can view, edit, and manage your data assets through granular access policies and user roles. Get started â Follow these steps to implement access control in Atlan: Create personas Create Purpose Concepts â What are personas? : Detailed explanation of personas. What are Purposes? : Detailed explanation of purposes. What are Sidebar Tabs? : Detailed explanation of purposes. Tags: access control permissions security governance atlan Next Invite new users Get started Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/add-users-to-groups",
    "text": "Configure Atlan Access control Manage users and groups Add users to groups On this page Add users to groups Who can do this? You will need to be an admin user in Atlan to manage group membership. Add users to a group â To add many users to one group: From the left menu on any screen, clickÂ Admin . UnderÂ Workspace , or from the tiles, click Groups . To the right of a group row, click the user button. Check all users to add to the group. Click theÂ Save button. Add groups to a user â To add many groups to one user: From the left menu on any screen, clickÂ Admin . UnderÂ Workspace , or from the tiles, clickÂ Users . To the right of a user row, click the group button. Check all groups to add to the user. Click theÂ Add button. Map users to SSO groups â Atlan supports configuring SSO group mappings. You will first need to create groups in Atlan that correspond to the groups you want to map from your SSO provider to Atlan. To automatically assign users to Atlan groups based on their SSO groups, refer to the documentation for supported SSO providers: Azure AD Google JumpCloud Okta OneLogin SAML 2.0 You can also set default roles for new users joining the Atlan workspace via SSO. Tags: atlan documentation Previous Manage users Next Manage user authentication Add users to a group Add groups to a user Map users to SSO groups"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/additional-connectivity-to-data-sources",
    "text": "Connect data Connectivity Framework Connector Framework References Additional connectivity to data sources On this page Additional connectivity to data sources In addition to connecting to your data sources directly, Atlan also supports connecting through: Private network link â AWS PrivateLink â AWS PrivateLink creates a secure, private connection between services running in AWS: Amazon Athena Amazon MSK Amazon Redshift Databricks Hive Microsoft SQL Server   - Amazon RDS and Amazon EC2 MySQL PostgreSQL Snowflake Tableau Trino Azure Private Link â Azure Private Link creates a secure, private connection between services running in Azure: Databricks Snowflake Private Service Connect â Private Service Connect creates a secure, private connection between services running in Google Cloud Platform: Google BigQuery Docker-based offline extraction â Atlan supports the offline extraction method for fetching metadata from supported sources. You will need to first extract the metadata yourself and then make it available in S3. For offline extraction, Atlan uses the following: Base image   -  Ubuntu for SQL-based extraction, Alpine Linux for REST API extraction. Programming language   -  Kotlin for SQL sources, Python for BI sources and event buses. Databases â Amazon Redshift Databricks Hive Microsoft SQL Server MySQL Oracle PostgreSQL SAP HANA Snowflake Teradata BI tools â IBM Cognos Analytics Looker Tableau ThoughtSpot Data movement tools â dbt Core Event buses â Aiven Kafka Apache Kafka Confluent Kafka Redpanda Kafka Miners â Databricks Teradata S3 miner â Amazon Redshift Google BigQuery Hive Microsoft Azure Synapse Analytics Microsoft SQL Server Snowflake Teradata Kubernetes-based offline extraction â Refer to How to connect on-premises databases to Kubernetes , and then request sample ConfigMap and CronJob files for the following supported SQL connectors: Microsoft SQL Server MySQL Oracle PostgreSQL Tags: atlan documentation Previous What is the crawler logic for a deprecated asset? Next Connectors and capabilities Private network link Docker-based offline extraction Kubernetes-based offline extraction"
  },
  {
    "url": "https://docs.atlan.com/product/administration",
    "text": "Configure Atlan Administration Administration Atlan's administration features provide comprehensive tools for managing your data workspace. Configure user access, monitor system logs, and customize workspace settings to help your organization's data environment operate efficiently and securely while meeting your specific requirements. Key features â ð Logs Monitor system events and user activities through comprehensive logs. Track changes and troubleshoot issues with detailed event history. ð§ª Labs Access experimental features and beta functionality. Test new capabilities before they're generally available. ð README templates Standardize documentation across your data assets with customizable templates. Maintain consistent information structure. Featured guides â View event logs Learn how to access and interpret system event logs Enable labs features Discover how to activate and use experimental features Create README templates Set up standardized templates for asset documentation Tags: atlan documentation Next Allow guests to request updates"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-guests-to-request-updates",
    "text": "Configure Atlan Administration Get Started Allow guests to request updates On this page Allow guests to request updates Who can do this? You will need to be an admin user in Atlan to allow guest users to request metadata updates. Guest users in Atlan can only suggest changes to asset metadata if enabled from the admin center. To allow guest users to request Â updates, follow these steps. Enable guest users to request updates â To enable your guest users to request metadata updates: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under theÂ Access control heading of the Labs page, turn on Allow guests to raise requests for metadata updates .Â Your guest users will now be able to raise requests for metadata updates on assets! ð If you'd like to disable this option for your guest users, follow the steps above and then turn it off. Tags: atlan documentation Previous Administration Next Allow members to view reports Enable guest users to request updates"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/authenticate-sso-credentials-to-query-data",
    "text": "Configure Atlan Integrations Identity Management SSO Guides Authenticate SSO credentials to query data On this page Authenticate SSO credentials to query data Who can do this? Any Atlan user with data access to the asset and SSO credentials for the connection. Once your connection admins have configured SSO authentication, you can query data using your SSO credentials. Atlan currently supports the following connectors for SSO authentication: Amazon Redshift -  currently only Okta is supported as the identity provider. Google BigQuery -  Atlan uses Google OAuth 2.0, which handles integration with all identity providers. However, Atlan has only validated the integration with Okta. Snowflake -  Atlan uses Snowflake External OAuth for SSO authentication, thus supporting all Snowflake-supported identity providers . To query data using shared credentials instead, refer to Provide credentials to query data . Did you know? Connections that require you to provide SSO credentials have a small icon next to them. If the connection you want to query has no icon, you can query it with its default shared credentials. You only need to authenticate for connections with this icon. Set up your SSO credentials â Atlan supports SSO authentication for querying data from the following connections: Amazon Redshift â Atlan supports Okta SSO authentication for Amazon Redshift connections. Before you can query data with SSO credentials, you will first need to enable Okta SSO authentication for Amazon Redshift in Atlan. To set up your Okta SSO credentials for an Amazon Redshift connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the Amazon Redshift connection that requires SSO credentials. A Set up SSO authentication for Redshift dialog will appear. Click Get started to set up your SSO credentials for the connection: For Authentication , Okta is the default selection. For Username , enter your Okta username. For Password , enter the password for your Okta username. Click the Test Authentication button to confirm connectivity. Once authentication is successful, click Done . Close the SSO authentication completed dialog to return to the query editor. You can now run queries using your Okta SSO credentials! ð Google BigQuery â Atlan supports Google OAuth 2.0 SSO authentication for Google BigQuery connections. Before you can query data with SSO credentials, you will first need to enable SSO authentication for Google BigQuery in Atlan. To set up your SSO credentials for a Google BigQuery connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the Google BigQuery connection that requires SSO credentials. An Authorizing dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click Allow to enable authentication. Once authorization is successful, close the Authorizing dialog to return to the query editor. You can now run queries using your SSO credentials! ð Snowflake â Atlan supports SSO authentication via Snowflake External OAuth for Snowflake connections. Before you can query data with SSO credentials, you will first need to enable SSO authentication for Snowflake in Atlan. To set up your Snowflake OAuth credentials for a Snowflake connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the Snowflake connection that requires Snowflake OAuth credentials. An Authorizing dialog will appear. Once authorization is successful, close the Authorizing dialog to return to the query editor. (Optional) To change roles and warehouses, click the connection name in the left menu or the Editor context tab with the connection name: For Role , click the Select role dropdown to select a granted role assigned to you in Snowflake. If no role is selected, Atlan will use the default PUBLIC role in Snowflake for authentication. For Warehouse , click the Select warehouse dropdown to change warehouses.Â You can now run queries using your Snowflake OAuth credentials! ð Remove your SSO credentials â To remove your SSO credentials for a connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the connection with SSO authentication enabled. From the upper right of the query editor, click the Editor context tab with the connection name. In the Editor context dialog, hover over the timestamp and then click Log out . In the Log out from Insights dialog, click Log out to confirm. Did you know? You can refer to troubleshooting connector-specific SSO authentication to troubleshoot any errors. Tags: atlan documentation Previous How to enable SAML 2.0 for SSO Next Authenticate SSO credentials to view sample data Set up your SSO credentials Remove your SSO credentials"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/authenticate-sso-credentials-to-view-sample-data",
    "text": "Configure Atlan Integrations Identity Management SSO Guides Authenticate SSO credentials to view sample data On this page Authenticate SSO credentials to view sample data Who can do this? Any Atlan user with data access to the asset and SSO credentials for the connection. Atlan will display a 100-row sample of the data . Once your connection admins have configured SSO authentication, you can view sample data using your SSO credentials. Atlan currently supports the following connectors for SSO authentication: Amazon Redshift -  currently only Okta is supported as the identity provider. Google BigQuery -  Atlan uses Google OAuth 2.0, which handles integration with all identity providers. However, Atlan has only validated the integration with Okta. Snowflake -  Atlan uses Snowflake External OAuth for SSO authentication, thus supporting all Snowflake-supported identity providers . To view sample data using shared credentials instead, refer to Provide credentials to view sample data . Set up your SSO credentials â Atlan supports SSO authentication for viewing sample data from the following connections: Amazon Redshift â Atlan supports Okta SSO authentication for Amazon Redshift connections. Before you can view sample data with SSO credentials, you will first need to enable Okta SSO authentication for Amazon Redshift in Atlan. To set up your Okta SSO credentials for viewing sample data, for Amazon Redshift: On the Assets page, click an Amazon Redshift asset to view its asset profile. From the asset profile, click Sample data . A Set up SSO authentication for Redshift dialog will appear. Click Get started to set up your SSO credentials for the connection: For Authentication , Okta is the default selection. For Username , enter your Okta username. For Password , enter the password for your Okta username. Click the Test Authentication button to confirm connectivity. Once authentication is successful, click Done . You can now view sample data using your Okta SSO credentials! ð Google BigQuery â Atlan supports Google OAuth 2.0 SSO authentication for Google BigQuery connections. Before you can view sample data with SSO credentials, you will first need to enable SSO authentication for Google BigQuery in Atlan. To set up SSO credentials for viewing sample data, for Google BigQuery: On the Assets page, click a Google BigQuery asset to view its asset profile. From the asset profile, click Sample data . To set up your SSO credentials for viewing sample data, click Get started . An Authorizing dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click Allow to enable authentication. Once authorization is successful, close the Authorizing dialog. You can now view sample data using SSO credentials! ð Snowflake â Atlan supports SSO authentication via Snowflake External OAuth for Snowflake connections. Before you can view sample data with SSO credentials, you will first need to enable SSO authentication for Snowflake in Atlan. To set up Snowflake OAuth credentials for viewing sample data, for Snowflake: On the Assets page, click a Snowflake asset to view its asset profile. From the asset profile, click Sample data . To set up your Snowflake OAuth credentials for viewing sample data, click Get started . An Authorizing dialog will appear. Once authorization is successful, close the Authorizing dialog. You can now view sample data using Snowflake OAuth credentials! ð Did you know? If your Atlan admin has enabled sample data download , you will be able to export sample data in a CSV file. Remove SSO credentials â To remove your SSO credentials for a connection: On the Assets page, click an asset to view its asset profile. From the asset profile, click Sample data . At the bottom of the Sample data screen, hover over the timestamp and then click Log out . In the Log out from sample data preview dialog, click Log out to confirm. Did you know? You can refer to troubleshooting connector-specific SSO authentication to troubleshoot any errors. Tags: atlan documentation Previous Authenticate SSO credentials to query data Next Limit SSO automatically creating users when they log in Set up your SSO credentials Remove SSO credentials"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/automate-policy-compliance",
    "text": "Build governance Stewardship Policy Management Automate policy compliance On this page Automate policy compliance â Available via the Advanced Policy & Compliances package Who can do this? You must be an admin user in Atlan to enable , create , manage , and approve data governance policies. Data governance policies form the backbone of effective data governance. These help you define how to store, manage, access, and use data within an organization. You can establish guiding principles, validation rules, and best practices for monitoring data and policy compliance. An organization may have data that can be publicly available, secure and confidential, or a combination of both. Atlan can help you ensure that all your data assets are governed by data governance policies and used in compliance with applicable laws and regulations. The policy center in Atlan helps you build policies to align with your organizational goals for securing and managing data. You can currently choose from six different types of policies: Data quality -  maintain data accuracy, consistency, and reliability by establishing standards and processes for data validation, cleansing, and quality control. Data privacy -  govern the collection, processing, and sharing of personal and/or sensitive data to ensure compliance with data protection regulations and safeguarding privacy rights. Data security -  outline measures and controls to protect data from unauthorized access, breaches, and loss, often including encryption, access controls, and incident response procedures. Data lifecycle -  define the stages of data from creation and usage to archival and disposal. This ensures data is retained only as long as necessary and compliant with legal and business requirements. Data ethics -  set guidelines for responsible, ethical, and acceptable data use, and address issues such as bias, fairness, and responsible AI implementation. Data definitions and models -  standardize data definitions, taxonomies, and models to ensure consistent and accurate understanding and usage of data across the organization. Enable policy center â Who can do this? You must be an admin user in Atlan to enable the policy center module for your organization. As a prerequisite, you must have the governance workflows and inbox module enabled . The policy center provides a single control plane to link your data governance policies to your assets in Atlan. To enable the policy center for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, under Governance center , configure the following: Ensure that the Governance Workflows and Inbox module is enabled . Turn on Policy Center to create and enforce governance policies on your assets in Atlan. In the Enable Policy Center dialog, click Enable to confirm. If you'd like to disable the Policy Center Â module from your organization's Atlan workspace, follow the steps above to turn it off. Once enabled, you can also temporarily disable the module and turn it on again as needed. For any policies you may have created, this will not result in any data loss. Tags: atlan documentation Previous Manage tasks Next Create policies Enable policy center"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/multiple-azure-ad-tenants",
    "text": "Configure Atlan Integrations Identity Management SSO FAQ Can Atlan integrate with multiple Azure AD tenants within a single instance? Can Atlan integrate with multiple Azure AD tenants within a single instance? Atlan can currently only connect to any one Azure AD tenant. However, you can reach out to Atlan support to share more details about your organizational setup and requirements. For example, are your users currently assigned to separate tenants? This will help us better understand your use case. Tags: atlan documentation faq-integrations Previous Microsoft Defender SSO error Next Can we use a Microsoft SSO login?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/restrict-querying-data-warehouse",
    "text": "Use data Insights FAQ Can we restrict who can query our data warehouse? Can we restrict who can query our data warehouse? You can restrict who can query data at different levels of granularity: Block all querying Block all querying of a source Block specific users from querying specific assets by asset or tag Tags: atlan documentation faq-insights Previous Can I turn off sample data preview for the entire organization? Next How can I identify an Insights query in my database access log?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/configure-custom-domains-for-microsoft-excel",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Configure custom domains for Microsoft Excel On this page Configure custom domains for Microsoft Excel Who can do this? You will need your Microsoft 365 administrator to complete these steps   -  you may not have access yourself. Before you begin, you may need to Determine if Centralized Deployment of add-ins works for your organization . If your Atlan tenant is hosted on a custom domain   -  for example, https://<your-tenant-name>.mycompany.com Â   -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel. Prerequisites â The Atlan add-in must be centrally deployed from the Microsoft 365 admin center . You must have access to a Windows machine to install Microsoft PowerShell. Install PowerShell â Install PowerShell to deploy the Atlan add-in for custom domains: Windows: PowerShell comes pre-installed on most modern Windows systems. If you need to upgrade the package, you can download it from the Microsoft website . macOS: Open Terminal and run: brew install --cask powershell Linux: Open Ubuntu and run: sudo apt-get update sudo apt-get install -y powershell For other distributions, refer to Microsoft's PowerShell installation guide . Configure the add-in for Microsoft Excel â To install the Atlan add-in directly in Microsoft Excel: Open PowerShell and run the following command to install the required module: Install-Module -Name O365CentralizedAddInDeployment -Scope CurrentUser Run the following command to import the module: Import-Module -Name O365CentralizedAddInDeployment Run the following command to connect to the organization add-in service: Connect-OrganizationAddInService In the Microsoft authorization dialog, select an account to authenticate the connection. (Optional) Run the following command to list existing add-ins in the organization: Get-OrganizationAddIn Run the following command to set a custom domain for the Atlan add-in: Set-OrganizationAddInOverrides -ProductId <your-product-ID>  -AppDomains \"<your-custom-domain>\" Replace <your-product-ID> with the product ID of the Atlan add-in. Replace <your-custom-domain> with your organization's custom domain. (Optional) Troubleshooting add-in connectivity â Who can do this? Any individual in your organization with access to Microsoft Excel and Atlan tenant on a custom domain. This section is optional if you deployed the Atlan add-in for the first time in your organization after completing the steps above. Your users will be able to connect Atlan to Microsoft Excel from your custom domain. However, if any user tried to set up the add-in prior to the configuration above, they may not able to connect Atlan to Microsoft Excel. In that case, Atlan recommends clearing the add-in cache using the following steps. Clear Microsoft Excel Online cacheÂ â To clear the local storage of your Microsoft Excel online app, from Google Chrome: Open a blank Microsoft Excel workbook. From the top right of your browser, click the vertical three dots icon for more menu options. From the more options menu, click More Tools and then click Developer Tools . In the top menu of developer tools, click Application . In the left menu of the Application tab, under Storage , click Local storage . Find an entry for https://***-excel.officeapps.live.com . Right-click on the entry and then click Clear . Refresh the Microsoft Excel Online app in your browser. Clear Microsoft Excel cache on Mac â To clear the Microsoft Excel cache on Mac: Close Microsoft Excel. Use the Finder to navigate to the /Users/<user>/Library/Containers/com.microsoft.Excel/Data/Library/Application   Support/Microsoft/Office/16.0/Wef. folder. Search for a313dc5c-6ca5-4346-abfc-c84c57d4b9dc and delete all the files and folders returned in the search results. Restart your Microsoft Excel app. Clear Microsoft Excel cache on Windows â To clear the Microsoft Excel cache on Windows: Open Microsoft Excel. From the ribbon, navigate to File > Options > Trust Center > Trust Center Settings > Trusted Add-in Catalogs . In Trusted Web Add-in Catalogs , select the checkbox Next time Office starts, clear all previously-started web add-ins cache . Close Microsoft Excel and then restart it to clear the add-in cache. Once it has restarted, the checkbox will be deselected automatically. Tags: atlan documentation Previous Bulk enrich metadata Next Download impacted assets in Google Sheets Prerequisites Install PowerShell Configure the add-in for Microsoft Excel (Optional) Troubleshooting add-in connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/connections",
    "text": "Connect data Connectivity Framework Connector Framework Connectors Atlan's connectors enable you to integrate, catalog, and govern metadata from leading data platforms. These connectors help you discover, document, and manage your data assets within your organization's data ecosystem, providing a unified view of your data landscape. Core offerings â ð Preflight Checks Verify permissions and configuration before running crawlers for successful metadata extraction. ð Workflow Management Follow recommended sequences for running workflows to ensure proper metadata extraction and lineage construction. ð Connectivity Options Choose from various connectivity methods including direct connections, private network links, and secure agent extraction. ð Metadata Extraction Comprehensive crawling of metadata including schemas, tables, views, and lineage information from supported sources. ð¡ For optimal results, follow the recommended workflow order: crawl data stores first, then run data quality tools, mine query logs, run extract-load tools, transformation tools, and finally crawl business intelligence tools. Tags: connect data integration connectors atlan data sources Next Manage connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-forms",
    "text": "Build governance Stewardship Form Management Create forms On this page Create forms Who can do this? You must be an admin user in Atlan to create and manage forms. Anyone with access to Atlan   -  admin, member, or guest user   -  can fill out forms while raising requests . You can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more. You can use forms to: Standardize request details, improving governance and automation. Track responses and enable structured data collection from user requests. Forms can currently be embedded within the following governance workflow templates only: Access management New entity creation Create a form â To create a new form: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Forms . Click the + New form button to create a new form. For Heading , configure the following: For Heading text , enter a name indicative of the form's purpose   -  for example, Data Access Request . (Optional) For Subheading text , enter a brief description. Click anywhere on the screen to save the name and description. To add a field to your form, in the Configure form, enter the following details: For Type , select the type of value you expect users to use for this field: Text type allows free-form text values. Dropdown type allows creating a list of predefined set of values. For Options , click Add option to set as many values as you want for your dropdown list. Email type allows creating a text field for email addresses. Number type allows numeric values. Date type allows date values in the UTC date format   -  year, month, day. Custom metadata property type allows defining existing custom metadata properties. For Property and Options , select the associated property and options for your custom metadata structure. For Name , enter a name that describes the purpose of the input field. For Description , enter a description for your input field. For Placeholder text , add instructions to help users complete the field. (Optional) Toggle on Required to prevent form submission if the field is empty. (Optional) Click the + Add another field button to add more fields to your form. (Optional) Click an existing field to perform additional operations: Click the reorder icon to drag and drop the selected field to reorder your form. Click the trash icon to delete the selected field. Click the copy icon to create a duplicate copy of the selected field. (Optional) Once you have completed your form, in the top right of the screen, you can: Click Preview to view a draft of the form before publishing it. Click Save as draft to save your changes in a draft version and publish when ready. Â Click Publish to publish your completed form immediately. Your form is now live! ð Once published, you will be able to embed forms in governance workflows to collect more information from requesters. Manage forms â Once you have created forms, you can manage and modify your forms and monitor responses from the Forms dashboard. Form deletion is currently not supported. To manage forms: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Forms . From the Overview tab, you can view the following: Filter your existing forms by Published , Draft , or Disabled status. To edit a form, click the name of your form and edit it. To disable a form, select a form and then change the status from Draft or Published to Disabled . Change to the Monitor responses tab to view all form responses: Click the Submitted by filter to filter responses from specific users. Click any response to view more details. You can also view all other responses to a specific form. Frequently asked questions â Can Markdown syntax be used in the form description? â Atlan currently does not support Markdown syntax in the form description or any other fields. Tags: atlan documentation Previous Revoke data access Next Troubleshooting policies Create a form Manage forms Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/create-groups",
    "text": "Configure Atlan Access control Get started Create groups Create groups Who can do this? You will need to be an admin user in Atlan to create groups. To create a group in Atlan: From the left menu of any screen, click Admin . Under Workspace click Groups . Click the Create Group button. Enter details for the new group in Create Group : For Name enter a descriptive name for the group. (Optional) ForÂ Description enter an explanation of the group. (Optional) ForÂ Slack enter the name of an existing Slack channel for the group. (You do not need to include the   in the channel name.) Click Test your slack link to attempt to open the channel in your integrated Slack workspace . (Optional) ForÂ Users search for and add any users that should be in the group. (You can also add users later.) (Optional) To add this group for all new users automatically, enable Mark as default . Click Create Group button. You now have a new group in Atlan! ð Did you know? Once you've integrated Slack and added the Slack channel for your group, simply click the Slack icon next to the group name in Atlan and you'll be redirected to the group channel on Slack. You can then post questions and share assets directly on that channel. Tags: atlan documentation Previous Invite new users Next Create persona"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-policies",
    "text": "Build governance Stewardship Policy Management Create policies On this page Create policies â Available via the Advanced Policy & Compliances package Who can do this? You must be an admin user in Atlan to enable , create , manage , and approve data governance policies. You can create a policy to document guidelines for the following: How's data processed and managed within your organization? Who is responsible for the data under various circumstances? What can you do to reduce potential business problems from the improper use of data? Before you can create a data governance policy, you must have an Atlan admin enable the policy center module in your Atlan workspace. To create a policy, complete the following steps. Create a policy â To create a new policy: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . From the Policy Center , click the + New policy button to create a new policy. In the Create a new policy dialog, enter the following details: For Policy name , enter a meaningful name for your policy. For Policy type , choose a policy type. For Owners , assign individual users or groups as policy owners. Click Create to get started. Define the policy â Add a purpose â Once you have created a policy, you can define its purpose. This is the mission statement of your policy, where you can outline what you want the policy to accomplish. To define the purpose of your policy: In the Overview tab of the policy page, under Purpose , you can either: Click Edit to manually describe the purpose of your policy and then click Save . Click ask Atlan AI to add an Atlan AI-generated description. In the Generate purpose using Atlan AI form, enter the following details: For Enter industry , enter the name of your industry   -  for example, Finance . To define the area of impact of your policy, click the dropdown to select Global , Regional , or Local policy . For compliance type, select Standard , Regulation , or Other . For Enter compliance , enter the compliance regulation. For Describe the policy , enter a brief description of your policy. Click Generate Purpose to generate an Atlan AI-generated purpose. (Optional) Click + Add resource to add resources to your policy description. Describe the policy â You can set out the best practices, goals, and guidelines for your policy document. To describe your policy: Switch to the Purpose tab of your policy page. In the Policy section, click Edit to write your policy. You can either manually draft the policy description or use Atlan AI to do the same and then edit as needed. Click Save to save your changes. (Optional) Add policy exceptions â A policy exception is a method for maintaining a policy but granting exceptions to authorized individuals or entities. Doing so will allow them to circumvent one or more restrictions. To add a policy exception: In the Purpose tab of the policy page, under Policy Exceptions , click Add policy exception . In the New policy exception form, enter the following details: For Exception name , enter a meaningful name. For Purpose , briefly describe the purpose of this exception. For Users , select the individual users or groups to whom this exception should apply. Click Add exception to save your changes. (Optional) Click + Add new to add more policy exceptions. Define scope and rules â Did you know? Your selected assets will not be linked to the draft policy until after it has been approved. It may also take a few hours after the policy has been approved for the assets to be linked while the linkage workflow runs in the background. Select asset scope â You can determine the assets within the scope of your policy. Policy rules will only apply to the filtered subset of assets you select. To select assets: Switch to the Scope & Rules tab of your policy page. For Asset scope , use the asset filters to select the relevant assets. The operators and values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter . (Optional) To preview the assets included in the scope of your policy, click View all . Click Save scope to save asset selection. (Optional) To the right of any filter, click the three horizontal dots and then: To remove a filter, click Delete . To turn off a filter, click Disable . Click Enable to turn on any disabled filters. Create compliance rules â To implement and enforce your policy, you can create a set of rules to specify permitted or restricted actions, enable compliance with data standards, and ensure accountability. If the assets scoped to the policy do not comply with all the rules, Atlan will trigger an incident to alert you. This incident can help you understand the specific rules that have been violated by the assets, making them noncompliant with the policy. Atlan currently supports creating 10 rules per policy. To define compliance rules: In the Scope & Rules tab of your policy page, Assets must comply with the scope defined above is the default rule for all policies. You must first determine your asset scope before you can create compliance rules. For Compliance rules , use the attribute filters to create a rule with which scoped assets must comply. The operators and values will vary depending on the selected attributes. Atlan currently supports creating policy rules based on the following metadata attributes: Certificates Owners Terms Tags Custom metadata (Optional) To add more rules, click Add another rule . Click Save rules to save the rules you created for the policy. Atlan will scan scoped assets to ensure that these match all the rules. An incident will be triggered for any asset that does not comply with all the policy rules. (Optional) To the right of any rule, click the three horizontal dots and then: To remove a rule, click Delete . To turn off a rule, click Disable . Click Enable to turn on any disabled rules. Define policy validity â To define the validity period of your policy: In the Policy Details sidebar of the Overview tab, for Valid till , click the pencil icon to set a validity period. From the calendar, set a date for when the policy will expire. For Review period , click the pencil icon to set a review period. For ...days before expiry , enter a numeric value for when the policy should be reviewed before its expiration date. By default, Atlan will display a warning message on the policy 30 days prior to its expiration date. You can adjust the review period to set a different timeline. During the review period, you can either revise the expiring policy or extend its validity period. Select approval workflow â To select an approval workflow: Switch to the Relationships Â tab of your policy page. From the left menu, select Approval Workflows . In the Approval Workflows section, click Add approval workflow . In the Select Approval Workflow dialog, click the relevant approval workflow for your policy. (Optional) Hover over Approvers to view a list of approvers. Click Save to save your selections. (Optional) Add terms related to this policy â You can add business context to your policies in Atlan. In the Overview tab of the policy page, under Linked Terms , click the + button to add related terms. (Optional) Add related policies â You may want to group data governance policies by policy type, business function, and more. You can optionally create relationships between your policies in Atlan to build a more comprehensive framework of data governance. To add related policies: Switch to the Relationships Â tab of your policy page. From the left menu, select Related Policies . In the Related Policies Â section, click Add related policies . In the left menu of the Add policies related to ... dialog, click the relevant policies to connect to your policy. Click Add policies to save your selections. Submit for approval â Once you have reviewed your policy, in the top right of your screen, click Submit for approval to submit your policy for approval. If the policy has been approved and the workflow linking the policy to your selected assets has run successfully, the policy you created will become active and govern linked assets. For governed assets, linked policies will appear on the asset sidebar . You can hover over a linked policy in the asset sidebar to view details in a popover, including policy type, purpose, and certification status, and even navigate to the policy in the policy center. Did you know? If you have any questions about setting up policies, head over to Troubleshooting policies . Tags: atlan documentation Previous Automate policy compliance Next Manage policies Create a policy Define the policy Define scope and rules Define policy validity Select approval workflow (Optional) Add terms related to this policy (Optional) Add related policies Submit for approval"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/how-tos/create-a-purpose",
    "text": "Configure Atlan Access control Get started Create purpose On this page Create purpose Who can do this? You will need to be an admin user to create purposes. danger A purpose acts on at least one tag. This tag must be created before creating the purpose. To create a purpose: From the left menu of any screen, clickÂ Governance . UnderÂ Access Control , clickÂ Purposes . If this is the first purpose, click the Get started button. Otherwise, in the top right, click the New Purpose button. For Purpose name , enter a meaningful name for the purpose. (Optional) Add a description for the purpose. In the lower-left corner of the dialog, click Select tag . Select one or more tags from the list, and then click on the purpose box again to close the list. ClickÂ Create to create the purpose. You now have an empty purpose. Did you know? The purpose will not yet control any access. Your users can still use the purpose to quickly browse assets with any of the tags selected, though. (Optional) Add rich documentation â To add rich documentation describing the purpose: UnderÂ Summary , thenÂ Channels , add any Slack channels relevant to the purpose. UnderÂ Resources ,Â add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive   -  anything that has a URL. Under Readme , write a richly formatted description of the purpose. Add policies â Did you know? For the purpose to control access, you need to define one or more policies. Repeat the following steps for each set of users and permissions you want to control through the purpose. To add policies to the purpose, from within the purpose: Change to theÂ Policies tab. Click theÂ New Policy button and choose the type of policy. Add a metadata policy â To grant or restrict permissions to change metadata: ChooseÂ Metadata policy . Under Name , briefly describe the policy's intention. (Optional) Under Users and Groups , choose the users to whom to apply the policy. By default, all users will be included. To select others: In theÂ Users and Groups box, click theÂ x . UnderÂ Users and Groups , click theÂ Add link. Search for and select the users and groups to control with the policy, and then click anywhere in theÂ Metadata policy sidebar. (Optional) For Configure permissions , choose the permissions the policy will grant . By default, all permissions will be granted. To select others: To the right ofÂ Configure permissions , click theÂ Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. At the bottom of the list, click Save . (Optional) ForÂ Deny selected permissions , choose whether you want to explicitly deny these permissions. danger If enabled, this will override all grants _take_priority) of those permissions from any other policies for the same users. At the bottom of theÂ Metadata policy sidebar, clickÂ Save . Add a data policy â To grant or restrict permissions to query or preview data: ChooseÂ Data policy . Under Name , briefly describe the policy's intention. (Optional) Under Users and Groups , choose the users to whom to apply the policy. By default, all users will be included. To select others: In theÂ Users and Groups box, click theÂ x . UnderÂ Users and Groups , click theÂ Add link. Search for and select the users and groups to control with the policy, and then click anywhere in theÂ Data policy sidebar. (Optional) ForÂ Querying Permissions , choose whether you want to explicitly deny the ability to query and preview data on these assets. danger If enabled, this will override all grants _take_priority) of those permissions from any other policies for the same users. This will also deny at the table level   -  even if only 1 out of 100 columns in a table has the tag, previewing and querying will be denied for the entire table. (Optional) For Configure permissions , choose the masking policy to apply. By default, no masking will be applied. To apply masking, under Masking (Optional) , select the type of masking to apply. If you are unsure what they do, hover over each one to see a more detailed description and an example. At the bottom of theÂ Data policy sidebar, clickÂ Save . (Optional) Set preferences â Did you know? You can also personalize the details users will see in the sidebar or filters menu when in a purpose. This is great to limit information overload, by showing only what is relevant to a given set of users. To set preferences for the purpose: Change to theÂ Preferences tab of the purpose. From the left menu, configure the following: To limit the asset types that should be visible to the purpose: Click Asset types to view asset type preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose. To limit the out-of-the-box tabs that should be visible to the purpose: Click Asset sidebar to view asset sidebar preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose. To limit the asset filters that should be visible to the purpose: ClickÂ Asset filters to view asset filter preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose. To limit the custom metadata that should be visible to the purpose: Click Custom metadata to view custom metadata preferences. Click the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the purpose. Tags: atlan documentation Previous Create persona Next Manage users (Optional) Add rich documentation Add policies (Optional) Set preferences"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-pipelines",
    "text": "Data Pipelines Atlan's Data Pipelines connectors enable you to integrate, catalog, and govern metadata from leading ETL tools and workflow orchestration platforms. These connectors help you document data transformations, track lineage, and manage the complete lifecycle of your data as it moves through various systems. Key concepts â ETL : Extract, transform, load processes that move and reshape data between systems Orchestration : Platforms that schedule, coordinate, and monitor data workflows Lineage : Visual representation of data movement from source to destination Transformation : Rules and logic applied to data as it moves through pipelines Core offerings â ð Process Tracking Document ETL processes and monitor data movement across your organization ð End-to-End Lineage Visualize complete data lineage from source through transformation to consumption ð Impact Analysis Understand how changes to pipelines affect downstream systems and reports ð Code Integration Link transformation code and business logic to your data assets in the catalog Get started â 1 Select your platform Choose the ETL tool or orchestration platform you want to connect from the sidebar â 2 Set up credentials Configure authentication and access permissions following the platform-specific guide â 3 Extract metadata Run the crawler to capture lineage, transformation logic, and pipeline structure ð¡ Use the sidebar navigation to browse available connectors, including popular tools like dbt, Fivetran, Apache Airflow, Alteryx and Azure Data Factory. Tags: connect data integration etl workflows orchestration lineage atlan connectors data"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks",
    "text": "Build governance Data Quality Studio Databricks Data Quality On this page Databricks Data Quality Studio Private Preview Overview: Monitor the quality of your Databricks assets in Atlan using Databricks' native data quality capabilities. This integration lets you create rules, track metrics, and view data quality insights directly within Atlan's discovery, lineage, and data products. Get started â Follow these steps to set up Databricks as your data quality studio: Set up Databricks Enable data quality on a connection FAQ â Setup and configuration : Common questions about Databricks setup and configuration. Tags: databricks data-quality governance atlan Next Set up Databricks Get started FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains",
    "text": "Build governance Domains On this page Domains Overview: Domains help you organize data assets in Atlan into logical, business-aligned structures. They provide a way to group related assets and establish ownership and governance boundaries across your data ecosystem. Get started â Follow these steps to implement domains in Atlan: Manage domains Guides â Organize assets : Learn how to organize your data assets into domains to improve discoverability and governance. Tags: domains organization governance data assets atlan Next Manage domains Get started Guides"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-discovery-of-process-assets",
    "text": "Configure Atlan Administration Feature Management How to enable discovery of process assets On this page Enable  discovery of process assets Who can do this? You will need to be an admin user in Atlan to enable discovery and tracking of process assets. Processes represent the movement or transformation of assets in Atlan. By default, process assets are hidden on the assets page and reporting center to ensure an efficient asset search, filtering , and discovery experience. To create a more customizable experience for your users, you can turn on discovery and tracking of process assets. Did you know? Even if discovery of process assets is turned off, users will still be able to view process assets on the lineage graph and sidebar. Enable process asset discovery â To enable discovery and tracking of process assets: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Assets heading of the Labs page, turn on Discover and track processes .Â Your users will now be able to search, filter, discover, and track process assets ! ð If you'd like to disable this feature, follow the steps above to turn it off. Tags: atlan documentation Previous How to enable associated terms Next How to enable sample data download Enable process asset discovery"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-sample-data-download",
    "text": "Configure Atlan Administration Feature Management How to enable sample data download On this page Enable  sample data download Who can do this? You will need to be an admin user in Atlan to enable sample data downloads. Atlan allows admin users to enable or disable downloading sample data . This can help you enforce better governance across your organization. To enable sample data download, follow these steps . Enable sample data download â To enable sample data download: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Insights heading of the Labs page, turn on Download data . Your users will now be able to export sample data in a CSV file! ð If you'd like to disable sample data download, follow the steps above to turn it off. Tags: atlan documentation Previous How to enable discovery of process assets Next How to enable scheduled queries Enable sample data download"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-scheduled-queries",
    "text": "Configure Atlan Administration Feature Management How to enable scheduled queries On this page Enable  scheduled queries Who can do this? You will need to be an admin user in Atlan to enable scheduled queries. To enable scheduled queries, follow these steps. Enable scheduled queries â To enable scheduled queries: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Insights heading of the Labs page, turn on Schedule queries .Â In the Turn on scheduled queries dialog box, click Yes to confirm. Your users will now be able to schedule queries in Insights ! ð If you'd like to disable scheduled queries, follow the steps above to turn it off.Â Did you know? Once scheduled queries has been enabled, your users will also be able to export large results via email . Tags: atlan documentation Previous How to enable sample data download Next Restrict asset visibility Enable scheduled queries"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/how-tos/enable-data-quality",
    "text": "Build governance Data Quality Studio Databricks Data Quality Configure data quality Enable data quality on connection On this page Enable data quality on connection Private Preview Enable data quality on your Databricks connection in Atlan to start monitoring data quality. This guide helps you configure the connection with the necessary credentials and permissions. Prerequisites â Before you begin, complete the following steps: Set up Databricks for data quality completed Have the service principal credentials created during Databricks setup Identify the Databricks connection where you want to enable data quality Enable data quality â Follow these steps to enable data quality on your Databricks connection. Turn on the data quality feature: Navigate to Settings in Atlan Find the Labs section Turn on the Data Quality toggle Select your connection and configure credentials: IMPORTANT Currently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection, raise a support request . Data Quality Page Connection Settings Navigate to Governance > Data Quality Select your Databricks connection from the list Click Enable data quality for your selected connection Enter the following credential details: Client ID : The service principal client ID created in Databricks setup Client Secret : The service principal client secret Tenant ID : The tenant ID (Azure only) Workspace URL : Your Databricks workspace URL SQL Warehouse : Your preferred SQL warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Databricks Databricks setup completed correctly Click Update to save the credentials Navigate to Governance > Connections Select your Databricks connection Open Connection settings from the sidebar Enter the following credential details: Client ID : The service principal client ID created in Databricks setup Client Secret : The service principal client secret Tenant ID : The tenant ID (Azure only) Workspace URL : Your Databricks workspace URL SQL Warehouse : Your preferred SQL warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Databricks Databricks setup completed correctly Click Update to save the credentials Next steps â After completing these steps: Atlan takes approximately 10 minutes to complete the setup in the background Once finished, you'll see data quality options available on your Databricks assets You can start creating data quality rules on tables and views Need help â If you have questions or need assistance with enabling data quality on your connection, reach out to Atlan Support by submitting a support request . See also â Data quality permissions - Learn about the data quality permission scopes and configuration Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: databricks data-quality setup atlan Previous Set up Databricks Next Setup and configuration Prerequisites Enable data quality Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/enable-data-quality",
    "text": "Build governance Data Quality Studio Snowflake Data Quality Configure data quality Enable data quality on connection On this page Enable data quality on connection Private Preview Enable data quality on your Snowflake connection in Atlan to start monitoring data quality. This guide helps you configure the connection with the necessary credentials and permissions. Prerequisites â Before you begin, complete the following steps: Set up Snowflake for data quality completed Have the credentials for the atlan_dq_user created during Snowflake setup Identify the Snowflake connection where you want to enable data quality Enable data quality â Follow these steps to enable data quality on your Snowflake connection. Turn on the data quality feature: Navigate to Settings in Atlan Find the Labs section Turn on the Data Quality toggle Select your connection and configure credentials: IMPORTANT Currently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection, raise a support request . Data Quality Page Connection Settings Navigate to Governance > Data Quality Select your Snowflake connection from the list Click Enable data quality for your selected connection Enter the following credential details: Username : The atlan_dq_user created in Snowflake setup Password : The password for your Atlan DQ user Role : atlan_dq_service_role Warehouse : Your preferred warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Snowflake Snowflake setup completed correctly Click Update to save the credentials Navigate to Governance > Connections Select your Snowflake connection Open Connection settings from the sidebar Enter the following credential details: Username : The atlan_dq_user created in Snowflake setup Password : The password for your Atlan DQ user Role : atlan_dq_service_role Warehouse : Your preferred warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Snowflake Snowflake setup completed correctly Click Update to save the credentials Next steps â After completing these steps: Atlan takes approximately 10 minutes to complete the setup in the background Once finished, you'll see data quality options available on your Snowflake assets You can start creating data quality rules on tables and views Need help â If you have questions or need assistance with enabling data quality on your connection, reach out to Atlan Support by submitting a support request . See also â Operations - Learn about the data quality operations and monitoring capabilities Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: snowflake data-quality setup atlan Previous Set up Snowflake Next Enable auto re-attachment of rules Prerequisites Enable data quality Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary",
    "text": "Build governance Glossary On this page Glossary Overview: Create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization. Gain visibility into business context and meaning for your data assets. Get started â Follow these steps to create and manage your business glossary in Atlan: Set up glossaries Guides â Bulk upload terms in the glossary Link terms to assets Concepts â What is a glossary Troubleshooting â Why am I unable to approve a glossary update request FAQ â Can I add duplicate glossary terms What is the default permission for a glossary Can I use personas to update a term in a glossary Can I create backups of glossaries How to fully delete glossary terms or archived items Tags: glossary terminology definitions governance atlan Next Set up glossaries Get started Guides Concepts Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/google-dashboard-login-error",
    "text": "Configure Atlan Integrations Identity Management SSO Troubleshooting Google Dashboard login error On this page Google Dashboard login error Why do I get an error while logging in via Google Dashboard? â When logging in from your Google Dashboard, you may receive an Invalid request error. This is a known problem with Google for which there isn't a clear solution. Here are some references about this known issue: reference 1 and reference 2 . While the login initiated via your Atlan URL works, the IdP initiated login unfortunately doesn't due to the problem described in the above links. We currently do not have a path to resolution from Google. Tags: atlan documentation faq-integrations Previous Okta first-time login authentication error Next Microsoft Defender SSO error"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/faq/reporting-materialized-views",
    "text": "Use data Reporting FAQ How do I see views instead of materialized views in the reporting center? How do I see views instead of materialized views in the reporting center? On the Assets dashboard in the reporting center , click View in the All Asset Types dropdown menu toÂ only see views. You can further filter by selecting a specific connector or connection as well. Tags: atlan documentation Previous Report on assets Next Is there a dashboard to see how my metadata is populated?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/how-do-i-use-the-filters-menu",
    "text": "Use data Discovery Configuration How do I use the filters menu? How do I use the filters menu? To learn how to use the filters menu in Atlan, follow the instructions in this guide . Tags: atlan documentation Previous Add custom metadata Next How to interpret timestamps"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/parameterized-queries",
    "text": "Use data Insights FAQ How to use parameterized queries? How to use parameterized queries? When querying in Insights, you can define custom parameters to make your query interactive . You can define values for your custom parameters, specify data types, and incorporate them into your query. Tags: atlan documentation faq-insights Previous Monitor for runaway queries? Next What controls the frequency of queries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/faq/dashboard-metadata",
    "text": "Use data Reporting FAQ Is there a dashboard to see how my metadata is populated? Is there a dashboard to see how my metadata is populated? Atlan's reporting center provides you with a composite view of your metadata across all the sources integrated with Atlan. You can track metrics for asset enrichment, see metadata updates over time, review personas and purposes, and much more. Tags: atlan documentation Previous How do I see views instead of materialized views in the reporting center?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/limit-sso-automatically-creating-users-when-they-log-in",
    "text": "Configure Atlan Integrations Identity Management SSO Guides Limit SSO automatically creating users when they log in Limit SSO automatically creating users when they log in Only users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan. To restrict access to certain users, edit the list of users configured for Atlan in your SSO provider to a limited set of users. Each time a new user needs to be onboarded, they will need to be added to this list in your SSO provider before they can access Atlan. Without being added to this list, no user in Atlan will be automatically created even if they attempt to log in. Tags: atlan documentation Previous Authenticate SSO credentials to view sample data Next Set default user roles for SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-policies",
    "text": "Build governance Stewardship Policy Management Manage policies On this page Manage policies Who can do this? You must be an admin user in Atlan to enable , create , manage , and approve data governance policies. Once you have created policies, you can manage and revise your policies, monitor policy breaches, report incidents, and more from the Policy center dashboard. Monitor policies â To monitor your policies and take action: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . From the Overview tab in the Policy Center , you can: Review the Action Required section: Click Open incidents to take action on any open incidents. You can filter incidents by Closed , Open , or In-progress status. Select any incident to view more details and assets linked to the incident. Click Policy breaches to examine policy breaches, whichÂ occur when any one condition is breached or violated for a particular policy. View the total count of ungoverned assets in your Atlan workspace. Ungoverned assets refer to assets that are not governed by any policy in Atlan. Click Policies to approve to review and approve policies pending your approval. In the Policies section, view recent, draft, or starred policies. In the At a glance section, view policies by policy type. Under My Policies , view all the policies you have either created or are designated as an owner. Change to the All policies tab to view all the policies in your Atlan workspace. You can filter policies by status   - Active , Draft , or Deprecated . Select any policy to view more details. Change to the Reporting tab to monitor activity related to all policies in Atlan. For All Policies , visualize your policies in Atlan by type or status. For Assets governed over time , visualize trends in your governed assets over time. For Active policies by type , view active policies by policy type. For Assets with multiple policies , view assets that fall under the purview of multiple policies. For Policies with exceptions , view policies with defined exceptions. For Activity , view an activity log for all your policies in Atlan. Revise a policy â Atlan currently only supports revising an active policy that is within its validity period. If a policy comes to the end of its validity period and you decide not to extend or revise it, the policy will be deprecated at the end of its validity period. Any assets within the scope of that expired policy will be automatically delinked and no new incidents will be generated. You will still be able to view the deprecated policy and assets within the scope of that policy in the policy center. If you revise an existing policy, Atlan will create a new draft of that existing policy with the same details. You can then revise the policy details and set a new validity period. Only when the new version has been approved and becomes active, the previous version will be deprecated, and stop scanning assets and generating any new incidents. Note that the workflow linking the revised policy to your selected assets has to run successfully in the background for the revised policy to become active and govern linked assets. To revise an existing policy: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . Select a policy to revise. From the top right, click the Revise policy button. This will create a new draft version of your existing policy. The previous policy will still be under enforcement until the new policy is approved, becomes active, and successfully linked to selected assets. Edit the draft policy to make any changes, such as updating the validity period. Submit the revised policy for approval. Only when the new version has been approved and becomes active, the previous version will be deprecated. You can only delete a draft or published policy in Atlan if you're an owner of that policy. Report an incident â Once you have created data governance policies, Atlan will scan governed assets for any incidents and report them in near real time. If any changes not compliant with your policy definition and compliance rules are detected among governed assets, Atlan will generate incidents automatically and notify the policy owners from the policy center. You can then take action on open incidents. In addition to automated incident reporting, you can manually report incidents that may warrant attention. To manually report an incident: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . Under Action Required , click Open incidents to open the Incidents sidebar. From the Incidents sidebar, click Add new incident to manually report an incident. In the Report new incident , enter the following details: For Brief Description , enter a brief description of the issue you have detected. For In-depth description , add details about the issue, including steps to reproduce the issue. (Optional) For Related Policy , select an impacted policy, if any. (Optional) For Add assets , select any and all impacted assets that may apply. Click Submit incident to submit your incident report. Tags: atlan documentation Previous Create policies Next Revoke data access Monitor policies Revise a policy Report an incident"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/manage-system-announcements",
    "text": "Configure Atlan Integrations Communication SMTP and Announcements Manage system announcements On this page Manage system announcements Who can do this? You will need to be an admin user in Atlan to manage system announcements. Have you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape. System announcements appear on the homepage of all users, in an announcement box under Recent updates . Did you know? You can only create one system announcement per instance. To add more information to your system announcement, you can either edit the existing one to update it or delete the old one and create a new system announcement. Add a system announcement â To add a system announcement: From the left menu of any screen, clickÂ Admin . In the upper right of the page, click the New announcement button. (Optional) At the top of the dialog, click on Information to change the style of announcement: To give general information, in blue, choose Information . To display a problem, in red, chooseÂ Issue . To give a warning, in yellow, chooseÂ Warning . ForÂ Add announcement header... enter a brief title for your announcement. ForÂ Add description... enter the detailed explanation for the announcement. (Optional) You can use Markdown syntax to write a description. Any headings will be rendered in heading 6 ( <h6> ). danger Atlan currently does not support adding images to your announcements. At the bottom of the dialog, clickÂ Save . Your announcement will now appear on the home page of every user that visits Atlan! ð Remove a system announcement â To remove a system announcement: From the left menu of any screen, clickÂ Admin (or from the homepage where the announcement is displayed). In the upper-right corner of the announcement box, click the 3-dot button. From the options, clickÂ Delete . Your announcement has now been removed from the home page of every user that visits Atlan! ð Tags: atlan documentation Previous Create announcements Next Identity Management Integrations Add a system announcement Remove a system announcement"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/monitor-runaway-queries",
    "text": "Use data Insights FAQ Monitor for runaway queries? Monitor for runaway queries? Atlan provides a default 10-minute timeout for all connectors . For example, if a query runs for more than 10 minutes, it will be aborted to safeguard against query abuse. Tags: atlan documentation faq-insights Previous How can I identify an Insights query in my database access log? Next How to use parameterized queries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains/how-tos/organize-assets",
    "text": "Build governance Domains Asset Organization How to organize assets On this page Organize assets Who can do this? Any non-guest user with edit access to an asset's metadata can add assets to domains. This only includes admin and member users. Domain policies currently do not have any impact outside the products module . You can map and organize your assets into domains and subdomains . Domains provide you with a logical structure to group and govern your assets that aligns with business needs and ensures a curated discovery experience. To add assets to a domain, note the following: You can link assets to domains irrespective of whether or not you use data products . You can only add assets to any one specific domain or subdomain. Assets may be used across multiple domains, but can only belong to one domain or subdomain. You can filter assets by a single domain, multiple domains, or no domains. Atlan currently does not support adding glossaries, categories, and terms to domains. Atlan currently does not support raising a request to add assets to domains. Admin users can bulk add assets to domains using playbooks . Add an asset to a domain â Did you know? You can also set up playbooks to bulk add assets to your domains and subdomains. You will need to be an admin user in Atlan to create playbooks. To add an asset to a domain, complete the following steps. To add an asset to a domain: From the left menu of any screen in Atlan, click Assets . On the Assets page, click an asset to open the asset sidebar. In the Overview sidebar, under Domains , click Add to domain . In the popup, check the boxes to select the domain or subdomain to which you want to add the asset. You can only select any one parent domain or nested subdomain. (Optional) Hover over the linked domain or subdomain to view details in a popover, including the user that added the domain. You can also: Click View domain to view the domain profile from the governance center. If the products module is turned off , you will need to be an admin user in Atlan to view the domain. If the products module is turned on , domain policies will determine your ability to view the domain. Click the unlink icon to remove the asset from the linked domain. (Optional) Click the pencil icon to change to a different domain or remove it from the asset. (Optional) In the Filters menu on the left, click Domains to filter assets by domains: Check the boxes to select one or more domains or subdomains to filter your assets. Click No domains to filter assets not mapped to any domain. Did you know? To programmatically add assets to a domain or remove them from a linked domain, refer to our developer documentation . Tags: atlan documentation Previous Manage domains Add an asset to a domain"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/set-up-a-private-network-link-to-amazon-msk",
    "text": "Connect data Event/Messaging Amazon MSK Get Started Set up a private network link to Amazon MSK On this page Set up a private network link to Amazon MSK Who can do this? You will need your Amazon MSK or AWS administrator involved   -  you may not have access to run these tasks. AWS PrivateLink creates a secure, private connection between services running in AWS, ensuring that traffic between services remains within the AWS network. This document describes the steps to set this up between Amazon MSK and Atlan. Prerequisites â Before you can set up private network connectivity, ensure the following: Amazon MSK version: Apache Kafka 2.7.1 or higher. Authentication type: only IAM role-based authentication is supported. Cluster instance type: must be larger than t3.small. Region alignment: both your Amazon MSK cluster and Atlan tenant must reside in the same AWS region. For more information, refer to Requirements and Limitations for Multi-VPC Private Connectivity . Request Atlan's details â To configure private network connectivity between your AWS account and Atlan, contact Atlan support for the following details: Atlan's AWS account ID Enable private network link â To verify or enable AWS PrivateLink for Amazon MSK: Sign in to the AWS Management Console and open the Amazon MSK Console . From the left menu, click Clusters . On the Clusters page, under Cluster name , select the cluster for which you want to enable private network link. On your cluster page, below the overview section, click the Properties tab. In the Properties tab, navigate to the Networking settings section to verify or enable AWS PrivateLink connectivity: If you have verified that AWS PrivateLink is turned on, skip to the next section. If AWS PrivateLink is turned off, click the Edit button and then click Turn on multi-VPC connectivity to enable it. In the Turn on multi-VPC connectivity page, for Authentication type , click IAM role-based authentication . At the bottom of the screen, click Turn on selection . The cluster will undergo a rolling update, which may take several minutes to a few hours to complete. Grant access to Atlan â Once AWS PrivateLink is enabled for your Amazon MSK cluster, you will need to update the cluster policy to grant access to Atlan. To update your Amazon MSK cluster policy: Sign in to the AWS Management Console and open the Amazon MSK Console . From the left menu, click Clusters . On the Clusters page, under Cluster name , select the cluster for which you enabled private network link. On your cluster page, below the overview section, click the Properties tab. In the Properties tab, navigate to the Security settings section and then click Edit cluster policy . In the Edit cluster policy page, under Cluster policy , configure the following: Click Basic as the new cluster policy. For Account ID(s) that need cluster access , enter Atlan's AWS account ID. Click the Include Kafka service principal checkbox to allow Atlan access to Kafka services only. Click Save changes to save your selections. Notify Atlan support team â Once you've completed the steps above, contact the Atlan support team again and provide the following details for your Amazon MSK cluster: Amazon MSK Cluster ARN   -  the unique identifier of your cluster Atlan will create a managed VPC connection to your Amazon MSK cluster . Once completed, Atlan support will send you the cluster connection string (bootstrap servers) required for accessing Amazon MSK via AWS PrivateLink. You can now enter the cluster connection string for the Bootstrap servers field to crawl Amazon MSK . Atlan will securely connect to your Amazon MSK cluster using AWS PrivateLink. Tags: atlan documentation Previous Set up Amazon MSK Next Crawl Amazon MSK Prerequisites Request Atlan's details Enable private network link Grant access to Atlan Notify Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-a-private-network-link-to-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift Get Started Set up a private network link to Amazon Redshift On this page Set up a private network link to Amazon Redshift Who can do this? You will need your Amazon Redshift administrator or AWS administrator involved   -  you may not have access to run these tasks. Redshift-managed VPC endpoints create a secure, private connection between services running in AWS. This document describes the steps to set this up between Amazon Redshift and Atlan, when you use our Single Tenant SaaS deployment. Prerequisites â Your Redshift cluster must be an RA3 node type. Your Redshift cluster must have cluster relocation turned on . Your Redshift cluster must be available through port 5439. You must have spare capacity in your VPC endpoint quota . (For all details, see Working with Redshift-managed VPC endpoints in Amazon Redshift .) Request Atlan's details â Before granting access to your Redshift cluster to Atlan, you will need the following: Atlan's AWS account ID Atlan's VPC ID for the connection Request these from Atlan support . Grant access to Atlan â Once you've received the details above, to grant Atlan access to your Redshift cluster : Sign in to the AWS Management Console and open the Amazon Redshift console . From the navigation menu, click Clusters . From the table, click the name of the cluster to which you want to grant access. Change to theÂ Properties tab of the cluster. Under theÂ Granted accounts section, clickÂ Grant access . In theÂ Grantee information form: ForÂ AWS account ID , enter the Atlan AWS account ID. ForÂ VPC , choose Grant access to specific VPCs and enter the Atlan VPC ID. At the bottom right, click the Grant access button. Notify Atlan support team â Once you've completed the steps above, contact the Atlan support team again and provide the following details for your Redshift cluster: AWS account ID Redshift cluster identifier   -  the unique identifier of your cluster Atlan will create a Redshift-managed VPC endpoint , and then reply to you with a hostname. When you use this hostname in the configuration for crawling , Atlan will connect to Redshift over the private network. Tags: atlan documentation Previous How to enable SSO for Amazon Redshift Next Crawl Amazon Redshift Prerequisites Request Atlan's details Grant access to Atlan Notify Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-a-private-network-link-to-microsoft-sql-server-on-amazon-ec2",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server Private Network Set up a private network link to Microsoft SQL Server on Amazon EC2 On this page Set up a private network link to Microsoft SQL Server on Amazon EC2 Who can do this? You will need your AWS administrator to complete these tasks   -  you may not have access yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Microsoft SQL Server on Amazon EC2 and Atlan. Prerequisites â You should already have the following: Your own non-default VPC configured in AWS. A Microsoft SQL Server on Amazon EC2 instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from Atlan support . Create security group â You will need to create a security group for the following: Microsoft SQL Server on Amazon EC2 instance Network Load Balancer (NLB) Microsoft SQL Server on Amazon EC2 instance â You can either create a new security group or add the following rule to an existing security group already attached to your Microsoft SQL Server on Amazon EC2 instance. To create a security group for your Microsoft SQL Server on Amazon EC2 instance: Open the Amazon VPC console . From the left menu, under Security , click Security Groups . Click the Create security group button. Enter a name and description for the new security group. From the VPC list, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. For Inbound rules , leave this blank until after you have created a security group for the Network Load Balancer . Return to this step once you have created it, click the Add rule button, and then add the following rule: For Type , use MSSQL if you are using the default port (1433), or use Custom and enter your port under Port range . For Destination , add the security group you created for the NLB . Click Create security group to finish setup. Network Load Balancer â To create a security group for the Network Load Balancer: Open the Amazon VPC console . From the left menu, under Security , click Security Groups . Click the Create security group button. Enter a name and description for the new security group. From the VPC list, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. For Outbound rules , click the Add rule button and then add the following rule: For Type , use MSSQL if you are using the default port (1433), or use Custom and enter your port under Port range . For Destination , add the security group you created for your Microsoft SQL Server on Amazon EC2 instance . Click Save . Click Create security group to finish setup. Create a target group â To create a target group for the NLB: Open the Amazon EC2 console . From the left menu, under Load Balancing , click Target Groups . Click Create target group . For Basic configuration , enter the following details: For Choose a target type , keep Instances . For Target group name , enter a unique name for the new target group. For Protocol , select TCP . For Port ,Â enterÂ 1433 . For IP address type , selectÂ IPv4 . For VPC , select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. In the Health checks section, change the protocol to TCP and keep Advanced health check settings as the default. Click Next to proceed. To register your Amazon EC2 instance, on the Register targets page: For Available instances , select your Amazon EC2 instance running Microsoft SQL Server. Keep the default port 1433 and then choose Include as pending below . At the bottom of the form, click theÂ Create target group button. Create internal Network Load Balancer â To create an NLB: Open the Amazon EC2 console . From the left menu, underÂ Load Balancing , click Load Balancers . At the top of the screen, click theÂ Create Load Balancer button. Under theÂ Network Load Balancer option, click theÂ Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name , enter a unique name. ForÂ Scheme , selectÂ Internal . ForÂ IP address type , selectÂ IPv4 . Enter the followingÂ Network mapping settings for the load balancer: ForÂ VPC , select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. ForÂ Mappings , select the availability zones with private subnets. For Security groups , select the security group you created for the Network Load Balancer. info ðª Did you know? The Enforce inbound rules on PrivateLink traffic setting is turned on by default and cannot be modified until after the load balancer has been created. If this setting is left on, you will need to contact Atlan support and request the CIDR range of Atlan's cluster to add as an inbound rule on the NLB security group . To turn it off, follow these instructions . Enter the following Listeners and routing settings for the load balancer: For Protocol , select TCP . For Port , enter 1433 . For Target group , select the target group you created. Review your configuration, and click Create load balancer . Verify target group is healthy â To verify that the target group is healthy: From the EC2 menu on the left, underÂ Load Balancing , clickÂ Target Groups . From theÂ Target groups table, click the link to the target group you created above. At the bottom of the screen, under theÂ Details tab, check that there is a 1 under bothÂ Total targets andÂ Healthy . (Note: This number could be more than 1 if you have a multi-node deployment.) Create endpoint service â To create an endpoint service: Open the Amazon VPC console . From the left menu, under Virtual private cloud , click Endpoint services . At the top of the page, click theÂ Create endpoint service button. Enter the followingÂ Endpoint service settings : ForÂ Name , enter a meaningful name. ForÂ Load balancer type , chooseÂ Network . ForÂ Available load balancers , select the load balancer you created above. Enter the followingÂ Additional settings : ForÂ Require acceptance for endpoint ,Â enableÂ Acceptance required to require manual acceptance of connection requests to your endpoint service. Otherwise, these requests will be accepted automatically. For Enable private DNS name , leave unchecked. ForÂ Supported IP address types , enableÂ IPv4 . At the bottom of the form, click theÂ Create button. Once the endpoint service has been created, navigate to the Details page. From the Details page: Under Service Name , copy the value to send to Atlan. Under Availability Zones , copy the zones to send to Atlan. Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of theÂ Allow principals table, click theÂ Allow principals button. UnderÂ Principals to add andÂ ARN , enter the Atlan account ID and root principal   -  for example, arn:aws:iam::<account_id>:root . At the bottom of the form, click theÂ Allow principals button. Notify Atlan support â Once all of the above steps have been completed, contact Atlan support and provide the following details: Service name of the endpoint service Availability zones for the endpoint service There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS : Navigate toÂ Services , thenÂ Networking & Content Delivery , thenÂ VPC . From the menu on the left, underÂ Virtual private cloud , clickÂ Endpoint services . From theÂ Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to theÂ Endpoint connections tab. You should see a row in theÂ Endpoint connections table with aÂ State ofÂ Pending acceptance . Select this row, and click theÂ Actions button and thenÂ Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. Request DNS name from Atlan â Contact Atlan support to request the regional DNS name of the VPC endpoint that Atlan created in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to your Microsoft SQL Server on Amazon EC2 instance from within Atlan. ð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl Microsoft SQL Server in Atlan! ð Tags: atlan documentation Previous Crawl Microsoft SQL Server Next Set up a private network link to Microsoft SQL Server on Amazon RDS Prerequisites Create security group Create a target group Create internal Network Load Balancer Verify target group is healthy Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request Request DNS name from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-a-private-network-link-to-microsoft-sql-server-on-amazon-rds",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server Private Network Set up a private network link to Microsoft SQL Server on Amazon RDS On this page Set up a private network link to Microsoft SQL Server on Amazon RDS Who can do this? You will need your AWS administrator to complete these tasks   -  you may not have access yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Microsoft SQL Server on Amazon RDS and Atlan. Prerequisites â You should already have the following: Your own non-default VPC configured in AWS. A Microsoft SQL Server on Amazon RDS instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from Atlan support . Set up network to RDS (in AWS) â To set up the private network of your Microsoft SQL Server instance, from within AWS : Copy network settings â Navigate to Services , then Database , and then RDS . On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, copy the following values: Endpoint and Port values VPC value Subnet group value On the left, click Subnet groups . From the table, click the row whose Name matches the subnet group copied above. From the Subnets table, copy each value under the CIDR block column for private subnets. Create inbound rule â To create an inbound rule allowing your private subnet access to your RDS instance: On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, under the Security column and the VPC security groups heading, click the link to your security group. At the bottom of the screen, change to the Inbound rules tab and then click the Edit inbound rules button. At the bottom of the table, click the Add rule button and create the following rule: For Type , use SQL Server if you are using the default port (1433), or use Custom and enter your port under Port range . For Source , use Custom and enter your CIDR range (see Copy network settings ). Repeat these sub-steps for each of your CIDR ranges. Below the table, click the Save rules button. (Optional) Create RDS proxy â Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This method uses a username and password to connect to the RDS database. To create an RDS proxy for your RDS instance: On the left, under Amazon RDS , click on Proxies . In the upper right of the Proxies table, click the Create proxy button. Under Proxy configuration , enter the following details: For Engine family , select SQL Server . For Proxy identifier , enter a meaningful name for your proxy. Under Target group configuration for Database , choose your RDS instance. Under Authentication for the Secrets Manager secrets : If you have an existing secret for your RDS instance's database credentials, select it from the dropdown. If not, click the Create a new secret link and enter these details in the new tab: For Secret type , select Credentials for Amazon RDS database . For Credentials , enter the Username and Password of the database user. Under Database , select your RDS instance. At the bottom of the form, click the Next button. For Secret name , enter a name for the secret. At the bottom of the form, click the Next button. Leave the automatic secret rotation off and click the Next button. Review the secret definition and click the Store button. Return to the tab where you started creating the RDS proxy. Under Authentication for IAM authentication : If IAM authentication is set to Required , Atlan will use an IAM role to connect to the RDS proxy. If IAM authentication is set to Not Allowed , basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy. Under Connectivity , expand the Additional connectivity configuration : For VPC security group , select Choose existing . For Existing VPC security groups , select the security group you edited with the inbound rules above. At the bottom right of the form, click the Create proxy button. From the Proxies table, click the link for the proxy you just created. Under Proxy endpoints section, copy the hostname in the Endpoint column. Create internal Network Load Balancer â Retrieve IP address of the RDS â From an EC2 instance in your AWS account, run the following command: nslookup <endpoint> Replace <endpoint> with the fully-qualified endpoint hostname copied from the RDS endpoint or RDS proxy created above. Copy the IP address that comes back from the command, under Non-authoritative answer and to the right of Address . Start creating NLB â To create an NLB, from within AWS : Navigate to Services , then Compute , and then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name , enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where the RDS instance is located (see Copy network settings ). For Mappings , select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port , enter 1433 (or the non-default port value from Copy network settings ). For Default action , click the Create target group link. This will open the target group creation in a new browser tab. Create target group â To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type , select IP addresses . For Target group name , enter a name. For Port , enter 1433 (or the non-default port value from Copy network settings ). For IP address type , select IPv4 . For VPC , select the VPC where the RDS instance is located (see Copy network settings ). At the bottom of the form, click the Next button. Enter the following IP addresses settings for the target group: For Network , select the VPC where the RDS instance is located (see Copy network settings ). For IPv4 address , enter the IP address returned by the nslookup command (see Retrieve IP address of the RDS ). For Ports , enter 1433 (or the non-default port value from Copy network settings ). At the bottom of the IP addresses section, click the Include as pending below button. Confirm the following Review targets settings for the target group: Confirm IP address matches the IP address returned by the nslookup command. Confirm Port is 1433 (or the non-default port value used by your RDS instance). At the bottom of the form, click the Create target group button. Finish creating NLB â Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action dropdown box. Select the target group you created above in the Default action drop-down. At the bottom of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy â To verify that the target group is healthy: From the EC2 menu on the left, under Load Balancing , click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service â To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , and then VPC . From the menu on the left, under Virtual private cloud , click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name , enter a meaningful name. For Load balancer type , choose Network . For Available load balancers , select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint , enable Acceptance required . For Supported IP address types , enable IPv4 . At the bottom of the form, click the Create button. Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN , enter the Atlan account ID and root principal -  for example, arn:aws:iam::<account_id>:root . At the bottom of the form, click the Allow principals button. Notify Atlan support â Once all of the above steps are complete, contact Atlan support . You will need to provide Atlan support: The RDS proxy or RDS endpoint DNS -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively. Once this is done, there are additional steps that Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. Request DNS name from Atlan â Contact Atlan support to request the regional DNS name of the VPC endpoint that Atlan created in the following format - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to your Microsoft SQL Server on Amazon RDS instance from within Atlan. ð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl Microsoft SQL Server in Atlan! ð Tags: atlan documentation Previous Set up a private network link to Microsoft SQL Server on Amazon EC2 Next What does Atlan crawl from Microsoft SQL Server? Prerequisites Set up network to RDS (in AWS) (Optional) Create RDS proxy Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request Request DNS name from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
    "text": "Connect data Databases SQL Databases MySQL Get Started Set up a private network link to MySQL On this page Set up a private network link to MySQL Who can do this? You will need your AWS administrator to complete these tasks   -  you may not have access yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between MySQL (RDS) and Atlan. Prerequisites â You should already have the following: Your own non-default VPC configured in AWS. A MySQL RDS instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Setup network to RDS (in AWS) â To setup the private network of your MySQL instance, from within AWS : Copy network settings â Navigate to Services , then Database , then RDS . On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, copy the following values: Endpoint and Port values VPC value Subnet group value On the left, click Subnet groups . From the table, click the row whose Name matches the subnet group copied above. From the Subnets table, copy each value under the CIDR block column for private subnets. Create inbound rule â To create an inbound rule allowing your private subnet access to your RDS instance: On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, under the Security column and the VPC security groups heading click the link to your security group. At the bottom of the screen, change to the Inbound rules tab and click the Edit inbound rules button. At the bottom of the table, click the Add rule button and create the following rule: For Type use MySQL/Aurora if you are using the default port (3306), or use Custom and enter your port under Port range . For Source use Custom and enter your CIDR range (see Copy network settings ). Repeat these sub-steps for each of your CIDR ranges. Below the table, click the Save rules button. (Optional) Create RDS proxy â Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This method uses a username and password to connect to the RDS database. To create an RDS proxy for your RDS instance: On the left, under Amazon RDS , click on Proxies . In the upper right of the Proxies table, click the Create proxy button. Under Proxy configuration enter the following details: For Engine family select MySQL . For Proxy identifier enter a meaningful name for your proxy. Under Target group configuration for Database choose your RDS instance. Under Authentication for the Secrets Manager secrets : If you have an existing secret for your RDS instance's database credentials, select it from the drop-down. If not, click the Create a new secret link and enter these details in the new tab: For Secret type select Credentials for Amazon RDS database . For Credentials enter the Username and Password of the database user. Under Database select your RDS instance. At the bottom of the form, click the Next button. For Secret name enter a name for the secret. At the bottom of the form, click the Next button. Leave the automatic secret rotation off and click the Next button. Review the secret definition and click the Store button. Return to the tab where you started creating the RDS proxy. Under Authentication for IAM authentication : If IAM authentication is set to Required , Atlan will use an IAM role to connect to the RDS proxy. If IAM authentication is set to Not Allowed , basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy. Under Connectivity expand the Additional connectivity configuration : For VPC security group select Choose existing . For Existing VPC security groups select the security group you edited with the inbound rules above. At the bottom right of the form, click the Create proxy button. From the Proxies table, click the link for the proxy you just created. Under Proxy endpoints section, copy the hostname in the Endpoint column. Create internal Network Load Balancer â Retrieve IP address of the RDS â From an EC2 instance in your AWS account, run the following command: nslookup <endpoint> Replace <endpoint> with the fully-qualified endpoint hostname copied from the RDS endpoint or RDS proxy created above. Copy the IP address that comes back from the command, under Non-authoritative answer and to the right of Address . Start creating NLB â To create an NLB, from within AWS : Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name enter a unique name. For Scheme select Internal . For IP address type select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC select the VPC where the RDS instance is located (see Copy network settings ). For Mappings select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port enter 3306 (or the non-default port value from Copy network settings ). For Default action click the Create target group link. This will open the target group creation in a new browser tab. Create target group â To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type select IP addresses . For Target group name enter a name. For Port enter 3306 (or the non-default port value from Copy network settings ). For IP address type select IPv4 . For VPC select the VPC where the RDS instance is located (see Copy network settings ). At the bottom of the form, click the Next button. Enter the following IP addresses settings for the target group: For Network select the VPC where the RDS instance is located (see Copy network settings ). For IPv4 address enter the IP address returned by the nslookup command (see Retrieve IP address of the RDS). For Ports enter 3306 (or the non-default port value from Copy network settings ). At the bottom of the IP addresses section, click the Include as pending below button. Confirm the following Review targets settings for the target group: Confirm IP address matches the IP address returned by the nslookup command. Confirm Port is 3306 (or the non-default port value used by your RDS instance). At the bottom of the form, click the Create target group button. Finish creating NLB â Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy â To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service â To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom of the form, click the Create button. Did you know? Under the Details of the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to the RDS instance from within Atlan. Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID and specified principal. At the bottom of the form, click the Allow principals button. Notify Atlan support â Once all of the above steps are complete, contact Atlan support . You will need to provide Atlan support: The RDS proxy or RDS endpoint DNS   -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively. Once this is done, there are additional steps that Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. ð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl MySQL in Atlan! ð Tags: atlan documentation Previous Set up MySQL Next Crawl MySQL Prerequisites Setup network to RDS (in AWS) (Optional) Create RDS proxy Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
    "text": "On this page Set up a private network link to PostgreSQL AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between PostgreSQL (RDS) and Atlan, when you use our Single Tenant SaaS deployment. Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. Prerequisites â You should already have the following: Your own non-default VPC configured in AWS. A PostgreSQL RDS instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Setup network to RDS (in AWS) â To setup the private network of your PostgreSQL instance, from within AWS : Copy network settings â Navigate to Services , then Database , then RDS . On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, copy the following values: Endpoint and Port values VPC value Subnet group value On the left, click Subnet groups . From the table, click the row whose Name matches the subnet group copied above. From the Subnets table, copy each value under the CIDR block column for private subnets. Create inbound rule â To create an inbound rule allowing your private subnet access to your RDS instance: On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, under the Security column and the VPC security groups heading click the link to your security group. At the bottom of the screen, change to the Inbound rules tab and click the Edit inbound rules button. At the bottom of the table, click the Add rule button and create the following rule: For Type use PostgreSQL if you are using the default port (5432), or use Custom and enter your port under Port range . For Source use Custom and enter your CIDR range (see Copy network settings ). Repeat these sub-steps for each of your CIDR ranges. Below the table, click the Save rules button. (Optional) Create RDS proxy â Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This method uses a username and password to connect to the RDS database. To create an RDS proxy for your RDS instance: On the left, under Amazon RDS , click on Proxies . In the upper right of the Proxies table, click the Create proxy button. Under Proxy configuration enter the following details: For Engine family select PostgreSQL . For Proxy identifier enter a meaningful name for your proxy. Under Target group configuration for Database choose your RDS instance. Under Authentication for the Secrets Manager secrets : If you have an existing secret for your RDS instance's database credentials, select it from the drop-down. If not, click the Create a new secret link and enter these details in the new tab: For Secret type select Credentials for Amazon RDS database . For Credentials enter the Username and Password of the database user. Under Database select your RDS instance. At the bottom of the form, click the Next button. For Secret name enter a name for the secret. At the bottom of the form, click the Next button. Leave the automatic secret rotation off and click the Next button. Review the secret definition and click the Store button. Return to the tab where you started creating the RDS proxy. Under Authentication for IAM authentication : If IAM authentication is set to Required , Atlan will use an IAM role to connect to the RDS proxy. If IAM authentication is set to Not Allowed , basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy. Under Connectivity expand the Additional connectivity configuration : For VPC security group select Choose existing . For Existing VPC security groups select the security group you edited with the inbound rules above. At the bottom right of the form, click the Create proxy button. From the Proxies table, click the link for the proxy you just created. Under Proxy endpoints section, copy the hostname in the Endpoint column. Create internal Network Load Balancer â Retrieve IP address of the RDS â From an EC2 instance in your AWS account, run the following command: nslookup <endpoint> Replace <endpoint> with the fully-qualified endpoint hostname copied from the RDS proxy created above. Copy the IP address that comes back from the command, under Non-authoritative answer and to the right of Address . Start creating NLB â To create an NLB, from within AWS : Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name enter a unique name. For Scheme select Internal . For IP address type select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC select the VPC where the RDS instance is located (see Copy network settings ). For Mappings select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port enter 5432 (or the non-default port value from Copy network settings ). For Default action click the Create target group link. This will open the target group creation in a new browser tab. Create target group â To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type select IP addresses . For Target group name enter a name. For Port enter 5432 (or the non-default port value from Copy network settings ). For IP address type select IPv4 . For VPC select the VPC where the RDS instance is located (see Copy network settings ). At the bottom of the form, click the Next button. Enter the following IP addresses settings for the target group: For Network select the VPC where the RDS instance is located (see Copy network settings ). For IPv4 address enter the IP address returned by the nslookup command (see Retrieve IP address of the RDS ). For Ports enter 5432 (or the non-default port value from Copy network settings ). At the bottom of the IP addresses section, click the Include as pending below button. Confirm the following Review targets settings for the target group: Confirm IP address matches the IP address returned by the nslookup command. Confirm Port is 5432 (or the non-default port value used by your RDS instance). At the bottom of the form, click the Create target group button. Finish creating NLB â Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy â To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service â To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom of the form, click the Create button. Did you know? Under the Details of the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to the RDS instance from within Atlan. Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID. At the bottom of the form, click the Allow principals button. Notify Atlan support â Once all of the above steps are complete, contact Atlan support . You will need to provide Atlan support: The name of the endpoint service created in the previous step . Go to Endpoint Services and copy the \" Service Name \". The RDS proxy or RDS endpoint DNS   -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively. Once this is done, there are additional steps that Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. ð The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl PostgreSQL in Atlan! ð Tags: atlan documentation Prerequisites Setup network to RDS (in AWS) (Optional) Create RDS proxy Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-a-private-network-link-to-tableau-server",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau Get Started Set up a private network link to Tableau server On this page Set up a private network link to Tableau server AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Tableau serverÂ and Atlan, when you use our Single Tenant SaaS deployment. Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. Prerequisites â You should already have the following: Tableau instance running in AWS (private EC2 instance). Atlan hosted in the same region as the Tableau instance. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Setup network to EC2 instance â To setup the private network of your Tableau EC2 instance, from within AWS : Copy network settings â To copy the network settings of your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click on your Tableau EC2 instance. Under the instance's Details tab: Under VPC ID copy the VPC identifier. Under Subnet ID click the subnet for the instance. In the Subnets table, copy the value under the IPv4 CIDR column. Create inbound rule â To create an inbound rule allowing your private subnet access to your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click on your Tableau EC2 instance. Under the instance's details, change to the Security tab. Under Security groups click the security group for the instance. Under the Inbound rules tab, click the Edit inbound rules button. At the bottom left of the Inbound rules table, click the Add rule button. For Type , select Custom TCP . For Port range , enter the port on which Tableau is accessible (for example, default port 80 and TLS port 443 ). For Source , choose Custom and enter the CIDR range for your Tableau instance (see Copy network settings ). Below the bottom right of the Inbound rules table, click the Save rules button. Create internal Network Load Balancer â Start creating NLB â To create an NLB, from within AWS: Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name enter a unique name. For Scheme select Internal . For IP address type select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC select the VPC where the Tableau instance is located (see Copy network settings ). For Mappings select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port enter 80 (or the non-default port value used in Created inbound rule ). For Default action click the Create target group link. This will open the target group creation in a new browser tab. Create target group â To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type select Instances . For Target group name enter a name. For Port enter 80 (or the non-default port value used in Create inbound rule ). For VPC select the VPC where the Tableau instance is located (see Copy network settings ). At the bottom of the form, click the Next button. From the Available instances table: Click the checkbox next to your Tableau instance. Enter the port for the instance (80 or non-default value used in steps above). Click the Include as pending below button. At the bottom right of the form, click the Create target group button. Finish creating NLB â Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom right of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy â danger As a prerequisite for TLS configuration on Tableau Server only, ensure that the health check Protocol of the target group is set to HTTPS or modify the health check settings as required. To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing , click Target Groups . From the Target groups table, click the row for the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service â To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom right of the form, click the Create button. Under the Details of the endpoint service, copy the hostname under Service name . Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID. At the bottom right of the form, click the Allow principals button. Notify Atlan support â Once all the above steps are complete, provide Atlan support with the following information: The hostname for the endpoint service created above. The port number for the Tableau instance. For SSL certificates only, the private DNS name for which you have issued an SSL certificate on your Tableau Server instance. There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. For SSL certificates only, creating a DNS CNAME record pointing the private DNS name shared above to the VPC endpoint URL. This will allow Atlan to use your private DNS name with the SSL certificate. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS: Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending . Select this row, and click the Actions button and then Accept endpoint connection request . If prompted to confirm, type accept into the field and click the Accept button. Wait for this to complete, it could take about 30 seconds. ð The connection is now established. You can now use the service endpoint provided by Atlan support (or the private DNS name for SSL certificates ) as the hostname to crawl Tableau in Atlan! ð Tags: atlan documentation Previous Set up on-premises Tableau access Next Crawl Tableau Prerequisites Setup network to EC2 instance Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/set-up-amazon-athena",
    "text": "Connect data Databases Query Engines Amazon Athena Get Started Set up Amazon Athena On this page Set up Amazon Athena warning ð¤ Who can do this? You will probably need your Amazon Athena administrator to run these commands   -  you may not have access yourself. Did you know? Prefixing all resources created for Atlan with atlan- will help you better identify them. You should also add AWS tags and descriptions to these resources for later reference. Create IAM policy â To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" , \"glue:GetTables\" , \"glue:GetDatabases\" , \"glue:GetTable\" , \"glue:GetDatabase\" , \"glue:SearchTables\" , \"glue:GetTableVersions\" , \"glue:GetTableVersion\" , \"glue:GetPartition\" , \"glue:GetPartitions\" , \"glue:GetUserDefinedFunctions\" , \"glue:GetUserDefinedFunction\" , \"athena:GetTableMetadata\" , \"athena:StartQueryExecution\" , \"athena:GetQueryResults\" , \"athena:GetDatabase\" , \"athena:GetDataCatalog\" , \"athena:ListQueryExecutions\" , \"athena:GetWorkGroup\" , \"athena:StopQueryExecution\" , \"athena:GetQueryResultsStream\" , \"athena:ListDatabases\" , \"athena:GetQueryExecution\" , \"athena:ListTableMetadata\" , \"athena:BatchGetQueryExecution\" ] , \"Resource\" : [ \"arn:aws:glue:<region>:<account_id>:tableVersion/*/*/*\" , \"arn:aws:glue:<region>:<account_id>:table/*/*\" , \"arn:aws:glue:<region>:<account_id>:catalog\" , \"arn:aws:glue:<region>:<account_id>:database/*\" , \"arn:aws:athena:<region>:<account_id>:datacatalog/*\" , \"arn:aws:athena:<region>:<account_id>:workgroup/*\" , \"arn:aws:s3:::<data_bucket>\" , \"arn:aws:s3:::<data_bucket>/*\" ] } , { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:ListBucketMultipartUploads\" , \"s3:AbortMultipartUpload\" , \"s3:ListBucket\" , \"s3:GetBucketLocation\" , \"s3:ListMultipartUploadParts\" ] , \"Resource\" : [ \"arn:aws:s3:::<s3_bucket>/*\" , \"arn:aws:s3:::<s3_bucket>\" ] } , { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : \"athena:ListDataCatalogs\" , \"Resource\" : \"*\" } ] } Replace <region> with the AWS region of your Athena instance. Replace <account_id> Â with your account ID. Replace <data_bucket> with the S3 bucket where your actual data resides, such as your Glue tables. Replace <s3_bucket> with the S3 bucket where Athena can store temporary Athena query results. info ðª Did you know? We recommend using Atlan's deployment bucket to store these results. This ensures all Atlan assets are managed in a single bucket. If you have an external Hive metastore connected to Athena, also add these policies: { \"Sid\" : \"VisualEditor3\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:InvokeFunction\" , \"lambda:GetFunction\" ] , \"Resource\" : [ \"arn:aws:lambda:<region>:<account_id>:function:<lambda_fn_name>\" ] } Replace <region> with the AWS region of your Athena instance. Replace <account_id> Â with your account ID. Replace <lambda_fn_name> with the name of the Lambda function used to configure the catalog. These allow Atlan to trigger the Lambda function. danger If you're using AWS Lake Formation to manage access to your AWS resources, you will need to grant permissions in AWS Lake Formation as well as to the objects you want to crawl. Choose authentication mechanism â Using the policy created above, configure one of the following options for authentication. User-based authentication â To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On theÂ Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication â To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security: Generate the external ID within Atlan . Paste the external ID into the policy (replace <atlan_generated_external_id> with it): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_generated_external_id>\" } } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: atlan documentation Previous Amazon Athena Next Set up a private network link to Amazon Athena Create IAM policy Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/how-tos/set-up-apache-kafka",
    "text": "Connect data Event/Messaging Apache Kafka Get Started Set up Apache Kafka On this page Set up Apache Kafka Who can do this? You will probably need your Apache Kafka administrator to run these commands   -  you may not have access yourself. Atlan supports different authentication mechanisms to securely access your Apache Kafka cluster. If the cluster is configured with \"No Auth\" (authentication not needed), Atlan connects directly. If the cluster requires authentication, you must configure it for Atlan to access your Apache Kafka cluster. Atlan supports the following authentication flows: Basic authentication using a username and password with SASL_PLAIN mechanism SCRAM authentication using a username and password with SASL_SCRAM mechanism Use basic authentication with SASL_PLAIN mechanism â With basic authentication using SASL_PLAIN , Atlan authenticates with Kafka using a username and password. To authenticate Atlan with Kafka using SASL_PLAIN , complete the following steps on each broker: Create user by defining the user credentials in a `JAAS` login configuration file: KafkaServer { org.apache.kafka.common.security.plain.PlainLoginModule required username=\"<kafka admin username>\" password=\"<kafka admin password>\" user_<username> = \"<password>\"; }; Replace <kafka admin username> and <kafka admin password> with the administrator credentials for Kafka. Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Pass the JAAS file as a JVM configuration option when running the broker: export KAFKA_OPTS=\"-Djava.security.auth.login.config=<path-to-jaas-file>/jaas-kafka-server.conf\" Atlan does not make any API requests or queries that update the resources in your Kafka cluster. Set the minimum necessary permissions for resources: Grant topic permissions to read and describe topics with the following command: ./bin/kafka-acls.sh --topic '*' --add --allow-principals user:atlan --operations Read,Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/server.properties Grant consumer group permissions to read and describe consumer groups with the following command: ./bin/kafka-acls.sh --consumer-group '*' --add --allow-principals user:atlan --operations Read,Describe --allow-host '*' --config /<broker-config-path>/config/server.properties Grant cluster permissions to describe cluster configurations with the following command: ./bin/kafka-acls.sh --cluster --add --allow-principals user:atlan --operations Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/config/server.properties Once you have configured the Apache Kafka brokers, restart your brokers to pick up the new configuration. Use SCRAM authentication with SASL_SCRAM mechanism â SCRAM (Salted Challenge Response Authentication Mechanism) provides more security than SASL_PLAIN . To use SCRAM authentication, complete the following steps on each broker: Create the SCRAM user using SCRAM-SHA-256 or SCRAM-SHA-512 mechanism based on the mechanism set up on your Apache Kafka cluster: Use SCRAM-SHA-256 mechanism: bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-256=[iterations=4096,password=<password>]\" Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Use SCRAM-SHA-512 mechanism: kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-512=[iterations=4096,password=<password>]\" Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Use both SCRAM-SHA-256 and SCRAM-SHA-512 mechanisms: kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-256=[iterations=4096,password=<password>],SCRAM-SHA-512=[iterations=4096,password=<password>]\" Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Verify the user configuration: kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type users --entity-name <username> Replace <username> with the username you want to use in Atlan. The SCRAM authentication needs a JAAS file. If the file doesn't exist, create one with the following content: KafkaServer { org.apache.kafka.common.security.plain.PlainLoginModule required username=\"<kafka admin username>\" password=\"<kafka admin password>\" }; Pass in the JAAS file as a JVM configuration option when running the broker: export KAFKA_OPTS=\"-Djava.security.auth.login.config=<path-to-jaas-file>/jaas-kafka-server.conf\" Atlan does not make any API requests or queries that update the resources in your Kafka cluster. Set the minimum necessary permissions for resources: Grant topic permissions to read and describe topics with the following command: ./bin/kafka-acls.sh --topic '*' --add --allow-principals user:atlan --operations Read,Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/server.properties Grant consumer group permissions to read and describe consumer groups with the following command: ./bin/kafka-acls.sh --consumer-group '*' --add --allow-principals user:atlan --operations Read,Describe --allow-host '*' --config /<broker-config-path>/config/server.properties Grant cluster permissions to describe cluster configurations with the following command: ./bin/kafka-acls.sh --cluster --add --allow-principals user:atlan --operations Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/config/server.properties Once you have configured the Apache Kafka brokers, restart your brokers to pick up the new configuration. Tags: atlan documentation Previous Apache Kafka Next Crawl Apache Kafka Use basic authentication with SASL_PLAIN mechanism Use SCRAM authentication with SASL_SCRAM mechanism"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda",
    "text": "Configure Atlan Integrations Automation AWS Lambda Set up AWS Lambda On this page Set up AWS Lambda warning ð¤ Who can do this? You will probably need your AWS Lambda administrator to run these commands   -  you may not have access yourself. Create IAM policy â To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:InvokeFunction\" , \"lambda:InvokeAsync\" \"lambda:ListFunctions\" ] , \"Resource\" : \"*\" } ] } Choose authentication mechanism â Using the policy created above, configure one of the following options for authentication. User-based authentication â To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On theÂ Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role-based authentication â To configure role-based authentication, attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster. Please raise a support ticket to use this option. Role delegation-based authentication â To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the Lambda function. Tags: atlan documentation Previous AWS Lambda Next Create an AWS Lambda trigger Create IAM policy Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/how-tos/set-up-cloudera-impala",
    "text": "Connect data Databases Query Engines Cloudera Impala Get Started Set up Cloudera Impala On this page Set up Cloudera Impala Who can do this? You will probably need your Cloudera Impala instance administrator to complete these steps â you may not have access yourself. This guide provides step-by-step instructions to configure user access and grant the required permissions in Cloudera Impala so that Atlan can crawl metadata. Create user â Create a user in your LDAP system for Atlan to authenticate with Impala. You can use identity providers like OpenLDAP, Active Directory, or any other service your organization uses to create this user. Based on the authorization service your organization uses with Impala, sync the created user with either Ranger or Sentry. For Ranger, follow the Ranger Authentication and User Sync documentation . For Sentry, refer to the Sentry Overview documentation . Connect to Impala using the admin user from either Ranger or Sentry to manage permissions. Grant permissions to assets â There are three ways in which you can grant permissions to assets, depending on your requirements for crawling assets. Who can do this? The Impala or Ranger administrator likely needs to complete these steps, as you may not have the required access. Grant permission to crawl schema â To provide the SELECT privilege for the entire schema, run the following command: GRANT SELECT ON SCHEMA <schema_name> TO USER <atlan-user>; Repeat the above command for each schema you want to crawl. Grant permission to crawl specific tables â To grant access to a specific table, run the following command: GRANT SELECT ON TABLE <table_name> TO USER <atlan-user>; Replace <table_name> with the name of the table. Grant permission to crawl specific columns â To grant column-level access, use the following command: GRANT SELECT(column1, column2) ON TABLE <table_name> TO USER <atlan-user>; Replace column1 , column2 with the relevant column names. Replace <table_name> the relevant table name. (Optional) Grant permission to calculate specific attributes â Run the following SQL commands: GRANT ALTER ON TABLE <table_name> TO USER <atlan-user>; GRANT SELECT ON TABLE <table_name> TO USER <atlan-user>; Replace <table_name> with the name of the table. These permissions are needed to calculate attributes like rowCount and sizeBytes for the tables. Tags: atlan documentation Previous Cloudera Impala Next Crawl Cloudera Impala Create user Grant permissions to assets (Optional) Grant permission to calculate specific attributes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core",
    "text": "Connect data ETL Tools dbt Get Started Set up dbt Core On this page Set up dbt Core This guide explains how to set up dbt Core in Atlan, including configuring access, organizing your storage bucket, and uploading the necessary metadata files so Atlan can process and analyze your dbt project data. Setup and access management â In this section, learn how to configure access for dbt Core so Atlan can connect to your storage location and read the required metadata. Choose between using your own cloud storage bucket or an Atlan-managed bucket. Use your own bucket (recommended) Use Atlan bucket Depending on the cloud provider in use, go to Marketplace â search for dbt â click to set up dbt â select Object Storage, and then choose the desired cloud provider. Atlan supports reading from AWS, Azure, and GCP. The setup process prompts for the information required for each cloud provider. For authentication, refer to the following: Amazon S3 â Please follow the instructions below in order to create the right IAM Role with the right permissions Azure ADLS â Please follow the instructions below in order to create the right Service principle with the right permissions Google GCS â Please follow the instructions below in order to create the right Service account with the right permissions To avoid access issues, Atlan can help you uploading the required files to the same bucket where your tenant is hosted. Amazon S3 â Raise a support request to get the details of your Atlan S3 bucket and include the ARN value of the IAM user or IAM role that Atlan can provision access to. You need to create an IAM policy and attach it to the IAM user or role to upload the required files to your Atlan bucket. To create an IAM policy with the necessary permissions, follow the steps below Google Cloud Storage â To use Atlan's Google Cloud Storage (GCS) bucket, first you have to create a new service account . Then Raise a support request to share the username of the service account with Atlan. The username is in the following format: [email protected] . The Atlan support team provides you with read and write access to a particular folder in the Atlan GCS bucket. Once Atlan has granted access, you can use the service account to upload the required files. Structure the bucket â Once you have configured access, the next step is to organize your storage bucket so that Atlan can correctly identify and process uploaded files. info Atlan uses the metadata.invocation_id and metadata.project_id attributes to uniquely identify and link the uploaded files. Atlan doesn't use the file paths to identify a project or job that the file belongs to. The following directory structure is provided as a guideline Atlan supports extracting dbt metadata from multiple or single dbt projects. The main-prefix has the following format gcs|s3://<BUCKET_NAME>/<PATH_PREFIX> or abfss://<CONTAINER>/<PATH> , if you used Atlan's bucket, the Atlan support team provides it after setting up access policies on your bucket. You need to use the following directory structure, even if you have a single dbt project: main-prefix - project1 - job1 - manifest.json - other files - job2 - manifest.json - other files - job4 - manifest.json - other files - project3 - job5 - manifest.json - other files Upload project files â To load correct metadata, Atlan processes the manifest.json and run_results.json files for each job. There are many ways to load the metadata, below are suggested approaches from Atlan. You need to upload the files from the target directory of the dbt project into distinct folders. Upload the run artifacts generated from the following commands: (Required) Compilation results: dbt compile --full-refresh This command generates files that contain a full representation of your dbt project's resources, including models, tests, macros, node configurations, resource properties, and more. Files to upload: manifest.json and run_results.json Alternatively, you can upload the same files by running the dbt run --full-refresh command. (Optional) Test results: dbt test This command executes all dbt tests in a dbt project and generates files that contain the test results. Files to upload: manifest.json and run_results.json (Optional) Catalog: dbt docs generate This command generates metadata about the tables and views produced by the models in your dbt project, for example, column data types and table statistics. Files to upload: manifest.json and catalog.json Tags: atlan documentation Previous Set up dbt Cloud Next Crawl dbt Setup and access management Structure the bucket Upload project files"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase",
    "text": "Connect data BI Tools Cloud-based BI Metabase Get Started Set up Metabase On this page Set up Metabase Who can do this? You will probably need your Metabase administrator to follow the below steps   -  you may not have access yourself. Create a user â To create a user for Atlan to use when integrating with Metabase: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . At the top of the page, change to the People tab. To the upper right of the table, click the Invite someone button and enter their details: For First name enter the user's first name, for example Atlan . For Last name enter the user's last name, for example User . For Email enter the user's email address, for example a service account email address. At the bottom of the dialog, click the Create button. When prompted, click Done . Create a group â You can only attach Metabase permissions to groups . To create a group for Atlan to use when integrating with Metabase: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the People tab. From the left of the page, open the Groups tab. At the top right, click the Create a group button. For Group name enter Atlan . On the right of the row click the Add button. To add the user to the group: Click the Atlan group you created. To the upper right of the table, click the Add members button. Under Members start typing the name used above (for example, Atlan User ) and select it. On the right of the row, click the Add button. Set permissions â Did you know? We do not make any API requests or queries that will update the dashboards , collections or questions in your Metabase instance. Minimum permissions â To set the minimum permissions required to crawl Metabase : From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the Permissions tab. From the top of the page, change to the Collection permissions tab. For each collection you want to crawl in Atlan: Under the Collections heading on the left, click the collection. Under Permissions for <collection name> , for the Atlan group, under Collection access click the No access drop-down. (Optional) To crawl sub-collections, toggle the Also change sub-collections option. Select the View permission. In the upper-right of the page, click the Save changes button. When prompted with Save permissions? click the Yes button to confirm. Partial lineage permissions â danger When a Metabase question uses native queries, these permissions cannot capture lineage to source tables and columns. To set the minimal permissions for extracting lineage from Metabase: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the Permissions tab. From the top of the page, change to the Data permissions tab. Below the tab, click the Groups pill. Below the pill, select the Atlan group. Under Permissions for the Atlan group , for each database: Under Data access change the drop-down value to Unrestricted . Although Atlan does not query data, this permission is necessary to enable the next option. Under Native query editing change the drop-down value to Yes . This permission is necessary for Atlan to parse the queries that power your Metabase questions, to generate lineage. In the upper-right of the page, click the Save changes button. When prompted with Save permissions? click the Yes button to confirm. Complete lineage permissions â To set permissions for extracting lineage from all your Metabase questions: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the People tab. On the row for the Atlan user you created above, under Groups change the drop-down value to Administrators . Did you know? Administrative access is necessary to get the default source database name used for queries. This is only available to Administrators. The unrestricted data access and native query editing permissions above are insufficient. (Optional) Allowlist the Atlan IP â If you are using the IP allowlist in your Metabase instance, you must add your Atlan IP to the allowlist. Please raise a support ticket to learn your Atlan IP. Tags: atlan documentation Previous Metabase Next Crawl Metabase Create a user Create a group Set permissions (Optional) Allowlist the Atlan IP"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake",
    "text": "Build governance Data Quality Studio Snowflake Data Quality On this page Snowflake Data Quality Studio Private Preview Overview: Monitor the quality of your Snowflake assets in Atlan using Snowflake's native data metric functions . This integration lets you create rules, track metrics, and view data quality insights directly within Atlan's discovery, lineage, and data products. Get started â Follow these steps to set up Snowflake as your data quality studio: Set up Snowflake Enable data quality on a connection Guides â Enable auto re-attachment for data quality rules : Configure automatic re-attachment of data quality rules to assets. Upgrade Snowflake Data Quality Setup : Update existing Snowflake data quality integration to the latest version. References â Operations : Technical reference for data quality operations between Atlan and Snowflake. FAQ â Roles and permissions : Common questions about data quality roles and permissions. Tags: snowflake data-quality governance atlan Next Set up Snowflake Get started Guides References FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship",
    "text": "Build governance Stewardship On this page Stewardship Overview: Implement data stewardship in Atlan through automated workflows, policies, and task management. Enable data teams to efficiently govern and maintain data assets through structured processes and compliance measures. Get started â Follow these steps to implement stewardship in Atlan: Automate data governance Guides â Create governance workflows Manage governance workflows Manage tasks Automate policy compliance Create policies Manage policies Revoke data access Create forms Troubleshooting â Troubleshooting policies Tags: stewardship governance workflows policies automation atlan Next Automate data governance Get started Guides Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/always-on/references/suggestions-from-similar-assets",
    "text": "Configure Atlan Integrations Automation Always On Suggestions from similar assets On this page Suggestions from similar assets As a data team ourselves, we know that metadata curation can be time-consuming. To simplify that process, each time you fill in a metadata gap, Atlan looks for other places to reuse that information. For example: You add the description Information about customers to a table called CUSTOMER . Atlan will look for any other tables called CUSTOMER without a description. Atlan suggests Information about customers as the description for these other CUSTOMER tables. Metadata suggestions are currently available for: Descriptions Terms Tags Owners Atlan is always working behind the scenes to help fill in the gaps! ð Our goals: Simplify metadata curation Reduce human error Improve consistency across the data stack Did you know? Even if a field is blank, everyone can still see potential information for an asset from these suggestions. However, only users with the necessary edit permissions for that asset will be able to apply the metadata suggestions. Frequently asked questions â Where can I see suggestions? â On the asset sidebar, where the user generally visits the widgets for metadata updates. Are suggestions available for everything in my instance? â Suggestions are only available for assets of the same type with the same name, with one exception. Metadata suggestions can be propagated across tables and views with the same name.Â For example: You are on a CUSTOMER table asset with no description. You will only see a suggestion if there is another CUSTOMER table or view asset with a description already in place. If there is a column called CUSTOMER with a description, you will not see its description as a suggestion on a table named CUSTOMER . If my asset already has that field populated, will there be a suggestion to update it? â No, suggestions are currently only available for assets without that field populated. Do suggestions work across asset types (for example, from database to BI dashboard)? â No, currently Atlan only provides suggestions for the same asset type. Will Atlan make suggestions for similar but not identical assets? â Not exactly. Currently, Atlan makes suggestions for assets that have exactly the same name. For example: Two tables named CUSTOMER and CUSTOMERS will not share suggestions. But two tables named CUSTOMER , even if one is in Snowflake and the other is in BigQuery, will share suggestions. Does Atlan come up with its own suggestions? â No, Atlan only suggests what has already been filled in for some other asset. Atlan does not create its own suggestions. How are suggestions provided for owners? â When you update an owner for an asset type, for example, a table asset: Atlan automatically checks other table assets with the same name and provides the owner you updated as a suggestion. This works even if the owner has been updated via a workflow or API-based bulk updates. Tags: atlan documentation Previous Always On Next Tag propagation"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/troubleshooting/troubleshooting-datastax-enterprise-connectivity",
    "text": "Connect data Databases NoSQL Databases Datastax Enterprise Troubleshooting Troubleshoot permission issues On this page Troubleshoot permission issues This guide helps you resolve common permission-related errors that may occur while setting up the DataStax connector. User doesn't have permission to access any keyspaces â Error message User does not have permission to access any keyspaces in the DataStax cluster. Please ensure the user has the necessary permissions to access the keyspaces. Possible causes The user doesn't have the necessary permissions to access keyspaces. The keyspaces don't exist or aren't available. Resolution Verify that the user has the required permissions to access the keyspaces. For more information, see permissions required to set up the DataStax connector . Confirm that the relevant keyspaces exist and are reachable from the connector environment. User doesn't have permission to fetch the cluster name â Error message User does not have permission to fetch the cluster name from the DataStax cluster. Please ensure the user has the necessary permissions to access the keyspaces. Possible causes The user lacks permissions to read system-level metadata. The user is missing DESCRIBE permission on system tables or the cluster. Resolution Verify that the user has access to system tables in the cluster. Confirm that the user has DESCRIBE permission on the cluster. Contact your Cassandra administrator to grant necessary system-level permissions if required. Tags: atlan documentation Previous Preflight checks for DataStax Enterprise User doesn't have permission to access any keyspaces User doesn't have permission to fetch the cluster name"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/troubleshooting/troubleshooting-exporting-large-query-results",
    "text": "Use data Insights Troubleshooting Troubleshooting exporting large query results On this page Troubleshooting exporting large query results If you'd like to export large query results of more than 100,000 rows, here are a few things to keep in mind: Is there a compute cost for executing long-running queries? â Long-running queries usually involve scanning a large number of rows, which can result in incurring cost against source compute. This is especially true when the query limit is increased significantly. Once the query limit has been increased, it will be applicable for all users who have access to execute such queries. To avoid incurring a high compute cost, it is recommended that you exercise caution while increasing the query row limit . What is the maximum time limit for long-running queries? â Atlan will issue a hard stop for queries running longer than 24 hours. Is there an expiration time for the download URL for results? â The generated download link in your email inbox has an expiry limit of 24 hours. What is the email attachment limit for CSV files? â Email providers typically set a limit of 25 MB on the attached files. If the file size crosses the limit, please use the download option in the email to download the results. How long can it take for results to appear in the inbox? â Depending on the file size of the query results, it may take a few minutes for the file to be available for download. If clicking on the download URL displays a file key error in the browser, please check the query status in Atlan at a later time. Tags: atlan documentation Previous Troubleshooting bring your own credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/troubleshooting/troubleshooting-google-bigquery-connectivity",
    "text": "Connect data Data Warehouses Google BigQuery Troubleshooting Troubleshooting Google BigQuery connectivity On this page Troubleshooting Google BigQuery connectivity Does Atlan support nested columns beyond level 1? â Atlan gets the raw structure for nested columns beyond level 1 from JSON files, which are then parsed and rendered on the table asset sidebar. However, nested columns beyond level 1 can only be viewed and not enriched with metadata. For example, if you attach a tag to a level 1 column, the tag will not be propagated to the nested columns. What are the known limitations of generating lineage for nested columns? â The following examples illustrate the known limitations of generating lineage for Google BigQuery nested columns in Atlan: Lineage will not be generated if you use the nested column of a RECORD in the source table or view to create another nested column for a RECORD REPEATED column in the target table or view. For example: CREATE VIEW orders_view AS SELECT ARRAY ( SELECT AS STRUCT customer . name AS customer_name , order_id AS order_id ) AS orders FROM orders_raw ; Lineage will be generated for: orders_raw.order_id â orders_view.orders.order_id Lineage will not be generated for: orders_raw.customer.name â orders_view.orders.customer_name Lineage will not be generated for a nested column if you use the RECORD column as well as a level 1 nested column in the source table or view to create a target column in another table or view. For example: CREATE VIEW orders_view AS SELECT customer AS customer ; customer . name AS customer_name , order_id AS order_id FROM orders_raw ; Lineage will be generated for: orders_raw.customer â orders_view.customer orders_raw.customer.address.city â orders_view.city Lineage will not be generated for: orders_raw.customer.name â orders_view.customer_name . Instead, lineage will be generated between orders_raw.customer â orders_view.customer_name . Lineage will not be generated for a nested column if you use a table alias to refer to nested columns in a source table or view to create a target column in another table or view. For example: CREATE VIEW orders_view AS SELECT o . order_id AS order_id o . customer . name as customer_name o . customer . address . city as city FROM orders_raw o ; Lineage will be generated for: orders_raw.order_id â orders_view.order_id Lineage will not be generated for: orders_raw.customer.name â orders_view.customer_name orders_raw.customer.address.city â orders_view.city Lineage will not be generated for a nested column if you use a source table or view CROSS JOIN with UNNEST to create a target table or view. For example: CREATE VIEW orders_view AS SELECT order_id , customer . name as customer_name , customer . address . city as city , i . item_name AS item_name FROM orders_raw CROSS JOIN UNNEST ( items ) AS i ; Lineage will be generated for: orders_raw.order_id â orders_view.order_id Lineage will not be generated for: orders_raw.customer.name â orders_view.customer_name orders_raw.customer.address.city â orders_view.city orders_raw.items.item_name â orders_view.item_name How to debug test authentication and preflight check errors? â Invalid project ID Provided GCP project ID is invalid, please check and try again. Ensure that the project ID is non-empty and matches the expected project in your Google Cloud console. Invalid service account JSON Following are the possible error messages for this issue: ProvidedÂ Service account JSON is invalid, please check and try again. Private key in the service account JSON is invalid, please check and try again. Failed to sign service account access token request with the provided private key. Check the service account JSON and try again. Malformed JSON, please check and try again. These indicate issues with the service account JSON key, possibly invalid or malformed data, or incorrect private key or formatting. Verify that the service account JSON is correctly formatted, and the private key is correctly specified. Regenerate the service account key if needed, and ensure that all required fields are included. Ensure that the file is not corrupted and follows a proper JSON structure. Insufficient permissions Service account doesn't have permission to create jobs, please ensure that the service account has the 'bigquery.jobs.create' permission. Ensure that you have assigned the bigquery.jobs.create permission to the service account. Review the roles and permissions assigned to the service account in your Google Cloud IAM settings. Cloud Resource Manager API disabled Cloud Resource Manager API has not been used in the configured project before or it is disabled. Please enable it and try again after some time. Open your Google Cloud console and enable the Cloud Resource Manager API for the project. Wait for the API to be fully activated before retrying the operation. Invalid grant, service account not found Unable to get access token for the provided service account, ensure the service account is active and try again. The service account is either inactive or does not exist. Ensure that the service account you created still exists in your Google Cloud console and has neither been disabled nor deleted. General connection failure Following are the possible error messages for this issue: Unable to connect to the configured BigQuery instance, please check your credentials and configs and then try again. Cannot create poolable connection factory. These indicate a general connection failure to your Google BigQuery instance, possibly due to misconfigured credentials or network issues. Verify that your credentials are correctly configured. Ensure that there are no network issues blocking the connection. If the problem still persists after verifying all of the above, contact Atlan support . Tags: atlan documentation Previous Preflight checks for Google BigQuery"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/troubleshooting/troubleshooting-hive-connectivity",
    "text": "On this page Troubleshooting Hive connectivity Can the Hive crawler connect to an independent Hive metastore? â Atlan does not support using only independent Hive metastore components for the Hive crawler . Atlan also does not support crawling the metastore database specifically   -  for example, using the PostgreSQL crawler. Doing so would bring in metadata for the database itself and not Hive's metadata. Tags: atlan documentation"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/troubleshooting/troubleshooting-microstrategy-connectivity",
    "text": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy Troubleshooting Troubleshooting MicroStrategy connectivity On this page Troubleshooting MicroStrategy connectivity Is the certified status in MicroStrategy mapped to the certificates field in Atlan? â No, the certified status in MicroStrategy is not mapped to the certificates field in Atlan. While the MicroStrategy certified field only supports a yes or no status, the certification field in Atlan supports multiple values. However, if your assets have been certified in MicroStrategy, you can view that information in the Properties tab of the asset sidebar in Atlan. Is the owner field in MicroStrategy mapped to the owners field in Atlan? â No, the asset owner in MicroStrategy is displayed as the source owner in the Overview section of the asset sidebar in Atlan. How is the upstream lineage for source tables calculated for cubes and reports? â Cubes or reports do not directly reference SQL tables or columns   -  they import attributes and metrics. Attributes can directly reference columns of SQL tables while metrics may reference an attribute, a fact, or another metric. Upstream lineage for cubes and reports is created by aggregating all the directly referenced SQL tables of the attributes and metrics thatÂ cubes and reports are sourced from. Why am I not seeing upstream lineage for attributes, facts, and metrics? â Atlan currently does not support upstream lineage for schema objects like attributes, facts, and metrics. Why are datasets created in a dossier missing from lineage? â Atlan currently does not support lineage for datasets directly created in a dossier due to limitations of the MicroStrategy REST APIs. The complete metadata required for creating lineage is unavailable. Does Atlan support document visualizations? â Atlan currently does not support document visualizations due to limitations of the MicroStrategy REST APIs. The complete metadata required for supporting document visualizations is unavailable. Tags: atlan documentation Previous Preflight checks for MicroStrategy"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/troubleshooting/troubleshooting-mysql-connectivity",
    "text": "Connect data Databases SQL Databases MySQL Troubleshooting Troubleshooting MySQL connectivity On this page Troubleshooting MySQL connectivity Is using RDS Proxy recommended while connecting to RDS? â Amazon RDS Proxy is a fully-managed, highly-available database proxy for Amazon Relational Database Service (RDS) . It makes applications more scalable, secure, and resilient to database failures. Amazon RDS Proxy sits between your application and relational database to efficiently manage connections to the database and improve scalability of the application. When setting up a private network link to MySQL , Amazon RDS Proxy can help with: Connection pooling   -  RDS Proxy helps manage database connections by pooling them and reusing them across multiple application connections. This reduces the overhead of establishing new connections for each database request and improves performance. Scalability   -  it automatically scales connection capacity based on demand, allowing applications to handle a high number of concurrent database connections without overwhelming the database instance. It also helps distribute the workload across multiple database instances, enabling horizontal scalability. Connection multiplexing   -  RDS Proxy multiplexes multiple database connections over a single secure connection. This reduces the number of connections to the database, conserving system resources and reducing the chance of hitting connection limits. Security   -  it integrates with AWS Identity and Access Management (IAM) , allowing fine-grained control over who can access the proxy and underlying database. It also supports Amazon virtual private cloud (VPC) endpoints, enhancing security through private connectivity. For more questions, head over to Amazon RDS Proxy FAQs . Tags: atlan documentation Previous Preflight checks for MySQL"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/troubleshooting/troubleshooting-on-premises-database-connectivity",
    "text": "Connect data Databases On-premises On-premises Databases Troubleshooting Troubleshooting on-premises database connectivity On this page Troubleshooting on-premises database connectivity No internet access on server running metadata-extractor â The metadata-extractor will attempt to download missing JDBC drivers. If there is no internet connection available on the server running Docker Compose, the metadata-extractor will not be able to get the required drivers. To make these drivers available without an internet connection: Download the latest JDBC driver from the database vendor. Copy the JDBC driver files to the server running Docker Compose into a dedicated directory. This should be a sub-directory under where your compose file exists (for example, ./jdbc/ ). Extract the files in that sub-directory: tar -zxvf postgres.tar.gz mysql.tar.gz Change your compose file to mount this sub-directory for each service you've defined. For example, the compose file would look something like this: services: my-database:   ... volumes: - <LOCAL-PATH-TO-DRIVERS>:/jars - ./output/my-database:/output (Replace <LOCAL-PATH-TO-DRIVERS> with the sub-directory where you extracted the drivers: ./jdbc in our example.) Once you've followed these steps you should be able to run sudo docker-compose up as usual. The metadata-extractor now use these local drivers, so internet access is no longer necessary. I need local DNS to resolve my server addresses â To specify a DNS server for the metadata-extractor to use in resolving server names: Add a dns element to your service definitions in the compose file. For example, the compose file would look something like this: services: my-database: <<: *extract environment:   ... volumes:   ... dns: - <DNS-SERVER-ADDRESS> (Replace <DNS-SERVER-ADDRESS> with the IP address of your local DNS server.) In need to connect to localhost â To allow metadata-extractor to access a service running on the same server as Docker Compose: On Linux, use localhost as the HOST value. On Windows and Mac, use host.docker.internal as the HOST value. More details are available in the Docker article: I want to connect from a container to a service on the host Tags: atlan documentation Previous Supported connections for on-premises databases"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/troubleshooting/troubleshooting-policies",
    "text": "Build governance Stewardship Troubleshooting Troubleshooting policies On this page Troubleshooting policies â Available via the Advanced Policy & Compliances package Here are a few things to know when setting up data governance policies in the policy center: What is the maximum number of assets that can be scoped to a policy? â There is currently no upper limit on the total number of assets that can be scoped to a policy. However, bear in mind that a higher asset volume can slow down the workflows for linking assets to the policy and completing compliance checks. It may also adversely impact the overall performance of Atlan, including search and discovery, workflow runtime, playbook execution, and more. What is the maximum number of rules supported in a policy? â Atlan currently supports creating 10 rules per policy. However, as with a higher volume of assets, more rules can also slow down the completion of compliance checks. Note that checks for each rule run one at a time during the scanning process. How many active policies can Atlan support? â There is no upper limit on the total number of active policies in Atlan. However, Atlan recommends that you proceed with caution. Multiple active or overlapping policies may delay the completion of compliance checks or adversely impact overall performance. Can running playbooks on governed assets affect compliance checks? â Yes, compliance checks must operate in isolation from playbook runs. For any overlapping assets, compliance checks should not coincide with any ongoing or scheduled playbook runs. Failure to heed this warning may result in false incidents. Can I use newly created metadata attributes to define the asset scope or compliance rules? â For any newly created glossary, category, term, tag, or custom metadata, you must wait a minimum of three hours before you can use them to define the asset scope or compliance rules in a policy. For any changes to a linked asset that violate a rule, how long will it take to be reported as an incident? â The violation check operates in near real time. However, if there is a high volume of updates and the queue becomes congested, new updates will have to wait until existing ones have been processed. Tags: atlan documentation Previous Create forms"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/troubleshooting/troubleshooting-postgresql-connectivity",
    "text": "Connect data Databases SQL Databases PostgreSQL Troubleshooting Troubleshooting PostgreSQL connectivity On this page Troubleshooting PostgreSQL connectivity Can Atlan crawl future tables created by any user? â PostgreSQL does not provide a single command to grant access to future tables created by any user on a global level. You will have to alter the default privileges of every current and future user that creates tables. This is to ensure that the database role you created for integrating with Atlan has access to the tables created by those users by default. For example: ALTER DEFAULT PRIVILEGES FOR USER < USER_CREATING_TABLES > IN SCHEMA < SCHEMA > GRANT SELECT , REFERENCES ON TABLES TO atlan_user_role ; However, altering the default privileges of every current and future user may not be sustainable or controlled from a single place. To automate the granting of permissions, you can: Grant function You can automate the granting of privileges to the database role you created for integrating with Atlan. Note that the function below is located in the public schema. You can use any schema you want to store this function: Set custom conditions using PL/pgSQL to skip or allow only certain schemas or tables: CREATE OR REPLACE FUNCTION public . grant_permissions_on_all_schemas ( ) RETURNS void AS $$ DECLARE schema_name text ; BEGIN FOR schema_name IN ( SELECT nspname FROM pg_namespace WHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema' ) LOOP EXECUTE format ( 'GRANT USAGE ON SCHEMA %I TO atlan_user_role' , schema_name ) ; -- grant access to all tables, including views, materialized views EXECUTE format ( 'GRANT SELECT, REFERENCES ON ALL TABLES IN SCHEMA %I TO atlan_user_role' , schema_name ) ; END LOOP ; END ; $$ LANGUAGE plpgsql ; Next, set up a periodic schedule and execute this function on a daily or hourly basis to ensure that the database role has access to all new schemas or tables: select public . grant_permissions_on_all_schemas ( ) ; Event triggers You can create an event trigger on any CREATE SCHEMA or CREATE TABLE command. This automation will ensure minimal lag, and you will not have to set up a schedule to run the above grant function. Note that the event trigger only listens to new create event triggers. You will still need to run the grant function above to ensure that the database role has access to all current schemas or tables. -- Function to grant permissions on a specific schema CREATE OR REPLACE FUNCTION public . grant_permissions_on_schema ( ) RETURNS event_trigger AS $$ DECLARE obj record ; BEGIN FOR obj IN SELECT * FROM pg_event_trigger_ddl_commands ( ) WHERE command_tag = 'CREATE SCHEMA' LOOP EXECUTE format ( 'GRANT USAGE ON SCHEMA %I TO atlan_user_role' , obj . object_identity ) ; RAISE NOTICE 'Granted USAGE on schema % to atlan_user_role' , obj . object_identity ; END LOOP ; END ; $$ LANGUAGE plpgsql ; -- Event trigger for new schemas CREATE EVENT TRIGGER grant_permissions_on_new_schema ON ddl_command_end WHEN TAG IN ( 'CREATE SCHEMA' ) EXECUTE FUNCTION public . grant_permissions_on_schema ( ) ; -- Function to grant permissions on a specific table CREATE OR REPLACE FUNCTION public . grant_permissions_on_table ( ) RETURNS event_trigger AS $$ DECLARE obj record ; BEGIN FOR obj IN SELECT * FROM pg_event_trigger_ddl_commands ( ) WHERE command_tag = 'CREATE TABLE' or command_tag = 'CREATE VIEW' or command_tag = 'CREATE TABLE AS' or command_tag = 'CREATE MATERIALIZED VIEW' LOOP EXECUTE format ( 'GRANT SELECT, REFERENCES ON TABLE %s TO atlan_user_role' , obj . object_identity ) ; RAISE NOTICE 'Granted SELECT, REFERENCES on table % to atlan_user_role' , obj . object_identity ; END LOOP ; END ; $$ LANGUAGE plpgsql ; -- Event trigger for new tables, views, mat views CREATE EVENT TRIGGER grant_permissions_on_new_table ON ddl_command_end WHEN TAG IN ( 'CREATE TABLE' , 'CREATE VIEW' , 'CREATE TABLE AS' , 'CREATE MATERIALIZED VIEW' ) EXECUTE FUNCTION public . grant_permissions_on_table ( ) ; Tags: atlan documentation Previous Preflight checks for PostgreSQL"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-tag-management",
    "text": "Connect data Data Warehouses Snowflake Troubleshooting Troubleshooting Snowflake tag management On this page Troubleshooting Snowflake tag management Here are a few things to know about managing Snowflake tags in Atlan: If a Snowflake tag is not attached to an asset, will enabling reverse sync trigger any updates? â When reverse tag sync is enabled, updates in Snowflake will only be triggered if an imported tag is attached to a Snowflake asset in Atlan or an existing Atlan tag for an asset is updated with an imported Snowflake tag. Will Snowflake tags show up as native assets in Atlan? â No, Atlan currently does not support Snowflake tags as native assets. Do corresponding tags in Atlan and Snowflake need to be of the same case for the sync to work? â Tag sync happens through case-insensitive name match. Even if the Snowflake tag and the corresponding Atlan tag have the same name but are of a different case, the tags will be synced and remain consistent between Atlan and Snowflake. When reverse tag sync is enabled, will all Snowflake assets with tags in Atlan be updated in Snowflake? â Enabling reverse tag sync will not trigger any updates unless tag updates are made for the Snowflake assets either in Atlan or Snowflake. If a tag is deleted in Snowflake, will the corresponding tag in Atlan also be deleted? â Only the imported Snowflake tag associated with the corresponding Atlan tag and its association with linked assets will be removed. The Atlan tag and the Snowflake assets to which the Atlan tag is attached will remain unaffected. How will tag propagation work for Snowflake tags? â Atlan supports tag propagation based on asset hierarchy and lineage. This existing functionality will remain unaffected by imported Snowflake tags. To learn about tag propagation in Snowflake, see here . How can I find out if reverse tag sync was successful? â If you're using the account usage method , expect latency of data for up to 3 hours. You can also use a table function like TAG_REFERENCES to confirm reverse tag sync. What happens if a tag is deleted in Atlan but remains in Snowflake? â The tag will be recreated during the next Snowflake crawler run and linked to assets where the relationship still exists in Snowflake. Does enabling reverse sync automatically push all updates to Snowflake? â No. However, you can set up playbooks in Atlan to update your Snowflake assets with imported tags at scale. If reverse sync is enabled for the Snowflake tags, updates will be pushed to Snowflake as well. What is the default propagation setting for synced Snowflake tags and can it be changed? â When imported Snowflake tags are attached to assets after a crawler run , tag propagation is turned off by default in Atlan. Can I restrict who can push asset-specific tag syncs to Snowflake? â No, this functionality is currently unavailable. However, stay tuned for more updates. Will reverse sync remove a tag from an asset in Snowflake if it is removed in Atlan? â If a tag is removed from a Snowflake asset in Atlan, it will not delete the tag in Snowflake. Instead, the asset will no longer be linked to the Snowflake tag in Atlan only. If tag propagation and reverse sync are enabled, will my assets in Snowflake display multiple values of the same tag? â If both tag propagation and reverse sync are enabled, and multiple values of the same tag are being propagated via different paths to a Snowflake asset: Atlan will preserve all values of the attached tag on the asset. Snowflake will only preserve the last modified value of the attached tag on the asset. Is it possible to filter and select specific tags to import in Atlan? â No, Atlan currently does not support filtering and selecting specific Snowflake tags to import. Atlan will crawl all the tags associated with an asset in Snowflake. Can Atlan reverse sync tags to dropped tables in Snowflake? â If tables are dropped in Snowflake, then the corresponding link between the table and tag is also removed. If new tables are created in their place, you will need to recreate the link. You can either: Apply tags to the new tables in Snowflake Crawl the new Snowflake tables in Atlan , run a playbook for tag attachment , and then push tag updates to Snowflake If you're also using dbt, you can use pre-hooks and post-hooks to preserve metadata like attached tags while building your dbt models. Refer to dbt documentation to learn more about hooks. Tags: atlan documentation Previous Troubleshooting Snowflake connectivity Next Snowflake warehouse configuration"
  },
  {
    "url": "https://docs.atlan.com/faq/user-management-and-access-control",
    "text": "Configure Atlan Frequently Asked Questions User Management and Access Control On this page User Management and Access Control Complete guide to managing users, configuring access controls, and understanding permissions in Atlan. How can I find all users in Atlan? â You must be an admin user to see the full user list. In the Admin centre open Users & Groups â Users to view, filter, and export all users in your workspace. Can I change my access level as an admin to test member and guest permissions? â Changing your own admin role is risky because any background workflows or playbooks you own might fail. Instead, create separate member and guest test accounts (for example, with disposable email addresses) and use an incognito browser window to verify permissions. How can a user with a member role query tables? â Give the member a persona or purpose that contains the necessary data access policies for the tables they need to query. How do you become an admin for a connection? â Only an existing connection admin (or a workspace admin) can assign additional connection admins. Ask a current connection admin to add you via the connection sidebar or by editing the connector workflow. Why do some assets have a lock or a request to get approval from an admin? â A member sees a lock icon when they're neither a connection admin nor part of any persona or purpose that permits asset changes. To gain edit rights, become a connection admin or be added to a suitable persona or purpose. Which permissions take priority when policies have overlapping assets? â When more than one policy affects the same asset, Atlan applies the most restrictive rule. An explicit deny always overrides a permit. See How do I control access to metadata and data? for details. Can I restrict business users to only view verified assets? â No. Access policies can currently target connections and tags but not certification status. What's the default landing page for users with two or more personas? â After you set landing page preferences : If exactly one persona applies to the user, they land on that persona's configured page. If multiple personas apply and the \"All assets\" view is disabled , Atlan chooses the first persona alphabetically. If the \"All assets\" view is available, the default landing page is All assets . Can I restrict metadata access to a subset of columns? â Yes. Create a metadata policy and, in the asset selector, drill down to choose only the columns that should be visible. How's the atlan_user id used for authentication in Redshift? â Atlan connects via the Amazon Redshift JDBC driver, using the IAM credentials you configure. The atlan_user identifier is not used to log in; instead, Atlan attributes each query to the actual user in Redshift's logs. Is the data for queries or sample preview masked in memory? â Yes. Atlan rewrites the SQL at run-time so that masked values (for example, via REPLACE ) are returned directly from the source database; the unmasked data never reaches Atlan's memory. Can users link terms to assets without glossary access? â Yesâprovided their metadata policy allows them to add or remove terms. They can link terms from the asset sidebar even if they don't have edit rights on the glossary itself. Why do I get permission denied when running an API request? â Add one or more personas to the API token so it can access the connection's assets. You can assign personas to a token in Admin â API authentication . For steps, see API authentication . Does Atlan have a password policy? â Yes. Atlan enforces these minimum requirements: Minimum length: 12 characters Must include at least 1 digit Must include both lowercase and uppercase letters Must include at least 1 special character Generated passwords expire after 90 days , and you can't reuse your last 5 passwords. What assets can be transferred from a removed user and how? â Removing a user from Atlan and transferring ownership of their assets may entail one of the following actions or a combination thereof: Remove the user from a list of owners. Delete the associated asset. Transfer ownership of assets to a new user. Category Condition Action Persona User is present Remove user from the persona Purpose User is present Remove user from the purpose Owner metadata Sole owner Transfer ownership to transferee Multiple owners Remove user from owner metadata attribute Connection admin Sole connection admin Transfer role to transferee Multiple connection admins Remove user from list of connection admins Query collection owner If query collection is private Delete query collection along with its folders and queries If query collection is shared and user has view permissions Remove user from query collection If query collection is shared and user is sole owner Transfer query collection to transferee If query collection is shared and has multiple owners Remove user from list of owners Query owner If parent collection of the query is to be deleted Delete the query If parent collection of the query is not to be deleted and user is sole owner Transfer ownership to transferee If parent collection of the query is not to be deleted and query has multiple owners Remove user from owner metadata of the query Starred assets User is present in the starredBy attribute of an asset Remove user from starredBy attribute API tokens User has created API tokens Delete all API tokens created by user SCIM tokens User has created SCIM tokens Delete all SCIM tokens created by user User-level integrations User has created an integration with Jira, Slack, Teams, or more Delete all user-level integrations Requests User has submitted requests Delete all requests from user Playbooks One-time playbooks No action Scheduled playbooks If user is the creator of the playbook and playbook schedule, transfer playbook and schedule to transferee Workflows One-time workflows No action Scheduled workflows If user is the creator of the workflow and workflow schedule, transfer workflow and cron to transferee Scheduled queries If results are shared with other users Remove user from the list of query result recipients, transfer the workflow, cron, and parent collection to transferee, and remove deleted user from owner metadata in queries If results are not shared with other users Delete the workflow Can I remove users if SSO or SCIM is enforced? â Yes, you can remove users irrespective of whether you're using basic authentication, SSO, or SCIM provisioning in Atlan. Will the activity log include metadata updates made by a removed user? â The activity log will retain historical information on any metadata updates made by a removed user, logged under their username. This is crucial to maintain data integrity for auditing purposes. Is it possible to reactivate a removed user? â No, it is not possible to reactivate a removed user. Since the user will be hard-deleted from Atlan, there will be no trace of the user in the identity system. Atlan maintains historical records of removed users for auditing purposes only. Whether you're using basic authentication, SSO, or SCIM provisioning, any returning user with the same username will be treated as a new user in Atlan. Tags: atlan documentation faq-administration Previous Tags and Metadata Management Next Workflows and Data Processing"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups",
    "text": "On this page Users and groups Overview: Manage users and groups in Atlan to control access and organize your data team. Set up user roles, create groups, and delegate administration to maintain a secure and efficient data platform. Get started â Follow these steps to implement user and group management in Atlan: Invite new users Guides â Manage users Manage user authentication Create groups Add users to groups Delegate administration Concepts â What are groups What are user roles Tags: users groups authentication administration governance atlan Get started Guides Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/connections/concepts/what-are-preflight-checks",
    "text": "Connect data Connectivity Framework Connector Framework Concepts What are preflight checks? On this page What are preflight checks? Preflight checks makes sure that all the required grants and permissions are in place while setting up a new workflow in Atlan. The framework of these comprehensive preflight checks may vary by connection source. When setting up a new workflow, you can run preflight checks to: Perform the necessary technical validations during setup Help improve your workflow success rate Supported sources â Atlan currently supports preflight checks for the following connection sources: Airflow, Amazon MWAA, Astronomer, and Google Cloud Composer Aiven Kafka Amazon MSK Amazon Redshift Amazon QuickSight Anomalo Databricks dbt Domo Confluent Schema Registry Fivetran Google BigQuery Hive Looker Metabase Microsoft Azure Data Factory Microsoft Azure Synapse Analytics Microsoft Power BI Microsoft SQL Server MicroStrategy Mode Monte Carlo MySQL Oracle PostgreSQL PrestoSQL Qlik Sense Cloud Redash Redpanda Kafka Salesforce SAP HANA Sigma Sisense Snowflake Tableau Teradata Trino Tags: atlan documentation Previous How to provide SSL certificates Next What is the crawler logic for a deprecated asset? Supported sources"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/concepts/what-are-the-sidebar-tabs",
    "text": "Configure Atlan Access control References What are the sidebar tabs? On this page What are the sidebar tabs? Who can do this? You will need to be an admin user in Atlan to personalize the asset sidebar for personas and purposes. Personas and purposes help you manage data access and curate assets for your users. You can also limit the details in the asset sidebar to only what is relevant to the persona or purpose.Â Personalize the asset sidebar â Once you've created a persona or purpose , you can set preferences for each sidebar tab. If enabled, the tab will display relevant information for a supported asset in the sidebar. You can personalize the asset sidebar for the following asset types and more: All assets â Lineage â The Lineage tab allows users to: View the upstream sources and downstream transformations of an asset. Search and filter by upstream and downstream directions or asset types. Open the lineage graph . Relations â The Relations tab allows users to: View assets related to each other beyond a parent-child relationship. Search and filter related assets by asset type. Activity â The Activity Â tab allows users to: View the changelog for an asset. Search and filter activities by type of metadata update. Resources â The Resources tab allows users to: View internal or external URLs for contextual information on an asset. Requests â The Requests Â tab allows users to: View requests for metadata updates on an asset. Search and filter requests by request status. Properties â The Properties tab allows users to: View essential properties of an asset, including unique identifier, qualified name, and more. SQL assets â Columns â The Columns tab allows users to: View columns for table assets. Search and filter columns by data type. View usage and popularity metrics , column keys , and data lineage. Open the column sidebar for a selected column. Usage â The Usage tab allows users to: View usage metadata for Snowflake and Google BigQuery assets   -  including top users, top five queries by context, compute cost , and more. Fact-Dim Relations â The Fact-Dim Relations tab allows users to: View assets associated with each other by fact-dimension relationships . Search and filter related assets by asset type. Profile â The Profile tab allows users to: View data profiling metrics for profiled column assets -  including column quality, summary statistics, and timestamp for last profiled. Queries â The Queries tab allows users to: View saved queries for an asset. Use the search bar for quick search. BI assets â Fields â The Fields tab allows users to: View data type fields for supported asset types   -  data source fields and calculated fields for Tableau data sources, columns for Microsoft Power BI tables, fields for Looker explores and views, Sigma datasets and data elements, dbt models, and Salesforce objects. Search and filter fields by lineage. Visuals â The Visuals tab allows users to: View visualization assets for Redash queries . Use the search bar for quick search and view lineage information. Visualizations â The Visualizations tab allows users to: View visualization assets for ThoughtSpot liveboards . Use the search bar for quick search and view lineage information. Schema Objects â The Schema Objects tab allows users to: View schema objects for MicroStrategy projects , reports , and cubes. Search and filter schema objects by asset type. ETL assets â Runs â The Runs tab allows users to: View run history for task and Airflow DAG assets. Filter runs by run status and type. Copy run IDs and view runs directly in Airflow. Tasks â The Tasks tab allows users to: View task assets for Airflow DAGs or ETL processes. Object storage assets â Objects â The Objects tab allows users to: View bucket assets for object storage connectors like Google Cloud Storage or S3 . Use the search bar for quick search. Data quality indicators â Incidents â The Incidents tab allows users to: View alerts for Monte Carlo monitors when anomalies are detected. Examine incidents by total count of incidents and timestamp for last synced. Filter incidents by incident type. Open incidents directly in Monte Carlo. Monte Carlo â The Monte Carlo tab allows users to: View timestamp for last synced and total count of incidents and custom monitors for table assets monitored by Monte Carlo . Expand the Incidents and Custom monitors tabs to view more details. Soda â The Soda tab allows users to: View timestamp for last synced and scanned and a total count of Soda checks on the asset. Examine Soda checks in detail and open them directly in Soda. dbt Test â The dbt Test tab allows users to: View a total count of dbt tests on the asset and also grouped by test status. Open the dbt tests in a sidebar or view directly in dbt. Glossary assets â Assets â The Assets tab allows users to: View linked assets for glossary terms . Search and filter linked assets by asset type. Query assets â Schedules â The Schedules tab allows users to: View a list of schedules for a saved query, if any. Tags: atlan documentation Previous What are user roles? Next User Role Sync Personalize the asset sidebar All assets SQL assets BI assets ETL assets Object storage assets Data quality indicators Glossary assets Query assets"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/column-keys-crawled",
    "text": "Connect data Connectivity Framework Connector Framework FAQ What column keys does Atlan crawl? On this page What column keys does Atlan crawl? If the following column keys are defined in the SQL database at source, Atlan will crawl and display them as attributes for your assets: Primary key -  uniquely identifies each row in a table. Foreign key -  links together two tables. Partition key -  determines logical partitions in a table. Sort key -  determines the order in which rows are stored in a table. Index key -  defines the order for an index in the database. Cluster key -  determines the order in which the database is partitioned. Distributed key -  determines where data is stored in a database. View column keys â Navigate to the left menu of any screen in Atlan and click Assets to begin: From the asset preview â To view column keys in the asset preview: From the Assets page, navigate to the asset preview section. The asset preview for column assets will display available column keys. From the asset profile â To view column keys in the asset profile: From the Assets page, right-click a table or a column asset and select Open profile . Navigate to Column preview to view available column keys. From the sidebar â To view column keys in the sidebar: From the Assets page, click a table or a column asset. In the sidebar to the right, click the Columns tab to view available column keys in the sidebar. Create foreign key relationships â You can create foreign key relationships only through APIs for your column assets in Atlan. You can use the foreignKeyTo and foreignKeyFrom statements to create column references for your foreign keys and maintain the referential integrity of your assets. Refer to our developer documentation to assign foreign key relationships. Once you have created foreign key relationships, your users will be able to view the column references and better understand the relationships between assets. Filter assets by column keys â You can filter your asset search results by type-specific property filters, such as column keys for your column assets.Â To filter column assets by column keys: From the left menu on any screen in Atlan, click Assets . Under the search bar on the Assets page, click the Column tab to filter for column assets. In the Filters menu on the left, click the Column filter. To add a type-specific property filter to refine your search, select a column-type property filter   -  for example, Primary key . In the filter dialog, click Yes to view column assets with a primary key or click No to only view column assets without a primary key. Supported sources â Atlan supports column keys for the following connectors: Amazon Athena Amazon Redshift AWS Glue Databricks Google BigQuery Hive Microsoft Azure Synapse Analytics Microsoft SQL Server MySQL Oracle PostgreSQL SAP HANA Snowflake Teradata Tags: atlan documentation faq-connections Previous How often does Atlan crawl Snowflake? Next What's the difference between connecting to Athena and Glue? View column keys Create foreign key relationships Filter assets by column keys Supported sources"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/query-frequency",
    "text": "Use data Insights FAQ What controls the frequency of queries? What controls the frequency of queries? Access to querying assets can be controlled through Atlan's persona-based data policies . In order to prevent query abuse, there is a default limit of 100 on the number of rows in each query. Tags: atlan documentation faq-insights Previous How to use parameterized queries? Next Why do I only see tables from the same schema to join from in a visual query?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
    "text": "Connect data Data Warehouses Snowflake Manage Snowflake in Atlan Configure Snowflake data metric functions On this page Configure Snowflake data metric functions Private Preview To use system data metric functions (DMFs) from Snowflake, you need to configure your Snowflake setup. Prerequisites â Before proceeding with the configuration, make sure the following prerequisites are met: Snowflake editions -  Atlan data quality for Snowflake is only supported for Enterprise and Business Critical editions of Snowflake. Administrative access   -  the user configuring Snowflake must have an ACCOUNTADMIN role or equivalent administrative privileges. Dedicated warehouse   -  your organization must have a dedicated Snowflake warehouse for running data quality-related queries. Create roles in Snowflake â You will need to create the following two roles in Snowflake for the Atlan data quality integration: Data quality admin role ( dq_admin )   -  a high-privilege role responsible for managing DMF associations on tables and views. This role is used to create and manage the database objects required for data quality operations. Atlan data quality service role ( atlan_dq_service_role )   -  a service role that Atlan will use to interact with Snowflake objects related to data quality operations. This role will be assigned to the Atlan service user. Create data quality admin role â Run the following commands to create the dq_admin role and grant access to the Snowflake warehouse: CREATE OR REPLACE ROLE dq_admin ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE dq_admin ; Replace <warehouse-name> with the name of your dedicated Snowflake warehouse for running data quality-related queries. Create Atlan data quality service role â Run the following commands to create the atlan_dq_service_role and grant access to the Snowflake warehouse: CREATE OR REPLACE ROLE atlan_dq_service_role ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_dq_service_role ; Replace <warehouse-name> with the name of your dedicated Snowflake warehouse for running data quality-related queries. Create a user in Snowflake â A dedicated Snowflake user is required for Atlan to connect to your Snowflake instance. You will need to create this integration user and assign the Atlan data quality service role to this user. Refer to the Create a user documentation to create the new user. After creating the user, grant the Atlan data quality service role to the new user you created in Snowflake: GRANT ROLE atlan_dq_service_role TO USER < atlan_dq_user > ; Grant privileges â The following privileges must be granted to the roles created in Snowflake for the Atlan data quality integration: Privileges for data quality admin â Grant the dq_admin role the ability to create databases and access system DMFs in Snowflake: GRANT CREATE DATABASE ON ACCOUNT TO ROLE dq_admin ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE dq_admin ; Privileges for table owners â For each role that owns tables in your Snowflake environment, grant the following privileges: GRANT ROLE < table_owner > TO ROLE dq_admin ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE < table_owner > ; GRANT EXECUTE DATA METRIC FUNCTION ON ACCOUNT TO ROLE < table_owner > ; Replace <table_owner> with the role that owns Snowflake tables. Privileges for Atlan data quality service role â Grant the following privileges to the atlan_dq_service_role : GRANT APPLICATION ROLE SNOWFLAKE . DATA_QUALITY_MONITORING_VIEWER TO ROLE atlan_dq_service_role ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE atlan_dq_service_role ; GRANT EXECUTE TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; Set up required objects â Once you have created roles and granted the required privileges, you will need to create the necessary objects such as a dedicated database, schema, and stored procedure to be used for managing DMF operations. Change to the dq_admin role: USE ROLE dq_admin ; Create the database and schema in Snowflake for the Atlan data quality integration: CREATE DATABASE ATLAN_DQ ; CREATE SCHEMA ATLAN_DQ . SHARED ; The ATLAN_DQ database serves as a container for all objects related to the Atlan data quality integration. The ATLAN_DQ.SHARED schema provides a separate namespace for shared procedures and functions. Create the store procedure in Snowflake to manage DMFs: /** * Manages Data Metric Functions (DMF) attachment operations for Snowflake tabular entities. Runs with the privileges of the procedure owner. * @param {string} ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE) * @param {string} ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param {string} ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param {string} [DMF_NAME=null] - Fully qualified name of the DMF (database.schema.name) * @param {string} [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param {string} [SCHEDULE_TYPE=null] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param {string} [SCHEDULE_VALUE=null] - Schedule value based on type * @returns {string} - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( ACTION STRING , ENTITY_TYPE STRING , ENTITY_NAME STRING , DMF_NAME STRING DEFAULT NULL , DMF_ARGUMENTS_JSON STRING DEFAULT '[]' , SCHEDULE_TYPE STRING DEFAULT NULL , SCHEDULE_VALUE STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- /** * Executes a SQL query with parameters * @param {string} sqlText - SQL statement to execute * @param {Array} [binds=[]] - Array of bind parameters for the query * @param {boolean} [returnFirstRow=false] - Whether to return only the first row * @returns {Object} Object containing execution result or error information */ function executeQuery ( sqlText , binds = [ ] , returnFirstRow = false ) { try { if ( ! sqlText ) return { isErrored: true , message: \"SQL Text is required\" , result: null , } ; const statement = snowflake . createStatement ( { sqlText , binds } ) ; const result = statement . execute ( ) ; const response = { isErrored: false , message: \"\" , result: null , } ; if ( returnFirstRow ) { response . result = result . next ( ) ? result : null ; return response ; } response . result = result ; return response ; } catch ( err ) { return { isErrored: true , message: ` ${err.code} - ${err.message} - ${sqlText} with binds: ${binds.join(\", \")} ` , result: null , } ; } } /** * Safely parses a JSON string * @param {string} jsonString - JSON string to parse * @returns {Object} Parsed JSON object or null if invalid */ function safelyParseJSON ( jsonString ) { try { return JSON . parse ( jsonString ) ; } catch ( err ) { return null ; } } /** * Validates a number within a range * @param {string} value - Number to validate * @param {number} min - Minimum value * @param {number} max - Maximum value * @returns {boolean} True if number is valid * @returns {boolean} False if number is invalid */ function isNumberValid ( value , min , max ) { const num = parseInt ( value , 10 ) ; return ! isNaN ( num ) && num = min && num & le ; max ; = \"max;\" } = \"}\" * * = \"**\" * = \"*\" escapes = \"Escapes\" and = \"and\" quotes = \"quotes\" a = \"a\" snowflake = \"Snowflake\" identifier = \"identifier\" @param = \"@param\" {string} = \"{string}\" - = \"-\" raw = \"Raw\" to = \"to\" normalize = \"normalize\" @returns = \"@returns\" properly = \"Properly\" quoted = \"quoted\" safe = \"safe\" for = \"for\" sql = \"SQL\" function = \"FUNCTION\" normalizeidentifier ( identifier ) = \"normalizeIdentifier(identifier)\" { = \"{\" return = \"return\" ` =\" ` \" ${identifier . replace ( = \"${identifier.replace(\" g , = \"g,\" ) } = \")}\" ` ;=\" ` ; \" retrieves=\" Retrieves \" all=\" all \" columns=\" columns \" given=\" given \" entity.=\" entity . \" validates=\" Validates \" that=\" that \" the=\" the \" entityexists=\" entityexists \" procedure=\" procedure \" owner=\" owner \" has=\" has \" access=\" access \" it.=\" it . \" entityname=\" entityName \" fully=\" Fully \" qualified=\" qualified \" entity=\" entity \" name=\" name \" {array}=\" {Array} \" array=\" Array \" of=\" of \" column=\" column \" objects=\" objects \" with=\" with \" datatype=\" dataType \" properties=\" properties \" @throws=\" @throws \" {error} = \"{Error}\" if = \"if\" doesn = \"doesn\" t = \"t\" exist = \"exist\" or = \"or\" is = \"is\" inaccessible = \"inaccessible\" getallcolumnsforentity ( entityname ) = \"getAllColumnsForEntity(entityName)\" const = \"const\" sqltext = \"SHOW COLUMNS IN IDENTIFIER(?)\" ; = \";\" binds = \"[entityName];\" result , = \"result,\" iserrored , = \"isErrored,\" message = \"message\" binds ) ; = \"binds);\" ( iserrored ) = \"(isErrored)\" exists = \"exists\" it = \"it\" throw = \"throw\" new = \"new\" error ( message ) ; = \"Error(message);\" while = \"while\" ( result . next ( ) ) = \"(result.next())\" name: = \"name:\" result . getcolumnvalue ( = \"result.getColumnValue(\" column_name = \"column_name\" ) , = \"),\" datatype: = \"dataType:\" json . parse ( result . getcolumnvalue ( = \"JSON.parse(result.getColumnValue(\" data_type = \"data_type\" ) ) . type , = \")).type,\" } ; = \"};\" ( column . datatype = \"==\" fixed = \"FIXED\" ) = \")\" column . datatype = \"NUMBER\" columns . push ( column ) ; = \"columns.push(column);\" columns ; = \"columns;\" dmf = \"DMF\" valid = \"valid\" dmfname = \"dmfName\" dmfarguments = \"dmfArguments\" arguments = \"arguments\" { boolean } = \"{boolean}\" whether = \"Whether\" invalid = \"invalid\" isdmfvalid ( dmfname , = \"isDMFValid(dmfName,\" dmfarguments ) = \"dmfArguments)\" identifier ( ? ) ( ${dmfarguments} ) ` ,=\"IDENTIFIER(?)(${dmfArguments}) ` , \" [dmfname],=\" [ dmfName ] , \" true);=\" true ) ; \" true;=\" true ; \" checks=\" Checks \" timezone=\" Timezone \" validate=\" validate \" true=\" True \" false=\" False \" istimezonevalid(timezone)=\" isTimezoneValid ( timezone ) \" result=\" executeQuery ( ` SELECT\" convert_timezone(?,=\"CONVERT_TIMEZONE(?,\" current_timestamp()) ` , = \"CURRENT_TIMESTAMP())`,\" [ timezone ] , = \"[timezone],\" ! result . iserrored ; = \"!result.isErrored;\" generates = \"Generates\" type = \"type\" signature = \"signature\" based = \"based\" on = \"on\" {object} = \"{Object}\" entitycolumnsmap = \"entityColumnsMap\" map = \"Map\" names = \"names\" in = \"in\" format = \"format\" > : [ { name: , dataType:  } ] } * @param {string} baseEntityName - Name of the base entity * @returns {string} DMF type signature * @throws {Error} If entity not found in the cache * / function generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) { if ( ! dmfArguments || ! dmfArguments . length ) return \"\" ; const baseEntityColumns = entityColumnsMap [ baseEntityName ] ; if ( ! baseEntityColumns ) { throw new Error ( ` Entity ${baseEntityName} not found in the cache ` ) ; } const baseEntityColumnArguments = dmfArguments . filter ( param = param . type = = = \"COLUMN\" ) . map ( param = { const column = baseEntityColumns . find ( col = col . name = = = param . name ) ; return column ? column . dataType : null ; } ) . join ( \", \" ) ; const baseEntityArguments = ` TABLE(${baseEntityColumnArguments}) ` ; const referencedEntityArguments = dmfArguments . filter ( param = param . type = = = \"TABLE\" ) . map ( entityParam = { const entityName = entityParam . name ; const entityColumns = entityColumnsMap [ entityName ] ; if ( ! entityColumns ) { throw new Error ( ` Entity ${entityName} not found in the cache ` ) ; } const columnTypes = entityParam . nested . map ( nestedParam = { const column = entityColumns . find ( col = col . name = = = nestedParam . name ) ; return column ? column . dataType : null ; } ) . filter ( Boolean ) . join ( \", \" ) ; return ` TABLE(${columnTypes}) ` ; } ) ; const arguments = [ baseEntityArguments , . . . referencedEntityArguments ] . join ( \", \" ) ; return arguments ; } /** * Generates DMF arguments for SQL statements * @param {string} dmfArguments - Array of DMF arguments * @returns {string} Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments ( dmfArguments ) { return dmfArguments . map ( param = { if ( param . type = = = \"COLUMN\" ) { return normalizeIdentifier ( param . name ) ; } // Handle TABLE type with nested columns return ` ${normalizeIdentifier(param.name)}(${ param.nested .map(nested = normalizeIdentifier(nested.name)) .join(\", \") }) ` ; } ) . join ( \", \" ) ; } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- /** * Validates that mandatory arguments are provided and valid * @throws {Error} If any mandatory argument is missing or invalid */ function validateMandatoryArguments ( ) { const VALID_ACTIONS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" , \"UPDATE_SCHEDULE\" ] ) ; const VALID_ENTITY_TYPES = new Set ( [ \"TABLE\" , \"VIEW\" , \"MATERIALIZED VIEW\" , \"EXTERNAL TABLE\" , \"ICEBERG TABLE\" ] ) ; const DMF_OPS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] ) ; const VALID_SCHEDULE_TYPES = new Set ( [ \"MINUTES\" , \"CRON\" , \"ON_DATA_CHANGE\" , \"NOT_SCHEDULED\" ] ) ; const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set ( [ \"MINUTES\" , \"CRON\" ] ) ; if ( ! VALID_ACTIONS . has ( ACTION ) ) throw new Error ( ` Invalid ACTION: \"${ACTION}\". Valid options are ${Array.from(VALID_ACTIONS).join(\", \")} ` ) ; if ( ! VALID_ENTITY_TYPES . has ( ENTITY_TYPE ) ) throw new Error ( ` Invalid ENTITY_TYPE: \"${ENTITY_TYPE}\". Valid options are ${Array.from(VALID_ENTITY_TYPES).join(\", \")} ` ) ; if ( DMF_OPS . has ( ACTION ) && ! DMF_NAME ) throw new Error ( \"DMF_NAME is required for DMF related actions\" ) ; if ( ACTION = = = \"UPDATE_SCHEDULE\" ) { if ( ! SCHEDULE_TYPE ) throw new Error ( \"SCHEDULE_TYPE is required for SCHEDULE action\" ) ; if ( ! VALID_SCHEDULE_TYPES . has ( SCHEDULE_TYPE ) ) throw new Error ( ` Invalid schedule type: \"${SCHEDULE_TYPE}\". Valid options are ${Array.from(VALID_SCHEDULE_TYPES).join(\", \")} ` ) ; if ( SCHEDULE_TYPES_THAT_REQUIRE_VALUE . has ( SCHEDULE_TYPE ) && ! SCHEDULE_VALUE ) throw new Error ( \"SCHEDULE_VALUE is required for SCHEDULE action\" ) ; } } /** * Parses a fully qualified name into its components * @param {string} fullyQualifiedName - Fully qualified name to parse * @returns {Object} Object with database, schema, and name properties * @throws {Error} If invalid fully qualified name */ function validateFullyQualifiedName ( fullyQualifiedName ) { const parts = fullyQualifiedName . split ( \".\" ) . map ( part = part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length != = 3 ) throw new Error ( ` Invalid fully qualified name: ${fullyQualifiedName}. Expected format: database.schema.name ` ) ; } /** * Validates the structure of DMF arguments JSON * @param {string} rawDMFArguments - Raw JSON string of DMF arguments * @throws {Error} If DMF arguments structure is invalid */ function validateDMFArgumentsStructure ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) = param . type = = = \"TABLE\" ) ; if ( referencedEntities . length 1 ) throw new Error ( \"Only one referenced entity is allowed\" ) ; const validationFunctions = { arrayItem: ( param ) = [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) && param . name , nestedItem: ( param ) = [ \"COLUMN\" ] . includes ( param . type ) && param . name , } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; if ( referencedEntities . length 0 ) { for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } } /** * Validates that all specified columns exist in an entity * @param {Array} columnsToCheck - Array of column names to validate * @param {Array} entityColumns - Array of column metadata from the entity * @param {string} entityName - Name of the entity for error message * @throws {Error} If any column doesn't exist in the entity */ function validateColumnsExistInEntity ( entityName , allColumnsInEntity , columnsToCheck ) { for ( const column of columnsToCheck ) { if ( ! allColumnsInEntity . some ( col = col . name = = = column ) ) throw new Error ( ` Column ${column} not found in entity ${entityName} ` ) ; } } /** * Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param {string} entityName - Fully qualified name of the entity * @param {string} dmfName - Fully qualified name of the DMF * @param {Array} dmfArguments - Array of DMF arguments * @throws {Error} If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers ( entityName , dmfName = \"\" , dmfArguments = [ ] ) { validateFullyQualifiedName ( entityName ) ; // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName ; const baseEntityAllColumns = getAllColumnsForEntity ( entityName ) ; const entityColumnsMap = { [ baseEntityName ] : baseEntityAllColumns } ; const allReferencedEntities = dmfArguments . filter ( param = param . type = = = \"TABLE\" ) ; for ( const referencedEntity of allReferencedEntities ) { const columns = getAllColumnsForEntity ( referencedEntity . name ) ; entityColumnsMap [ referencedEntity . name ] = columns ; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments . filter ( param = param . type = = = \"COLUMN\" ) . map ( param = param . name ) ; validateColumnsExistInEntity ( baseEntityName , baseEntityAllColumns , allBaseEntityColumnsInArguments ) ; for ( const referencedEntity of allReferencedEntities ) { const columnsInArguments = referencedEntity . nested . map ( nestedParam = nestedParam . name ) ; validateColumnsExistInEntity ( referencedEntity . name , entityColumnsMap [ referencedEntity . name ] , columnsInArguments ) ; } if ( dmfName ) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) ; isDMFValid ( dmfName , generatedTypeSignature ) ; } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } /** * Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param {string} cronExpression - CRON expression to validate * @throws {Error} If CRON expression is invalid */ function validateCronExpression ( cronExpression ) { if ( cronExpression . length 100 ) throw new Error ( \"Cron expression is too long\" ) ; const cronFields = cronExpression . trim ( ) . split ( / \\s + / ) ; if ( cronFields . length != = 6 ) throw new Error ( \"Invalid cron expression. Expected 6 fields\" ) ; const [ minute , hour , dayOfMonth , month , dayOfWeek , timezone ] = cronFields ; const isTimezoneValidResult = isTimezoneValid ( timezone ) ; if ( ! isTimezoneValidResult ) throw new Error ( \"Invalid timezone provided in the cron expression\" ) ; const regexPatterns = { minute : / ^ ( \\ * | \\d + | \\ * \\ / \\d + | \\d + \\ - \\d + | \\d + ( , \\d + ) * ) $ / , hour : / ^ ( \\ * | \\d + | \\ * \\ / \\d + | \\d + \\ - \\d + | \\d + ( , \\d + ) * ) $ / , dayOfMonth: / ^ ( \\ * | L | \\d + | \\ * \\ / \\d + | \\d + \\ - \\d + | \\d + ( , \\d + ) * ) $ / , month : / ^ ( \\ * | \\d + | JAN | FEB | MAR | APR | MAY | JUN | JUL | AUG | SEP | OCT | NOV | DEC | \\ * \\ / \\d + | \\d + \\ - \\d + | [ A - Z ] { 3 }\\ - [ A - Z ] { 3 } | \\d + ( , \\d + ) * | ( [ A - Z ] { 3 } ( , [ A - Z ] { 3 } ) * ) ) $ / i , dayOfWeek: / ^ ( \\ * | \\d + | SUN | MON | TUE | WED | THU | FRI | SAT | \\d + L | [ A - Z ] { 3 }L | \\ * \\ / \\d + | \\d + \\ - \\d + | [ A - Z ] { 3 }\\ - [ A - Z ] { 3 } | \\d + ( , \\d + ) * | ( [ A - Z ] { 3 } ( , [ A - Z ] { 3 } ) * ) ) $ / i , } ; if ( minute . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( minute , 0 , 59 ) ) throw new Error ( \"Invalid minute value\" ) ; if ( hour . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( hour , 0 , 23 ) ) throw new Error ( \"Invalid hour value\" ) ; if ( dayOfMonth . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfMonth , 1 , 31 ) ) throw new Error ( \"Invalid day of month value\" ) ; if ( month . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( month , 1 , 12 ) ) throw new Error ( \"Invalid month value\" ) ; if ( dayOfWeek . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfWeek , 0 , 6 ) ) throw new Error ( \"Invalid day of week value\" ) ; if ( ! regexPatterns . minute . test ( minute ) || ! regexPatterns . hour . test ( hour ) || ! regexPatterns . dayOfMonth . test ( dayOfMonth ) || ! regexPatterns . month . test ( month ) || ! regexPatterns . dayOfWeek . test ( dayOfWeek ) ) throw new Error ( \"Invalid cron expression\" ) ; } /** * Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws {Error} If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule ( ) { const VALID_MINUTES = new Set ( [ \"5\" , \"15\" , \"30\" , \"60\" , \"720\" , \"1440\" ] ) ; if ( SCHEDULE_TYPE = = = \"MINUTES\" && ! VALID_MINUTES . has ( SCHEDULE_VALUE ) ) throw new Error ( ` Invalid SCHEDULE_VALUE for MINUTES. Valid options are ${Array.from(VALID_MINUTES).join(\", \")} ` ) ; if ( SCHEDULE_TYPE = = = \"CRON\" ) validateCronExpression ( SCHEDULE_VALUE ) ; // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } /** * Validates all provided arguments * Performs comprehensive validation on input parameters * @throws {Error} If any validation fails */ function validateAllArguments ( ) { validateMandatoryArguments ( ) ; // Validates all mandatory arguments are provided in the correct format if ( ACTION = = = \"UPDATE_SCHEDULE\" ) validateProvidedArgumentsForSchedule ( ) ; // Validates the provided schedule type and value else { validateDMFArgumentsStructure ( DMF_ARGUMENTS_JSON ) ; } validateProvidedIdentifiers ( ENTITY_NAME , DMF_NAME , safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- /** * Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns {string} JSON string with operation status and result message */ function main ( ) { validateAllArguments ( ) ; // Validate all provided arguments // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments ( safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; const SQL_TEMPLATES = { ATTACH_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} ADD DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) ` , DETACH_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} DROP DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) ` , SUSPEND_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) SUSPEND ` , RESUME_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) RESUME ` , UPDATE_SCHEDULE: { MINUTES: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = '${SCHEDULE_VALUE} MINUTE' ` , CRON: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'USING CRON ${SCHEDULE_VALUE}' ` , ON_DATA_CHANGE: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES' ` , NOT_SCHEDULED: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} UNSET DATA_METRIC_SCHEDULE ` , } , } ; let sqlText = \"\" ; let returnMessage = \"\" ; if ( ACTION = = = \"UPDATE_SCHEDULE\" ) { sqlText = SQL_TEMPLATES [ ACTION ] [ SCHEDULE_TYPE ] ; returnMessage = ` Successfully updated schedule for ${ENTITY_NAME} to ${SCHEDULE_TYPE} ${SCHEDULE_VALUE} ` ; } else { sqlText = SQL_TEMPLATES [ ACTION ] ; returnMessage = ` ACTION: ${ACTION} performed successfully on ${ENTITY_NAME} with DMF: ${DMF_NAME} and DMF Arguments: ${dmfArguments} ` ; } const result = executeQuery ( sqlText ) ; return JSON . stringify ( { isSuccessful: ! result . isErrored , message: result . isErrored ? result . message : returnMessage , } ) ; } // Execute the main function and return the result try { return main ( ) ; } catch ( err ) { return JSON . stringify ( { isSuccessful: false , message: err . message , } ) ; } $$ ; Grant access to Atlan data quality service role â Finally, grant permissions to the Atlan data quality service role to access the database, schema, and stored procedure you created in Snowflake: USE ROLE dq_admin ; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . SHARED TO ROLE atlan_dq_service_role ; GRANT USAGE ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE atlan_dq_service_role ; GRANT CREATE SCHEMA ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; Tags: data integration setup configuration Previous Manage Snowflake tags Next Multiple tag values and concatenation Prerequisites Create roles in Snowflake Create a user in Snowflake Grant privileges Set up required objects Grant access to Atlan data quality service role"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/preflight-checks-for-snowflake",
    "text": "Connect data Data Warehouses Snowflake References Preflight checks for Snowflake On this page Preflight checks for Snowflake Before running the Snowflake crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Database and schema check â Information schema â â Check successful â Check failed for $missingObjectName Account usage â â Check successful â Check failed for $missingObjectName Warehouse access â â Check successful â Operation cannot be performed / User is not authorized to perform this action / Cannot be resumed because resource monitor has exceeded its quota Miner â Did you know? Once you have crawled assets from Snowflake, you can mine query history . Query history view â â Check successful â Cannot access query history table. Please run the command in your Snowflake instance: GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role; Access history view â â Check successful â Check failed. Something went wrong with your request. Sessions view â â Check successful â Check failed. Something went wrong with your request. S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors data crawl Previous What does Atlan crawl from Snowflake? Next Troubleshooting Snowflake connectivity Database and schema check Warehouse access Miner"
  },
  {
    "url": "https://docs.atlan.com/tags/connectivity",
    "text": "64 docs tagged with \"connectivity\" View all tags Aiven Kafka Integrate, catalog, and govern Aiven Kafka assets in Atlan. Amazon Athena Integrate, catalog, and govern Amazon Athena assets in Atlan. Amazon DynamoDB Integrate, catalog, and govern Amazon DynamoDB assets in Atlan. Amazon MSK Integrate, catalog, and govern Amazon MSK assets in Atlan. Amazon MWAA OpenLineage Integrate, catalog, and visualize Amazon MWAA lineage in Atlan. Amazon QuickSight Integrate, catalog, and govern Amazon QuickSight assets in Atlan. Amazon Redshift Integrate, catalog, and govern Amazon Redshift assets in Atlan. Amazon S3 Integrate, catalog, and govern Amazon S3 assets in Atlan. Anomalo Integrate, catalog, and govern Anomalo assets in Atlan. Apache Airflow OpenLineage Integrate, catalog, and visualize Apache Airflow lineage in Atlan. Apache Kafka Integrate, catalog, and govern Apache Kafka assets in Atlan. Apache Spark OpenLineage Integrate, catalog, and visualize Apache Spark lineage in Atlan. Astronomer OpenLineage Integrate, catalog, and visualize Astronomer lineage in Atlan. AWS Glue Integrate, catalog, and govern AWS Glue assets in Atlan. BigID Integrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata. Cloudera Impala Integrate, catalog, and govern Cloudera Impala assets in Atlan. Confluent Kafka Integrate, catalog, and govern Confluent Kafka assets in Atlan. Confluent Schema Registry Integrate, catalog, and govern Confluent Schema Registry assets in Atlan. Crawl Cloudera Impala Learn how to crawl metadata from Cloudera Impala into Atlan. Dagster Integrate, catalog, and visualize Dagster lineage in Atlan. Databricks Integrate, catalog, and govern Databricks assets in Atlan. DataStax Enterprise Catalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data. dbt Integrate, catalog, and govern dbt assets in Atlan. Domo Integrate, catalog, and govern Domo assets in Atlan. Fivetran Integrate, catalog, and govern Fivetran assets in Atlan. Google BigQuery Integrate, catalog, and govern Google BigQuery assets in Atlan. Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan. Hive Catalog and govern Hive assets in Atlan for discovery and governance. IBM Cognos Analytics Integrate, catalog, and govern IBM Cognos Analytics assets in Atlan. Informatica CDI Integrate, catalog, and govern Informatica Cloud Data Integration assets in Atlan. Looker Integrate, catalog, and govern Looker assets in Atlan. Matillion Integrate, catalog, and govern Matillion assets in Atlan. Metabase Integrate, catalog, and govern Metabase assets in Atlan. Microsoft Azure Cosmos DB Catalog and govern Cosmos DB assets in Atlan for discovery and governance. Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan. Microsoft Azure Event Hubs Integrate, catalog, and govern Azure Event Hubs assets in Atlan. Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan. Microsoft Power BI Integrate, catalog, and govern Power BI assets in Atlan. Microsoft SQL Server Integrate, catalog, and govern Microsoft SQL Server assets in Atlan. MicroStrategy Integrate, catalog, and govern MicroStrategy assets in Atlan. Mode Integrate, catalog, and govern Mode assets in Atlan. MongoDB Catalog and govern MongoDB assets in Atlan for discovery and governance. Monte Carlo Integrate, catalog, and govern Monte Carlo assets in Atlan. MySQL Integrate, catalog, and govern MySQL assets in Atlan. On-Premises Databases Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required. Oracle Integrate, catalog, and govern Oracle assets in Atlan. PostgreSQL Integrate, catalog, and govern PostgreSQL assets in Atlan. PrestoSQL Integrate, catalog, and govern PrestoSQL assets in Atlan. Qlik Sense Cloud Integrate, catalog, and govern Qlik Sense Cloud assets in Atlan. Qlik Sense Enterprise (Windows) Integrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan. Redash Integrate, catalog, and govern Redash assets in Atlan. Redpanda Kafka Integrate, catalog, and govern Redpanda Kafka assets in Atlan. Salesforce Integrate, catalog, and govern Salesforce assets in Atlan. SAP ECC Integrate, catalog, and govern SAP ECC assets in Atlan. SAP HANA Catalog and govern SAP HANA assets in Atlan for discovery and governance. SAP S/4HANA Integrate, catalog, and govern SAP S/4HANA assets in Atlan. Sigma Integrate, catalog, and govern Sigma assets in Atlan. Sisense Integrate, catalog, and govern Sisense assets in Atlan. Snowflake Integrate, catalog, and govern Snowflake assets in Atlan. Soda Integrate, catalog, and govern Soda assets in Atlan. Tableau Integrate, catalog, and govern Tableau assets in Atlan. Teradata Catalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage. ThoughtSpot Integrate, catalog, and govern ThoughtSpot assets in Atlan. Trino Integrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/snowflake",
    "text": "12 docs tagged with \"snowflake\" View all tags Data quality permissions Reference for data quality permission scopes and configuration in Atlan. Enable auto re-attachment of rules Learn how to enable automatic re-attachment of data quality rules to Snowflake tables and views. Enable data quality on connection Enable and configure data quality for your Snowflake connection in Atlan. Multiple tag values and concatenation Learn how Atlan handles multiple tag values for Snowflake objects, including concatenation, sorting, and reverse synchronization. Operations Atlan crawls and manages the following data quality operations and results from Snowflake. Roles and permissions Explanation of Snowflake's security model and role requirements for data quality operations. Rules and dimensions Reference for available data quality rules and classification dimensions in Snowflake data quality. Set up Snowflake Configure Snowflake to enable data quality monitoring through Atlan. Snowflake Integrate, catalog, and govern Snowflake assets in Atlan. Snowflake Data Quality Studio Set up and configure Snowflake for data quality monitoring through Atlan. Snowflake warehouse configuration Recommended Snowflake warehouse configuration to enable reliable Atlan workflow execution. Upgrade to Snowflake data quality studio Update existing Snowflake data quality integration to the latest version"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/troubleshooting/troubleshooting-connector-specific-sso-authentication",
    "text": "Configure Atlan Integrations Identity Management SSO Troubleshooting Troubleshooting connector-specific SSO authentication On this page Troubleshooting connector-specific SSO authentication Atlan currently supports the following connectors for SSO authentication to query data and preview sample data : Amazon Redshift Google BigQuery Snowflake General â How will SSO authentication interact with any data policies in Atlan? â Atlan supports data policies mandated at source if using SSO authentication. Explicit restrictions will take precedence, unless otherwise configured. Let's examine two scenarios using the example of a masking policy: If you have a data policy in Atlan to mask sensitive data and are also using SSO authentication with no masking policy at source, the data will be masked in Atlan. However, if you have toggled on Enable data policies created at source to apply for querying in Atlan while configuring SSO authentication in Atlan, only source policies will take effect and the data will not be masked in Atlan as per the source policy. If you do not have any data policy in Atlan but are using SSO authentication with a masking policy at source for sensitive data, the data will be masked in Atlan. Snowflake â Why am I getting an incorrect username or password error message? â If you receive the following error message: Cannot create PoolableConnectionFactory (Incorrect username or password was specified.) The security integration in Snowflake maps Atlan email addresses to Snowflake login names. First, check if a user with an Atlan email address exists in Snowflake. If a user exists and the Snowflake login name is not an email address, your Snowflake administrator will have to manually update the user-mapping in the security integration to use email addresses instead. To do so, add the following command to the security integration in Snowflake : EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'EMAIL_ADDRESS' Refer to Snowflake documentation . Why am I getting a role error message? â If you receive the following error message: Cannot create PoolableConnectionFactory (Role <'ACCOUNTADMIN'/'ORGADMIN'/'SECURITYADMIN'> specified in the connect string is not granted to this user. Contact your local system administrator, or attempt to login with another role, e.g. PUBLIC By default, Snowflake blocks the ACCOUNTADMIN , ORGADMIN , and SECURITYADMIN roles from being assumed in the security integration . Therefore, a user with any of these Snowflake roles will not be able to run queries with Snowflake OAuth-based authentication . To allow users with the ACCOUNTADMIN , ORGADMIN , or SECURITYADMIN role to query with Snowflake OAuth-based authentication, you will need to add the following command to set account-level permissions for the security integration in Snowflake : ALTER ACCOUNT SET EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST = FALSE ; Refer to Snowflake documentation . Tags: integration connectors Previous Troubleshooting SSO Next PingFederate SSO 404 error"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/order-workflows",
    "text": "Connect data Connectivity Framework Connector Framework How-tos How to order workflows On this page order workflows The order of operations you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling data tools . The right order particularly ensures that lineage is constructed without needing to rerun crawlers. Order of operations â To have lineage across tools, you need to: Crawl data stores first -  for example, SQL data sources , NoSQL data sources , event buses , and schema registries . Run data quality tools -  for example, Monte Carlo and Soda . Mine query logs - mine queries through S3 or run miner packages for supported sources. Run extract-load tools -  for example, Fivetran , Airflow/OpenLineage and other supported distributions , and data processing tools like Apache Spark/OpenLineage , Alteryx . Run transformation tools -  for example, dbt and Matillion . Crawl business intelligence tools last -  for example, supported BI tools like Looker , Microsoft Power BI , Tableau , and more. If you use a different order, the upstream assets (data stores) might not yet exist when you load the BI metadata. In that case, you may see lineage within the BI metadata, but not between the BI metadata and data sources. If this happens, no worriesâjust rerun your existing workflows following the recommended order and Atlan can resolve it. Did you know? As a general rule of thumb, start by crawling the data sourceâincluding BI toolsâbefore mining query logs. For example, when aiming to mine Microsoft Power BI, begin with a crawl of Microsoft Power BI. Workflow recommendations â The following are general guidelines and best practices for running workflows in Atlan: Schedule your workflows based on how often you want your metadata in Atlan to be updated   -  weekly, monthly, and so on. To configure custom cron schedules, learn more here . Avoid any overlaps between workflow schedules to ensure consistent workflow run times. Remember that the first workflow run can typically take much longer than subsequent runs. The first run establishes the connection, queries the source, extracts and transforms the metadata, and then publishes your assets for the first time in Atlan. If running a miner for the first time, set a start date around 3 days prior to the current date and then schedule it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause workflows to time out or hit resource consumption errors. For all subsequent miner runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run preflight checks before running the crawler to check for any permissions or other configuration issues, including testing authentication. Troubleshooting tips â Here are a few tips to help you troubleshoot workflow failures in Atlan: If test authentication or preflight checks fail, check the source to ensure that your credentials are correct and you have requisite access to crawl the metadata. If you're connecting to Atlan via private link and experience any network-related errors or timeouts during test authentication, it may mean that there is a network connectivity issue between the source and Atlan. Reach out to Atlan support to help you investigate further. If both test authentication and preflight checks fail and succeed intermittently when tried multiple times, this may mean that your cluster is in an unstable state and needs to be restarted. Notify Atlan support to restart your cluster. Tags: connectors data crawl Previous Mine queries through S3 Next How to provide SSL certificates Order of operations Workflow recommendations Troubleshooting tips"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access",
    "text": "Connect data Databases On-premises On-premises Databases Get Started Set up on-premises database access On this page Set up on-premises database access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your database access details, including credentials. In some cases you won't be able to expose your databases for Atlan to crawl and ingest metadata. This may happen for various reasons: Transactional databases may have high-load mechanisms. That could make direct connection problematic. Security requirements may restrict accessing sensitive, mission critical transactional databases from outside. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your on-premises databases you will need to use Atlan's metadata-extractor tool. Did you know? Atlan uses exactly the same metadata-extractor behind the scenes when it connects to cloud databases. danger If you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the metadata-extractor tool â To get the metadata-extractor tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. To load the image: For Docker Image, load the image to the server you'll use to crawl databases: sudo docker load -i /path/to/jdbc-metadata-extractor-master.tar For OCI Image: Docker: Install Skopeo . Load the image to the server you'll use to crawl databases: skopeo copy oci-archive:/path/to/jdbc-metadata-extractor-master-oci.tar docker-daemon:jdbc-metadata-extractor-master:latest Podman: Load the image to the server you'll use to crawl databases: podman load -i /path/to/jdbc-metadata-extractor-master-oci.tar podman tag <loaded image hash> jdbc-metadata-extractor-master:latest Get the compose file â Atlan provides you a configuration file for the metadata-extractor tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises databases. The file is docker-compose.yml . Define database connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your database connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises database, define an entry under services in the compose file. Each entry will have the following structure: services: CONNECTION-NAME: <<: *extract environment: <<: *CONNECTION-TYPE   Credentials   Database address volumes:   Output folder Replace CONNECTION-NAME with the name of your connection. <<: *extract tells the metadata-extractor tool to run. environment contains all parameters for the tool. <<: *CONNECTION-TYPE applies default arguments for the corresponding connection type. Refer to Supported connections for on-premises databases for full details of each connection type. Example â Let's explain in detail using an example: services: inventory:                          1. Call this connection \"inventory\" <<: *extract environment: <<: *psql                       2. Connect to PostgreSQL using basic authentication USERNAME: some-username         3. Credentials PASSWORD: some-password HOST: inventory.local           4. Database address PORT: 5432 DATABASE: inventory volumes: - *shared-jdbc-drivers - ./output/inventory:/output    5. Store results in ./output/inventory The name of this service is inventory . You can use any meaningful name you want. In this example, we are using the same name as the database we're going to crawl. The <<: *psql sets the connection type to Postgres using basic authentication. USERNAME and PASSWORD specify the credentials required for the psql connection. HOST , PORT and DATABASE specify the database address. The PORT is 5432 by default, so you can omit it most of the time. The ./output/inventory:/output line specifies where to store results. You will need to replace inventory with the name of your connection. We recommend you to output metadata for different databases in separate folders. You can add as many database connections as you want. Did you know? Docker's documentation describes the services format in more detail. Secure credentials â danger If you decide to keep database credentials in the compose file, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To create and use Docker secrets: Create a JSON file and add the credentials that you want to use in Docker secrets. For example: { \"USERNAME\" : \"my-secret-user\" , \"PASSWORD\" : \"my-secret-password\" } info ðª Did you know? The keys here will be the environment variable names, hence consider migrating them from the compose file to secrets. Once set to secrets, the environment variables in secrets will take precedence over the ones in the compose file. If not provided in secrets, the values will be parsed from the compose file instead. Create a new Docker secret: docker secret create my_database_credentials credentials.json At the top of your compose file, add a secrets element to access your secret: secrets : my_database_credentials : external : true Within the service section of the compose file, add a new secrets element and specify CREDENTIAL_SECRET_PATH to use it as credentials. danger If you have added database credentials directly to the compose file, Atlan recommends that you leave CREDENTIAL_SECRET_PATH as blank. For example, your compose file would now look something like this: secrets : my_database_password : external : true x-templates :   ... services : my-database : << : *extract environment : << : *psql CREDENTIAL_SECRET_PATH : \"/run/secrets/my_database_credentials\"   ... volumes :   ... secrets : - my_database_password volumes : jars : Troubleshooting secure credentials â Atlan recommends the following troubleshooting measures: If you're unable to create Docker secrets, ensure that Swarm mode is enabled. Secrets are encrypted during transit and at rest in a Docker swarm. Run the following command to enable Swarm mode: docker swarm init If running the compose file after providing the credentials secret results in Unsupported external secret <secret_name> , complete the following steps: Modify the compose file as follows: secrets : my_database_password : external : true x-templates :   ... services : my-database : << : *extract environment : << : *psql CREDENTIAL_SECRET_PATH : \"/run/secrets/my_database_credentials\"   ... volumes :   ... secrets : - my_database_password deploy : replicas : 1 restart_policy : condition : none volumes : jars : Run the compose file using the following command: docker stack deploy -c docker-compose.yml <stack_name> Replace the <stack_name> with the name you provided while deploying the stack. Verify deployment status using the following command: docker stack ps --no-trunc <stack_name> Replace the <stack_name> with the name you provided while deploying the stack. If stack deployment has been successfully completed, monitor the docker service logs using the following command: docker service logs <stack_name>_<service_name> --follow Replace the <stack_name> with the name you provided while deploying the stack. Replace the <service_name> with the service name in Docker. danger The docker stack deploy command will run all the services in the docker-compose.yml file, so ensure that the docker-compose.yml only contains the service you intend to run. Tags: data crawl Previous On-Premises Databases Next Crawl on-premises databases Prerequisites Get the compose file Define database connections Secure credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/crawl-on-premises-databases",
    "text": "Connect data Databases On-premises On-premises Databases Crawl On-premises Assets Crawl on-premises databases On this page Crawl on-premises databases Once you have set up the metadata-extractor tool , you can extract metadata from your on-premises databases using the following steps. Run metadata-extractor â Crawl all databases â To crawl all databases using the metadata-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up . Crawl a specific database â To crawl a specific database using the metadata-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Save the compose file and use the command sudo docker-compose up <CONNECTION-NAME> within the local folder where the compose file is stored. (Replace <CONNECTION-NAME> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The metadata-extractor tool will generate the following JSON files for each service : columns-<DATABASE>.json databases.json extras-procedures-<DATABASE>.json procedures-<DATABASE>.json schemas-<DATABASE>.json table-<DATABASE>.json view-<DATABASE>.json You can inspect the metadata and make sure it is acceptable to provide the metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata, you need to upload the metadata to an S3 bucket. Did you know? To avoid access issues, upload to the same S3 bucket that Atlan uses. Raise a support request to get your Atlan bucket details, and include the ARN of the IAM user or IAM role so access can be provisioned. To create a separate bucket, see Option 1: Use your own bucket in the dbt documentation (the steps are the same). To upload the metadata to S3: Confirm that all the files for a particular service have the same prefix. For example, metadata/inventory/columns-inventory.json , metadata/inventory/databases.json , etc. Upload the files to the S3 bucket using your preferred method. For example, to upload all the files using the AWS CLI : aws s3 cp output/inventory s3://my-bucket/metadata/inventory --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: Amazon Redshift Hive Microsoft SQL Server MySQL Oracle PostgreSQL SAP HANA Snowflake Teradata For all of the above cases, select Offline for the extraction method. Tags: connectors data crawl Previous Set up on-premises database access Next Supported connections for on-premises databases Run metadata-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases",
    "text": "Connect data Databases On-premises On-premises Databases References Supported connections for on-premises databases On this page Supported connections for on-premises databases The metadata-extractor tool supports the following connection types. These describe the details required when setting up on-premises database access . Amazon Redshift with basic authentication â Use <<: *redshift under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 5439 ) DATABASE -  database name ( required ) USERNAME -  database username ( required ) PASSWORD -  database user password ( required ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-redshift-database: <<: *extract environment: <<: *redshift USERNAME: my-database-username PASSWORD: my-database-password HOST: redshift-host PORT: redshift-database-port DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/redshift-example:/output Amazon Redshift with IAM user authentication â Use <<: *redshift under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 5439 ) DATABASE -  database name ( required ) DATABASE_USER -  database username of the IAM user ( required ) AWS_ACCESS_KEY_ID -  AWS access key ID ( required ) AWS_SECRET_ACCESS_KEY -  AWS secret access key ( required ) CLUSTER_ID -  cluster identifier of your private Amazon Redshift cluster ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: redshift-iam-user-example: <<: *extract environment: <<: *redshift-iam-user AWS_ACCESS_KEY_ID: aws-access-key-id AWS_SECRET_ACCESS_KEY: aws-secret-access-key HOST: redshift-host DATABASE: my-database-name DATABASE_USER: my-database-user   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' CLUSTER_ID: private-cluster-id USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/redshift-iam-user-example:/output Hive with basic authentication â Use <<: *hive under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 10000 ) DEFAULT_SCHEMA -  default schema name ( optional, default is default ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) SCHEMA_EXCLUDE_REGEX -  regex to exclude schemas ( optional ) SCHEMA_INCLUDE_REGEX -  regex to include schemas ( optional ) TEMP_TABLE_REGEX -  regex to exclude tables ( optional ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example hive-example: <<: *hive-extract environment: <<: *hive HOST: hive-host PORT: hive-port DEFAULT_SCHEMA: default USERNAME: my-hive-username PASSWORD: my-hive-password   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/hive-example:/output Microsoft SQL Server with basic authentication â Use <<: *mssql under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-mssql-database: <<: *extract environment: <<: *mssql USERNAME: db-user PASSWORD: db-user-password HOST: mssql-database-host DATABASE: northwind   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/mssql-database:/output MySQL with basic authentication â Use <<: *mysql under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 3306 ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-mysql-database: <<: *extract environment: <<: *mysql USERNAME: db-user PASSWORD: db-user-password HOST: mysql-database-host.internal   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-mysql-database:/output MySQL with IAM authentication â Use <<: *mysql-iam under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 3306 ) USERNAME -  database user name ( required ) AWS_ACCESS_KEY_ID -  AWS access key id ( required ) AWS_SECRET_ACCESS_KEY -  AWS secret access key ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-mysql-database: <<: *extract environment: <<: *mysql-iam AWS_ACCESS_KEY_ID: my-access-key-id AWS_SECRET_ACCESS_KEY: my-secret-access-key USERNAME: db-user HOST: mysql-database-host.internal   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-mysql-database:/output Oracle with basic authentication â Use <<: *oracle under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 1521 ) SERVICE_NAME -  database service name ( required ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-oracle-database: <<: *extract environment: <<: *oracle USERNAME: db-user PASSWORD: db-user-password HOST: oracle-database-host.internal SERVICE_NAME: my-service-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-oracle-database:/output PostgreSQL with basic authentication â Use <<: *postgresql under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 5432 ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-postgresql-database: <<: *extract environment: <<: *postgresql USERNAME: db-user PASSWORD: db-user-password HOST: postgresql-database-host.internal DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-postgresql-database:/output PostgreSQL with IAM authentication â Use <<: *postgresql-iam under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 5432 ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) AWS_ACCESS_KEY_ID -  AWS access key id ( required ) AWS_SECRET_ACCESS_KEY -  AWS secret access key ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-postgresql-database: <<: *extract environment: <<: *postgresql-iam AWS_ACCESS_KEY_ID: my-access-key-id AWS_SECRET_ACCESS_KEY: my-secret-access-key USERNAME: db-user HOST: postgresql-database-host.internal DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-postgresql-database:/output SAP HANA with basic authentication â Use <<: *sap-hana under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 39015 ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-sap-hana-database: <<: *extract environment: <<: *sap-hana USERNAME: db-user PASSWORD: db-user-password HOST: sap-hana-database-host.internal DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-sap-hana-database:/output Snowflake with basic authentication â Use <<: *snowflake under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 443 ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-snowflake-database: <<: *extract environment: <<: *snowflake USERNAME: db-user PASSWORD: db-user-password HOST: snowflake-database-host.internal DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-snowflake-database:/output Snowflake with OAuth authentication â Use <<: *snowflake-oauth under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 443 ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) CLIENT_ID -  OAuth client ID ( required ) CLIENT_SECRET -  OAuth client secret ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-snowflake-database: <<: *extract environment: <<: *snowflake-oauth USERNAME: db-user CLIENT_ID: my-client-id CLIENT_SECRET: my-client-secret HOST: snowflake-database-host.internal DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-snowflake-database:/output Teradata with basic authentication â Use <<: *teradata under the environment section to use this connection type. Available attributes: HOST -  database host name or IP address ( required ) PORT -  database port ( optional, default is 1025 ) DATABASE -  database name ( required ) USERNAME -  database user name ( required ) PASSWORD -  database user password ( required ) USE_SOURCE_SCHEMA_FILTERING -  Boolean to specify if schema-level filtering needs to be enabled while fetching schemas, tables, and columns ( optional ) EXCLUDE_FILTER_TEMPLATE -  exclude filter pattern ( optional ) INCLUDE_FILTER_TEMPLATE -  include filter pattern ( optional ) USE_JDBC_INTERNAL_METHODS - Boolean to specify if JDBC internal methods need to be used as part of the extraction ( optional ) Example services: my-teradata-database: <<: *extract environment: <<: *teradata USERNAME: db-user PASSWORD: db-user-password HOST: teradata-database-host.internal DATABASE: my-database-name   If using Docker Swarm mode for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$$\": [\"^SCHEMA1$$\", \"^SCHEMA2$$\"]}'   If using Docker Compose for offline extraction, format the filters as follows: INCLUDE_FILTER_TEMPLATE: '{\"^DB1$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' EXCLUDE_FILTER_TEMPLATE: '{\"^DB2$\": [\"^SCHEMA1$\", \"^SCHEMA2$\"]}' USE_SOURCE_SCHEMA_FILTERING: \"false\" USE_JDBC_INTERNAL_METHODS: \"false\" volumes: - *shared-jdbc-drivers - ./output/my-teradata-database:/output Tags: connectors data authentication Previous Crawl on-premises databases Next Troubleshooting on-premises database connectivity Amazon Redshift with basic authentication Amazon Redshift with IAM user authentication Hive with basic authentication Microsoft SQL Server with basic authentication MySQL with basic authentication MySQL with IAM authentication Oracle with basic authentication PostgreSQL with basic authentication PostgreSQL with IAM authentication SAP HANA with basic authentication Snowflake with basic authentication Snowflake with OAuth authentication Teradata with basic authentication"
  },
  {
    "url": "https://docs.atlan.com/tags/multiple-concatenation",
    "text": "One doc tagged with \"multiple-concatenation\" View all tags Multiple tag values and concatenation Learn how Atlan handles multiple tag values for Snowflake objects, including concatenation, sorting, and reverse synchronization."
  },
  {
    "url": "https://docs.atlan.com/tags/warehouse",
    "text": "One doc tagged with \"warehouse\" View all tags Snowflake warehouse configuration Recommended Snowflake warehouse configuration to enable reliable Atlan workflow execution."
  },
  {
    "url": "https://docs.atlan.com/tags/configuration",
    "text": "19 docs tagged with \"configuration\" View all tags Administration and Configuration Complete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization. Configure SMTP Atlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and [scheduled queries](/product/capabilities/insights/how-tos/schedule-a-query). We provide an embedded SMTP server to do this, out-of-the-box. Configure Snowflake data metric functions Configure Snowflake data metric functions <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Crawl Fivetran Learn about crawl fivetran. Integrate Amazon MWAA/OpenLineage To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Apache Airflow/OpenLineage To integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Apache Spark/OpenLineage Atlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Integrate Astronomer/OpenLineage To integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/. Integrate Slack To integrate Slack and Atlan, follow these steps. Preflight checks for Fivetran Learn about preflight checks for fivetran. Schedule a query You must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients. Set up an Azure private network link to Databricks For all details, see [Databricks documentation](https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/private-link-simplified?source=recommendations create-the-workspace-and-private-endpoints-in-the-azure-portal-ui). Set up Anomalo Atlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata. Set up Fivetran Learn about set up fivetran. Snowflake warehouse configuration Recommended Snowflake warehouse configuration to enable reliable Atlan workflow execution. What does Atlan crawl from Fivetran? Learn about what does atlan crawl from fivetran?. What does Atlan crawl from SAP ECC? What does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> What does Atlan crawl from SAP S/4HANA? What does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Workflows and Data Processing Everything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan."
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/monitor-connectivity",
    "text": "Connect data Connectivity Framework Connector Framework How-tos Monitor connectivity On this page Monitor connectivity Atlan runs its crawlers through an orchestrated set of automated tasks. To monitor these orchestrated set of tasks follow these steps. Monitor the crawling process â You can visualize the individual tasks a workflow runs as a directed acyclic graph (or \"DAG\"). To visualize the crawling process: When running a workflow immediately, you will be redirected to the monitoring page within 5 seconds. At any other moment: From the left menu, click Workflows to navigate to the Workflow center : By default, workflow runs from the last 24 hours will be shown. (Optional) Use the filters along the top to narrow down to the workflow run you want to monitor. From the Workflow run history table, click the workflow run you want to check. On the left of the screen under the Summary tab, you can also see: The current status of the workflow run. The start and finish time of the workflow run. The elapsed time (duration) of the workflow run. Who triggered the workflow run and how (manually or automatically). Identify errors â If a crawler fails due to an error, Atlan will show where the failure occurred in the visualization. To review the failure of any workflow with an error: Open the workflow run visualization (using either option above). Under theÂ Summary tab on the left of the screen, click theÂ View Failed tasks button. Atlan will take you to the Failed Tasks tab on the left of the screen. Here you can review details about the specific activity or activities that failed. Review log files â Each task in the DAG may produce a log file containing additional details. To review the log file for a specific activity: Click the task (activity node) in the DAG visualization. Open the Failed Tasks tab of a workflow run visualization (see steps above). To the right of each failed step, click theÂ Logs button. If there are any logs available, Atlan will display them on the screen. Did you know? Not every failed activity will produce a log. Look at the Message field of failed tasks for ideas about what went wrong when there is no log file available. Manage all workflows â You can monitor and manage all your workflows in Atlan from the workflow center. To manage all your workflows: From the left menu of any screen in Atlan, click Workflows . From the tabs along the top in the Workflow center , click Manage . Search for a specific workflow from the search bar or click Select package to filter by supported connectors . (Optional) In the Filters menu on the left, select a filter to drill down further: Click Created by to filter workflows created by specific users in Atlan. Click Workflow type to filter workflows by type of workflow   -  connectors, utilities, and miners. Each workflow type also displays the total count of workflow runs for that type. Click Schedule to filter workflows by scheduled or unscheduled runs. The workflow preview includes a summary of workflow details. Navigate to the workflow sidebar on the right, from the sidebar: The Overview tab displays run count, when the workflow was created and by whom, workflow schedules if applicable, and last 5 runs. (Optional) Click Run workflow to run the workflow directly from the sidebar. The Runs tab displays a summary of past workflow runs. (Optional) Select a workflow run to view more details or modify connectivity . Select any workflow to open the workflow. Tags: integration connectors Previous Manage connectivity Next Connect data sources for Azure-hosted Atlan instances Monitor the crawling process Identify errors Review log files Manage all workflows"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/connect-data-sources-for-azure-hosted-atlan-instances",
    "text": "Connect data Connectivity Framework Connector Framework How-tos Connect data sources for Azure-hosted Atlan instances On this page Connect data sources for Azure-hosted Atlan instances This document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following: Data sources hosted on Microsoft Azure Data sources hosted in data centers Azure-managed data sources â To connect your Atlan instance hosted on Microsoft Azure with a Microsoft Azure-managed data source, Atlan recommends the following method. For this purpose, we'll consider a Microsoft Azure-managed Snowflake instance: For data sources like Snowflake, you can use Azure Private Link . Atlan will create a private endpoint in your Atlan instance to connect to your Snowflake instance using the the resource ID. This will create a request in your Atlan instance. Accept the request to proceed. Once the request has been accepted, Atlan will be able to access the data source using a private endpoint over the Azure backbone network, bypassing the internet. Atlan will also create a private DNS and add an A record for the private endpoint previously created in the Azure-managed Atlan instance and share the details with you. You can use this DNS record to connect to the Azure-hosted Snowflake data source. Each data source will require a separate Azure private endpoint. On-premises data sources â To connect your Atlan instance hosted on Microsoft Azure with an on-premises data source, Atlan recommends the following method. For this purpose, we'll consider an on-premises MySQL server hosted in a data center: For an on-premises MySQL database, you can consider a combination of Azure Private Link , Azure Load Balancer , and Azure Virtual Machines . For this method, the data source must be accessible from your Microsoft Azure subscription. You will need to create a virtual machine in your Azure-managed Atlan instance to port forward the request to the corresponding data source. Add a network load balancer to the virtual machine and create a Private Link service to the load balancer. Atlan will create a private endpoint in your Atlan instance to connect to the Private Link service of the load balancer. Atlan will also create a private DNS and add an A record for the private endpoint previously created in the Azure-managed Atlan instance and share the details with you. You can use this DNS record to connect to the on-premises data source using the load balancer. Only one private endpoint will be required to connect to all the on-premises data sources through port forwarding. Tags: connectors data Previous Monitor connectivity Next Mine queries through S3 Azure-managed data sources On-premises data sources"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/provide-ssl-certificates",
    "text": "Connect data Connectivity Framework Connector Framework How-tos How to provide SSL certificates On this page provide SSL certificates SSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for crawling Tableau . The following types of SSL certificates are supported: Self-signed â Paste the public .crt or .cert part of your TLS certificate in the Privacy Enhanced Mail (PEM) format. For example: ----BEGIN CERTIFICATE---- MIIDazCCAlOgAwIBAgIJAOqRDRz0BxIAMA0GCSqGSIb3DQEBCwUAMIGZMQswCQYD ... ... ... u1Q== ----END CERTIFICATE---- Internal CA â An SSL certificate chain is a sequence of certificates consisting of three parties: A root certificate authority, One or more intermediate certificate authorities, And the server certificate. Paste the root, intermediate, and server certificates in the following format: ----BEGIN CERTIFICATE---- ABCDE...... ----END CERTIFICATE---- ----BEGIN CERTIFICATE---- EFGHT...... ----END CERTIFICATE---- ----BEGIN CERTIFICATE---- NAMNOP...... ----END CERTIFICATE---- ----BEGIN CERTIFICATE---- KROPS...... ----END CERTIFICATE---- The top certificate is the root certificate Followed by the hops in the right sequence Ending with server certificate Tags: connectors data crawl Previous How to order workflows Next What are preflight checks? Self-signed Internal CA"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/odbc-jdbc-driver",
    "text": "Connect data Connectivity Framework Connector Framework FAQ Can I connect to any source with an ODBC/JDBC driver? Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's supported connectors use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, contact Atlan support to share more details about your use case. You can also use Atlan APIs to integrate sources for crawling data assets , generating lineage , or enriching existing assets with additional metadata . Tags: connectors data integration crawl api faq-connections Previous Supported sources Next Can the Hive crawler connect to an independent Hive metastore?"
  },
  {
    "url": "https://docs.atlan.com/tags/workflow",
    "text": "13 docs tagged with \"workflow\" View all tags Automate data governance You can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Configure workflow execution Learn about configure workflow execution. Crawl Informatica CDI assets Configure and run the crawler to discover and catalog your Informatica CDI assets Create governance workflows :::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/. Delegate administration The workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows. How often does Atlan crawl Snowflake? Learn about how often does atlan crawl snowflake?. Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Manage governance workflows :::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/. Manage requests If your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected. Requests Requests allow users to suggest changes to assets that they cannot directly change themselves. Revoke data access As an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the [scope of governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows). Set up Alteryx Set up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run. Troubleshooting Anomalo connectivity Learn about troubleshooting anomalo connectivity."
  },
  {
    "url": "https://docs.atlan.com/tags/orchestration",
    "text": "12 docs tagged with \"orchestration\" View all tags Automate data governance You can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Configure workflow execution Learn about configure workflow execution. Create governance workflows :::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. Delegate administration The workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows. How often does Atlan crawl Snowflake? Learn about how often does atlan crawl snowflake?. Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Manage governance workflows :::warning Who can do this? You must be an [admin user](/product/capabilities/governance/users-and-groups/concepts/what-are-user-roles) in Atlan to [enable](/. Manage requests If your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected. Requests Requests allow users to suggest changes to assets that they cannot directly change themselves. Revoke data access As an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the [scope of governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows). Troubleshooting Anomalo connectivity Learn about troubleshooting anomalo connectivity."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-github",
    "text": "Connect data ETL Tools dbt Impact Analysis Add impact analysis in GitHub On this page Add impact analysis in GitHub danger For existing users, the dbt-action is no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the atlan-action . Refer to How to migrate from dbt to Atlan action to learn more and complete the migration. If you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard,Â Atlan provides a GitHub Action to help you out. This action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request. Prerequisites â Before running the action, you will need to create an Atlan API token . You will also need to assign a persona to the API token and add a metadata policy that provides the requisite permissions on assets for the Atlan dbt action to work. For example, you can add the following permissions: dbt   - Read and Update Materialized layer, such as Snowflake   - Read and Update Any downstream connections, such as Microsoft Power BI   - Read only When a pull request with changes to one or more dbt models is merged, the Atlan dbt action will link the pull request as a resource to the assets in Atlan. To ensure that the pull request is linked as a resource, you will need to assign the right persona with requisite permissions to the API token. You will need to configure the default GITHUB_TOKEN permissions. Grant Read and write permissions to the GITHUB_TOKEN in your repository to allow the atlan-action to seamlessly add or update comments on pull requests. Refer to GitHub documentation to learn more. Configure the action â To set up the Atlan action in GitHub: Create repository secrets in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token. Add the GitHub Action to your workflow: Create a workflow file in your repository   - .github/workflows/atlan-dbt.yml . Add the following code to your workflow file: name : Atlan action on : pull_request : types : [ opened , edited , synchronize , reopened , closed ] jobs : get-downstream-impact : name : Get Downstream Assets runs-on : ubuntu - latest steps : - name : Run Action uses : atlanhq/atlan - action@v1 with : GITHUB_TOKEN : $ { { secrets.GITHUB_TOKEN } } ATLAN_INSTANCE_URL : $ { { secrets.ATLAN_INSTANCE_URL } } ATLAN_API_TOKEN : $ { { secrets.ATLAN_API_TOKEN } } Test the action â After you've completed the configuration above, create a pull request with a changed dbt model file to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request: The GitHub workflow will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. The comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms. View the impacted assets in Atlan or open the source URL   -  for example, view an impacted Looker dashboard directly in Looker. Once you have merged the pull request, it will be added as a resource to the dbt model and its materialized assets. You can view the linked pull request from the Resources tab of the asset sidebar. For example: Inputs â Name Description Required GITHUB_TOKEN For writing comments on PRs to print downstream assets true ATLAN_INSTANCE_URL For making API requests to the user's tenant true ATLAN_API_TOKEN For authenticating API requests to the user's tenant true DBT_ENVIRONMENT_BRANCH_MAP For mapping the GitHub branch with a specific dbt environment false IGNORE_MODEL_ALIAS_MATCHING For turning off matching aliases using this variable false Troubleshooting the action â Why does the action fetch a model from an incorrect environment? â If there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitHub branch with a specific dbt environment. This will allow the Atlan GitHub action to parse lineage for that specific environment. For example, you can provide the mapping in this format   - branch name : dbt environment name jobs: get-downstream-impact: name: Get Downstream Assets runs-on: ubuntu-latest steps: - name: Run Action uses: atlanhq/atlan-action@v1 with: GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}} ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}} ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}} +         DBT_ENVIRONMENT_BRANCH_MAP: | +           main: dbt-prod +           beta: dbt-test Why does the action fetch a model by its alias and not model name? â By default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the IGNORE_MODEL_ALIAS_MATCHING input to true. For example: jobs: get-downstream-impact: name: Get Downstream Assets runs-on: ubuntu-latest steps: - name: Run Action uses: atlanhq/atlan-action@v1 with: GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}} ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}} ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}} +         IGNORE_MODEL_ALIAS_MATCHING: true Tags: connectors api authentication model Previous Migrate from dbt to Atlan action Next Add impact analysis in GitLab Prerequisites Configure the action Test the action Inputs Troubleshooting the action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-gitlab",
    "text": "Connect data ETL Tools dbt Impact Analysis Add impact analysis in GitLab On this page Add impact analysis in GitLab danger For existing users, the dbt-action is no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the atlan-action . Refer to How to migrate from dbt to Atlan action to learn more and complete the migration. If you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard, Atlan provides a GitLab CI/CD pipelineÂ to help you out. This pipeline places Atlan's impact analysis right into your merge request. So, you can view the potential downstream impact of your changes before merging the request. Prerequisites â Before running the action, you will need to create an Atlan API token . Assign a persona to the API token and add a metadata policy that provides requisite permissions on assets for the Atlan dbt action to work. For example, you can add the following permissions: dbt   - Read and Update Materialized layer, such as Snowflake   - Read and Update Any downstream connections, such as Microsoft Power BI   - Read only When a merge request with changes to one or more dbt models is merged, the Atlan dbt action will link the merge request as a resource to the assets in Atlan. To ensure that the merge request is linked as a resource, you will need to assign the right persona with requisite permissions to the API token . Configure the action â To set up the Atlan dbt action in GitLab: Define CI/CD variables in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token . GITLAB_TOKEN with the value of the project access token. Click the checkboxes for Mask variable and Expand variable only. Leave the Protect variable checkbox unchecked   -  merge request pipelines do not have access to protected variables . Add the GitLab pipeline to your workflow: Create a workflow file in the root directory of your repository   - .gitlab-ci.yml . Add the following code to your workflow file: stages : - get - downstream - impact get-downstream-impact-open : stage : get - downstream - impact image : node : 20 script : - git clone - branch v1 https : //github.com/atlanhq/atlan - action.git - cd atlan - action - npm install - npm run build - node ./adapters/index.js environment : name : get - downstream - impact rules : - if : '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if : '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when : never - if : '$CI_COMMIT_BRANCH' Test the action â After you've completed the configuration above, create a merge request with a changed dbt model file to test the action. You should see the Atlan GitLab CI/CD pipeline running and adding comments in your merge request: The GitLab CI/CD pipeline will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. The comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms. View the impacted assets in Atlan or open the source URL   -  for example, view an impacted Looker dashboard directly in Looker. Once you have merged the merge request, it will be added as a resource to the dbt model and its materialized assets. You can view the linked merge request from the Resources tab of the asset sidebar. For example: Inputs â Name Description Required GITLAB_TOKEN For writing comments on PRs to print downstream assets true ATLAN_INSTANCE_URL For making API requests to the user's tenant true ATLAN_API_TOKEN For authenticating API requests to the user's tenant true DBT_ENVIRONMENT_BRANCH_MAP For mapping the GitLab branch with a specific dbt environment false IGNORE_MODEL_ALIAS_MATCHING For turning off matching aliases using this variable false Troubleshooting the action â Why does the action fetch a model from an incorrect environment? â If there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitLab branch with a specific dbt environment. This will allow the Atlan GitLab CI/CD pipeline to parse lineage for that specific environment. For example, you can provide the mapping in this format   - branch name : dbt environment name stages: - get-downstream-impact get-downstream-impact-open: stage: get-downstream-impact image: node:20 variables: +    DBT_ENVIRONMENT_BRANCH_MAP: | +      main: [Enter Your Branch name] script: - git clone   - branch v1 https://github.com/atlanhq/atlan-action.git - cd atlan-action - npm install - npm run build - node ./adapters/index.js environment: name: get-downstream-impact rules: - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when: never - if: '$CI_COMMIT_BRANCH' Why does the action fetch a model by its alias and not model name? â By default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the IGNORE_MODEL_ALIAS_MATCHING input to true. For example: stages: - get-downstream-impact get-downstream-impact-open: stage: get-downstream-impact image: node:20 variables: +    IGNORE_MODEL_ALIAS_MATCHING: \"true\" script: - git clone   - branch v1 https://github.com/atlanhq/atlan-action.git - cd atlan-action - npm install - npm run build - node ./adapters/index.js environment: name: get-downstream-impact rules: - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when: never - if: '$CI_COMMIT_BRANCH' Tags: connectors data api authentication model Previous Add impact analysis in GitHub Next What does Atlan crawl from dbt Cloud? Prerequisites Configure the action Test the action Inputs Troubleshooting the action"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/bulk-enrich-metadata",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Bulk enrich metadata On this page Bulk enrich metadata Atlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan. Atlan currently supports the following options for bulk metadata enrichment: Export assets to spreadsheets â For a fairly large number of assets to be documented, you can export your assets from Atlan to a supported spreadsheet tool and bulk enrich metadata directly from spreadsheets. Atlan currently supports exporting assets to: Google Sheets Microsoft Excel online Here is a quick summary of this option: An Atlan admin must integrate a supported spreadsheet tool in Atlan to export assets. Once assets have been exported, you must install the Atlan extension to enrich assets and sync changes to Atlan. Only supported for online versions of Google Sheets and Microsoft Excel. Must use an organizational email address to export assets from Atlan. Exports basic as well as custom metadata. Supports exporting impacted assets . Creation of new terms or tags is not supported. To create new terms, admins and members with edit access can bulk upload glossaries . Deletion of terms is not supported. Does not provide cron support, use custom packages instead. Refer to How to export assets to get started. Use Atlan extension in spreadsheets â For a small number of assets to be documented, you can install the Atlan extension in a supported spreadsheet tool and bulk enrich metadata directly from spreadsheets. Atlan currently supports installing the Atlan extension in: Google Sheets Microsoft Excel online and desktop versions Here is a quick summary of this option: Must install the Atlan extension for Google Sheets or Microsoft Excel. Supports both online and desktop versions of supported spreadsheet tools. Only supports basic metadata   -  custom metadata is not supported, use export assets Â option instead. Supports importing and updating impacted assets. Creation of new terms or tags is not supported. To create new terms, admins and members with edit access can bulk upload glossaries . Glossary assets are not supported, use export assets option instead.Â Refer to How to integrate Atlan with Google Sheets or How to integrate Atlan with Microsoft Excel to get started. Use custom packages â Atlan's basic and advanced asset export custom packages can power miscellaneous use cases: Export to your file storage for reporting and analytics outside Atlan. Export to a CSV file, enrich metadata, and then use the asset import package to update in Atlan. Here is a quick summary of this option: Exports to a CSV file. Provides an alternative to out-of-the-box solutions if your administrative policies prohibit such integrations. The basic package exports almost all backend attributes, recommended for one-off migrations. The advanced package offers filtering capabilities, recommended if the scope of the export is limited. Both packages are compatible with the asset import package. Only admin users can run custom package workflows. All attributes except asset relationships are supported. Refer to Asset export (basic) or Asset export (advanced) to get started. Tags: integration connectors Previous What is included in the Slack integration? Next Configure custom domains for Microsoft Excel Export assets to spreadsheets Use Atlan extension in spreadsheets Use custom packages"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/disable-sample-data-preview",
    "text": "Use data Insights FAQ Can I turn off sample data preview for the entire organization? Can I turn off sample data preview for the entire organization? Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the Snowflake crawler to prevent users from previewing any Snowflake data. Simply change Allow Data Preview to No . There is currently no global toggle to turn off sample data on a tenant level. Sample data preview also does not fall under the purview of the Insights toggle . Tags: connectors data crawl faq faq-insights Previous Can I query any DW/DL? Next Can we restrict who can query our data warehouse?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/faq/site-renaming",
    "text": "Configure Atlan Integrations Project Management Jira FAQ Can site renaming affect the Jira integration? Can site renaming affect the Jira integration? Refer to our troubleshooting Jira documentation to learn more. Tags: integration connectors faq-integrations Previous Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Next ServiceNow"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/hive-metastore-connection",
    "text": "Connect data Connectivity Framework Connector Framework FAQ Can the Hive crawler connect to an independent Hive metastore? Can the Hive crawler connect to an independent Hive metastore? See Troubleshooting Hive connectivity . Tags: integration connectors faq-connections Previous Can I connect to any source with an ODBC/JDBC driver? Next How often does Atlan crawl Snowflake?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/microsoft-sso-login",
    "text": "Configure Atlan Integrations Identity Management SSO FAQ Can we use a Microsoft SSO login? Can we use a Microsoft SSO login? Yes, Atlan supports Azure AD for SSO . Tags: integration connectors faq-integrations Previous Can Atlan integrate with multiple Azure AD tenants within a single instance? Next What type of user provisioning does Atlan support for SSO integrations?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/connect-on-premises-databases-to-kubernetes",
    "text": "On this page Connect on-premises databases to Kubernetes Who can do this? You will need access to a machine that can run Kubernetes on-premises. You will also need your database access details, including credentials. You can configure and use Atlan's metadata-extractor tool to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose. Get the metadata-extractor tool â To get the metadata-extractor tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load image to Kubernetes cluster â You cannot upload the extractor image directly to a Kubernetes cluster. You must upload the extractor image to a container registry that your Kubernetes cluster can access. This ensures that Kubernetes can readily deploy pods with the metadata-extractor tool. Apply configuration maps â ConfigMaps contain essential settings that enable the metadata-extractor tool to connect to your database, including connection details and extraction parameters. This can help you customize the extraction process to fit your database environment. Deploy configurations to your specific database setup: kubectl apply -f config-maps.yml Create a config-maps.yml containing your database settings, for example: apiVersion: v1 kind: ConfigMap metadata: name: atlan-extractor-config-mysql data: DOWNLOAD_JDBC: \"true\" DOWNLOAD_JDBC_URL: \"https://example.com/path/to/jdbc-driver.tar.gz\" DRIVER: \"com.example.jdbc.Driver\"   Add other necessary configurations as key-value pairs Replace example values with details of your database connection and the JDBC driver. Deploy extraction job â Set up the CronJob for metadata extraction from the database: kubectl apply -f job.yml Example â Create a job.yml for the extraction job with details like the following: apiVersion: batch/v1 kind: CronJob metadata: name: atlan-extractor-cron-job spec: schedule: \"@weekly\" jobTemplate: spec: template: spec: containers: - name: crawler image: your-registry/path-to-extractor-image:latest   Define environment variables and volume mounts as required (Optional) Configure CronJob schedule â The CronJob is configured to execute weekly by default. To configure the CronJob schedule: Open the job.yml file. In the spec section, for schedule: , replace the \"@weekly\" cron expression with your preferred schedule. For example, use \"@daily\" for daily executions or provide a custom cron schedule . For more information on CronJob schedules, refer to Kubernetes documentation . (Optional) Trigger the job manually â To trigger an immediate metadata extraction, execute the CronJob manually: kubectl create job --from=cronjob/atlan-extractor-cron-job crawl-mysql-$(date '+%Y-%m-%d-%H-%M-%S') Request files from Atlan â To get started, contact Atlan support to request sample ConfigMap and CronJob files for supported SQL connectors: Microsoft SQL Server MySQL Oracle PostgreSQL Tags: connectors data Get the metadata-extractor tool Load image to Kubernetes cluster Apply configuration maps Deploy extraction job (Optional) Trigger the job manually Request files from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/troubleshooting/connection-issues",
    "text": "Connect data Databases SQL Databases CrateDB Troubleshooting Connection issues On this page Connection issues This guide helps you resolve common connection and authentication issues when setting up the CrateDB connector in Atlan. Unable to connect to CrateDB cluster â If Atlan can't establish a connection to your CrateDB cluster, you see connection errors during the initial setup or test connection phase. â Error messages Check failed - Unable to connect to CrateDB cluster Connection timed out ð Cause Incorrect HTTP/HTTPS endpoint format Firewall blocking port 4200 (or your configured port) SSL/TLS certificate errors when using HTTPS ð ï¸ How to fix Verify endpoint format : Make sure you're using the correct HTTP or HTTPS endpoint with proper format: http://your-host:4200 https://your-cluster.crate.io:4200 Check firewall and port access : Port 4200 must be open between Atlan and your CrateDB instance. Validate SSL certificate : If using HTTPS, make sure the certificate is valid and trusted by your network. Authentication failed â When credentials are incorrect or the user account has issues, authentication fails preventing Atlan from accessing your CrateDB cluster. â Error messages Check failed - Authentication failed for user <username> 401 Unauthorized ð Cause Wrong username or password User not present in CrateDB Password expired or changed ð ï¸ How to fix Verify credentials : Double-check that the username and password are correct. Veriy user exists : Confirm user exists in CrateDB with: SELECT name FROM sys . users WHERE name = 'atlan_user' ; Update password : Reset user password with: ALTER USER atlan_user SET ( password = 'new_password' ) ; Tags: connectors cratedb database troubleshooting Previous Preflight checks for CrateDB Next Permissions and limitations Unable to connect to CrateDB cluster Authentication failed"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/connectors-and-capabilities",
    "text": "Connect data Connectivity Framework Connector Framework References Connectors and capabilities On this page Connectors and capabilities The matrix below lists the supported capabilities for Atlan's current integrations: Connector   - supported sources Discovery   - native assets support Lineage   - asset lineage support through SQL parsing, API crawling, orchestrator tools, or parsing JSON files from S3 Secure agent extraction   -  Use a secure agent to extract metadata. Popularity   - asset usage and popularity metrics support Browser extension   - Atlan's browser extension support â   -  capability supported â   -  capability not supported â   -  capability will be a paid addition Data sources â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Amazon Athena â â â â â Amazon Redshift â â â â â AWS Glue â â â â â Cloudera Impala â â â â â Databricks â â â â â Google BigQuery â â â â â Hive â â â â â Microsoft Azure Synapse Analytics â â â â â Microsoft SQL Server â â â â â MySQL â â â â â Oracle â â â â â PostgreSQL â â â â â PrestoSQL â â â â â Redash â â â â â Salesforce â â â â â SAP ECC â â â â â SAP HANA â â â â â Snowflake â â â â â Teradata â â â â â Trino â â â â â NoSQL data sources â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Amazon DynamoDB â â â â â DataStax Enterprise â â â â â Microsoft Azure Cosmos DB â â â â â MongoDB â â â â â Business intelligence tools â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Amazon QuickSight â â â â â Domo â â â â â IBM Cognos Analytics â â â â â Looker â â â â â Metabase â â â â â Microsoft Power BI â â â â â MicroStrategy â â â â â Mode â â â â â Qlik Sense Cloud â â â â â Qlik Sense Cloud Enterprise â â â â â Sigma â â â â â Sisense â â â â â Tableau â â â â â ThoughtSpot â â â â â Data movement tools â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension dbt Cloud â â â â â dbt Core â â â â â Fivetran â â â â â Matillion â â â â â Microsoft Azure Data Factory â â â â â Data quality tools â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Anomalo â â â â â Monte Carlo â â â â â Soda â â â â â Event buses â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Aiven Kafka â â â â â Amazon MSK â â â â â Apache Kafka â â â â â Confluent Kafka â â â â â Microsoft Azure Event Hubs â â â â â Redpanda Kafka â â â â â Schema registry â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Confluent Schema Registry â â â â â Orchestration tools â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Apache Airflow/OpenLineage â â â â â Amazon MWAA â â â â â Astronomer â â â â â Dagster â â â â â Google Cloud Composer â â â â â Alteryx â â â â â Data processing tools â Connector Discovery Lineage Secure Agent extraction Popularity Browser extension Apache Spark â â â â â Tags: lineage data-lineage impact-analysis integration connectors api rest-api graphql Previous Additional connectivity to data sources Next OpenLineage configuration and facets Data sources NoSQL data sources Business intelligence tools Data movement tools Data quality tools Event buses Schema registry Orchestration tools Data processing tools"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka",
    "text": "Connect data Event/Messaging Aiven Kafka Crawl Aiven Kafka Assets Crawl Aiven Kafka On this page Crawl Aiven Kafka Once you have configured the Aiven Kafka permissions , you can establish a connection between Atlan and Aiven Kafka. Did you know? Atlan currently supports the offline extraction method for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. To crawl metadata from Aiven Kafka after uploading the results to S3 , review the order of operations and then complete the following steps. Select the source â To select Aiven Kafka as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Aiven Kafka Assets . In the right panel, click Setup Workflow . Provide credentials â Atlan supports the offline extraction method for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Extraction method , Offline is the default selection. ForÂ Bucket name , enter the name of your S3 bucket. ForÂ Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the Aiven Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Aiven Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, clickÂ Exclude topics regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Aiven Kafka crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run Â button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run Â button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Aiven Kafka Next What does Atlan crawl from Aiven Kafka? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/crawl-amazon-athena",
    "text": "Connect data Databases Query Engines Amazon Athena Crawl Athena Assets Crawl Amazon Athena On this page Crawl Amazon Athena Once you have configured the Amazon Athena access permissions , you can establish a connection between Atlan and Amazon Athena. (If you are also using a private network for Amazon Athena, you will need to set that up first , too.) To crawl metadata from Amazon Athena, review the order of operations and then complete the following steps. Select the source â To select Amazon Athena as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Athena Assets , and click Setup Workflow . Provide credentials â To enter your Amazon Athena credentials: ForÂ Host enter the host name (or PrivateLink endpoint ) for your Amazon Athena instance. ForÂ Authentication choose the method you configured when setting up the Amazon Athena access permissions : At the bottom, enter the AWS Role ARN and S3 Output Location you configured. The S3 Output Location is where you store temporary Athena query results. For IAM User authentication, enter the AWS Access Key and AWS Secret Key you configured. For IAM Role authentication, enter the following: Set the AWS Role ARN to the ARN of the role you created in your AWS account . (Optional) Under External ID , click the Generate button. Click the button to the right of this field to copy the generated ID and use it in setting up your trust policy . (Optional) For Workgroup , you can override the defaultÂ primary workgroup for tracking compute costs, granular permission controls, and more. Click Test Authentication to confirm connectivity to Amazon Athena. Once successful, at the bottom of the screen, click Next . Configure the connection â To complete the Amazon Athena connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Amazon Athena data, change Allow SQL Query toÂ No . (Optional) To prevent users from previewing any Amazon Athena data, change Allow Data Preview toÂ No . At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the Amazon Athena crawler, you can further configure it. You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a Java regular expression in theÂ Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False Â to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Amazon Athena crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen click the Run button. To schedule the crawler to run hourly, daily, weekly or monthly, at the bottom of the screen click the Schedule & Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up a private network link to Amazon Athena Next What does Atlan crawl from Amazon Athena? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/how-tos/crawl-amazon-dynamodb",
    "text": "Connect data Databases NoSQL Databases Amazon DynamoDB Crawl DynamoDB Assets Crawl Amazon DynamoDB On this page Crawl Amazon DynamoDB Once you have configured the Amazon DynamoDB permissions , you can establish a connection between Atlan and Amazon DynamoDB. To crawl metadata from Amazon DynamoDB, review the order of operations and then complete the following steps. Select the source â To select Amazon DynamoDB as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Amazon DynamoDB Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Amazon DynamoDB credentials: For Extraction method , Direct is the default extraction method. For Authentication, choose the method you configured when setting up the Amazon DynamoDB access permissions : For IAM User authentication, enter the AWS Access Key and AWS Secret Key you downloaded . For IAM Role authentication, enter the AWS Role ARN you configured . (Optional) Enter the AWS External ID only if you have configured an external ID in the role definition. For AWS Region , enter the AWS region of your Amazon DynamoDB instance. Click the Test Authentication button to confirm connectivity to Amazon DynamoDB. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Amazon DynamoDB connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Amazon DynamoDB crawler, you can further configure it. On the Metadata Â page, you can override the defaults for any of these options: To have the crawler ignore tables based on a naming convention, specify a regular expression in the Exclude tables regex field. To have the crawler include tables based on a naming convention, specify a regular expression in the Include tables regex field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Amazon DynamoDB crawler, after completing the steps above: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Amazon DynamoDB Next What does Atlan crawl from Amazon DynamoDB? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk",
    "text": "Connect data Event/Messaging Amazon MSK Crawl Amazon MSK Assets Crawl Amazon MSK On this page Crawl Amazon MSK Once you have configured the Amazon MSK permissions , you can establish a connection between Atlan and Amazon MSK. (If you are also using a private network for Amazon MSK, you will need to set that up first , too.) To crawl metadata from Amazon MSK, review the order of operations and then complete the following steps. Select the source â To select Amazon MSK as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Amazon MSK Assets . In the right panel, click Setup Workflow . Provide credentials â To enter your Amazon MSK credentials: For Extraction method , Direct is the default selection. For Bootstrap servers , enter the hostname(s) (or PrivateLink cluster connection string ) of your Amazon MSK broker(s)   -  for multiple hostnames, separate each entry with a comma , or semicolon ; . For Authentication , IAM Role is the default authentication method. For Deployment Type , Provisioned is the default deployment type. For Security protocol , SASL_SSL is the default security protocol. For AWS Role ARN , enter the ARN of the IAM role you created in your AWS account . For AWS Region , enter the AWS region of your Amazon MSK cluster. Click the Test Authentication button to confirm connectivity to Amazon MSK. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Amazon MSK connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Amazon MSK crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal topics , keep the default option Yes to skip internal Apache Kafka topics or click No to enable crawling them. To have the crawler ignore topics based on a naming convention, specify a regular expression in the Exclude topic regex field. To have the crawler include topics based on a naming convention, specify a regular expression in the Include topic regex field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Amazon MSK crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run Â button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run Â button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up a private network link to Amazon MSK Next What does Atlan crawl from Amazon MSK? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/crawl-amazon-quicksight",
    "text": "Connect data BI Tools Cloud-based BI Amazon QuickSight Crawl QuickSight Assets Crawl Amazon QuickSight On this page Crawl Amazon QuickSight Once you have configured the Amazon QuickSight permissions , you can establish a connection between Atlan and Amazon QuickSight. To crawl metadata from Amazon QuickSight, review the order of operations and then complete the following steps. Select the source â To select Amazon QuickSight as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click QuickSight Assets . In the right panel, click Setup Workflow . Provide your credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Amazon QuickSight credentials: For Authentication, IAM User is the default authentication method. For AWS Access Key , enter the AWS access key you downloaded . For AWS Secret Key , enter the AWS secret key you downloaded . At the bottom, enter the Region and AWS Account ID of your Amazon QuickSight instance. Click the Test Authentication button to confirm connectivity to Amazon QuickSight. Once authentication is successful, navigate to the bottom of the screen and click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Amazon QuickSight. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Amazon QuickSight data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the Amazon QuickSight connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Amazon QuickSight crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: For Fetch all assets without folder , click Yes to fetch assets not linked to any folders, including datasets, analyses, and dashboards, or click No to only fetch assets linked to folders. To select the folders you want to include in crawling, click Include Folders . (This will default to all folders, if none are specified.) To select the folders you want to exclude from crawling, click Exclude Folders . (This will default to no folders, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Amazon QuickSight crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Amazon QuickSight Next What does Atlan crawl from Amazon QuickSight? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift Crawl Redshift Assets Crawl Amazon Redshift On this page Crawl Amazon Redshift Once you have configured the Amazon Redshift access permissions , you can establish a connection between Atlan and Amazon Redshift. To crawl metadata from Amazon Redshift, review the order of operations and then complete the following steps. Select the source â To select Amazon Redshift as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Redshift Assets , and click Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . Direct extraction method â To enter your Amazon Redshift credentials: For Host Name , enter the host name of your Amazon Redshift instance. From your Redshift cluster you can find the host name in the Configuration section as a variable called Endpoint . For Port , enter the port number for your Amazon Redshift instance. You can find this next to the host name in the Configuration section of your Redshift cluster. For Deployment Type , click Provisioned if your Amazon Redshift instance is deployed on provisioned clusters or click Serverless if deployed on a serverless workgroup . For Authentication , choose the method you configured when setting up the Amazon Redshift access permissions : For Basic authentication, enter the Username and Password you configured. For IAM User authentication, enter the AWS Access Key , AWS Secret Key , and Username you configured for the database. (Optional) This is only required if you are accessing a private cluster on provisioned deployment, using a Network Load Balancer (NLB), and connecting via IAM, for Cluster ID , enter the name of the Amazon Redshift cluster that you want to connect to. (Optional) This is only required if you are accessing a private cluster on serverless deployment, for Workgroup , enter the name of your workgroup . For IAM Role authentication, enter the Username you configured for the database only if your deployment type is Provisioned . For Serverless deployment type, you do not need to enter a username. Set the AWS Role ARN to the ARN of the role you created in your AWS account . (Optional) For Region , enter the AWS region of your Amazon Redshift instance. Offline extraction method â Atlan supports the offline extraction method for fetching metadata from Amazon Redshift. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the Amazon Redshift connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Amazon Redshift data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Amazon Redshift data, change Allow Data Preview to No . At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the Amazon Redshift crawler, you can further configure it. You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to configure the crawler: For Cross Connection , click Yes to extract lineage across all available Amazon Redshift connections or click No to limit lineage extraction to the current connection. For Control Config , if Atlan support has provided you with a custom control configuration, select Custom and enter the configuration into the Custom Config box. You can also: Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. If you've configured a cloned schema to provide access to Atlan, add the following key-value pair to the Custom Config field: {\"clonedPgCatalogSchema\": \"cloned_schema_name\"} Replace cloned_schema_name with the name of your cloned schema. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Amazon Redshift crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up a private network link to Amazon Redshift Next Mine Amazon Redshift Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka",
    "text": "Connect data Event/Messaging Apache Kafka Crawl Apache Kafka Assets Crawl Apache Kafka On this page Crawl Apache Kafka Atlan crawls metadata from your Apache Kafka clust er, allowing you to discover, classify, and govern your Kafka topics and schemas. This guide walks you through the steps to configure and run the Apache Kafka crawler in Atlan. Prerequisites â Before you begin, complete the following prerequisites: Apache Kafka setup: You have configured the Apache Kafka permissions , you can establish a connection between Atlan and Apache Kafka. Order of operations: Review the order of operations to understand the sequence of tasks for crawling metadata. Access to Atlan workspace: You must have the required permissions in Atlan to create and manage a connection. Select the source â To select Apache Kafka as your source: In Atlan, click New , and from the menu, select New Workflow . From the Marketplace page, click Apache Kafka Assets . Click Setup Workflow in the right panel to proceed with configuration. Provide credentials â InÂ Direct extraction, Atlan connects to Apache Kafka and crawls metadata directly. InÂ Offline extraction, you need to first extract metadata yourself and make it available in S3 . Direct extraction method â To enter your Apache Kafka credentials: For Bootstrap servers , enter the hostname(s) of your Apache Kafka broker(s)   -  for multiple hostnames, separate each entry with a comma , or semicolon ; . For Authentication , Atlan provides the following authentication methods: No Authentication: If your Apache Kafka cluster does not require authentication, Atlan can connect without any credentials.. Basic Authentication (SASL/PLAIN): Uses a username and password with the SASL_PLAIN mechanism for authentication. SCRAM Authentication (SASL/SCRAM): Uses a username and password with the SASL_SCRAM mechanism (SCRAM-SHA-256 or SCRAM-SHA-512) for secure authentication. Username, enter the username for your Apache Kafka brokers. Password, enter the password for the username. For Security protocol , select Plaintext or SSL for No Auth, and SASL_PLAINTEXT or SASL_SSL for Basic and SCRAM authentication. For SASL Mechanism (optional for SCRAM authentication), choose the appropriate mechanism for your Kafka cluster. Click Test Authentication to confirm connectivity. Once authentication is successful, click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Apache Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: ForÂ Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the Apache Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Apache Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal topics , keep the default option Yes to skip internal Apache Kafka topics or click No to enable crawling them. To select the Apache Kafka assets you want to exclude from crawling, clickÂ Exclude topics regex . (This will default to no assets, if none specified.) To select the Apache Kafka assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Apache Kafka crawler, after completing the steps above: Click Preflight checks to verify configuration. Choose one of the following options: To run the crawler once immediately, click Run . To schedule the crawler, click Schedule & Run . Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Apache Kafka Next Set up on-premises Kafka access Prerequisites Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/how-tos/crawl-aws-glue",
    "text": "Connect data ETL Tools AWS Glue Crawl AWS Glue Assets Crawl AWS Glue On this page Crawl AWS Glue Once you have configured the AWS Glue access permissions , you can establish a connection between Atlan and AWS Glue. To crawl metadata from AWS Glue, review the order of operations and then complete the following steps. Select the source â To select AWS Glue as your source: In the top right corner of any screen, navigate to New Â and then click New Workflow . From the list of packages, select Glue Assets , and click Setup Workflow . Provide credentials â To enter your AWS Glue credentials: For Authentication , choose the method you configured when setting up the AWS Glue access permissions : At the bottom, enter the Region of your AWS Glue deployment. ForÂ IAM User authentication, enter theÂ AWS Access Key and AWS Secret Key you configured. ForÂ IAM Role authentication, enter the following: Set the AWS Role ARN to the ARN of the role you created in your AWS account . (Optional) Under External ID , click the Generate button. Click the button to the right of this to copy the generated ID and use this in setting up your trust policy . ClickÂ Test Authentication to confirm connectivity to AWS Glue. Once successful, at the bottom of the screen, click Next . Configure the connection â To complete the AWS Glue connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the AWS Glue crawler, you can further configure it. You can override the defaults for any of these options: Select assets you want to include in crawling in the Include Metadata field. (This will default to all assets, if none are specified.) Select assets you want to exclude from crawling in the Exclude Metadata field. (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the AWS Glue crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen click the Run button. To schedule the crawler to run hourly, daily, weekly or monthly, at the bottom of the screen click the Schedule & Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up AWS Glue Next What does Atlan crawl from AWS Glue? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/how-tos/crawl-bigid",
    "text": "Connect data Privacy & Security BigID Crawl BigID Metadata Crawl BigID On this page Crawl BigID Configure the Atlan BigID workflow to crawl metadata from your BigID instance and discover privacy-related data assets in Atlan. This guide walks through setting up the workflow, configure connection, map data sources, and running the crawler. Prerequisites â Before you begin, make sure you have: Set up a BigID system user account. If not, follow the Set up BigID guide for detailed instructions. Your BigID domain name and API token which is needed to configure the workflow. To crawl metadata from BigID, review the order of operations . Permissions required â To successfully configure the BigID workflow, make sure that your user role has below permissions: Atlan : Admin or Workflow Admin permissions BigID : System user with API access Set up workflow â Follow these steps in Atlan to create a BigID workflow: Click New and then click New Workflow to set up a new workflow From the list of packages, select BigID and click on Setup Workflow . Provide below details: Workflow Name : Enter a unique name to help recognize and manage the workflow Host FQDN : Enter the BigID domain name. For private-network setups, use the private DNS associated with the link. Personal Access Token Value : Provide the API token created for the system user in the Set up BigID SSL certificate : Enter the root certificate PEM value if your BigID instance exposes a self-signed certificate Click Test Authentication to confirm connectivity to BigID When successful, at the bottom of the screen click Next Set up connection â Set up the connection details and specify who can manage this connection. Follow these steps to configure the connection: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, nobody can manage the connection - not even admins. At the bottom of the screen, click Next to proceed Map data sources â Map BigID Data Sources to Atlan Connections to establish the relationship between your data assets. Follow these steps to map data sources: Atlan Connection : Select the Atlan Connection that houses the data assets that you're trying to bring BigID metadata for BigID Datasources : Select one or more BigID Data Sources that contain assets associated with the mapped Atlan Connection Click Next to configure custom metadata Configure custom metadata â Set up custom metadata to store BigID-discovered attributes in Atlan. Follow these steps to configure custom metadata: Custom Metadata : Create a new Custom Metadata on Atlan named BigID Metadata with a text-based property named Attributes . This is used to house the BigID-discovered, scan-related attributes that the workflow brings over to Atlan Attribute Custom Metadata : Enter the value as BigID Metadata Attribute Custom Metadata Property : Enter the value as Attributes Run crawler â Execute the BigID crawler to discover and import metadata. Follow these steps to run crawler: Run immediately : Click the Run button to run the crawler once immediately Schedule run : Click the Schedule Run button to schedule the crawler to run hourly, daily, weekly, or monthly Troubleshooting â If you encounter issues during the BigID crawl process: Authentication issues : Verify your API token is valid and has the correct permissions SSL certificate errors : Check that you've provided the correct root certificate PEM value No metadata appears : Check that data source mapping is correct and BigID contains assets for the mapped connections Need help â Contact Atlan support : For issues related to Atlan integration, contact Atlan support See also â What does Atlan crawl from BigID Tags: connectors data crawl privacy bigid Previous Set up BigID Next What does Atlan crawl from BigID? Prerequisites Permissions required Set up workflow Troubleshooting Need help See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/how-tos/crawl-confluent-kafka",
    "text": "Connect data Event/Messaging Confluent Kafka Crawl Confluent Kafka Assets Crawl Confluent Kafka On this page Crawl Confluent Kafka Atlan crawls metadata from your Confluent Kafka cl uster, allowing you to discover, classify, and govern your Kafka topics and schemas. This guide walks you through the steps to configure and run the Confluent Kafka crawler in Atlan. Prerequisites â Before you begin, complete the following prerequisites: Confluent Kafka setup: You have configured the Confluent Kafka permissions , you can establish a connection between Atlan and Confluent Kafka. Order of operations: Review the order of operations to understand the sequence of tasks for crawling metadata. Access to Atlan workspace: You must have the required permissions in Atlan to create and manage a connection. Select the source â To select Confluent Kafka as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Confluent Kafka Assets . In the right panel, click Setup Workflow . Provide credentials â In Direct extraction , Atlan connects to Confluent Kafka and crawls metadata directly. In Offline extraction , you need to first extract metadata yourself and make it available in S3. Direct extraction method â To enter your Confluent Kafka credentials: For Bootstrap servers , enter the hostname(s) of your Confluent Kafka broker(s). Separate multiple hostnames with a comma , or semicolon ; . For API Key , enter the API key you copied. For API Secret , enter the API secret you copied. For Security protocol, click SASL_PLAINTEXT to connect to Confluent Kafka through a non-encrypted channel or click SASL_SSL to connect via a Secure Sockets Layer (SSL) channel. Click the Test Authentication button to confirm connectivity to Confluent Kafka. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Confluent Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the Confluent Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, use values like production , development , gold , or analytics . (Optional) To change the users who can manage this connection, update the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Confluent Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal topics , keep the default option Yes to skip internal Kafka topics or click No to enable crawling them. To select the assets you want to exclude from crawling, clickÂ Exclude topics regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Confluent Kafka crawler: To run the crawler once, immediately, click the Run button at the bottom of the screen. To schedule the crawler to run hourly, daily, weekly, or monthly, click the Schedule & Run button at the bottom of the screen. Once the crawl completes, your assets appear in Atlan! ð Tags: connectors data crawl setup Previous Set up Confluent Kafka Next Set up on-premises Kafka access Prerequisites Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry",
    "text": "Connect data Event/Messaging Confluent Schema Registry Crawl Schema Registry Assets Crawl Confluent Schema Registry On this page Crawl Confluent Schema Registry Once you have configured the Confluent Schema Registry access permissions , you can establish a connection between Atlan and Confluent Schema Registry. To crawl metadata from Confluent Schema Registry, review the order of operations and then complete the following steps. Select the source â To select Confluent Schema Registry as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Confluent Schema Registry Assets . In the right panel, click Setup Workflow . Provide credentials â To enter your Confluent Schema Registry credentials: For Host , enter your schema registry endpoint . For API Key , enter the API key you copied . For API Secret , enter the API secret you copied . Click the Test Authentication button to confirm connectivity to Confluent Schema Registry. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Confluent Schema Registry connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Confluent Schema Registry crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the subjects you want to exclude from crawling, click Exclude subjects . (This will default to no subjects, if none specified.) To select the subjects you want to include in crawling, click Include subjects . (This will default to all subjects, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Confluent Schema Registry crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Confluent Schema Registry Next What does Atlan crawl from Confluent Schema Registry? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/how-tos/crawl-cratedb",
    "text": "Connect data Databases SQL Databases CrateDB Crawl CrateDB Assets Crawl CrateDB On this page Crawl CrateDB Extract metadata from your CrateDB database and make it available in Atlan for data discovery, governance, and lineage tracking. This guide walks you through setting up authentication and running your first crawl. Prerequisites â Before you begin, make sure you have: Set up CrateDB with proper user permissions Network connectivity between Atlan and your CrateDB instance Your CrateDB cluster HTTP endpoint and port information Set up workflow â Create a new CrateDB Assets workflow to extract metadata from your database. Select New > New Workflow . From the list of packages, select CrateDB Assets . Click Setup Workflow . Configure extraction method â Choose how to connect to your CrateDB environment: Direct extraction Agent extraction Select Direct for the extraction method. Enter your CrateDB connection details: Host : Your CrateDB cluster HTTP endpoint (for example, https://your-cluster.crate.io ) Port : The port number of your CrateDB instance Authentication : Choose Basic authentication Username : Enter the username you configured in CrateDB Password : Enter the password you configured in CrateDB Database : Enter the name of the database to crawl Click Test Authentication to confirm connectivity to CrateDB using these details. When successful, click Next . Select Agent for the extraction method. Add the secret keys for your secret store configuration. Follow the Secure Agent configuration guide . Click Next . Configure connection details â Enter a Connection Name to identify your CrateDB environment. For example, production-cratedb , analytics-db , data-warehouse . Assign Connection Admins to manage access. At least one admin is required. Configure crawler settings â Before running the CrateDB crawler, you can configure additional settings: Exclude Metadata : Select assets you want to exclude from crawling Include Metadata : Select assets you want to include in crawling Exclude regex for tables & views : Specify a regular expression to ignore tables and views based on naming conventions Advanced Config : Enable Source Level Filtering : Enable schema-level filtering at source Use JDBC Internal Methods : Enable JDBC internal methods for data extraction Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run crawler â You can now start extracting metadata from your CrateDB database: Run now : Click Run to start a one-time crawl. Schedule runs : Click Schedule Run to automate recurring crawls (hourly, daily, weekly, or monthly). Monitor crawl progress in the activity log. Once complete, your CrateDB assets appear in Atlan. Troubleshooting â If you encounter connection or authentication issues, see Connection issues . See also â What does Atlan crawl from CrateDB? Preflight checks for CrateDB Tags: connectors data crawl Previous Set up CrateDB Next What does Atlan crawl from CrateDB? Prerequisites Set up workflow Troubleshooting See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/how-tos/crawl-dagster",
    "text": "Connect data Orchestration & Workflow Dagster Get Started Crawl Dagster assets On this page Crawl Dagster assets Private Preview Create a crawler workflow in Atlan to capture lineage from your Dagster assets. This workflow connects to Dagster and begins lineage capture. Prerequisites â Before you begin, make sure you have: Admin access to your Atlan workspace Configured Dagster for Atlan integration. For more information, see Set up Dagster Create crawler workflow â Follow these steps to create a workflow in Atlan that captures lineage from Dagster. In Atlan, select New > New Workflow . From the package list, choose Dagster Assets . Select Setup Workflow . Configure connection â Follow these steps to configure the Dagster connection in Atlan and finalize lineage capture. Enter a Connection Name . For example, production , development , or analytics . Assign at least one Connection Admin . Select Run to create the connection. Track the workflow's progress in the Workflow center . After the workflow completes, the Dagster connection is ready to receive lineage events. See also â What does Atlan crawl from Dagster : Metadata available from Dagster after integration Tags: connectors lineage dagster Previous Set up Dagster Next What does Atlan crawl from Dagster Prerequisites Create crawler workflow Configure connection See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/how-tos/crawl-datastax-enterprise",
    "text": "Connect data Databases NoSQL Databases Datastax Enterprise Crawl Datastax Enterprise Assets Crawl DataStax Enterprise On this page Crawl DataStax Enterprise Once you have configured DataStax Enterprise , you can establish a connection between Atlan and DataStax Enterprise. To crawl metadata from DataStax Enterprise, review the order of operations and then complete the following steps. Select the source â To select DataStax Enterprise as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select DataStax Enterprise Assets and click the Setup Workflow button. Provide credentials â InÂ Direct extraction, Atlan connects to DataStax Enterprise and crawls metadata directly. InÂ Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your DataStax Enterprise credentials: For Host Name , enter the host name of your DataStax Enterprise instance. For Port , enter the port number of your DataStax Enterprise instance. For Authentication , Basic authentication, enter the Username and Password you use to log in to DataStax Enterprise. (Optional) For SSL , keep the default Enabled to use HTTPS or click Disabled to use HTTP. (Optional) For SSL certificate , this is only required if your DataStax Enterprise instance uses a self-signed or an internal CA SSL certificate , paste a supported SSL certificate in the recommended format . At the bottom of the form, click the Test Authentication button to confirm connectivity to DataStax Enterprise using these details. When successful, at the bottom of the screen click the Next button. Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from DataStax Enterprise. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the DataStax Enterprise data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the DataStax Enterprise connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Configure the crawler â Before running the DataStax Enterprise crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the DataStax Enterprise keyspaces you want to include in crawling, click Include Keyspaces . (This will default to all assets, if none are specified.) To select the DataStax Enterprise keyspaces you want to exclude from crawling, click Exclude Keyspaces . (This will default to no assets, if none are specified.) To have the crawler ignore DataStax Enterprise keyspaces based on a naming convention, specify a regular expression in the Exclude Keyspaces Regex field. To check for any permissions or other configuration issues before running the crawler, click Preflight checks . Did you know? If a keyspace appears in both the include and exclude filters, the exclude filter takes precedence. (The Exclude Keyspace Regex also takes precedence.) Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up DataStax Enterprise Next What does Atlan crawl from DataStax Enterprise? Select the source Provide credentials Configure the connection Configure the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/how-tos/crawl-domo",
    "text": "Connect data BI Tools Cloud-based BI Domo Crawl Domo Assets Crawl Domo On this page Crawl Domo Once you have configured the Domo permissions , you can establish a connection between Atlan and Domo. To crawl metadata from Domo, review the order of operations and then complete the following steps. Select the source â To select Domo as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Domo Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Domo credentials: For Host Name , enter the URL for your Domo instance. For Authentication , Basic is the default selection. For Client ID , enter the client ID you copied from the Domo developer portal. For Client Secret , enter the client secret you copied from the Domo developer portal. For Access Token , enter the access token you copied from your Domo instance. Click the Test Authentication button to confirm connectivity to Domo. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Domo connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Domo crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the dashboards you want to include in crawling, click Include dashboards . (This will default to all assets, if none are specified.) To select the dashboards you want to exclude from crawling, click Exclude dashboards . (This will default to no assets, if none are specified.) For DomoStats dataset ID to get cards metadata , enter the dataset ID for the dataset you created to import card metadata on the DomoStats connector. For DomoStats dataset ID to get card-dashboard relationship metadata , enter the dataset ID for the dataset you created to import card-dashboard relationship metadata on the DomoStats connector. For DomoStats dataset ID to get dataset-card relationship metadata , enter the dataset ID for the dataset you created to import dataset-card relationship metadata on the DomoStats connector. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Domo crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Domo Next What does Atlan crawl from Domo? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/how-tos/crawl-fivetran",
    "text": "Connect data ETL Tools Fivetran Crawl Fivetran Assets Crawl Fivetran On this page Crawl Fivetran Once you have configured the Fivetran permissions , you can establish a connection between Atlan and Fivetran. To enrich Atlan with metadata from Fivetran, review the order of operations and then complete the following steps. Select the source â To select Fivetran as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Fivetran Enrichment and then click Setup Workflow . Provide credentials â In order to run this package, you must ensure the following: The Fivetran Platform Connector is set up and has run successfully in Fivetran at least once. Fivetran logs are stored in a destination supported by Atlan. The above destination has been crawled in Atlan. To use the Fivetran Platform Connector: For Atlan Connection , Atlan will use the credentials of your selected connection to read the Fivetran Platform Connector tables associated with that connection. You can either: Create a connection in Atlan for the destination warehouse you configured while setting up the Fivetran Platform Connector in Fivetran. This connection in Atlan must have access to the Fivetran tables created by the Fivetran Platform Connector. Atlan supports the following destinations: Amazon Redshift Databricks Google BigQuery PostgreSQL Snowflake If you have already created a connection in Atlan, select the connection to extract. (To select a connection, the crawler must have already run for a supported destination.) danger If you have an existing connection, you must ensure that the user or other access permissions configured for that connection allow access to the Fivetran tables created by the Fivetran Platform Connector or update them accordingly. For Fivetran Platform Schema , select the destination schema where Fivetran logs are stored . You can only select one schema and must ensure that the connection above has access to all Fivetran log tables stored in this destination schema . Once successful, at the bottom of the screen, click Next . Configure the connection â To complete the Fivetran connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Run the enrichment â You can now enrich Atlan with Fivetran metadata: To check for any permissions or other configuration issues before running the enrichment, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run Â button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run Â button. Once the enrichment has completed running, you will see lineage extended upstream from your data platform, warehouse, or lake! ð Tags: connectors data crawl api configuration Previous Set up Fivetran Next What does Atlan crawl from Fivetran? Select the source Provide credentials Configure the connection Run the enrichment"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery Crawl BigQuery Assets Crawl Google BigQuery On this page Crawl Google BigQuery Once you have configured the Google BigQuery user permissions , you can establish a connection between Atlan and Google BigQuery. To crawl metadata from Google BigQuery, review the order of operations and then complete the following steps. Select the source â To select Google BigQuery as your source: In the top right corner of any screen, click New and then click New Workflow . From the list of packages, select BigQuery Assets and click Setup Workflow . Provide credentials â To enter your Google BigQuery credentials: For Authentication , Service Account is the default selection. For Connectivity , choose how you want Atlan to connect to Google BigQuery: To connect using a public endpoint from Google, click Public Network . To connect through a private endpoint, click Private Network Link . Next, contact Atlan support to request the DNS name of the Private Service Connect endpoint that Atlan created for the integration: For Host , enter the DNS name of the Private Service Connect endpoint received from Atlan in the following format   - https://bigquery-<privateserver>.p.googleapis.com . Replace <privateserver> with the DNS name. For Port , 443 is the default selection. For Project Id , enter the value of project_id from the JSON for the service account you created . This project ID is only used to authenticate the connection. You can configure the crawler to extract more than just the specified project. For Service Account Json , paste in the entire JSON for the service account you created . For Service Account Email , enter the value of client_email from the JSON for the service account you created . At the bottom of the form, click the Test Authentication button to confirm connectivity to Google BigQuery using these details. When successful, at the bottom of the screen click the Next button. Configure the connection â To complete the Google BigQuery connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Google BigQuery data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Google BigQuery data, change Allow Data Preview to No . At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the Google BigQuery crawler, you can further configure it. You can override the defaults for any of these options: For Filter Sharded Tables , keep No for the default configuration or click Yes to enable Atlan to catalog and display sharded tables with the same naming prefix as a single table in asset discovery and the lineage graph. Select assets you want to include in crawling in the Include Metadata field. (This will default to all assets, if none are specified.) Select assets you want to exclude from crawling in the Exclude Metadata field. (This will default to no assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. To import existing tags from Google BigQuery to Atlan , for Import Tags , click Yes . For Advanced Config , keep Default for the default configuration or click Custom if Atlan support has provided you with a custom control configuration. Enter the configuration into the Custom Config box. You can also enter {âignore-all-caseâ: true} to enable crawling assets with case-sensitive identifiers. For Hidden Assets , keep No for the default configuration or click Yes to crawl metadata from your hidden datasets in Google BigQuery. Did you know? If a folder or project appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Google BigQuery crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous How to enable SSO for Google BigQuery Next Mine Google BigQuery Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/crawl-hive",
    "text": "Connect data Databases Query Engines Hive Crawl Hive Assets Crawl Hive On this page Crawl Hive Once you have configured the Hive permissions , you can establish a connection between Atlan and Hive. (If you are also using a private network for Hive, you will need to set that up first , too.) To crawl metadata from Hive, review the order of operations and then complete the following steps. Select the source â To select Hive as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Hive Assets . In the right panel, click Setup Workflow . Provide your credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. InÂ Offline extraction, you need to first extract metadata yourself and make it available in S3 . Direct extraction method â To enter your Hive credentials: For Host Name , enter the host name (or PrivateLink endpoint ) for your Hive instance. For Port , enter the port number for your Hive instance. For Username , enter the username you created for that instance. For Password , enter the password for the username. For Default Schema , enter the default schema name for your Hive instance. Click the Test Authentication button to confirm connectivity to Hive. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Hive. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your bucket details: ForÂ Bucket name , enter the name of your S3 bucket or Atlan's bucket. ForÂ Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen clickÂ Next . Configure the connection â To complete the Hive connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Hive crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the Hive assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the Hive assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Hive crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Did you know? Once you have crawled assets from Hive, you can run the Hive Miner to mine query history through S3 . Tags: connectors data crawl Previous Set up a private network link to Hive Next What does Atlan crawl from Hive? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/crawl-ibm-cognos-analytics",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Crawl Cognos Analytics Assets Crawl IBM Cognos Analytics On this page Crawl IBM Cognos Analytics Once you have configured the IBM Cognos Analytics permissions , you can establish a connection between Atlan and IBM Cognos Analytics. To crawl metadata from IBM Cognos Analytics, revie w the order of operations and then complete the following steps. Select the source â To select IBM Cognos Analytics as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click IBM Cognos Analytics Assets .Â In the right panel, click Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . Direct extraction method â To enter your IBM Cognos Analytics credentials: For Host Name , enter the hostname of your IBM Cognos Analytics instance. For Port , enter the port number of your IBM Cognos Analytics instance. For Authentication , select the method you configured when setting up the IBM Cognos Analytics access permissions : For Basic , enter the Username and Password you configured for the new user. For API Key , enter the Username and API Key you configured for the new user. For OKTA , enter the Username and Password you configured for the new user in OKTA. For Namespace , enter the name of the namespace where you created the new user . Click the Test Authentication button to confirm connectivity to IBM Cognos Analytics. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method â Atlan supports the offline extraction method for fetching metadata from IBM Cognos Analytics. This method uses Atlan's cognos-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your bucket details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output/cognos-example/contents/0/result-0.json , output/cognos-example/contents-details/0/result-0.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the IBM Cognos Analytics connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, no one can manage the connection-not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the IBM Cognos Analytics crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Folders . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Folders . (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the IBM Cognos Analytics crawler, after completing the steps above: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up on-premises IBM Cognos Analytics access Next Crawl on-premises IBM Cognos Analytics Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/how-tos/crawl-informatica-cdi",
    "text": "Connect data ETL Tools Informatica CDI Crawl Informatica CDI Assets Crawl Informatica CDI assets On this page Crawl Informatica CDI assets Create a crawler workflow to automatically discover and catalog your Informatica Cloud Data Integration assets, including projects, workflows, and data lineage. Prerequisites â Before you begin, verify you have: Completed the Set up Informatica CDI guide Access to your Informatica Cloud environment Parameter files downloaded from your Secure Agent machines Create crawler workflow â Create a new workflow and select Informatica CDI as your connector source. In the top-right corner of any screen, select New > New Workflow . From the list of packages, select Informatica CDI Assets > Setup Workflow . Configure authentication â Set up secure access to your Informatica Cloud environment by providing connection credentials. In the Host field, enter your Informatica CDI domain without the protocol or sub-region. Example If your full URL is: https://usw1.dmp-us.informaticacloud.com/ Enter only: dmp-us.informaticacloud.com Enter the Username and Password for the user you created in the Set up Informatica CDI guide. Select Test Authentication to verify connectivity to Informatica CDI. After successful authentication, select Next . Configure connection â Set up connection management and define who can access and manage this connection. Enter a Connection Name that represents your source environment. For example, use values like production, development, gold, or analytics. To modify who can manage this connection, update the users or groups listed under Connection Admins . If you don't specify any user or group, no one can manage the connection, including admins. Select Next to continue. Configure crawler â Set up what to crawl and configure advanced options for accurate lineage generation. Configure metadata filters using the Include Metadata and Exclude Metadata fields. If an asset appears in both fields, the exclude metadata field takes precedence. Include Metadata : Select the projects or folders you want to include in crawling. This defaults to all assets if none are specified. Exclude Metadata : Select the projects or folders you want to exclude from crawling. This defaults to no assets if none are specified. Configure advanced options for uploading parameter files: Upload parameter files used by the Informatica CDI projects or folders in a compressed format. MIME types : Windows ZIP or Linux Zip Run crawler â Execute the crawler to discover and catalog your Informatica CDI assets. To run the crawler immediately, select Run . To schedule the crawler to run hourly, daily, weekly, or monthly, select Schedule & Run . After the crawler completes, you can view the assets on Atlan's asset page. See also â What does Atlan crawl from Informatica CDI : Understand the metadata and assets discovered during crawling Tags: connectors etl-tools informatica cdi crawl workflow Previous Set up Informatica CDI Next Transformations Prerequisites Create crawler workflow See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/how-tos/crawl-matillion",
    "text": "Connect data ETL Tools Matillion Crawl Matillion Assets Crawl Matillion On this page Crawl Matillion Once you have configured the Matillion user permissions , you can establish a connection between Atlan and Matillion. To crawl metadata from Matillion, review the order of operations and then complete the following steps. Select the source â To select Matillion as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Matillion Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Matillion credentials: For Extraction method , Direct is the default selection. For Hostname , enter the host name of your Matillion instance. For Authentication , Basic Authentication is the default method. For Username , enter the username you created in Matillion . For Password , enter the password you created for the username . For SSL , keep Enabled to connect via a Secure Sockets Layer (SSL) channel or click Disabled . Click the Test Authentication button to confirm connectivity to Matillion. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Matillion connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, no one can manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Matillion crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options. If an asset appears in both the include and exclude filters, the exclude filter takes precedence. To select the assets you want to include in crawling, click Include Projects . (This defaults to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Projects . (This defaults to no assets, if none specified.) For Enable Lineage , keep the default option Yes to crawl lineage or click No to disable it. End-to-end lineage is currently not supported for Matillion version 1.68 LTS due to limitations of the Matillion APIs   -  only lineage for asset transformations is supported at present. Run the crawler â To run the Matillion crawler, after completing the previous steps: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you can see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Matillion Next What does Atlan crawl from Matillion? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase",
    "text": "Connect data BI Tools Cloud-based BI Metabase Crawl Metabase Assets Crawl Metabase On this page Crawl Metabase Once you have configured the Metabase user permissions , you can establish a connection between Atlan and Metabase. To crawl metadata from Metabase, review the order of operations and then complete the following steps. Select the source â To select Metabase as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Metabase Assets and click on Setup Workflow . Provide credentials â To enter your Metabase credentials: For Host Name , enter the full URL for your Metabase instance, including the https:// . For Port , enter the port number of your Metabase instance. For Authentication , enter the Username and Password you configured. Click the Test Authentication button to confirm connectivity to Metabase using these details. Once successful, at the bottom of the screen, click Next . Configure the connection â To complete the Metabase connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the Metabase crawler, you can further configure it. You can override the defaults for any of the remaining options: Select collections you want to include in crawling in the Include Collections field. (This will default to all collections, if none are specified.) Select collections you want to exclude from crawling in the Exclude Collections field. (This will default to no collections, if none are specified.) Did you know? If a collection appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â You can now run the Metabase crawler. To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Metabase Next What does Atlan crawl from Metabase? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db",
    "text": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB Crawl Cosmos DB Assets Crawl Microsoft Azure Cosmos DB On this page Crawl Microsoft Azure Cosmos DB Once you have configured the Microsoft Azure Cosmos DB permissions , you can establish a connection between Atlan and Microsoft Azure Cosmos DB. To crawl metadata from Microsoft Azure Cosmos DB, review the order of operations and then complete the following steps. Select the source â To select Microsoft Azure Cosmos DB as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Cosmos DB Assets . In the right panel, click Setup Workflow . Provide your credentials â Choose your deployment method: In vCore Â deployment, you will need the primary connection string(s) of your vCore-based Microsoft Azure Cosmos DB account(s). In RU deployment, you will need the client ID, client secret, and tenant ID of the service principal you created for your RU-based Microsoft Azure Cosmos DB account. In vCore and RU deployment,Â you will need the primary connection string(s) of your vCore-based account(s) and client ID, client secret, and tenant ID of the service principal you created for yourÂ RU-based account. vCore deployment â To enter your Microsoft Azure Cosmos DB credentials: For Database API , MongoDB is the default selection. For Extraction method , Direct is the default selection. For Select the deployment types to crawl , click vCore . For Connection Strings , enter the primary connection string(s) you copied from your Microsoft Azure Cosmos DB account(s). Click the Test Authentication button to confirm connectivity to Microsoft Azure Cosmos DB. Once authentication is successful, navigate to the bottom of the screen and click Next . RU deployment â To enter your Microsoft Azure Cosmos DB credentials: For Database API , MongoDB is the default selection. For Extraction method , Direct is the default selection. For Select the deployment types to crawl , click RU . For Client ID , enter the application (client) ID you copied for your service principal. For Client Secret , enter the client secret you copied for your service principal. For Tenant ID , enter the directory (tenant) ID you copied for your service principal. Click the Test Authentication button to confirm connectivity to Microsoft Azure Cosmos DB. Once authentication is successful, navigate to the bottom of the screen and click Next . vCore and RU deployment â To enter your Microsoft Azure Cosmos DB credentials: For Database API , MongoDB is the default selection. For Extraction method , Direct is the default selection. For Select the deployment types to crawl , click vCore and RU . For Client ID , enter the application (client) ID you copied of the service principal for your RU-based account. For Client Secret , enter the client secret you copied of the service principal for your RU-based account. For Tenant ID , enter the directory (tenant) ID you copied of the service principal for your RU-based account. For Connection Strings , enter the primary connection string(s) you copied from your vCore-based account(s). Click the Test Authentication button to confirm connectivity to Microsoft Azure Cosmos DB. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Microsoft Azure Cosmos DB connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Microsoft Azure Cosmos DB crawler, you can further configure it. On the Metadata page, you can override the defaults for the following: For Extract Collection Schemas , change to Yes to enable Atlan to extract collection schemas by reading a subset of the documents in the collection and map them to column assets . For SchemaÂ extraction sample size , you can set a custom value of up to 1,000 for documents to be read for schema analysis. Run the crawler â To run the Microsoft Azure Cosmos DB crawler, after completing the steps above: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Microsoft Azure Cosmos DB Next What does Atlan crawl from Microsoft Azure Cosmos DB? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-factory",
    "text": "Connect data ETL Tools Microsoft Azure Data Factory Crawl Microsoft Azure Data Factory Assets Crawl Microsoft Azure Data Factory On this page Crawl Microsoft Azure Data Factory Once you have configured the Microsoft Azure Data Factory permissions , you can establish a connection between Atlan and Microsoft Azure Data Factory. To crawl metadata from Microsoft Azure Data Factor y, review the order of operations and then complete the following steps. Select the source â To select Microsoft Azure Data Factory as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Azure Data Factory Assets . In the right panel, click Setup Workflow . Provide credentials â To enter your Microsoft Azure Data Factory credentials: For Extraction method , Direct is the default selection. For Authentication , Service Principal is the default selection. For Client ID , enter the application (client) ID you copied for the service principal. For Client Secret , enter the client secret you copied for the service principal. For Tenant ID ,Â enter the directory (tenant) ID you copied for the service principal. Click the Test Authentication button to confirm connectivity to Microsoft Azure Data Factory. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Microsoft Azure Data Factory connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Run the crawler â To run the Microsoft Azure Data Factory crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Microsoft Azure Data Factory Next What does Atlan crawl from Microsoft Azure Data Factory? Select the source Provide credentials Configure the connection Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/crawl-microsoft-azure-event-hubs",
    "text": "Connect data Event/Messaging Microsoft Azure Event Hubs Crawl Microsoft Azure Event Hubs Assets Crawl Microsoft Azure Event Hubs On this page Crawl Microsoft Azure Event Hubs Once you have configured the Microsoft Azure Event Hubs permissions , you can establish a connection between Atlan and Microsoft Azure Event Hubs. To crawl metadata from Microsoft Azure Event Hubs, review the order of operations and then complete the following steps. Select the source â To select Microsoft Azure Event Hubs as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Azure Event Hubs Assets . In the right panel, click Setup Workflow . Provide credentials â Choose your authentication method: In SAS Key , you will need your event hub namespace and a connection string-primary key for authentication. In Service Principal , you will need your event hub namespace and the following: If only fetching metadata from Microsoft Azure Event Hubs, you will need a client ID, client secret, and tenant ID for authentication. If fetching metadata from both Microsoft Azure Event Hubs and Apache Kafka, you will need a connection string-primary key Â and client ID, client secret, and tenant ID for authentication. SAS key â To enter your Microsoft Azure Event Hubs credentials: For Extraction method , Direct is the default selection. For Select which metadata to fetch , click From Only Event hubs to only fetch metadata from Microsoft Azure Event Hubs or click From Both Kafka and Event hubs to fetch metadata from both Microsoft Azure Event Hubs and Apache Kafka. For Bootstrap servers , enter the event hub namespace you copied from Microsoft Azure Event Hubs in the following format   - <your event hub namespace>.servicebus.windows.net:9093 . For Connection string-primary key , enter the connection string-primary key you copied from Microsoft Azure Event Hubs. For Security protocol , SSL is the default selection for connecting via a Secure Sockets Layer (SSL) channel. Click the Test Authentication button to confirm connectivity to Microsoft Azure Event Hubs. Once authentication is successful, navigate to the bottom of the screen and click Next . Service principal â To enter your Microsoft Azure Event Hubs credentials: For Extraction method , Direct is the default selection. For Bootstrap servers , enter the event hub namespace you copied from Microsoft Azure Event Hubs in the following format   - <your event hub namespace>.servicebus.windows.net:9093 . For Select which metadata to fetch , you can either: Click From Only Event hubs to only fetch metadata from Microsoft Azure Event Hubs. If you choose not to fetch metadata from Apache Kafka, note that Atlan will not be able to display the message count for your event hubs and consumer groups. To enter your credentials: For Client ID , enter the application (client) ID you copied for the service principal. For Client Secret , enter the client secret you copied for the service principal. For Tenant ID ,Â enter the directory (tenant) ID you copied for the service principal. Click From Both Kafka and Event hubs to fetch metadata from both Microsoft Azure Event Hubs and Apache Kafka and then enter your credentials: For Connection string-primary key , enter the connection string-primary key you copied from Microsoft Azure Event Hubs. For Client ID , enter the application (client) ID you copied for the service principal. For Client Secret , enter the client secret you copied for the service principal. For Tenant ID ,Â enter the directory (tenant) ID you copied for the service principal. For Security protocol , SSL is the default selection for connecting via a Secure Sockets Layer (SSL) channel. Click the Test Authentication button to confirm connectivity to Microsoft Azure Event Hubs. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Microsoft Azure Event Hubs connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Microsoft Azure Event Hubs crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal event hubs , keep the default option Yes to skip internal event hubs or click No to enable crawling them. To select the assets you want to exclude from crawling, click Exclude event hubs regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include event hubs regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Microsoft Azure Event Hubs crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Microsoft Azure Event Hubs Next What does Atlan crawl from Microsoft Azure Event Hubs? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/crawl-microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Crawl Synapse Assets Crawl Microsoft Azure Synapse Analytics On this page Crawl Microsoft Azure Synapse Analytics Once you have configured the Microsoft Azure Synapse Analytics permissions , you can establish a connection between Atlan and Microsoft Azure Synapse Analytics. To crawl metadata from Microsoft Azure Synapse Ana lytics, review the order of operations and then complete the following steps. Select the source â To select Microsoft Azure Synapse Analytics as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Synapse Assets . In the right panel, click Setup Workflow . Provide your credentials â Choose your authentication method: In Basic authentication, you will need a username , password , and database name. In Service principal authentication, you will need a client ID, client secret, tenant ID , and database name. Basic authentication â To enter your Microsoft Azure Synapse Analytics credentials: For Host , enter the server name of your SQL pool . For Port , enter the port number where your SQL pool is available. For Username , enter the username you created when setting up user permissions. For Password , enter the password you created when setting up user permissions. For Database , enter the name of the database you want to crawl. Click the Test Authentication button to confirm connectivity to Microsoft Azure Synapse Analytics. Once authentication is successful, navigate to the bottom of the screen and click Next . Service principal authentication â Did you know? For service principal authentication, Atlan fetches Synapse pipeline metadata from the Azure Synapse Analytics REST API to generate lineage. Refer to What lineage does Atlan extract from Microsoft Azure Synapse Analytics? to learn more. To enter your Microsoft Azure Synapse Analytics credentials: For Host , enter the server name of your SQL pool . For Port , enter the port number where your SQL pool is available. For Client ID , enter the application (client) ID you copied for your service principal. For Client Secret , enter the client secret you copied for your service principal. For Tenant ID , enter the directory (tenant) ID you copied for your service principal. For Database , enter the name of the database you want to crawl. Click the Test Authentication button to confirm connectivity to Microsoft Azure Synapse Analytics. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Microsoft Azure Synapse Analytics connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Microsoft Azure Synapse Analytics crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Microsoft Azure Synapse Analytics crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up on-premises Microsoft Azure Synapse Analytics miner access Next What does Atlan crawl from Microsoft Azure Synapse Analytics? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server Crawl SQL Server Assets Crawl Microsoft SQL Server On this page Crawl Microsoft SQL Server Once you have configured the Microsoft SQL Server user permissions , you can establish a connection between Atlan and Microsoft SQL Server. (If you are also using a private network for Microsoft SQL Server, you will need to set that up first, too, for your Microsoft SQL Server on Amazon RDS or Amazon EC2 instance.) To crawl metadata from Microsoft SQL Server, revie w the order of operations and then complete the following steps. Select the source â Select Microsoft SQL Server as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select SQL Server Assets and click on Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Microsoft SQL Server credentials: ForÂ Host ,Â enter the availability group listener name (or PrivateLink endpoint for your Microsoft SQL Server on Amazon RDS or Amazon EC2 instance). ForÂ Port ,Â enter the port on which Microsoft SQL Server is available (default is 1433). ForÂ Username , enter the username created when setting up user permissions. ForÂ Password , enter the password created when setting up user permissions. For Database , enter the name of the database. Click Test Authentication to confirm connectivity to Microsoft SQL Server using these details. When successful, at the bottom of the screen click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Microsoft SQL Server. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include database.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. Once completed, navigate to the bottom of the screen and click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Microsoft SQL Server. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Microsoft SQL Server data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â Complete the Microsoft SQL Server connection configuration: Provide aÂ Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Microsoft SQL Server data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Microsoft SQL Server data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the Microsoft SQL Server crawler, you can further configure it. To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in theÂ Exclude regex for tables & views field. You can also specify the following system tables to exclude from crawling   - Â sys* , MSmerge* , dbo.sys* , MSrepl* , IH* , MSpeer* , cdc.* , MS*history , MS*agent , MSdist* , MSpub* , MSsubcri* , MSdbms* , MSdynamic* , and MSagent* . Note that this is not an exhaustive list, for more information refer to source documentation . For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False Â to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Microsoft SQL Server crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's assets page! ð Did you know? Once you have crawled assets from Microsoft SQL Server, you can run the SQL Server Miner to mine query history through S3 . Tags: connectors data crawl Previous Set up Microsoft SQL Server Next Set up a private network link to Microsoft SQL Server on Amazon EC2 Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/how-tos/crawl-microstrategy",
    "text": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy Crawl MicroStrategy Assets Crawl MicroStrategy On this page Crawl MicroStrategy Once you have configured the MicroStrategy permissions , you can establish a connection between Atlan and MicroStrategy. To crawl metadata from MicroStrategy, review the order of operations and then complete the following steps. Select the source â To select MicroStrategy as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click MicroStrategy Assets .Â In the right panel, click Setup Workflow . Provide credentials â To enter your MicroStrategy credentials: For Host , enter the hostname of your MicroStrategy instance. For Authentication , Basic Authentication is the default selection. For Username , enter the username you created for the instance. For Password , enter the password for the username. Click the Test Authentication button to confirm connectivity to MicroStrategy. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the MicroStrategy connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the MicroStrategy crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Projects . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Projects . (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the MicroStrategy crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run Â button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up MicroStrategy Next What does Atlan crawl from MicroStrategy? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/how-tos/crawl-mode",
    "text": "Connect data BI Tools Cloud-based BI Mode Crawl Mode Assets Crawl Mode On this page Crawl Mode Once you have configured the Mode user permissions , you can establish a connection between Atlan and Mode. To crawl metadata from Mode, review the order of operations and then complete the following steps. Select the source â To select Mode as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Mode Assets and click on Setup Workflow . Provide credentials â To enter your Mode credentials: For Authentication , Basic is the default selection. For API Key ID , enter the API key ID you copied for your API token . For API Secret , enter the API secret you copied for your API token . For Workspace , enter the name of your Mode workspace as retrieved from the API or workspace URL. For Exclude all personal collections , change to Yes to exclude your personal collections or keep the default selection No to include them. Click the Test Authentication button to confirm connectivity to Mode using these details. Once successful, at the bottom of the screen, click Next . Configure the connection â To complete the Mode connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler â danger If you're authenticating in Atlan using a member API token , Atlan currently does not distinguish between collections you have access to and ones restricted at source. This is due to limitations with the Mode API. However, Atlan will only extract basic metadata for restricted collections and no underlying assets such as reports or queries. Before running the Mode crawler, you can further configure it. You can override the defaults for any of these options: To select the collections you want to exclude from crawling, click Exclude Collections . (This will default to no collections, if none are specified.) To select the collections you want to include in crawling, click Include Collections . (This will default to all collections, if none are specified.) To disable crawling archived reports and associated queries and charts from Mode, for Crawl Archived Reports , change to No . Did you know? If a collection appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Mode crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Mode Next What does Atlan crawl from Mode? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/how-tos/crawl-mongodb",
    "text": "Connect data Databases NoSQL Databases MongoDB Crawl MongoDB Assets Crawl MongoDB On this page Crawl MongoDB Once you have configured the MongoDB permissions , you can establish a connection between Atlan and MongoDB. To crawl metadata from MongoDB, review the order of operations and then complete the following steps. Select the source â To select MongoDB as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click MongoDB Assets . In the right panel, click Setup Workflow . Provide your credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â For Extraction method , Direct is the default selection. To enter your MongoDB credentials: For SQL interface host name , enter the host name of the SQL (or JDBC) endpoint you copied from your MongoDB database. For Authentication , Basic is the default method. For Username , enter the username you created in your MongoDB database. For Password , enter the password you created for the username . For MongoDB native host , enter the host name of your MongoDB database you copied. For Default database , enter the name of the default database you copied from your MongoDB database. For Authentication database , enter the name of the authentication database you copied from your MongoDB database. admin is the default selection   -  learn more about authentication databases in MongoDB . For SSL , keep Yes to connect via a Secure Sockets Layer (SSL) channel or click No . Click the Test Authentication button to confirm connectivity to MongoDB. Once authentication is successful, navigate to the bottom of the screen and click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from MongoDB. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the MongoDB data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the MongoDB connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the MongoDB crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets, if none specified.) To have the crawler ignore collections based on a naming convention, specify a regular expression in the Exclude regex for collections field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the MongoDB crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up MongoDB Next What does Atlan crawl from MongoDB? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/crawl-mysql",
    "text": "Connect data Databases SQL Databases MySQL Crawl MySQL Assets Crawl MySQL On this page Crawl MySQL Once you have configured the MySQL user permissions , you can establish a connection between Atlan and MySQL. (If you are also using a private network for MySQL, you will need to set that up first , too.) To crawl metadata from MySQL, review the order of operations and then complete the following steps. Select the source â To select MySQL as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select MySQL Assets and click on Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your MySQL credentials: For Host Name enter the host for your MySQL instance. For Port enter the port number of your MySQL instance. For Authentication choose the method you configured when setting up the MySQL user : For Basic authentication, enter the Username and Password you configured in MySQL. For IAM User authentication, enter the AWS Access Key , AWS Secret Key , and database Username you configured. For IAM Role authentication, enter the AWS Role ARN of the new role you created and database Username you configured. (Optional) Enter the AWS External ID only if you have not configured an external ID in the role definition. Click Test Authentication to confirm connectivity to MySQL using these details. info ðª Did you know? If you get an Error: 1129: Host ... is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts' , ask your database admin to run the FLUSH HOSTS; command in the RDS instance, and then try again. When successful, at the bottom of the screen click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from MySQL. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from MySQL. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the MySQL data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the MySQL connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any MySQL data, change Allow SQL Query to No . (Optional) To prevent users from previewing any MySQL data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the MySQL crawler, you can further configure it. (Some of the options may only be available when using the direct extraction method .) You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the MySQL crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up a private network link to MySQL Next What does Atlan crawl from MySQL? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-on-premises-databricks",
    "text": "Connect data Data Warehouses Databricks On-premises Setup Crawl on-premises Databricks On this page Crawl on-premises Databricks Once you have set up the databricks-extractor tool , you can extract metadata from your on-premises Databricks instances by completing the following steps. Run databricks-extractor â Crawl all Databricks connections â To crawl all Databricks connections using the databricks-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection â To crawl a specific Databricks connection using the databricks-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The databricks-extractor tool will generate many folders with JSON files for each service . For example: catalogs schemas tables You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/databricks-example/catalogs/success/result-0.json , output/databricks-example/schemas/{{catalog_name}}/success/result-0.json , output/databricks-example/tables/{{catalog_name}}/success/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/databricks-example s3://my-bucket/metadata/databricks-example --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Databricks Be sure to select Offline for the Extraction method . Tags: connectors data crawl Previous Set up on-premises Databricks access Next Set up on-premises Databricks lineage extraction Run databricks-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/crawl-on-premises-ibm-cognos-analytics",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Crawl Cognos Analytics Assets Crawl on-premises IBM Cognos Analytics On this page Crawl on-premises IBM Cognos Analytics Once you have set up the cognos-extractor tool , you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps. Run cognos-extractor â Crawl all IBM Cognos Analytics connections â To crawl all IBM Cognos Analytics connections using the cognos-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection â To crawl a specific IBM Cognos Analytics connection using the cognos-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The cognos-extractor tool will generate many folders with JSON files for each service . For example: folders explorations reports modules data sources and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/cognos-example/contents/result-0.json , output/cognos-example/contents-details/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/cognos-example s3://my-bucket/metadata/cognos-example --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl IBM Cognos Analytics Be sure you select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl IBM Cognos Analytics Next What does Atlan crawl from IBM Cognos Analytics? Run cognos-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/on-premises-event-buses/how-tos/crawl-on-premises-kafka",
    "text": "Connect data Event/Messaging Redpanda Kafka Guides Crawl on-premises Kafka On this page Crawl on-premises Kafka Once you have set up the kafka-extractor tool , you can extract metadata from your on-premises Kafka instances by completing the following steps. Run kafka-extractor â Crawl all Kafka connections â To crawl all Kafka connections using the kafka-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection â To crawl a specific Kafka connection using the kafka-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The kafka-extractor tool will generate many folders with JSON files for each service . For example: topics topic-configs consumer-groups consumer-groups-members and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket.Â To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. Upload the files to the S3 bucket using your preferred method   -  include all the files from the output folder generated after running Docker Compose. For example, to upload all files using the AWS CLI : aws s3 cp output/kafka-example s3://my-bucket/metadata/kafka-example --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Apache Kafka How to crawl Confluent Kafka How to crawl Aiven Kafka How to crawl Redpanda Kafka Be sure you select S3 for the Extraction method . Tags: connectors data crawl Previous Set up on-premises Kafka access Next What does Atlan crawl from Redpanda Kafka? Run kafka-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/crawl-on-premises-looker",
    "text": "Connect data BI Tools Cloud-based BI Looker Crawl Looker Assets Crawl on-premises Looker On this page Crawl on-premises Looker Once you have set up the looker-extractor tool , you can extract metadata from your on-premises Looker instances using the following steps. Run looker-extractor â Crawl all Looker connections â To crawl all Looker connections using the looker-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection â To crawl a specific Looker connection using the looker-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <CONNECTION-NAME> (Replace <CONNECTION-NAME> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The looker-extractor tool will generate many JSON files for each service . For example: projects.json dashboards.json dashboard_tiles.json looks.json and many others You can inspect the metadata and make sure it is acceptable to provide the metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure all files for a particular database have the same prefix. For example, metadata/looker/projects.json , metadata/looker/dashboards.json , etc. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/looker-example s3://my-bucket/metadata/looker-example --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Looker Be sure you select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl Looker Next What does Atlan crawl from Looker? Run looker-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-on-premises-tableau",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau Crawl Tableau Assets Crawl on-premises Tableau On this page Crawl on-premises Tableau Once you have set up the tableau-extractor tool , you can extract metadata from your on-premises Tableau instances by completing the following steps. Run tableau-extractor â Crawl all Tableau connections â To crawl all Tableau connections using the tableau-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection â To crawl a specific Tableau connection using the tableau-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The tableau-extractor tool will generate many folders with JSON files for each service . For example: calculated_fields dashboards datasources workbooks and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/tableau-example/dashboards/result-0.json , output/tableau-example/workbooks/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/tableau-example s3://my-bucket/metadata/tableau-example --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Tableau Be sure you select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl Tableau Next What does Atlan crawl from Tableau? Run tableau-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-on-premises-thoughtspot",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot Crawl ThoughtSpot Assets Crawl on-premises ThoughtSpot On this page Crawl on-premises ThoughtSpot Once you have set up the thoughtspot-extractor tool , you can extract metadata from your on-premises ThoughtSpot instances by completing the following steps. Run thoughtspot-extractor â Crawl all ThoughtSpot connections â To crawl all ThoughtSpot connections using the thoughtspot-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection â To crawl a specific ThoughtSpot connection using the thoughtspot-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The thoughtspot-extractor tool will generate many folders with JSON files for each service . For example: tags answers liveboards answer-sql-queries liveboard-sql-queries You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/thoughtspot-example/filter/answers/result-0.json , output/thoughtspot-example/filter/liveboards/result-0.json , output/thoughtspot-example/filter/answer-sql-queries/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/thoughtspot-example/filter s3://my-bucket/metadata/thoughtspot-example --recursive Crawl metadata in Atlan â Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl ThoughtSpot Be sure to select Offline Â for the Extraction method . Tags: connectors data crawl Previous Crawl ThoughtSpot Next What does Atlan crawl from ThoughtSpot? Run thoughtspot-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/how-tos/crawl-oracle",
    "text": "Connect data Databases SQL Databases Oracle Crawl Oracle Assets Crawl Oracle On this page Crawl Oracle Once you have configured the Oracle user permissions , you can establish a connection between Atlan and Oracle. To crawl metadata from Oracle, review the order of operations and then complete the following steps. Select the source â To select Oracle as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Oracle Assets and click on Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Oracle credentials: For Host Name , enter the host for your Oracle instance. For Port , enter the port number of your Oracle instance. For Username and Password , enter the credentials you created when configuring the permissions . For SID , enter the Oracle system identifier for your database. For Default Database Name, Â enter the database name (usually the same as the SID). Click the Test Authentication button to confirm connectivity to Oracle using these details. Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Oracle. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Oracle. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Oracle data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the Oracle connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Oracle data, change Allow SQL Query to No . (Note: This option has no effect when using the S3 extraction method.) (Optional) To prevent users from previewing any Oracle data, change Allow Data Preview to No . (Note: This option has no effect when using the S3 extraction method.) At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the Oracle crawler, you can further configure it. (These options are only available when using the direct extraction method .) You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in theÂ Exclude regex for tables & views field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Oracle crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . (This option is only available when using the S3 extraction method.) You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Oracle Next What does Atlan crawl from Oracle? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/crawl-postgresql",
    "text": "Connect data Databases SQL Databases PostgreSQL Crawl PostgreSQL Assets Crawl PostgreSQL On this page Crawl PostgreSQL Once you have configured the PostgreSQL user permissions , you can establish a connection between Atlan and PostgreSQL. (If you are using a private network for PostgreSQL, you will need to set that up first , too.) To crawl metadata from PostgreSQL, review the order of operations and then complete the following steps. Select the source â To select PostgreSQL as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Postgres Assets and click on Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your PostgreSQL credentials: For Host enter the host for your PostgreSQL instance. For Port enter the port number of your PostgreSQL instance. For Authentication choose the method you configured when setting up the PostgreSQL user : For Basic authentication, enter the Username and Password you configured in PostgreSQL. For IAM User authentication, enter the AWS Access Key , AWS Secret Key , and database Username you configured. For IAM Role authentication, enter the AWS Role ARN of the new role you created and database Username you configured. (Optional) Enter the AWS External ID only if you have not configured an external ID in the role definition. For Database enter the name of the database to crawl. Click Test Authentication to confirm connectivity to PostgreSQL using these details. When successful, at the bottom of the screen click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from PostgreSQL. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include database.json , columns-<database>.json , and so on. When complete, at the bottom of the screen click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from PostgreSQL. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the PostgreSQL data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the PostgreSQL connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the PostgreSQL crawler, you can further configure it. (These options are only available when using the direct extraction method.) You can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the PostgreSQL crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up PostgreSQL Next What does Atlan crawl from PostgreSQL? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/how-tos/crawl-prestosql",
    "text": "Connect data Databases Query Engines PrestoSQL Crawl PrestoSQL Assets Crawl PrestoSQL On this page Crawl PrestoSQL Once you have configured the PrestoSQL user permissions , you can establish a connection between Atlan and PrestoSQL. Did you know? Atlan currently only supports PrestoSQL until version 349. PrestoDB is not supported at present. To crawl metadata from PrestoSQL, review the order of operations and then complete the following steps. Select the source â To select PrestoSQL as your source: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the list of packages, select PrestoSQLÂ Assets and click onÂ Setup Workflow . Provide credentials â To enter your PrestoSQL credentials: ForÂ Host enter the host for your PrestoSQL instance. ForÂ Port enter the port of your PrestoSQL instance. ForÂ Username enter the name of the user you created . ForÂ Password enter the password for the user you created . ClickÂ Test Authentication to confirm connectivity to PrestoSQL using these details. When successful, at the bottom of the screen clickÂ Next . Configure the connection â To complete the PrestoSQL connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the PrestoSQL crawler, you can further configure it. You can override the defaults for any of these options: To select the PrestoSQL assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the PrestoSQL assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select the relevant option. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the PrestoSQL crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up PrestoSQL Next What does Atlan crawl from PrestoSQL? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/crawl-qlik-sense-cloud",
    "text": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud Crawl Qlik Sense Cloud Assets Crawl Qlik Sense Cloud On this page Crawl Qlik Sense Cloud Once you have configured the Qlik Sense Cloud permissions , you can establish a connection between Atlan and Qlik Sense Cloud. To crawl metadata from Qlik Sense Cloud, review th e order of operations and then complete the following steps. Select the source â To select Qlik Sense Cloud as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Qlik Sense Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Qlik Sense Cloud credentials: For Host , enter the tenant URL for your Qlik Sense Cloud instance. For Port , enter the port number for your Qlik Sense Cloud instance. For Authentication , API key is the default option. For API Key , enter the API key you copied from your Qlik Sense Cloud instance. Click the Test Authentication button to confirm connectivity to Qlik Sense Cloud. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Qlik Sense Cloud connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Qlik Sense Cloud crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Spaces . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Spaces . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Qlik Sense Cloud crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Qlik Sense Cloud Next What does Atlan crawl from Qlik Sense Cloud? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/crawl-qlik-sense-enterprise-on-windows",
    "text": "Connect data BI Tools On-premises & Enterprise BI Qlik Sense Enterprise on Windows Crawl Qlik Sense Enterprise Assets Crawl Qlik Sense Enterprise on Windows On this page Crawl Qlik Sense Enterprise on Windows Once you have configured the Qlik Sense Enterprise on Windows permissions , you can establish a connection between Atlan and Qlik Sense Enterprise on Windows. To crawl metadata from Qlik Sense Enterprise on Wi ndows, review the order of operations and then complete the following steps. Select the source â To select Qlik Sense Enterprise on Windows as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Qlik Sense Enterprise Windows Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Qlik Sense Enterprise on Windows credentials: JWT authentication â For Host , enter the hostname for your Qlik Sense Enterprise on Windows instance. For Port , enter the port number for your Qlik Sense Enterprise on Windows instance. For Authentication , keep JWT as the authentication method. For JWT token , enter the JWT you generated . For Virtual proxy prefix , enter the prefix for your virtual proxy . Click the Test Authentication button to confirm connectivity to Qlik Sense Enterprise on Windows. Once authentication is successful, navigate to the bottom of the screen and click Next . Windows authentication â For Host , enter the hostname for your Qlik Sense Enterprise on Windows instance. For Port , enter the port number for your Qlik Sense Enterprise on Windows instance. For Authentication , click Windows Auth . For Username , enter the username for Qlik Sense Enterprise on Windows in the domain\\username format. For Password , enter the password for Qlik Sense Enterprise on Windows. For Virtual proxy prefix , enter the prefix for your virtual proxy . Click the Test Authentication button to confirm connectivity to Qlik Sense Enterprise on Windows. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Qlik Sense Enterprise on Windows connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Qlik Sense Enterprise on Windows crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Streams . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Streams . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Qlik Sense Enterprise on Windows crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Qlik Sense Enterprise on Windows Next What does Atlan crawl from Qlik Sense Enterprise on Windows? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/how-tos/crawl-redash",
    "text": "Connect data BI Tools On-premises & Enterprise BI Redash Crawl Redash Assets Crawl Redash On this page Crawl Redash Once you have configured the Redash permissions , you can establish a connection between Atlan and Redash. To crawl metadata from Redash, review the order of operations and then complete the following steps. Select the source â To select Redash as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Redash Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Redash credentials: For Host Name , enter the URL for your Redash instance. For Authentication , API Key is the default selection. For API Key , enter the API key you copied from your Redash instance. Click the Test Authentication button to confirm connectivity to Redash. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Redash connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Redash crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the queries with tags you want to include in crawling, click Include queries with tags . (This will default to all assets, if none are specified.) To select the queries with tags you want to exclude from crawling, click Exclude queries with tags . (This will default to no assets, if none are specified.) To select the dashboards with tags you want to include in crawling, click Include dashboards with tags . (This will default to all assets, if none are specified.) To select the dashboards with tags you want to exclude from crawling, click Exclude dashboards with tags . (This will default to no assets, if none are specified.) For Advanced Config , you can either keep Default to allow default options for asset inclusion or click Custom to further configure the crawler: For Include unpublished queries , click Yes to enable crawling unpublished queries or No to skip crawling them. For Include queries without tags , click Yes to enable crawling queries without tags or No to skip crawling them. For Include dashboards without tags , click Yes to enable crawling dashboards without tags or No to skip crawling them. For Alternate Host URL , enter the protocol and host name to be used for viewing your assets directly in Redash from Atlan. If added, the View in Redash option will redirect to the alternate host URL instead of the host URL used to run the crawler . Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Redash crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Redash Next What does Atlan crawl from Redash? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka",
    "text": "Connect data Event/Messaging Redpanda Kafka Crawl Redpanda Kafka Assets Crawl Redpanda Kafka On this page Crawl Redpanda Kafka Once you have configured the Redpanda Kafka permissions , you can establish a connection between Atlan and Redpanda Kafka. Did you know? Atlan currently supports the offline extraction method for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. To crawl metadata from Redpanda Kafka after uploading the results to S3 , review the order of operations and then complete the following steps. Select the source â To select Redpanda Kafka as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Redpanda Kafka Assets . In the right panel, click Setup Workflow . Provide credentials â In offline extraction, you need to first extract metadata yourself and make it available in S3 . To enter your S3 details: For Extraction method , Offline is the default selection. For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the Redpanda Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Redpanda Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, clickÂ Exclude topics regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Redpanda Kafka crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run Â button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run Â button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl Previous Set up Redpanda Kafka Next Set up on-premises Kafka access Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/crawl-salesforce",
    "text": "Connect data CRM Salesforce Crawl Salesforce Assets Crawl Salesforce On this page Crawl Salesforce Once you have configured the Salesforce user permissions , you can establish a connection between Atlan and Salesforce. To crawl metadata from Salesforce, review the order of operations and then complete the following steps. Select source â To select Salesforce as your source: In the top right of any screen in Atlan, navigate to New and click New Workflow . From the list of packages, click Salesforce Assets . In the right panel, click Setup Workflow . Provide credentials â Enter credentials for your Salesforce integration in Atlan. The required fields vary depending on the authentication flow you choose. JWT bearer flow Client credentials flow Username-password flow To enter your Salesforce credentials: For Host Name , enter the full URL for your Salesforce instance, including the https:// . For example, https://MyDomainName.my.salesforce.com . For Authentication , select OAuth 2.0 JWT Bearer . For Username , enter the integration user's email address . For Is this a Sandbox Org account? , change to Yes if your org is a copy of your production org . For Consumer Key , enter the consumer key for the connected app . For Encrypted Private Key , enter the private key from the server.key file in RSA256 format: -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG....... -----END PRIVATE KEY----- Click Test Authentication at the bottom of the form to confirm connectivity. When successful, click Next at the bottom of the screen. To enter your Salesforce credentials: For Host Name , enter the full URL for your Salesforce instance, including the https:// . For example, https://MyDomainName.my.salesforce.com . For Authentication , select OAuth 2.0 Client Credentials . For Consumer Key (Client ID) , enter the Consumer Key from the external client app . For Consumer Secret (Client Secret) , enter the Consumer Secret from the external client app . Click Test Authentication at the bottom of the form to confirm connectivity. When successful, click Next at the bottom of the screen. To enter your Salesforce credentials: For Host Name , enter the full URL for your Salesforce instance, including the https:// . For example, https://MyDomainName.my.salesforce.com . For Authentication , keep the default option Resource Owner . For Username , enter the integration user's email address. For Password , enter the concatenation of the user's password and the personal security token . Entering either password or personal security token alone is insufficient. For example, password xyz + token 123 â enter xyz123 . For Is this a Sandbox Org account? , change to Yes if your org is a copy of your production org . For Consumer Key , enter the consumer key for the connected app . For Consumer Secret , enter the consumer secret for the connected app . Click Test Authentication at the bottom of the form to confirm connectivity. When successful, click Next at the bottom of the screen. Configure connection â To complete the Salesforce connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, nobody can manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure crawler â Before running the Salesforce crawler, you can further configure it. On the Metadata page: For Extract reports , click Yes if you'd like to extract report metadata or click No . For Extract dashboards ,Â click Yes if you'd like to extract dashboard metadata or click No . Run crawler â To run the Salesforce crawler, after completing the previous steps: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you can see the assets in Atlan's asset page! ð Tags: connectors data crawl salesforce setup Previous Set up username-password flow Next What does Atlan crawl from Salesforce? Select source Provide credentials Configure connection Configure crawler Run crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/how-tos/crawl-sap-hana",
    "text": "Connect data Databases SQL Databases SAP HANA Crawl SAP HANA Assets Crawl SAP HANA On this page Crawl SAP HANA Once you have configured the SAP HANA permissions , you can establish a connection between Atlan and SAP HANA. To crawl metadata from SAP HANA, review the order of operations and then complete the following steps. Select the source â To select SAP HANA as your source: In the top right of any screen in Atlan, navigate to New and then click New workflow . From the Marketplace page, click SAP HANA Assets .Â In the right panel, click Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . Direct extraction method â To enter your SAP HANA credentials: For Host , enter the host name for your SAP HANA instance. For Port , enter the port number for your SAP HANA instance. For Username , enter the username you created for the instance. For Password , enter the password for the username. Click the Test Authentication button to confirm connectivity to SAP HANA. Once authentication is successful, navigate to the bottom of the screen and then click Next . Offline extraction method â Atlan supports the offline extraction method for fetching metadata from SAP HANA. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. ForÂ Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection â To complete the SAP HANA connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the SAP HANA crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the SAP HANA assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the SAP HANA assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the SAP HANA crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run Â button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run Â button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up SAP HANA Next What does Atlan crawl from SAP HANA? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/how-tos/crawl-sigma",
    "text": "Connect data BI Tools Cloud-based BI Sigma Crawl Sigma Assets Crawl Sigma On this page Crawl Sigma Once you have configured the Sigma permissions , you can establish a connection between Atlan and Sigma. To crawl metadata from Sigma, review the order of operations and then complete the following steps. Select the source â To select Sigma as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Sigma Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Sigma credentials: For Endpoint , from the dropdown, select your organization's cloud . For Authentication , API Token is the default method. For Client ID , enter the client ID associated with your API token . For API Token , enter the API token that you created. Click the Test Authentication button to confirm connectivity to Sigma. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Sigma connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Sigma crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: To select the Sigma workbooks you want to include in crawling, click Include Workbooks . (This will default to all workbooks, if none are specified.) To select the Sigma workbooks you want to exclude from crawling, click Exclude Workbooks . (This will default to no workbooks if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Sigma crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Sigma Next What does Atlan crawl from Sigma? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/how-tos/crawl-sisense",
    "text": "Connect data BI Tools Cloud-based BI Sisense Crawl Sisense Assets Crawl Sisense On this page Crawl Sisense Once you have configured the Sisense permissions , you can establish a connection between Atlan and Sisense. To crawl metadata from Sisense, review the order of operations and then complete the following steps. Select the source â To select Sisense as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Sisense Assets . In the right panel, click Setup Workflow . Provide credentials â To enter your Sisense credentials: For Host , enter the hostname of your Sisense instance. For Authentication , API Token is the default selection. For Username , enter the email address you created for the new user. For Password , enter the password you created for the username. Click the Test Authentication button to confirm connectivity to Sisense. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Sisense connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Sisense crawler, you can further configure it. Sisense allows you to organize your Sisense assets into folders and subfolders. Atlan currently supports filtering your dashboards and widgets at a folder level. However, if you select a parent folder to include or exclude, all the child folders will also be included or excluded, respectively. Folder-level filtering currently does not apply to data-models and data-model-tables , all such assets will be crawled into Atlan. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Folders . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Folders . (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Sisense crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Sisense Next What does Atlan crawl from Sisense? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/how-tos/crawl-soda",
    "text": "Connect data Data Quality & Observability Soda Crawl Soda Assets Crawl Soda On this page Crawl Soda Once you have configured the Soda permissions , you can establish a connection between Atlan and Soda. To crawl metadata from Soda, review the order of operations and then complete the following steps. Select the source â To select Soda as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Soda . In the right panel, click Setup Workflow . Provide your credentials â To enter your Soda credentials: For Host Name , enter the hostname or base URL of your Soda instance to connect to the Soda APIs   -  for example, cloud.soda.io , cloud.us.soda.io , or demo.soda.io . For Authentication , API Key Authentication is the default selection. For API Key ID , enter the API key ID you copied . For API Secret , enter the API secret you copied . Click the Test Authentication button to confirm connectivity to Soda. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Soda connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Soda crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click the Include filter . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click the Exclude filter . (This will default to no assets, if none specified.) (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the enrichment: To map Soda metadata enrichment to assets from specific connections only, for Include Connections , specify the connections in Atlan, or leave it blank to include all connections. If you have specified any connections, Atlan will map Soda checks only to the assets included in those connections. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Soda crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up Soda Next What does Atlan crawl from Soda? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/crawl-teradata",
    "text": "Connect data Databases SQL Databases Teradata Crawl Teradata Assets Crawl Teradata On this page Crawl Teradata Once you have configured the Teradata user permissions , you can establish a connection between Atlan and Teradata. To crawl metadata from Teradata, review the order of operations and then complete the following steps. Select the source â To select Teradata as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Teradata Assets . In the right panel, click Setup Workflow . Provide credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your Teradata credentials: ForÂ Host , enter hostname of your Teradata instance. ForÂ Port , enter the port number of your Teradata instance. For Authentication , Basic is the default method. ForÂ Username , enter the username created when setting up user permissions. ForÂ Password , enter the password created when setting up user permissions. Click the Test Authentication button to confirm connectivity to Teradata. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method â Atlan also supports the offline extraction method for fetching metadata from Teradata. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include database.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. Once completed, navigate to the bottom of the screen and click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from Teradata. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Teradata data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â Complete the Teradata connection configuration: Provide aÂ Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the Teradata crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets, if none are specified.) To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in theÂ Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Teradata crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's assets page! ð Tags: connectors data crawl setup Previous Set up Teradata Next Mine Teradata Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot Crawl ThoughtSpot Assets Crawl ThoughtSpot On this page Crawl ThoughtSpot Once you have configured the ThoughtSpot permissions , you can establish a connection between Atlan and ThoughtSpot. To crawl metadata from ThoughtSpot, review the order of operations and then complete the following steps. Select the source â To select ThoughtSpot as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click ThoughtSpot Assets . In the right panel, click Setup Workflow . Provide your credentials â Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlanâs secure agent executes metadata extraction within the organization's environment. Direct extraction method â To enter your ThoughtSpot credentials: For Extraction method , Direct is the default selection. For Hostname , enter the hostname of your ThoughtSpot cloud instance in the following format   - <company_name>.thoughtspot.cloud . For Authentication , select the method you configured when setting up the ThoughtSpot access permissions : For Basic Authentication , enter the username and password you created . For Trusted Authentication , enter the username of your ThoughtSpot instance and the secret key you created . Click the Test Authentication button to confirm connectivity to ThoughtSpot. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method â Atlan supports the offline extraction method for fetching metadata from ThoughtSpot. This method uses Atlan's thoughtspot-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output/thoughtspot-example/filter/answers/result-0.json , output/thoughtspot-example/filter/liveboards/result-0.json , output/thoughtspot-example/filter/answer-sql-queries/result-0.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method â Atlan supports using a Secure Agent for fetching metadata from ThoughtSpot. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the ThoughtSpot data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection â To complete the ThoughtSpot connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the ThoughtSpot crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include tags . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude tags . (This will default to no assets, if none specified.) For Assets without tags , keep the default option Yes to skip crawling assets without tags Â or click No to enable crawling them. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the ThoughtSpot crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks -  currently only supported for offline extraction method . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: connectors data crawl setup Previous Set up on-premises ThoughtSpot access Next Crawl on-premises ThoughtSpot Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/crawl-trino",
    "text": "Connect data Databases Query Engines Trino Crawl Trino Assets Crawl Trino On this page Crawl Trino Once you have configured the Trino user permissions , you can establish a connection between Atlan and Trino. (If you are also using a private network for Trino, you will need to set that up first , too.) To crawl metadata from Trino, review the order of operations and then complete the following steps. Select the source â To select Trino as your source: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the list of packages, selectÂ Trino Assets and click onÂ Setup Workflow . Provide credentials â To enter your Trino credentials: ForÂ Host , enter the hostname (or PrivateLink endpoint ) for your Trino instance. ForÂ Port , enter the port of your Trino instance. ForÂ Username , enter the username you created . ForÂ Password , enter the password for the user you created . For Enable TLS/HTTPS , change to True to only allow TLS or HTTPS connections or keep the default False . For Disable SSL verification , change to True to disable SSL verification for self-signed certificates or keep the default False. Click Test Authentication to confirm connectivity to Trino using these details. Once successful, at the bottom of the screen click Next . Configure the connection â To complete the Trino connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler â Before running the Trino crawler, you can further configure it. You can override the defaults for any of these options: To select the Trino assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the Trino assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False Â to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Trino crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! ð Tags: connectors data crawl Previous Set up Trino Next Set up a private network link to Trino Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/product/administration/readme-templates/how-tos/create-readme-templates",
    "text": "Configure Atlan Administration Templates Create README templates On this page Create README templates Who can do this? You will need to be an admin user in Atlan to create and manage README templates. Admin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and enrich their assets with READMEs . They will also be able to see a rich preview of each template before adding the relevant documentation. Create a README template â To create a README template: In the left menu in Atlan, click Governance . Under the Governance heading, click Readme templates . On the Readme templates page, click Get started . In the Untitled Template dialog box, enter the following details: For template name, enter a name for your template. (Optional) For Add description , add a description for your template. (Optional) To add an icon, click on the image icon. Click Create to proceed.Â In the text editor, add your template. Click Save . (Optional) To edit the template, in the upper right of the screen, click the pencil icon.Â Your README template is now available in the templates manager! ð Tags: integration connectors Previous How to view query logs Create a README template"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/custom-solutions",
    "text": "Get Started Quick Start Guides Developers Custom solutions Custom solutions Atlan provides custom packages tailored to your unique use cases. These publicly-available extensions broaden the scope of our out-of-the-box connectivity options . Reach out to your customer success manager to install any of the selected packages in your Atlan tenant: Asset change notification -  sends email notifications when assets are created, updated, and/or deleted. Asset export (advanced) -  exports a set of assets to different locations based on the filtering criteria you have applied. Asset export (basic) -  identifies and extracts assets that have been enriched in Atlan. You can modify the resulting CSV file by updating the metadata for your assets, and then load it back into Atlan using the asset import package. Asset import -  loads metadata from a CSV file that matches the format of one extracted using either of the asset export packages ( basic or advanced ). Lineage builder -  creates lineage between any source and target asset. Lineage generator (no transformations) -  automatically detects assets with the same (or similar) name between two connections and creates lineage between them. Relational assets builder -  creates (and updates) net-new relational assets: connections, databases, schemas, tables, views, materialized views, and columns. Tags: integration connectors Previous API authentication Next Software development kits (SDKs)"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/faq/faq-dagster",
    "text": "Connect data Orchestration & Workflow Dagster FAQ Dagster integration On this page Dagster integration Private Preview Frequently asked questions about Dagster integration with Atlan. Why is my Dagster asset linked to multiple SQL tables across different databases/schemas? â This happens when the dagster/table_name metadata field doesn't include the fully qualified table name. Atlan may link to multiple tables with the same name across different databases or schemas. Use the fully qualified table name in the dagster/table_name field with the format database.schema.table . For more details, see Asset metadata and tags | Dagster Docs . Tags: connectors lineage faq Previous What does Atlan crawl from Dagster"
  },
  {
    "url": "https://docs.atlan.com/faq/data-connections-and-integration",
    "text": "Configure Atlan Frequently Asked Questions Data Connections and Integration On this page Data Connections and Integration Complete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues. How can I set up on-prem databases if I don't use Docker Compose? â To use Atlan's metadata-extractor tool to extract metadata from on-premises databases with Kubernetes deployment architecture, refer to the How to connect on-premises databases to Kubernetes documentation. Does Atlan integrate with MongoDB? â Atlan currently supports native integration with MongoDB and Microsoft Azure Cosmos DB for MongoDB deployments. Can Atlan work with the IBM Informix database? â Atlan doesn't currently offer native support for IBM Informix database. Atlan is built on an Open API architecture, so you can catalog your IBM Informix assets programmatically. Refer to the developer documentation to publish database, schema, table, and column objects. Is Atlan compatible with data quality tools? â Yes, Atlan integrates with several data quality and observability tools: Native integrations with tools like Great Expectations, Monte Carlo, and other data quality platforms You can view data quality metrics and alerts directly within asset profiles Custom integrations can be built using Atlan APIs to connect additional data quality tools For the most current list of supported integrations, check the connections documentation . Does Atlan integrate with Talend or Matillion? â Atlan currently only supports native integration with Matillion . If Talend uses the ELT (Extract, Load, Transform) paradigm, this means that the SQL transformation queries are pushed down into the warehouse. In that case, Atlan automatically generates the lineage for these transformations by parsing the SQL statements extracted from the query history of the data warehouse. Lineage for the extract and load steps isn't automated for Talend but is supported for Fivetran . However, if the transformation is taking place outside the warehouse, Atlan needs access to the transformation queries to generate the lineage. This access can either be gained via a Git repository if utilized or by sharing the queries or stored procedures in a shared location, such as an S3 bucket. Why does Atlan require the site administrator explorer role in Tableau? â To learn why Atlan requires the Site Administrator Explorer role in Tableau, refer to this guide . Why is the view button for Tableau unavailable? â The view button for Tableau assets may be unavailable due to: Insufficient permissions to access the Tableau content The Tableau server being inaccessible from your current network Authentication issues between Atlan and Tableau The specific asset being moved or deleted in Tableau Check your Tableau permissions and network connectivity, or contact your Tableau administrator. Who is a source owner in Microsoft Power BI? â Microsoft Power BI provides metadata for who generated or configured reports, datasets, and dataflows , which is then mapped to the source owner field. This has no connection to Microsoft Power BI workspace admins. You can refer to the API response schema to learn more: createdBy in WorkspaceInfoReport configuredBy in WorkspaceInfoDataset configuredBy in WorkspaceInfoDataflow The API doesn't return source owner metadata for other asset types due to limitations at source, see Microsoft Power BI documentation . The value of the source owner is a string. While for sources like Looker, Atlan displays the username based on the metadata received, Microsoft Power BI APIs only return the user's email address, which is what Atlan displays for supported assets. In certain cases, assets that are connected to the same Microsoft Power BI workspace and have the same source owner may display a different source owner or none at all. This is because Atlan maps it to the metadata returned by the APIs. For example, if the response is null, Atlan won't display a source owner. How does Atlan work with dbt single tenant vs multi-tenant? â There are multiple deployment options available for dbt Cloud (see dbt documentation here and here ). When integrating with dbt Cloud , Atlan uses the following APIs to fetch metadata from all deployment options: dbt Cloud Administrative API - to fetch account, project, environment, job, and run metadata. dbt Cloud Discovery API - to fetch models, sources, and tests from each dbt environment. Are there any dbt assets that can't be viewed in dbt? â Atlan displays the View in dbt link for newly created or synced dbt assets only â including models, sources, and tests. Atlan doesn't display this link for assets without target URLs. How can I reuse my documentation from dbt? â Atlan's dbt connectivity provides two ways to ingest documentation from dbt: Automatically load details like descriptions , from your existing dbt project details. Any other (Atlan-specific) details you want to document, through dbt's meta field. What happens to asset metadata in Atlan if I switch to a new server? â If the assets in your new server retain the same qualifiedName , then these are recreated in Atlan with all the metadata attached. The qualifiedName determines asset uniqueness in Atlan (and influences the GUID of the asset). Following this logic, any asset recreated with the same name remains the same asset in Atlan - considering that the qualifiedName has remained the same. By contrast, if an asset is recreated with a new name, it becomes a new asset in Atlan - considering that the qualifiedName has changed. This means that the attached metadata is no longer available. If an asset is removed from the data source or no longer available, it'll be archived in Atlan as part of the crawler's cleanup policy. Why is the metadata getting lost when migrating from Snowflake to dbt? â Metadata loss during Snowflake to dbt migration typically occurs because: The qualifiedName changes between the source Snowflake table and the dbt model Asset lineage connections aren't properly maintained during migration Custom metadata and tags aren't transferred in the migration process To preserve metadata: Make sure to use consistent naming conventions between Snowflake and dbt Use dbt's meta fields to preserve custom metadata Re-run Atlan crawlers after migration to rebuild lineage connections What happens when an asset is removed from Redshift? â If an asset such as a table or a schema is removed from Redshift or any other source, it'll also be automatically removed from Atlan during the next workflow run. Such assets are archived in Atlan (soft-deleted), so that they don't appear in search results by default. You can only access archived assets through discovery. What data is Atlan actually bringing in? â Atlan extracts metadata only, not the actual data: Schema information: Table structures, column names, data types Usage statistics: Query patterns, user access logs (when available) Lineage data: Data flow and transformation logic Custom metadata: Tags, descriptions, business glossary terms Data profiles: example data previews (configurable, with privacy controls) Atlan never stores your actual business data - it only catalogues metadata to help you understand and govern your data landscape. Can offline extraction fail if there are spaces in the path? â Atlan currently doesn't support spaces in folder names for S3. The offline extraction workflow fails if you include any spaces in the folder name in S3. To follow documented guidelines for safe characters, refer to Amazon S3 documentation . What does API only mean? â In the Where in Atlan column of What does Atlan crawl from (connector name)? documentation , API only indicates source properties that have been crawled in Atlan but aren't published or discoverable on the product UI. Can you integrate with Jupyter notebook? â Yes, Atlan supports integration with Jupyter notebooks through: Python SDK for programmatic access to Atlan APIs Jupyter extensions for data discovery and cataloguing Ability to document and version notebook-based analyses Integration with notebook-based data science workflows Check the developer documentation for specific integration examples and code samples. Can I integrate Atlan with any web application? â Yes, Atlan provides comprehensive APIs and SDKs that enable integration with virtually any web application: REST APIs for all Atlan functionality Python, Java, and JavaScript SDKs Webhook support for real-time notifications OAuth2 and API key authentication methods Visit the developer portal for integration guides, API documentation, and code examples. What does the Snowflake workflow follow when gathering information? â The Snowflake workflow or any other connector workflow has built-in differential crawling capabilities, which means it will crawl updates that have been made across assets in the Snowflake system and sync them to Atlan. This way, any incremental metadata changes happening in Snowflake are made available in Atlan after each workflow run. What's a Snowflake process? â Snowflake process (in Atlan) = Snowflake transformation Data processing in a warehouse involves a combination of these three processes: Extract (from source). Load (to warehouse). Transformation (of the data between source format and a consolidated format in the warehouse). Whether run through a Python script, directly from Snowflake's UI, or via third-party programs like Matillion, these transformations occur within Snowflake through SQL. Regardless of what ran this transformation SQL, a Snowflake process in Atlan captures the transformation logic for use in lineage. Tags: connectors data integration faq-connections Previous AI and Automation Features Next Security and Compliance"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/connections/how-tos/delete-a-connection",
    "text": "Configure Atlan Integrations Automation Connections Delete a connection On this page Delete a connection To delete a connection and its assets, complete th e following steps. Select the utility â To select the connection delete utility: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Utility . From the list of packages, select Connection Delete and click on Setup Workflow . Select the connection â To select the connection to delete: Under Connection select the existing connection you want to delete. For Delete Strategy choose how you want to delete that connection and its assets: To soft-delete the connection and its assets, choose Archive . To fully purge the connection and its assets, choose Delete . Run the utility â To delete the connection and its assets, after completing the steps above: At the bottom of the page, click Run . Once the utility has completed running, you will no longer see the assets in Atlan's asset page! ð Tags: integration connectors Previous Connections Integration Next Webhooks Integration Select the utility Select the connection Run the utility"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/faq/admin-user-salesforce",
    "text": "Connect data CRM Salesforce FAQ Does Atlan require an admin user in Salesforce? Does Atlan require an admin user in Salesforce? No. However, it is recommended that a Salesforce administrator establishes a connection between Atlan and Salesforce . To learn more, see here . Tags: connectors salesforce Previous Troubleshooting Salesforce connectivity Next Why does the description from Salesforce not show up in Atlan?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/tableau-calculated-fields-lineage",
    "text": "Use data Lineage FAQ Does lineage only cover calculated fields for Tableau dashboards? Does lineage only cover calculated fields for Tableau dashboards? Atlan displays upstream as well as downstream lineage for Tableau dashboards . The lineage flow can be as follows: Upstream sources (for example, Snowflake tables) â Tableau data sources â workbooks â worksheets â dashboards. Tags: connectors data crawl faq faq-lineage Previous Does Atlan support field-level lineage for BI tools? Next How do you enable data lineage for different data sources?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/download-impacted-assets-in-google-sheets",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Download impacted assets in Google Sheets On this page Download impacted assets in Google Sheets Once you've connected Atlan with Google Sheets , you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for impact analysis . danger You need to be logged into your Atlan instance before you can download impacted assets from Atlan in Google Sheets. Download impacted assets in Google Sheets â To import lineage in Google Sheets: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan . From the list of options in the Atlan add-on, clickÂ Import Lineage to open a list of your assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, select the asset type. In the Atlan sidebar on your spreadsheet, select the asset(s) you want to import. To select the type of impacted assets you'd like to download, in the Atlan sidebar, from the Direction dropdown: To download downstream assets for impact analysis , click Downstream . To download upstream assets for root cause analysis , click Upstream . To download all impacted assets, click Both . To download the impacted assets in Google Sheets: Click Import Lineage to download all the impacted assets in one sheet.Â Click the vertical three dots and then click Import to new sheet to download the assets in separate sheet tabs. (Optional) Once download is successful, click the asset links in your spreadsheet to view the assets in Atlan. The impacted assets are now available in Google Sheets! ð Update column metadata for impacted assets â Once you've imported your impacted assets from Atlan, you can edit the column metadata for impacted assets in Google Sheets. You can make changes to the column metadata once all the columns have been imported successfully. You can only make changes to the metadata in the following columns: Description Owner Users Owner Groups Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the following columns: Source Asset Source Asset Connector Source Asset Type Impacted Asset Impacted Asset Connector Impacted Asset Type Direction Terms Propagated Tags Source URL Qualified Name Source Asset GUID Impacted Asset GUID Immediate Upstream and Immediate Downstream Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Did you know? When adding tags or owners for impacted assets in Google Sheets, the tag or user name for the owner user or group must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags or owners as comma-separated values. Push changes to Atlan â Once you've made changes to the column metadata, to push your changes: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Push to Atlan . A dialog box will appear once the changes have synced. danger If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Google Sheets. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Tags: lineage data-lineage impact-analysis integration connectors downstream-impact dependencies upstream-dependencies data-sources Previous Configure custom domains for Microsoft Excel Next Download impacted assets in Microsoft Excel Download impacted assets in Google Sheets Update column metadata for impacted assets Push changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift Get Started How to enable SSO for Amazon Redshift On this page Enable  SSO for Amazon Redshift Atlan supports SSO authentication for Amazon Redshift connections with Okta as the identity provider. Once you've configured SSO authentication for Amazon Redshift, your users can: Query data with Okta SSO credentials View sample data with Okta SSO credentials Did you know? If you have already configured Okta and AWS, skip to configure SSO authentication in Atlan . Otherwise, complete all the steps below. Create a client application in Okta â Who can do this? You will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your AWS administrator. You will need to create a client application in Okta to use for configuring the identity provider in AWS . To create a client application, within Okta: Log in to your Okta Admin Console . From the left menu of the Admin Console , click Applications . Under Applications , click the Browse App Catalog button. On the Browse App Integration Catalog page, search for and select Amazon Web Services Redshift . From the Amazon Web Services Redshift page, click the Add integration button to create an integration. For Add Amazon Web Services Redshift , enter the following details: For Application label , enter a meaningful name for your new app integration   -  for example, Atlan_SSO . Click Done to proceed. On your new app page, click the Assignments tab and then click the Assign button: Click Assign to People to select individual users to assign to the application. Click Assign to Groups to select groups to assign to the application. On your new app page, click the Sign On tab and then navigate to the SAML Signing Certificates section: Under Actions , click Actions to expand the menu, and then from the dropdown, click View IdP metadata . This will open an XML file in a new tab. Save or download this file to use for configuring the identity provider in AWS . For User Authentication , click the Edit button: From the Authentication policy dropdown, click Okta Dashboard . Click Save to save your changes. You will need the IdP metadata XML file to configure Okta as the identity provider in AWS. Configure identity provider in AWS â Who can do this? You will need your AWS administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Okta administrator. You will need to establish a trust relationship between Okta as the identity provider and AWS. You will also need to create a role that Okta can use to access Amazon Redshift and assign required permissions to that role. Create an identity provider â To create an identity provider , within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console, click Identity providers and then click the Add provider button. In the Add an Identity provider dialog, enter the following details: For Provider type , select SAML . For Provider name , enter a name for the identity provider   -  for example, Okta_AtlanSSO . Under Metadata document , click Choose file and upload the IdP metadata XML file you downloaded from Okta . At the bottom of the dialog, click Add provider to add Okta as the identity provider in AWS. Once you have configured Okta as the identity provider in AWS, you will need to create a role for Okta to access Amazon Redshift. Create a role â To create a role , within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console, click Roles , and then from the top right, click the Create role button. On the Create role page, enter the following details: For Select trusted entity , under Trusted entity type , click SAML 2.0 federation . Under SAML 2.0 federation , enter the following details:Â For SAML 2.0-based provider , select the identity provider you created in AWS -  for example, Okta_AtlanSSO . Click Allow programmatic access only . For the Attribute dropdown, select SAML:aud . For Value , enter https://signin.aws.amazon.com/saml . Click Next to continue. For Add permissions , click Next to proceed to the next step. For Name, review, and create , under Role details , enter the following details: For Role name , enter a name for the role   -  for example, Okta_AtlanSSO_role . (Optional) For Description , enter a description for the new role. Click Create role to finish role setup. This will create a new role for Okta to access Amazon Redshift. Once you have created a role for Okta to access Amazon Redshift, you will need to assign permissions to that role. Create a policy â You will need to create an access policy and assign the following required permissions to the newly created role: CreateClusterUser JoinGroup GetClusterCredentials To create a policy, within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console, click Roles Â and then search for and select the role you created in the previous step   -  for example, Okta_AtlanSSO_role . On the newly created role page, to the right of Permission policies , click Add permissions , and then from the dropdown, click Create inline policy . On the Create policy page, you will need to assign the following permissions for Redshift   - GetClusterCredentials , JoinGroup , and CreateClusterUser . Repeat the steps below to assign each permission: For Specify permissions , under Select a service , search for and select Redshift . Under Redshift , enter the following details: For Allowed actions , search for and select a permission   -  for example, GetClusterCredentials . For Resources , click All . Click Next to proceed. For Review and create , under Policy name , enter a name for the newly created policy   -  for example, Okta_AtlanSSO_rolepolicy . Retrieve identity provider and role ARN â Once you have configured Okta as the identity provider and created a role in AWS, you will need the identity provider ARN and role ARN for further configuration in Okta. To retrieve the identity provider and role ARN, within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console: Click Identity providers and then select the identity provider you created : On the identity provider page, under ARN , click the clipboard icon to copy the identity provider ARN value and store it in a secure location. Click Roles and then select the role you created : On the role page, under ARN , click the clipboard icon to copy the role ARN value and store it in a secure location. Configure the client application in Okta â Who can do this? You will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your AWS administrator. You will need the identity provider ARN and role ARN from AWS for further configuration in Okta. To further configure the client application in Okta: Log in to your Okta Admin Console . From the left menu of the Admin Console , click Applications . Under Applications , select the client application you created in Okta. On your new app page, click the Sign On tab. On the Sign On page, next to Settings , click Edit . Navigate to the Advanced Sign-on Settings section and enter the following details: For IdP ARN and Role ARN , enter the identity provider ARN and role ARN as comma-separated values   -  for example, arn:aws:iam::403973984390:role/oktaAtlan_SSO , arn:aws:iam::403976283490:saml-provider/oktaAtlan_SSO_role . For Allowed DB Groups (Redshift) , enter the names of the Okta groups that should be provided access to Amazon Redshift. Click Save to confirm. On your new app page, click the General tab and navigate to the App Embed Link section. Under Embed Link , copy the link   -  for example, https://**<example>.okta.com**/home/amazon_aws_redshift/**0oa78lx856GcTMDsa697/aln1dkqcfra0piaWa0g** -  and store the IdP host name and app ID in a secure location to use for configuring SSO authentication in Atlan . For example: IdP host name: <example>.okta.com App ID: 0oa78lx856GcTMDsa697/aln1dkqcfra0piaWa0g Configure SSO authentication in Atlan â Who can do this? You will need to be a connection admin in Atlan to complete these steps. You will also need inputs and approval from your Okta and AWS administrators. Once you have configured Okta and AWS, you can enable SSO authentication for your Amazon Redshift users to query data and view sample data Â in Atlan. To configure Okta SSO on a Amazon Redshift connection, from Atlan: From the left menu of any screen, click Assets . From the Assets page, click the Connector filter, and from the dropdown, select Redshift . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select an Amazon Redshift connection to enable SSO authentication. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , click Okta authentication to enforce SSO credentials for querying data : For SSO authentication , enter the following details: For IDP host , enter the IdP host name you copied from Okta . For App ID , enter the app ID you copied from Okta . For AWS Role ARN , enter the role ARN retrieved from AWS . UnderÂ Display sample data , for Source preview , click Okta authentication to enforce SSO credentials for viewing sample data : If SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data. If a different authentication method is enabled for querying data, enter the IdP host name and app ID you copied from Okta and role ARN retrieved from AWS . (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Your users will now be able to run queries and view sample data using their Okta SSO credentials! ð Tags: connectors data integration authentication Previous Set up Amazon Redshift Next Set up a private network link to Amazon Redshift Create a client application in Okta Configure identity provider in AWS Configure the client application in Okta Configure SSO authentication in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/enable-sso-for-google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery Get Started How to enable SSO for Google BigQuery On this page Enable  SSO for Google BigQuery Atlan supports SSO authentication for Google BigQuery connections. Once you've configured SSO authentication for Google BigQuery, your users can: Query data with SSO credentials View sample data with SSO credentials Did you know? When using OAuth 2.0 for authorization, Google displays a consent screen to the user that includes a summary of your project, policies, and scopes. If you have not configured the consent screen, complete the steps in configure OAuth consent screen . Otherwise, skip to create access credentials . (Optional) Configure OAuth consent screen in Google BigQuery â Who can do this? You will need your Google BigQuery administrator to complete these steps   -  you may not have access yourself. To configure the OAuth consent screen , from Google BigQuery: Open the Google Cloud console . In the left menu of the Google Cloud console, under APIs & Services , click OAuth consent screen . On the OAuth consent screen page, under User Type , select a preferred user type and then click Create . In the corresponding Edit app registration page, enter the following details: For App name , enter a meaningful name   -  for example, Atlan_SSO . For User support email , enter a support email for your users to troubleshoot. For Developer contact information , enter an email address where Google can notify you about any changes to your project. Click Save and continue to proceed to the next step. On the Scopes page, complete the following steps: Click Add or remove scopes to add a new scope. In the Update selected scopes dialog, click BigQuery API to add the /auth/bigquery scope and then click Update . Click Save and continue to finish setup. Once the OAuth consent screen configuration is successful, click Go back to dashboard . Create access credentials in Google BigQuery â Who can do this? You will need your Google BigQuery administrator to complete these steps   -  you may not have access yourself. Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan. To create access credentials , from Google BigQuery: Open the Google Cloud console . In the left menu of the Google Cloud console, under APIs & Services , click Credentials . From the upper right of the Credentials page, click Create credentials , and from the dropdown, click OAuth client ID .Â In the OAuth client ID screen, enter the following details: For Application type , click Web application . For Name , enter a meaningful name   -  for example, Atlan_client . Under Authorized JavaScript origins , click Add URI Â and enter your Atlan instanceÂ   -  for example, https://<company-name>.atlan.com . Under Authorized redirect URIs , click Add URI Â and enter your Atlan endpoint URI   -  for example, https://<company-name>.atlan.com/api/service/oauth . Click Create to finish setup. From the corresponding OAuth client created dialog, copy the Client ID and Client secret and store it in a secure location. Configure SSO authentication in Atlan â Who can do this? You will need to be a connection admin in Atlan to complete these steps. You will also need inputs and approval from your Google BigQuery administrator. Once you have configured access credentials in Google BigQuery , you can enable SSO authentication for your users to query data and view sample data Â in Atlan. To configure SSO on a Google BigQuery connection, from Atlan: From the left menu of any screen, click Assets . From the Assets page, click the Connector filter, and from the dropdown, select BigQuery . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select a Google BigQuery connection to enable SSO authentication. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , clickÂ SSO authentication to enforce SSO credentials for querying data : For SSO authentication , enter the following details: For Client ID , enter the client ID you copied from Google BigQuery. For Client secret , enter the client secret Â you copied from Google BigQuery. UnderÂ Display sample data , for Source preview , clickÂ SSO authentication to enforce SSO credentials for viewing sample data : If SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data. If a different authentication method is enabled for querying data, enter the client ID andÂ client secret you copied from Google BigQuery. (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Your users will now be able to run queries and view sample data using their SSO credentials! ð Tags: connectors data integration authentication Previous Set up Google BigQuery Next Crawl Google BigQuery (Optional) Configure OAuth consent screen in Google BigQuery Create access credentials in Google BigQuery Configure SSO authentication in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/update-atlan-through-dbt",
    "text": "Connect data ETL Tools dbt Manage dbt in Atlan Enrich Atlan through dbt Enrich Atlan through dbt Beyond the default mapped dbt Cloud or dbt Core properties, you can update any of Atlan's metadata attributes (except for name , tenantId , and qualifiedName ) through your dbt model's meta property. For example, you can set: Announcements, atlan domains, certificates, custom metadata, descriptions, owners, atlan readme, tags, and terms on dbt assets and the assets that dbt materializes whenever applicable. For more details onÂ how to do these updates, including various examples, see the dbt tabs in the Common asset actions snippets of our developer documentation: Certify assets Manage announcements Change descriptions Change owners Tag assets Change custom metadata Link terms to assets Link Atlan domains to assets Link Readme to assets Tags: connectors data crawl enrichment Previous Manage dbt tags Next Migrate from dbt to Atlan action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools",
    "text": "On this page ETL tools connectors Atlan's ETL tools connectors enable you to integrate, catalog, and govern metadata from leading ETL and data transformation platforms. These connectors help you discover, document, and manage ETL assets for analytics, governance, and business operations. Get started â To connect your ETL tool to Atlan, follow the relevant guide below. The connectors listed are currently supported. For detailed setup, crawling, and troubleshooting information, use the sidebar navigation or the guides below. Set up guides â Connect to AWS Glue Configure the connection between Atlan and your AWS Glue environment. Connect to Alteryx Configure the connection between Atlan and your Alteryx environment. Connect to dbt Cloud Configure the connection between Atlan and your dbt Cloud project. Connect to dbt Core Configure the connection between Atlan and your dbt Core project. Connect to Fivetran Configure the connection between Atlan and your Fivetran environment. Connect to Matillion Configure the connection between Atlan and your Matillion environment. Connect to Microsoft Azure Data Factory Configure the connection between Atlan and your Microsoft Azure Data Factory instance. Tags: connectors etl integration Get started Set up guides"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/extract-on-premises-databricks-lineage",
    "text": "Connect data Data Warehouses Databricks Lineage and Usage How to extract on-premises Databricks lineage On this page extract on-premises Databricks lineage Once you have set up the databricks-extractor tool , you can extract lineage from your on-premises Databricks instances by completing the following steps. Run databricks-extractor â To extract lineage for a specific Databricks connection using the databricks-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files â The databricks-extractor tool will generate many folders with JSON files for each service . For example: extracted-lineage extracted-query-history Â (if EXTRACT_QUERY_HISTORY is set to true) You can inspect the lineage and usage metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 â To provide Atlan access to the extracted lineage and usage metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/databricks-lineage-example/extracted-lineage/result-0.json , output/databricks-lineage-example/extracted-query-history/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/databricks-lineage-example s3://my-bucket/metadata/databricks-lineage-example --recursive Extract lineage in Atlan â Once you have extracted lineage on-premises and uploaded the results to S3, you can extract lineage in Atlan: How to extract lineage and usage from Databricks Be sure to select Offline for the Extraction method . Tags: connectors data Previous How to extract lineage and usage from Databricks Next Manage Databricks tags Run databricks-extractor (Optional) Review generated files Upload generated files to S3 Extract lineage in Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/how-tos/find-assets-by-usage",
    "text": "Use data Usage & Popularity Get Started How to find assets by usage On this page Find assets by usage Data teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance. With Atlan's usage and popularity metadata, you'll be able to check off all these boxes! You can view usage metrics for your assets collected over the last 30 days. Atlan currently supports usage and popularity metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Microsoft Power BI -  reports and dashboards Snowflake -  tables, views, and columns Filter assets by usage â Use the usage filters to filter your assets by usage metadata. For instance, you'll be able to filter assets with zero queries and archive them or find costly assets to better optimize your operations. To filter assets by usage metadata: From the left menu in Atlan, click Assets . In the Filters menu in Assets , click Usage to expand the list of filters. From the Usage menu: For SQL assets, use the following filters: Click Number of queries to filter by the number of queries at source in the last 30 days. Click Number of users to filter by the number of users who queried an asset at source in the last 30 days. Click Last queried to filter by the last queried timestamp at source. Click Last row updated at to filter by the last row updated timestamp at source. To filter assets by compute cost , click Snowflake credits for Snowflake assets or click BigQuery query cost for Google BigQuery assets. For BI assets, use the following filters: Click Views count to filter by the number of views at source in the last 30 days. Click Number of users to filter by the number of users who viewed an asset at source in the last 30 days. Click Last viewed to filter by the last viewed timestamp at source. Your search results will now be filtered by usage metadata! ð Sort assets by popularity â Sort your data assets by popularity metadata to view the most or least used tables, views, or columns. For example, sorting your assets by popularity can help you deprecate unused or stale data assets, helping you reduce operational costs for your organization. To sort assets by popularity: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Your assets in the search results will now be sorted by popularity of usage! ð Did you know? You can also deep dive into usage metrics for Snowflake, Databricks, Google BigQuery, and Microsoft Power BI in Atlan. Tags: connectors data Previous Usage and Popularity Next How to interpret usage metrics Filter assets by usage Sort assets by popularity"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/identify-insights-query-db-log",
    "text": "Use data Insights FAQ How can I identify an Insights query in my database access log? How can I identify an Insights query in my database access log? Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs. Example query: SELECT * FROM \"WIDE_WORLD_IMPORTERS\" . \"FCT_SALES\" /* atlan(e7674c86-efbe-4cec-a3dd-f9b3e3a7f929) */ You can also search for Atlan queries in Snowflake or Databricks using the query text filter contains atlan . Tags: connectors data faq faq-insights Previous Can we restrict who can query our data warehouse? Next Monitor for runaway queries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/spark-lineage-handling",
    "text": "Use data Lineage FAQ How does Atlan handle lineage from Spark jobs? How does Atlan handle lineage from Spark jobs? Atlan currently supports native integration with Apache Spark . Atlan crawls and catalogs Spark jobs as native assets in Atlan and reports OpenLineage operational metadata for Spark jobs. Tags: lineage data-lineage impact-analysis integration connectors catalog metadata discovery faq-lineage Previous How does a Snowflake connection know that a table referenced in LookML is actually the same table? Next How is the Atlan lineage graph depicted using Power BI measures?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/how-tos/implement-openlineage-in-airflow-operators",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage Implement OpenLineage How to implement OpenLineage in Airflow operators On this page Implement OpenLineage in Airflow operators This document helps you learn how to implement OpenLineage support for any Airflow operator. To implement OpenLineage support, consider the following types of operators: Supported operators â If you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage. To install OpenLineage, refer to the documentation for supported sources: Apache Airflow Amazon MWAA Astronomer Google Cloud Composer For Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation . This documentation is automatically updated when OpenLineage support is added to any operator from a provider package. You have to make sure that you're using the latest version of the provider package. For more information, see the recommended provider package versions for OpenLineage . Custom and unsupported operators â If you're using a custom or an unsupported operator, your Airflow tasks will still emit OpenLineage events but may not include task-specific metadata such as inputs and outputs, SQL query, and more. This may limit Atlan from being able to generate data lineage. To implement OpenLineage support for custom and unsupported operators, refer to Implementing OpenLineage in Operators documentation . To help you understand the process, following is an example: Sample implementation â This approach is recommended when working with your own operators, where you can directly implement OpenLineage methods. You can also refer to OpenLineage documentation for more details. To implement OpenLineage support for a custom or an unsupported operator: Open the Operator class definition to which you want to add OpenLineage support. Implement at least one of the following OpenLineage methods in the Operator class: get_openlineage_facets_on_start() get_openlineage_facets_on_complete() The function should return datasets in the form of inputs and outputs with OpenLineage-compliant dataset names. This allows an OpenLineage consumer such as Atlan to properly match dataset metadata collected from different sources. To learn more about naming conventions, refer to OpenLineage documentation . Example â Below is an example of a properly implemented get_openlineage_facets_on_complete method for the GCSToGCSOperator . In this example, since there is some level of processing included in the execute method with no relevant failure data, implementing this single method was sufficient. def get_openlineage_facets_on_complete(self , task_instance) : \"\" \" Implementing _on_complete because execute method does preprocessing on internals. This means we won't have to normalize self.source_object and self.source_objects , destination bucket and so on. \"\" \" from airflow.providers.common.compat.openlineage.facet import Dataset from airflow.providers.openlineage.extractors import OperatorLineage return OperatorLineage( inputs= [ Dataset(namespace=f \"gs://{self.source_bucket}\" , name=source) for source in sorted(self.resolved_source_objects) ] , outputs= [ Dataset(namespace=f \"gs://{self.destination_bucket}\" , name=target) for target in sorted(self.resolved_target_objects) ] , ) Test implementation â Atlan recommends that you test your changes locally by running Apache Airflow on local and setting the OpenLineage transport as the \"console\". You can use Astronomer on local as it is easy and quick, but feel free to use any other method. To test your implementation locally: Install the Docker Desktop application in your system. Install Astro CLI . In your root directory, create a directory for the following Astronomer files   - mkdir astro-airflow and cd astro-airflow . Initialize an Astronomer project with the command astro init . This will create the required files in the directory you created above. Open the .env file, add AIRFLOW__OPENLINEAGE__TRANSPORT='{\"type\": \"console\"}' to the file, and save it. Add a test DAG with tasks using your custom operator with OpenLineage support to the astro-airflow/dags folder. Start Astronomer Airflow with the command astro dev start . Open http://localhost:8080/home after Astronomer Airflow has started. Run the DAG that uses your custom operator with OpenLineage support. Open DAG run task logs and locate the OpenLineage events in the logs. (Optional) Format the JSON OpenLineage events in your IDE using this online tool . Ensure that the OpenLineage events contain input and output details. For example: { \"eventTime\" : \"2024-12-27T17:55:24.407459+00:00\" , \"eventType\" : \"COMPLETE\" , \"inputs\" : [ { \"facets\" : { } , \"name\" : \"dir1/dir2/sample.csv\" , \"namespace\" : \"s3a://atlan-test-bucket\" } ] , ... ... \"outputs\" : [ { \"facets\" : { } , \"name\" : \"wide_world_importers.astronomer_assets.sample\" , \"namespace\" : \"databricks://dbc-8d941db8-48cd.cloud.databricks.com\" } ] , ... ... \"schemaURL\" : \"https://openlineage.io/spec/2-0-2/OpenLineage.json /$defs/RunEvent\" } To view other implementation examples, refer to the following documentation: GCSToGCSOperator AzureBlobStorageToGCSOperator (Optional) Community contribution â If you add OpenLineage support to an operator from the list of commonly used provider packages, consider updating the Apache Airflow repository . This allows other users to implement your code and improve it over time. Here is an example of a contribution to the community from a member of the Atlan team. Frequently asked questions â Can Atlan extract lineage from PythonOperator or BashOperator? â OpenLineage supports both PythonOperator and BashOperator. However, these core operators function as \"black box\" operators, capable of running any code. This in turn may limit the extent of lineage extraction. If the lineage generated is incomplete, Atlan suggests that you use manually annotated lineage (inlets and outlets). Can Atlan extract lineage from KubernetesPodOperator? â OpenLineage neither supportsÂ KubernetesPodOperator nor a managed service such as EksPodOperator or GKEStartPodOperator . This is because these operators also function as \"black box\" operators, capable of running any code. Limited execution details are exposed to the operator, thus limiting Atlan's ability to extract lineage. Atlan suggests that you use manually annotated lineage (inlets and outlets). Are there other methods to implement OpenLineage support for lineage generation through events? â Yes, you can use manually annotated lineage , which requires updating the DAG code. Keep in mind that this is a fallback measure, only recommended for very specific use cases, such as when it is impossible to extract lineage from the operator itself. Manually annotated lineage is also difficult to update and prone to manual errors. Tags: connectors data Previous How to integrate Apache Airflow/OpenLineage Next What does Atlan crawl from Apache Airflow/OpenLineage? Supported operators Custom and unsupported operators Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage",
    "text": "Connect data Orchestration & Workflow Amazon MWAA OpenLineage Get Started How to integrate Amazon MWAA/OpenLineage On this page Integrate Amazon MWAA/OpenLineage To integrate Amazon Managed Workflows for Apache Airflow (MWAA) with Atlan, complete the following steps. (Alternatively, you can use the AWS Secrets Manager to store the environment variables and fetch them using the plugin, follow the steps here to do so.) To learn more about OpenLineage, refer to OpenLineage configuration and facets . Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . Create an API token in Atlan â Before running the workflow, you need to create an API token in Atlan. Configure the integration in Atlan â Select the source â To select Amazon MWAA/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Amazon MWAA Airflow Assets Â and then click Setup Workflow . Create the connection â danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Amazon MWAA/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host , enter the URL of your Apache Airflow UI   -  do not include any extra paths such as /home in the URL. This will allow Atlan to help you view your assets directly in Amazon MWAA from the asset profile. (Optional) For Port , enter the port number for your Apache Airflow UI. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Amazon MWAA â Did you know? You will need the Atlan API token and connection name to configure the integration in Amazon MWAA. This will allow Amazon MWAA to connect with the OpenLineage API and send events to Atlan. danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. To configure Amazon MWAA to send OpenLineage events to Atlan: Based on your Apache Airflow version on Amazon MWAA, there may be additional prerequisites for using OpenLineage: For Apache Airflow versions 2.7.0 onward, update the requirements.txt file of your Apache Airflow instance with: apache-airflow-providers-openlineage For Apache Airflow versions 2.5.0 onward and prior to 2.7.0, update the requirements.txt file of your Apache Airflow instance: openlineage-airflow To set environment variables, you will need to deploy a custom plugin to Amazon MWAA. Create an env_var_plugin.py file and add the following Python code in the plugin: For Apache Airflow versions 2.7.0 onward: from airflow . plugins_manager import AirflowPlugin import os os . environ [ \"AIRFLOW__OPENLINEAGE__NAMESPACE\" ] = \"<connection_name>\" os . environ [ \"AIRFLOW__OPENLINEAGE__TRANSPORT\" ] = '''{ \"type\": \"http\", \"url\": \"https://<instance>.atlan.com/events/openlineage/airflow-mwaa/\", \"auth\": { \"type\": \"api_key\", \"api_key\": \"<API_token>\" } }''' os . environ [ \"AIRFLOW__OPENLINEAGE__CONFIG_PATH\" ] = \"\" os . environ [ \"AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS\" ] = \"\" class EnvVarPlugin ( AirflowPlugin ) : name = \"env_var_plugin\" AIRFLOW__OPENLINEAGE__NAMESPACE : replace <connection_name> with the connection name as exactly configured in Atlan. AIRFLOW__OPENLINEAGE__TRANSPORT : specify details of where and how to send OpenLineage events. Replace <instance> with the name of your Atlan instance. Replace <API_token> with the API token generated in Atlan. AIRFLOW__OPENLINEAGE__CONFIG_PATH : specifies that the apache-airflow-providers-openlineage package read the OpenLineage config from environment variables instead of a config file. AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS : specifies that OpenLineage must send events for all operators   -  only required for the apache-airflow-providers-openlineage package. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0: from airflow . plugins_manager import AirflowPlugin import os os . environ [ \"OPENLINEAGE_URL\" ] = \"https://<instance>.atlan.com/events/openlineage/airflow-mwaa/\" os . environ [ \"OPENLINEAGE_NAMESPACE\" ] = \"<connection_name>\" os . environ [ \"OPENLINEAGE_API_KEY\" ] = \"<API_token>\" class EnvVarPlugin ( AirflowPlugin ) : name = \"env_var_plugin\" OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow-mwaa/ . OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. OPENLINEAGE_API_KEY : set the API token generated in Atlan. Amazon MWAA allows you to install a plugin through a zip archive. You can either: Use the following code to zip your env_var_plugin.py file: zip plugins.zip env_var_plugin.py If you already have a plugins.zip file, add the env_var_plugin.py file to your zip file. Upload the plugins.zip and requirements.txt files to the S3 bucket connected to your Amazon MWAA environment. Amazon MWAA requires your DAGs, plugins, and requirements.txt file to be in the same S3 bucket, which serves as the source location for your environment. You will need to specify the path for the latest versions of the plugins.zip and requirements.txt files in Amazon MWAA. To specify the path: Open the Environments page on the Amazon MWAA console. Select an environment and then click Edit . In the DAG code in Amazon S3 section, configure the following: For Plugins file - optional , select the plugins.zip file in the S3 bucket connected to your Amazon MWAA environment or choose the latest plugins.zip version from the dropdown list. For Requirements file - optional , select the latest requirements.txt file version from the dropdown list. Click Next, Update environment. or Next to save your configurations. Verify the Atlan connection in Amazon MWAA â To verify connectivity to Amazon MWAA: For Verify connection withÂ MWAA , click the clipboard icon to copy and run the preflight check DAG on your Amazon MWAA instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: connectors data configuration Previous Amazon MWAA OpenLineage Next What does Atlan crawl from Amazon MWAA/OpenLineage? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Amazon MWAA Verify the Atlan connection in Amazon MWAA"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/how-tos/integrate-anomalo",
    "text": "Connect data Data Quality & Observability Anomalo Get Started How to integrate Anomalo On this page Integrate Anomalo Once you have configured the Anomalo settings , you can establish a connection between Atlan and Anomalo. To integrate Anomalo with Atlan, review the order of operations and then complete the following steps. Create an API token in Atlan â Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan â Select the source â To select Anomalo as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the list of packages, select AnomaloÂ Assets Â and then click Setup Workflow . Create the connection â You will only need to create a connection once to enable Atlan to receive incoming events from Anomalo. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the Anomalo events as and when your checks run in Anomalo to catalog your check metadata. To configure the Anomalo connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. For Host Name , enter the URL of your Anomalo instance. For API Key , enter the API key you copied in Anomalo. Click the Test Authentication button to confirm connectivity to Anomalo. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â On the Metadata page, you can override the defaults for any of these options: To select the warehouses you want to include in crawling, click Include warehouses . (This will default to all warehouses, if none are specified.) To select the warehouses you want to exclude from crawling, click Exclude warehouses . (This will default to no warehouses, if none are specified.) To check for any permissions or other configuration issues before running the crawler, click Preflight checks . Navigate to the bottom of the screen and click Next . Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Configure the integration in Anomalo â Who can do this? You will need your Anomalo Deployment Admin Superuser to complete these steps   -  you may not have access yourself. As a Deployment Admin Superuser in Anomalo, you will need to create an Organization Integration or Deployment Integration linking a newly created Atlan integration to your Anomalo deployment. This configuration is required only after you have completed integrating Anomalo in Atlan. You will need the following for this configuration: URL of your Atlan instance API token generated in Atlan To create an Atlan integration in Anomalo: Log in to your Anomalo instance as a Deployment Admin Superuser . Create an Atlan integration in the admin interface. Enter your Atlan hostname and the API token you generated in Atlan . Create an Organization Integration or a Deployment Integration linking the new Atlan integration to your Anomalo deployment or a specific organization. Once you have integrated Anomalo and your checks have completed running in Anomalo, Atlan will start receiving and processing events from Anomalo. You will see your Anomalo checks cataloged in Atlan! ð Note that Atlan does not fetch any historical check metadata. As new events are received from Anomalo, Atlan will process these events and catalog your Anomalo checks. Did you know? You can also view event logs in Atlan to track and debug events received from Anomalo. Tags: connectors integration api authentication Previous Set up Anomalo Next What does Atlan crawl from Anomalo? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Anomalo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage Get Started How to integrate Apache Airflow/OpenLineage On this page Integrate Apache Airflow/OpenLineage To integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to OpenLineage configuration and facets . Atlan also supports other Apache Airflow distributions to enhance your data management and workflow capabilities: Amazon MWAA Astronomer Google Cloud Composer Did you know? You will need the Atlan API token and connection name to configure the integration in Apache Airflow/OpenLineage. This will allow Apache Airflow to connect with the OpenLineage API and send events to Atlan. Create an API token in Atlan â Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan â Select the source â To select Apache Airflow/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Airflow Assets Â and then click Setup Workflow . Create the connection â danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Apache Airflow/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host and Port , enter the URL and port number of your Apache Airflow UI, respectively. This will allow Atlan to help you view your assets directly in Apache Airflow from the asset profile. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Apache Airflow/OpenLineage â Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. To configure Apache Airflow to send OpenLineage events to Atlan: Based on your Apache Airflow version, there may be additional prerequisites for using OpenLineage: For Apache Airflow versions 2.7.0 onward, download and install the latest apache-airflow-providers-openlineage package and update the requirements.txt file of your Apache Airflow instance with: apache-airflow-providers-openlineage For Apache Airflow versions 2.5.0 onward and prior to 2.7.0, download and install the latest openlineage-airflow library and update the requirements.txt file of your Apache Airflow instance with: openlineage-airflow Add the following environment variables to your project's .env file: danger When deploying Apache Airflow on Kubernetes, set these environment variables in both the Scheduler and Triggerer pods to ensure proper integration. For Apache Airflow versions 2.7.0 onward: AIRFLOW__OPENLINEAGE__NAMESPACE : set the connection name as exactly configured in Atlan. AIRFLOW__OPENLINEAGE__TRANSPORT : specify details of where and how to send OpenLineage events in the following JSON string format: { \"type\" : \"http\" , \"url\" : \"https://<instance>.atlan.com/events/openlineage/airflow/\" , \"auth\" : { \"type\" : \"api_key\" , \"api_key\" : \"<API_token>\" } } Replace <instance> with the name of your Atlan instance. Replace <API_token> with the API token generated in Atlan. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0:Â OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow/ . OPENLINEAGE_API_KEY : set the API token generated in Atlan. OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. Verify the Atlan connection in Apache Airflow â To verify connectivity to Apache Airflow: For Verify connection with Airflow , click the clipboard icon to copy and run the preflight check DAG on your Apache Airflow instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: connectors data configuration Previous Apache Airflow OpenLineage Next How to implement OpenLineage in Airflow operators Create an API token in Atlan Configure the integration in Atlan Configure the integration in Apache Airflow/OpenLineage Verify the Atlan connection in Apache Airflow"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage/how-tos/integrate-astronomer-openlineage",
    "text": "Connect data Orchestration & Workflow Astronomer OpenLineage Get Started How to integrate Astronomer/OpenLineage On this page Integrate Astronomer/OpenLineage To integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to OpenLineage configuration and facets . Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . Create an API token in Atlan â Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan â Select the source â To select Astronomer/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Astronomer Airflow Assets Â and then click Setup Workflow . Create the connection â danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Astronomer/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host , enter the URL of your Astronomer Airflow UI. This will allow Atlan to help you view your assets directly in Astronomer from the asset profile. (Optional) For Port , enter the port number for your Astronomer Airflow UI. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Astronomer â Did you know? You will need the Atlan API token and connection name to configure the integration in Astronomer. This will allow Astronomer to connect with the OpenLineage API and send events to Atlan. danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. Astronomer has a built-in OpenLineage integration -  Atlan recommends using OpenLineage version 1.2.1 or latest. You will need to use environment variables in Astronomer to set custom values for the integration with Atlan. To configure Astronomer to send OpenLineage events to Atlan: Open your Astronomer console and select a workspace. In the left menu under Workspace , click Deployments and then select the required deployment. On your deployment page, click the Variables tab. On the Variables page, click the Edit variables button. Add the following environment variable keys and corresponding values: For Apache Airflow versions 2.7.0 onward: AIRFLOW__OPENLINEAGE__NAMESPACE : set the connection name as exactly configured in Atlan. OPENLINEAGE_DISABLED and AIRFLOW__OPENLINEAGE__DISABLED : set both to false to enable the OpenLineage listener in Apache Airflow, if disabled by default. AIRFLOW__OPENLINEAGE__TRANSPORT : specify details of where and how to send OpenLineage events in the following JSON string format: { \"type\" : \"http\" , \"url\" : \"https://<instance>.atlan.com/events/openlineage/airflow-astronomer/\" , \"auth\" : { \"type\" : \"api_key\" , \"api_key\" : \"<API_token>\" } } Replace <instance> with the name of your Atlan instance. Replace <API_token> with the API token generated in Atlan. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0: OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow-astronomer/ . OPENLINEAGE_API_KEY : set the API token generated in Atlan. OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. OPENLINEAGE_DISABLED and AIRFLOW__OPENLINEAGE__DISABLED : set both to false to enable the OpenLineage listener in Apache Airflow, if disabled by default. Â Click Update Environment Variables to save your changes. It can take up to two minutes for new variables to be applied to your deployment. Verify the Atlan connection in Astronomer â To verify connectivity to Astronomer: For Verify connection with Astronomer , click the clipboard icon to copy and run the preflight check DAG on your Astronomer instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: connectors configuration Previous Astronomer OpenLineage Next What does Atlan crawl from Astronomer/OpenLineage? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Astronomer Verify the Atlan connection in Astronomer"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to integrate Atlan with Google Sheets On this page Integrate Atlan with Google Sheets The Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.Â Integrating Atlan with Google Sheets allows you to: Import column metadata for your data assets into Google Sheets Update column metadata for your imported assets directly in Google Sheets View data asset profiles on Google Sheets Download downstream impacted assets in Google Sheets Who can do this? Any individual in your organization with access to Atlan can install the Atlan add-on for Google Sheets. You can even install the Atlan add-on if you have a Google account with a non-gmail address. However, the Atlan add-on can also be installed at the workspace level. To install the app for users in your organization, follow this guide . You'll also need the following permissions for Google Sheets within your organization: Display and run third-party web content in prompts and sidebars within Google apps View and manage the spreadsheets with the Atlan add-on Install Atlan in Google Sheets â To install the Atlan add-on from the Google Workspace Marketplace, use this link . To install the Atlan add-on directly in Google Sheets, follow these steps: In the menu bar of your Google spreadsheet, click Extensions . In the dropdown menu, click Add-ons . Click Get add-ons to view available add-ons in the Google Workspace Marketplace. In the search bar of your Google Workspace Marketplace, type Atlan and press enter. Click Install to install the Atlan add-on. If you see a dialog box pop up asking for permissions, click Allow to continue. Connect Atlan with Google Sheets â To connect Atlan with your Google spreadsheets: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Setup Atlan . In the dialog box, enter your Atlan instance URL and click Continue . Congrats on connecting Atlan with Google Sheets! ð danger For every new spreadsheet that you create using Google Sheets, you will need to follow the steps outlined above to connect Atlan with that spreadsheet. The Atlan add-on will remain connected for all tabs within an already connected spreadsheet. Tags: integration connectors downstream-impact dependencies Previous How to export assets Next How to integrate Atlan with Microsoft Excel Install Atlan in Google Sheets Connect Atlan with Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/integrate-jira-cloud",
    "text": "Configure Atlan Integrations Project Management Jira Get Started How to integrate Jira Cloud On this page Integrate Jira Cloud Who can do this? You will need to be an admin in Atlan to configure the Jira Cloud integration. You will also need inputs and approval from an administrator of your Jira Cloud workspace. To integrate Jira Cloud and Atlan, follow these st eps. Connect Atlan to Jira Cloud â danger You must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work. Atlan uses the following scopes for the Jira Cloud integration: read:jira-user -  view user profiles read:jira-work -  view Jira issue data write:jira-work -  create and manage Jira issues manage:jira-configuration -  manage Jira global settings offline_access -  allows the Atlan app to refresh the access token To connect Atlan to Jira Cloud from within Atlan: From the left menu, clickÂ Admin . UnderÂ Workspace , clickÂ Integrations . In theÂ Jira tile, click theÂ Connect button. A new window will open, where you'll be asked to log into your Atlassian account: To login with an email address, enter your email address, click Continue , and then enter your password and click Log in . To login with Google, Microsoft, or Apple, click the appropriate button and follow the instructions. Once you're logged in, you will be asked to allow Atlan to access your Atlassian account. Scroll to the bottom of the window and click theÂ Accept button. If you have access to multiple Jira Cloud sites, for the Connecting to Jira dialog, click the Select environment dropdown to select the Jira environment you want to connect to Atlan and then click Connect . Install Atlan Jira app â To install the Atlan Jira app: Open the Atlan Jira app's page in the Atlassian marketplace, through either of these ways: From theÂ Integrations page of Atlan, in the Jira tile, click the Add to Jira button. Directly navigate to the URL: https://marketplace.atlassian.com/apps/1225577/atlan In the upper right of the page, click theÂ Get it now button. At the bottom of the Add to Jira dialog, click theÂ Get it now button. (Optional) Request permission from your Jira Cloud admin â If you are not a workspace administrator in Jira, you will be prompted to request permission to install. To request permission to install the integration: UnderÂ Want this app? Let your admin know why enter an explanation for installing the app. At the bottom of the form, click theÂ Submit request button. Contact your Jira workspace administrator and ask them to approve the Atlan app. Once approved, you'll get an email from Jira telling you that the Atlan app is installed. Configure integration from Atlan to Jira Cloud â To configure the Jira Cloud integration from Atlan, from the Integrations sub-menu: Expand theÂ Jira tile. (You may need to refresh the page before the following options appear.) Under theÂ Configurations tab, you can configure the following: For Projects and issue type , select the Jira projects for which your users are allowed to create tickets in Atlan. For each selected project, next to Issue types , click Edit to select the allowed issue types. (This will default to all projects and issue types, if none are specified.) For Default Project , select the Jira project to use as your default project from the allowed list of projects. Click Update to save your configuration. (Optional) At any future time, you can review theÂ Overview tab to see the number of linked issues between Jira and Atlan. Atlan is now connected to Jira Cloud! ð Did you know? The default project is preselected when creating or linking issues to an asset in Atlan. You can change the project while creating or linking issues as needed. (Optional) Create a webhook for access management workflows â If your Atlan admin has enabled the governance workflows and inbox module in your Atlan workspace, you can either register a webhook in the Jira administration console or install it directly in Atlan to allow your requesters to view the latest status of their data access approval or revocation requests for governed assets. This is only required if you: Enable governance workflows . Want to use the access management workflow to grant or revoke data access in a source tool using Jira . In addition to the scopes mentioned here , Atlan uses the following scope to manage Jira webhooks   - manage:jira-webhook . From Atlan â You can directly install the webhook in Atlan if you're an admin in both your Atlan and Jira Cloud workspaces. To install a webhook: From the left menu, clickÂ Admin . UnderÂ Workspace , clickÂ Integrations . Expand theÂ Jira tile. For Get updates on data access requests (optional) , click Install . Atlan will automatically install the webhook. From Jira â Your Jira admin will need to register the webhook in the Jira administration console if your Atlan admin is not a Jira admin. To register a webhook: Contact Atlan support to provide you with the webhook secret. Log in to the Jira administration console with the Administer Jira global permission . From the top right of the Jira administration console, open the settings menu, and then under _Jira Setting_s, click System . From the left menu, under Advanced , click WebHooks . From the top right of your screen, click the Create a WebHook button. In the webhook creation form, enter the following details: For Name , enter a meaningful name   -  for example, Atlan_webhook . For Status , click Enabled . For URL , copy and paste https://{atlan-domain}.com/api/service/jira/events -  replace atlan-domain with the name of your Atlan tenant. For Secret , enter the webhook secret provided by Atlan support. Note that if an invalid secret is used or this field is kept blank, the webhook configuration will be automatically removed from Atlan once an event is received in Atlan. Under Events , in the text box for Issue related events , paste issue.property[atlan].guid is NOT EMPTY . For Issue , click the checkboxes for updated (jira :issue _updated) and deleted (jira :issue _deleted) . Skip the rest of the fields. Click Create to register your webhook. Tags: integration connectors Previous Jira Next How to integrate Jira Data Center Connect Atlan to Jira Cloud Configure integration from Atlan to Jira Cloud (Optional) Create a webhook for access management workflows"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/netsuite-to-snowflake-lineage",
    "text": "Use data Lineage FAQ Is there a way to build lineage from NetSuite to Snowflake? Is there a way to build lineage from NetSuite to Snowflake? While Atlan currently does not support native integration with NetSuite, you can create and catalog NetSuite assets using Atlan APIs . This will allow you to create NetSuite assets in Atlan and also generate lineage between these assets and any other sources (including Snowflake) using Atlan lineage APIs . Tags: lineage data-lineage impact-analysis integration connectors api rest-api graphql catalog metadata discovery faq-lineage Previous How to obtain upstream lineage if I connect to a Tableau data asset? Next What are Power BI processes on the lineage graph?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/link-your-jira-account",
    "text": "Configure Atlan Integrations Project Management Jira Get Started Link your Jira account On this page Link your Jira account To create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that set up the Jira integration , but not for other users. An Atlan admin must set up the tenant-level Jira integration in Atlan before any other user can perform a user-level integration. This is because the Atlan app installation requires inputs and approval from an administrator of your Jira workspace. Once the Jira integration has been completed, you can link your Jira account to Atlan without requiring any additional permissions from your Atlan or Jira admin. Link your Jira account â To link your Jira account: From any screen, in the upper right navigate to your name, then clickÂ Profile . Click the icon at the bottom of the resulting dialog to get to integrations. UnderÂ Jira click theÂ Connect link. In the resulting popup, log in if necessary, then scroll to the bottom and click Allow . Unlink your Jira account â To unlink your Jira account: From any screen, in the upper right navigate to your name, then clickÂ Profile . Click the icon at the bottom of the resulting dialog to get to integrations. UnderÂ Jira click theÂ Disconnect link. In the confirmation dialog, clickÂ Confirm . Tags: integration connectors Previous How to integrate Jira Data Center Next Troubleshooting Jira Link your Jira account Unlink your Jira account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/how-tos/link-your-microsoft-teams-account",
    "text": "Configure Atlan Integrations Collaboration Microsoft Teams How-tos Link your Microsoft Teams account On this page Link your Microsoft Teams account To get alerts for starred assets directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that set up the Microsoft Teams integration , but not for other users. Link your Microsoft Teams account â To link your Microsoft Teams account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Teams ,Â click theÂ Connect link. In the resulting popup, scroll to the bottom and clickÂ Allow . Unlink your Microsoft Teams account â To unlink your Microsoft Teams account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Teams , click theÂ Disconnect link. In the confirmation dialog, clickÂ Confirm . Tags: integration connectors alerts monitoring notifications Previous How to integrate Microsoft Teams Next Troubleshooting Microsoft Teams Link your Microsoft Teams account Unlink your Microsoft Teams account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/how-tos/link-your-slack-account",
    "text": "Configure Atlan Integrations Collaboration Slack How-tos Link your Slack account On this page Link your Slack account To see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that set up the Slack integration , but not for other users. Link your Slack account â To link your Slack account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. UnderÂ Slack ,Â click theÂ Connect link. In the resulting popup, scroll to the bottom and clickÂ Allow . Unlink your Slack account â To unlink your Slack account: From any screen, in the upper right navigate to your name, then clickÂ Profile . Click the four dots icon in the resulting dialog to get to integrations. UnderÂ Slack , click theÂ Disconnect link. In the confirmation dialog, clickÂ Confirm . Tags: integration connectors Previous How to integrate Slack Next Troubleshooting Slack Link your Slack account Unlink your Slack account"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/how-tos/manage-requests",
    "text": "Use data Requests Get Started Manage requests On this page Manage requests Did you know? Atlan supports governance workflows ! Once you have enabled governance workflows and inbox , Atlan will channel requests and approvals for your governed assets through governance workflows and land them in your inbox . Request changes to an asset â Who can do this? Any user without edit access to an asset's metadata can request changes to an asset. To request changes to an asset: Navigate to the asset you want to change. For example, use search or discovery to get there. Click on the part of the asset you want to change, as if you were making the change directly. You'll see that you do not have permission and are instead suggesting a change. Instead of saving your change, Atlan will prompt you to submit it as a request. Did you know? The lock icon and slightly transparent text show that you do not have access to an asset. Track your request(s) â For a specific asset â To track your requests for a specific asset: Navigate to the asset you want to track. For example, use search or discovery to get there. From the right navbar of the asset, click on the Request tab. (Optional) To the right of the Requests heading, change the drop down to All to see all requests you've made on the asset. Pending requests are those still awaiting approval (or rejection). Approved requests are those that someone has already accepted (and applied). Rejected requests are those that someone has already declined (and will not be applied). All your requests â To track all the requests you have raised, across all assets: Click the icon in the upper left of any page to navigate to the Atlan home screen. Scroll to the bottom of the screen to the My Requests card. (Optional) In the upper right of the card, change the drop down to narrow requests by status. Pending requests are those still awaiting approval (or rejection). Approved requests are those that someone has already accepted (and applied). Rejected requests are those that someone has already declined (and will not be applied). (Optional) Hover over any pending request to see who can approve or reject it. Get notified on Slack â If your organization's Slack account is integrated with Atlan , you will receive Slack notifications when your requests are approved or rejected. To receive Slack notifications on your requests: The email address used for Slack and Atlan should be the same, even if you haven't personally integrated the accounts. The Slack app should have been installed before August 12, 2022. If installed later, you'll need to update Slack. If different email addresses were used for Slack and Atlan, you'll first need to link your Slack account with Atlan . Approve or reject request(s) â Who can do this? Any non-guest user with edit access to an asset's metadata can approve the request. This only includes admin and member users. For a specific asset â To approve or reject a request for a specific asset: Navigate to the asset you want to manage. For example, use search or discovery to get there. From the right sidebar of the asset, click on the Request tab. (Optional) To the right of the Requests heading, change the drop-down to Pending to see open requests on the asset. Hover over any pending request to either approve or reject it: Click the arrow next to Approve to approve the change with a comment, or click Approve to approve it without a comment. Click the arrow next to Reject to reject the change with a comment, or click Reject to reject it without a comment. All requests â Who can do this? Currently only admin users have access to see all requests. To approve or reject requests against any asset: From the left menu of any screen, click Governance . Under the Governance heading, click Requests . (Optional) In the Requests table, click the search bar to search for requests to take action on or click the funnel icon to filter your requests: Select Connection to filter by a specific connector , or drill down further by connection, database, or schema. Select Status to filter by request status   -  pending, approved, or rejected. Select Requestor to filter by specific users. Select Tags to filter by tag update requests. Select Request type to filter by type of metadata update requested   - description , tag , certificate , term , or owner . Select Asset type to filter by specific asset types   -  tables, columns, queries, and more. Select Raised in to filter by a predefined or custom data range. Hover over any pending request to either approve or reject it: Click the arrow next to Approve to approve the change with a comment, or click Approve to approve it without a comment. Click the arrow next to Reject to reject the change with a comment, or click Reject to reject it without a comment. Did you know? You can also configure the Slack integration to receive notifications for metadata update requests raised in Atlan and take action directly from Slack . Use the requests widget â The requests widget brings together all your requests in one location to help you track, manage, and prioritize them more effectively. You can open the requests widget from anywhere in Atlan, find all the requests that need your attention and take action immediately, and view a summary of your requests and track their statuses. Take action on requests â To take action on requests, from the requests widget: From the top right of any screen in Atlan, click the Requests icon. In the Requests dialog, under Needs attention , hover over or click any request to view actions. From the request card, you can either: To reject the update request, click Reject Â or click the downward arrow to the right and then click Reject with comment to add a comment as well. To approve the update request, click Approve or click the downward arrow to the right and then click ApproveÂ with comment to add a comment as well. Track your requests â To track your requests, from the requests widget: From the top right of any screen in Atlan, click the Requests icon. In the Requests dialog, click My requests to track all your requests. To view your requests sorted by status, click Pending to view pending requests, Approved for approved requests, or Rejected for requests that have been rejected. Tags: integration connectors workflow automation orchestration Previous Requests Next What are requests? Request changes to an asset Track your request(s) Approve or reject request(s) Use the requests widget"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/migrate-from-dbt-to-atlan-action",
    "text": "Connect data ETL Tools dbt Manage dbt in Atlan Migrate from dbt to Atlan action On this page Migrate from dbt to Atlan action The dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a GitHub or GitLab repository. Atlan plans to enhance this customÂ action to provide additional capabilities, such as impact analysis for data contracts ) and more. Instead of creating separate custom actions for each new capability, Atlan has renamed the dbt-action to atlan-action to better reflect the multiple capabilities on offer and will eventually deprecate the dbt-action . If you're currently using the dbt-action , Atlan strongly recommends migrating to the atlan-action . Migration notice and timeline â Atlan is providing you with a window of over six months to complete the migration, with a deadline set for June 2025. However, rest assured that Atlan will not archive the dbt-action until every organization has successfully transitioned to the atlan-action . If you choose not to migrate, please be aware that the dbt-action will no longer receive any updates. This means no new fixes or features will be implemented. Impact of migration â You can expect a seamless transition   -  there will be no changes in terms of functionality. Your workflows will continue to operate as usual post-migration. Migrate to Atlan action â GitHub â To migrate to the atlan-action : Open your GitHub workflow file that currently uses the dbt-action . Replace the dbt-action@v1 with atlan-action@v1 as follows: name: Atlan action on: pull_request: types: [opened, edited, synchronize, reopened, closed] jobs: get-downstream-impact: name: Get Downstream Assets runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Run Action -       uses: atlanhq/dbt-action@v1 +        uses: atlanhq/atlan-action@v1 with: GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}} ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}} ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}} GitLab â To migrate to the atlan-action : Open your GitLab workflow file .gitlab-ci.yml that currently uses the dbt-action . Clone v1 tag of atlan-action instead of the main branch of dbt-action : stages: - get-downstream-impact get-downstream-impact-open: stage: get-downstream-impact image: node:20 script: -  - git clone https://github.com/atlanhq/dbt-action.git +   - git clone --branch v1 https://github.com/atlanhq/atlan-action.git -  - cd dbt-action +   - cd atlan-action - npm install - npm run build - node ./adapters/index.js environment: name: get-downstream-impact rules: - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when: never - if: '$CI_COMMIT_BRANCH' Tags: connectors data model Previous Enrich Atlan through dbt Next Add impact analysis in GitHub Migration notice and timeline Impact of migration Migrate to Atlan action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/mine-microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Get Started Mine Microsoft Azure Synapse Analytics On this page Mine Microsoft Azure Synapse Analytics danger Atlan currently only supports mining query history for dedicated SQL pools with the Microsoft Azure Synapse Analytics miner. Mining query history for serverless SQL pools is currently not supported. Once you have crawled assets from Microsoft Azure Synapse Analytics , you can mine query history to construct lineage. To mine lineage from Microsoft Azure Synapse Analy tics, review the order of operations and then complete the following steps. Select the miner â To select the Microsoft Azure Synapse Analytics miner: In the top right of any screen, navigate to + New and then click New workflow . Under Marketplace , from the filters along the top, click Miner . From the list of packages, select Synapse Miner and then click Setup Workflow . Configure the miner â To configure the Microsoft Azure Synapse Analytics miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run .) For Miner Extraction Method , choose your extraction method: In Query History , Atlan connects to your database and mines query history directly. In Offline , you will need to first mine query history yourself and make it available in S3 . Run the miner â To run the Microsoft Azure Synapse Analytics miner, after completing the steps above: To run the miner once, immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Microsoft Azure Synapse Analytics assets that were created in Microsoft Azure Synapse Analytics! ð Tags: connectors data crawl Previous Set up Microsoft Azure Synapse Analytics Next Set up on-premises Microsoft Azure Synapse Analytics miner access Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/mine-microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Crawl Power BI Assets Mine Microsoft Power BI On this page Mine Microsoft Power BI Once you have crawled assets from Microsoft Power BI , you can mine its activity events to generate usage metrics . To mine activity events from Microsoft Power BI, review the order of operations and then complete the following steps. Select the miner â To select the Microsoft Power BI miner: In the top right of any screen, navigate toÂ New and then clickÂ New Workflow . From the filters along the top, click Miner . From the list of packages, select Power BI Miner and then click Setup Workflow . Configure the miner â To configure the Microsoft Power BI miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to configure the miner: For Start time , choose the earliest date from which to mine activity events. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Microsoft Power BI assets. Press enter after each name to add more names. Run the miner â To run the Microsoft Power BI miner, after completing the steps above: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you can see usage metrics for Microsoft Power BI assets that were created in Microsoft Power BI between the start time and when the miner ran! ð Tags: connectors crawl Previous Crawl Microsoft Power BI Next What does Atlan crawl from Microsoft Power BI? Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/mine-teradata",
    "text": "Connect data Databases SQL Databases Teradata Mine Mine Teradata On this page Mine Teradata Once you have crawled assets from Teradata , you can mine its query history to construct lineage. To mine lineage from Teradata, review the order of operations and then complete the following steps. Select the miner â To select the Teradata miner: In the top right of any screen, navigate to +New and then clickÂ New workflow . Under Marketplace , from the filters along the top, click Miner . From the list of packages, selectÂ Teradata Miner and then click Setup Workflow . Configure the miner â To configure the Teradata miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , choose your extraction method: In Query History , Atlan connects to your database and mines query history directly. In Offline , you will need to first mine query history yourself and make it available in S3 . This method uses Atlan's teradata-miner tool to mine query history. For Start date , choose the earliest date from which to mine query history. info ðª Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to configure the miner: For Cross Connection , click Yes to extract lineage across all available data source connections or click No to only extract lineage from the selected Teradata connection. For Control Config , if Atlan support has provided you with a custom control configuration, select Custom and enter the configuration into theÂ Custom Config box. You can also: Enter {âignore-all-caseâ: true} to enable crawling assets with case-sensitive identifiers. danger If running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run the miner â To run the TeradataÂ miner, after completing the steps above: To run the miner once, immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Teradata assets that were created in Teradata between the start date and when the miner ran! ð Tags: connectors data crawl setup Previous Crawl Teradata Next Set up on-premises Teradata miner access Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/okta-first-time-login-error",
    "text": "Configure Atlan Integrations Identity Management SSO Troubleshooting Okta first-time login authentication error On this page Okta first-time login authentication error Why do I get an authentication error when logging in via Okta for the first time? â It is possible that your Okta account is not yet linked with your Atlan account. Here are two ways to troubleshoot: Ensure that the service provider metadata details are copied accurately. If you're logged into Atlan, please log out from Atlan. Once you've logged out of Atlan, clear your browser cache and then continue. Tags: integration connectors faq-integrations Previous PingFederate SSO 404 error Next Google Dashboard login error"
  },
  {
    "url": "https://docs.atlan.com/product/connections/references/openlineage-configuration-and-facets",
    "text": "Connect data Connectivity Framework Connector Framework References OpenLineage configuration and facets On this page OpenLineage configuration and facets OpenLineage is a lineage metadata extraction library that you can install in a target application such as Apache Airflow or Apache Spark . Once you have installed OpenLineage, you can configure the target application to integrate with Atlan. This will allow Atlan to receive OpenLineage events and catalog your assets from supported sources. You will neither have to clone a GitHub repository nor make any code changes to your DAGs. To install OpenLineage, refer to the documentation for supported sources: Apache Airflow Amazon MWAA Apache Spark Astronomer Google Cloud Composer Did you know? To add lineage support to sources other than the ones listed above, you can use OpenLineage's extensible specification. Refer to our developer documentation to learn more. Example â Apache Airflow â Once you have configured a supported Apache Airflow distribution, you can run a sample DAG to confirm that your assets are being crawled in Atlan. Although Atlan strongly recommends running the preflight check DAG to test your Apache Airflow connection, you can also use the example DAG below to verify your setup. For example: import json from pendulum import datetime from airflow . decorators import ( dag , task , ) @dag ( dag_id = \"example_dag_basic\" , schedule = \"@once\" , start_date = datetime ( 2023 , 1 , 1 ) , catchup = False , tags = [ \"example\" ] , ) def example_dag_basic ( ) : @task ( ) def extract ( ) : data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}' order_data_dict = json . loads ( data_string ) return order_data_dict @task ( multiple_outputs = True ) def transform ( order_data_dict : dict ) : total_order_value = 0 for value in order_data_dict . values ( ) : total_order_value += value return { \"total_order_value\" : total_order_value } @task ( ) def load ( total_order_value : float ) : print ( f\"Total order value is: { total_order_value : .2f } \" ) order_data = extract ( ) order_summary = transform ( order_data ) load ( order_summary [ \"total_order_value\" ] ) example_dag_basic ( ) Apache Spark â Once you have configured Apache Spark, you can run a sample Spark job to confirm that your assets are being crawled in Atlan. For example: from pyspark . sql import SparkSession from pyspark . sql . functions import col   Create a Spark session and configure the spark properties spark = ( SparkSession . builder . master ( 'local' ) . appName ( 'data_pipeline_sample' ) . getOrCreate ( ) ) snowflake_options = { \"sfURL\" : \".snowflakecomputing.com\" , \"sfUser\" : \"\" , \"sfPassword\" : \"\" , \"sfDatabase\" : \"\" , \"sfWarehouse\" : \"\" , \"sfSchema\" : \"\" , \"sfRole\" : \"\" , } instacart_df = spark . read \\ . format ( \"snowflake\" ) \\ . options ( ** snowflake_options ) \\ . option ( \"dbtable\" , \"table1\" ) \\ . load ( ) filtered_df = instacart_df . filter ( col ( '\"order_id\"' ) == '123456' ) filtered_df . write \\ . format ( \"snowflake\" ) \\ . options ( ** snowflake_options ) \\ . option ( \"dbtable\" , \"table2\" ) \\ . mode ( \"append\" ) \\ . save ( ) spark . stop ( ) Supported facets â An OpenLineage event will contain the following object model: dataset, job, and run entities. In addition, OpenLineage supports facets to provide contextual metadata for events. Atlan currently only processes the following facets for OpenLineage events: Apache Airflow â OpenLineage facet Description Where in Atlan job.facets.jobType Apache Airflow asset type (task or DAG) asset profile, preview, and sidebar run.facets.airflow DAG details, including runs, tasks, owner, and task group asset profile, overview sidebar, and pipeline graph run.facets.airflow_version Apache Airflow version and DAG metadata API only run.facets.parentRun parent DAG for tasks API only run.facets.processing_engine Apache Airflow and OpenLineage versions API only outputs.facets.columnLineage fetches column lineage lineage graph Apache Spark â OpenLineage facet Description Where in Atlan eventType job run status overview sidebar eventTime job start and end time asset profile job.namespace connection name asset profile and overview sidebar job.name Spark job name asset name run.runId Spark job name run ID API only run.facets.spark_version Spark version overview sidebar run.facets.spark_properties OpenLineage package version API only run.facets.processing_engine Spark cluster details API only inputs.facets.name links input facets related assets and pipeline graph outputs.facets.name links output facets related assets and pipeline graph inputs.facets.namespace input type related assets and pipeline graph outputs.facets.namespace output type related assets and pipeline graph inputs.facets.symlinks retrieves logical entity API only outputs.facets.symlinks retrieves logical entity API only outputs.facets.columnLineage fetches column lineage lineage graph Tags: lineage data-lineage impact-analysis integration connectors catalog metadata discovery Previous Connectors and capabilities Next Supported sources Example Supported facets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/faq/permissions-and-limitations",
    "text": "Connect data Databases SQL Databases CrateDB FAQ Permissions and limitations On this page How do I make sure Atlan can access new tables automatically? â CrateDB requires explicit permission configuration to access tables created after initial setup. Grant schema-level permissions (recommended) : This approach automatically includes all future tables: GRANT DQL ON SCHEMA doc TO atlan_user ; This single command covers all current and future tables in the schema, including metadata access. Grant wildcard permissions : Use this for specific table patterns: GRANT DQL ON TABLE doc . * TO atlan_user ; Regular permission audits : Since CrateDB has limited stored procedure support, consider regular permission audits: -- Check current user permissions SELECT privilege_type , table_schema , table_name FROM information_schema . table_privileges WHERE grantee = 'atlan_user' ORDER BY table_schema , table_name ; What CrateDB limitations affect the Atlan connector? â No explicit schema creation : CrateDB doesn't support the CREATE SCHEMA statement. Schemas are created implicitly when a table referencing a new schema is created. Manual permission management : Since schema-level permissions aren't supported, access must be manually granted for each new table. No metadata-only access : CrateDBâs DQL privileges apply to both metadata and data. This means the Atlan user can view and query all data in any tables they have permission to access. Limited automation : CrateDB doesn't support triggers or full stored procedure capabilities, which restricts options for automating permission management or other operational tasks. Tags: connectors cratedb database faq faq-connectors Previous Connection issues How do I make sure Atlan can access new tables automatically? What CrateDB limitations affect the Atlan connector?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/references/preflight-checks-for-aiven-kafka",
    "text": "Connect data Event/Messaging Aiven Kafka References Preflight checks for Aiven Kafka On this page Preflight checks for Aiven Kafka Before running the Aiven Kafka crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Bucket credentials â â Check successful if the bucket, region, and prefix combination is valid and the bucket credentials passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Bucket credentials: failed with error code: NoSuchBucket Tags: connectors crawl Previous What does Atlan crawl from Aiven Kafka? Bucket credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/references/preflight-checks-for-amazon-msk",
    "text": "Connect data Event/Messaging Amazon MSK References Preflight checks for Amazon MSK On this page Preflight checks for Amazon MSK Before running the Amazon MSK crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Cluster permission check â â Check successful if the IAM role has sufficient permissions to list all the brokers for the Amazon MSK cluster. â Internal server error if the IAM role is missing kafka-cluster:Connect permission. â failed to get list of kafka brokers if the IAM role has the permission to connect to the Amazon MSK cluster but is unable to fetch any broker details. Topic permission check â â Check successful if the IAM role has sufficient permissions to list or describe topics available in the Amazon MSK cluster. â failed to get list of kafka topics if the IAM role is missing the kafka-cluster:DescribeTopic permission. Tags: connectors crawl Previous What does Atlan crawl from Amazon MSK? Next Troubleshooting Amazon MSK connectivity Cluster permission check Topic permission check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/references/preflight-checks-for-amazon-quicksight",
    "text": "Connect data BI Tools Cloud-based BI Amazon QuickSight References Preflight checks for Amazon QuickSight On this page Preflight checks for Amazon QuickSight BeforeÂ running the Amazon QuickSight crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Analysis view permission â The ListAnalyses REST API is used to fetch the actual list of analyses for which the user has view permission. â Check successful. Analysis API is accessible. â Check failed for listed analyses Folder view permission â The ListFolders REST API is used to fetch the actual list of folders for which the user has view permission. â Check successful. Folder API is accessible. â Check failed for listed folders Dashboard view permission â The ListDashboards REST API is used to fetch the actual list of dashboards for which the user has view permission. â Check successful. Dashboard API is accessible. â Check failed for listed dashboards Dataset view permission â The ListDataSets REST API is used to fetch the actual list of datasets for which the user has view permission. â Check successful. Dataset API is accessible. â Check failed for listed datasets Tags: connectors crawl api Previous What does Atlan crawl from Amazon QuickSight? Analysis view permission Folder view permission Dashboard view permission Dataset view permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/references/preflight-checks-for-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift References Preflight checks for Amazon Redshift On this page Preflight checks for Amazon Redshift Before running the Amazon Redshift crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Assets â Database and schema â â Check successful â Check failed for $missingObjectName Tables count â â Check successful. Table count: <count> â Source returned error/UI default failure message Tables metadata â â Check successful â Check failed! Please grant select permission on pg_catalog.svv_table_info External tables metadata â â Check successful â Check failed! Please grant select permission on pg_catalog.svv_external_tables Miner â Did you know? Once you have crawled assets from Amazon Redshift, you can mine query history . Query history â DDL â â Check successful â Check failed! Please grant select permission on pg_catalog.stl_ddltext DML â â Check successful â Check failed! Please grant select permission on pg_catalog.stl_query Session â â Check successful â Check failed! Please grant select permission on pg_catalog.stl_connection_log Transaction rollback â â Check successful â Check failed! Please grant select permission on pg_catalog.stl_undone Insert query â â Check successful â Check failed! Please grant select permission on pg_catalog.stl_insert S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors data crawl Previous What does Atlan crawl from Amazon Redshift? Next Troubleshooting Amazon Redshift connectivity Assets Miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/references/preflight-checks-for-anomalo",
    "text": "Connect data Data Quality & Observability Anomalo References Preflight checks for Anomalo On this page Preflight checks for Anomalo Before integrating Anomalo , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Anomalo instance accessibility check â This check tests for the validity of the host name URL and API key you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid. â Check successful if the Anomalo instance is accessible. â Unable to connect to your Anomalo instance Permission (read access) check â This check tests whether or not you have the required permissions for querying the Anomalo API to fetch metadata. â Check successful â Unable to fetch warehouse metadata from your Anomalo instance. Please check if the provided API key has the required permissions. Tags: connectors data api Previous What does Atlan crawl from Anomalo? Next Troubleshooting Anomalo connectivity Anomalo instance accessibility check Permission (read access) check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/references/preflight-checks-for-apache-kafka",
    "text": "Connect data Event/Messaging Apache Kafka References Preflight checks for Apache Kafka On this page Preflight checks for Apache Kafka Before running the Apache Kafka crawler , run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Cluster permission â â Check successful if the user has sufficient permission to list all the brokers of the Apache Kafka cluster. â Failed to get list of Kafka brokers if the user has permission to connect to the Apache Kafka cluster but is unable to fetch any broker details. Topics permission â â Check successful if the user has sufficient permission to list or describe topics available in the Apache Kafka cluster. â Failed to get list of Kafka topics if the user has permission to connect to the Apache Kafka cluster but is unable to fetch any topic details. Tags: connectors crawl Previous What does Atlan crawl from Apache Kafka?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/references/preflight-checks-for-confluent-schema-registry",
    "text": "Connect data Event/Messaging Confluent Schema Registry References Preflight checks for Confluent Schema Registry On this page Preflight checks for Confluent Schema Registry Before running the ConfluentÂ Schema Registry crawler , you can run preflight checks to perform the necessary technical validations. The following preflights check will be completed: Subjects check â â Check successful â Check failed for $missingObjectName subject. Please grant at least read access to the subject Configs check â â Check successful â Source returned error message Schemas check â â Check successful â Source returned error message Tags: connectors crawl Previous What does Atlan crawl from Confluent Schema Registry? Subjects check Configs check Schemas check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/references/preflight-checks-for-cratedb",
    "text": "Connect data Databases SQL Databases CrateDB References Preflight checks for CrateDB On this page Preflight checks for CrateDB Before running the CrateDB crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks are completed: Database and schema check â â Check successful â Check failed for $missingObjectName Tables check â â Check successful. Table count: <count> â Check failed. Missing some of the required grants SELECT, SHOW VIEW, EXECUTE on tables: Tags: connectors cratedb database preflight-checks Previous What does Atlan crawl from CrateDB? Next Connection issues Database and schema check Tables check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/references/preflight-checks-for-databricks",
    "text": "Connect data Data Warehouses Databricks References Preflight checks for Databricks On this page Preflight checks for Databricks Before running the Databricks crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: JDBC â Schemas â â Check successful â Check failed for $missingObjectName Rest API (Unity Catalog) â User login/UC enabled â â Check successful â Source returned error For example: {\"error_code\":\"403\",\"message\":\"Invalid access token.\"} Catalog â â Check successful â Check failed for $missingObjectName catalog Databricks lineage and usage (miner) â Did you know? Once you have crawled assets from Databricks, you can extract lineage and usage and popularity metrics . Login â â Check successful â Check failed For example: {\"error_code\":\"403\",\"message\":\"Invalid access token.\"} Lineage API â â Check successful. Lineage API is enabled. â Check failed For example: Lineage is not enabled for this Databricks account: 47258391-b3c8-4ff9-a0d9-5afc02443806 Query history API endpoint check â â Check successful â Check failed Tags: connectors data crawl api Previous What does Atlan crawl from Databricks? Next Troubleshooting Databricks connectivity JDBC Rest API (Unity Catalog) Databricks lineage and usage (miner)"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/references/preflight-checks-for-datastax-enterprise",
    "text": "Connect data Databases NoSQL Databases Datastax Enterprise References Preflight checks for DataStax Enterprise On this page Preflight checks for DataStax Enterprise Before running the DataStax Enterprise crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks must be completed: Cluster permission â â Check successful if the connection to the DataStax Enterprise cluster is established. â Check failed if unable to connect to the DataStax Enterprise cluster. Keyspace permission â â Check successful if the user has read access to system_schema.keyspaces . â Check failed if the user does not have read access to system_schema.keyspaces . Table permission â â Check successful if the user has read access to system_schema.tables . â Check failed if the user does not have read access to system_schema.tables . View permission â â Check successful if the user has read access to system_schema.views . â Check failed if the user does not have read access to system_schema.views . Column permission â â Check successful if the user has read access to system_schema.columns . â Check failed if the user does not have read access to system_schema.columns . Index permission â â Check successful if the user has read access to system_schema.indexes . â Check failed if the user does not have read access to system_schema.indexes . Tags: connectors data crawl Previous What does Atlan crawl from DataStax Enterprise? Next Troubleshoot permission issues"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/preflight-checks-for-dbt",
    "text": "On this page Preflight checks for dbt Before running the dbt crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: dbt Core â Manifest file check on S3 â This checks if manifest files are present in the provided bucket and prefix. â Check successful if Atlan can access the bucket and prefix containing the manifest files. â Check failed For example: If Atlan cannot access the bucket   - Manifest file Check on S3: failed With error code: AccessDenied - Access Denied If Atlan can access the bucket but there are no manifest files present in the bucket   - Manifest file Check on S3: No manifest files found in the mentioned S3 Prefix Tags: connectors crawl dbt Core"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/references/preflight-checks-for-domo",
    "text": "Connect data BI Tools Cloud-based BI Domo References Preflight checks for Domo On this page Preflight checks for Domo Before running the Domo crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Datasets API â Atlan uses the DataSet API to fetch dataset metadata from Domo. â Check successful. Datasets API is working. â Check failed For example: {\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}} STATUS:403 Dashboards API â Atlan uses the Page API to fetch dashboard metadata from Domo. â Check successful. Dashboards API is working. â Check failed For example: {\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}} STATUS:403 Cards API â Atlan uses the DomoStats cards API to fetch card metadata from Domo. This checks the format of the response and tries to find the Id , name , and description columns. â Check successful. Cards API is working. â Check failed. <Response received from the API> Card-Dashboard Relationship API â Atlan uses the DomoStats card-dashboard relationship API to fetch card-dashboard relationship metadata from Domo. This checks the format of the response and tries to find the cardId and pageId columns. â Check successful. The card-dashboard relationship API is working. â Check failed. <Response received from the API> Dataset-Card Relationship API â Atlan uses the DomoStats dataset-card relationship API to fetch dataset-card relationship metadata from Domo. This checks the format of the response and tries to find the dataSourceId and cardId columns. â Check successful. The dataset-card relationship API is working. â Check failed. <Response received from the API> Tags: connectors data crawl api Previous What does Atlan crawl from Domo? Next Troubleshooting Domo connectivity Datasets API Dashboards API Cards API Card-Dashboard Relationship API Dataset-Card Relationship API"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/references/preflight-checks-for-fivetran",
    "text": "On this page Preflight checks for Fivetran Before running the Fivetran enrichment , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Fivetran Platform table existence and access check â This check verifies if a Fivetran table named table_lineage exists in the selected schema, and whether Atlan has been provided with read access to the metadata. â Check successful â Table named table_lineage does not exist. or Provided credentials do not have read access. Tags: connectors data crawl api configuration Fivetran Platform table existence and access check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/preflight-checks-for-google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery References Preflight checks for Google BigQuery On this page Preflight checks for Google BigQuery Before running the Google BigQuery crawler , you can run preflight checks to perform the necessary technical validations.Â Did you know? All Google BigQuery resources expose the testIamPermissions() method, which is used for permission testing in Atlan through REST API. The following preflight checks will be completed: Authorization â Each request requires an OAuth 2.0 access token generated via the service account key . Assets â Metadata crawling permission â â Check successful â Check failed. Not all permission granted. Missing permissions: Query and review permission â â Check successful â Check failed. Not all permission granted. Missing permissions: Miner â Did you know? Once you have crawled assets from Google BigQuery, you can mine query history . Miner policy â Query history â â Check successful â Check failed. Not all permission granted. Missing permissions: S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Crawler workflow â This checks if the selected connection exists in Atlan. â Check successful â Check failed. Connection does not exist. / Check failed. Workflow artifacts are missing. Please run the crawler workflow again. Tags: connectors data crawl api authentication Previous What does Atlan crawl from Google BigQuery? Next Troubleshooting Google BigQuery connectivity Authorization Assets Miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/references/preflight-checks-for-hive",
    "text": "Connect data Databases Query Engines Hive References Preflight checks for Hive On this page Preflight checks for Hive Before running the Hive crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Database and schema check â Information schema â â Check successful â Check failed for $missingObjectName Miner â Did you know? Once you have crawled assets from Hive, you can mine query history . S3 credentials â â Check successful â Check failed Tags: connectors data crawl Previous What does Atlan crawl from Hive? Database and schema check Miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/references/preflight-checks-for-looker",
    "text": "Connect data BI Tools Cloud-based BI Looker References Preflight checks for Looker On this page Preflight checks for Looker Before running the Looker crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Projects view capability â First, the list of projects in the Include Projects and Exclude Projects fields is determined. Next, the Query Projects REST API is used to fetch the actual list of projects for which the user has view capability . â Check successful if all the projects from the first list are in the second one. â Check failed for $missingProject Folders view capability â First, the list of folders in the Include Folders and Exclude Folders fields is determined. Next,Â the Query Projects REST API is used to fetch the actual list of folders for which the user has view capability . â Check successful if all the folders from the first list are in the second one. â Check failed for $missingFolder Git SSH key check â â Check successful if a valid SSH key is provided when field level lineage is enabled. â Field level lineage needs to clone your git repositories. Please provide your git ssh key or disable field level lineage or Please provide your private ssh key properly. Remember to include '-----BEGIN' and '-----END' blocks also. Please remember to specify the password for private key if encrypted S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors crawl api Previous What does Atlan crawl from Looker? Next Troubleshooting Looker connectivity Projects view capability Folders view capability Git SSH key check S3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/references/preflight-checks-for-metabase",
    "text": "Connect data BI Tools Cloud-based BI Metabase References Preflight checks for Metabase On this page Preflight checks for Metabase Before running the Metabase crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Collection count â â Collection count check passed. Total collections: <collection count> â Total collections: 0 or source returned error Dashboard count â â Dashboard count check passed. Total dashboards: <dashboard count> â Total dashboards: 0 or source returned error Question count â â Question count check passed. Total questions: <question count> â Total questions: 0 or source returned error Native query editing permission â â Check successful â Check failed. Missing native query editing permission on the following databases: <database list> Tags: connectors data crawl Previous What does Atlan crawl from Metabase? Next Troubleshooting Metabase connectivity Collection count Dashboard count Question count Native query editing permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/references/preflight-checks-for-microsoft-azure-data-factory",
    "text": "On this page Preflight checks for Microsoft Azure Data Factory Before running the Microsoft Azure Data Factory crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Subscription permissions check â â Check successful â No subscriptions available. This can either mean that no data factory exists or the service principal does not have access to the specified data factories. Please grant the Reader role to the service principal for at least one data factory. Tags: connectors data crawl Subscription permissions check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/references/preflight-checks-for-microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics References Preflight checks for Microsoft Azure Synapse Analytics On this page Preflight checks for Microsoft Azure Synapse Analytics Before running the Microsoft Azure Synapse Analytics crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Databases and schemas check â This check is performed for both basic and service principal authentication method. â Check successful â Check failed for $missingObjectName REST credentials check â This check is only performed when using the service principal authentication method. Atlan authenticates the service principal credentials by attempting to fetch the access token using Microsoft's authorization flow . â Check successful â Check failed. <Response received from the API> Tags: connectors data crawl authentication Previous What does Atlan crawl from Microsoft Azure Synapse Analytics? Next What lineage does Atlan extract from Microsoft Azure Synapse Analytics? Databases and schemas check REST credentials check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/preflight-checks-for-microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI References Preflight checks for Microsoft Power BI On this page Preflight checks for Microsoft Power BI Before running the Microsoft Power BI crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Credentials scopes â â Check successful Â â Source returned error For example: Credentials Scopes: Failed with response{\"error\":\"invalid_client\",\"error_description\":\"AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app '832d86c8-cd9b-43a5-b165-ab40fb49770b'.\\r\\nTrace ID: 654ce9b2-a626-4e0b-8598-0cac69970200\\r\\nCorrelation ID: 5be327fc-93cb-4bef-ab4e-0373f11a8017\\r\\nTimestamp: 2022-10-31 09:03:41Z\",\"error_codes\":[7000215],\"timestamp\":\"2022-10-31 09:03:41Z\",\"trace_id\":\"654ce9b2-a626-4e0b-8598-0cac69970200\",\"correlation_id\":\"5be327fc-93cb-4bef-ab4e-0373f11a8017\",\"error_uri\":\"https://login.microsoftonline.com/error?code=7000215\"} STATUS:401 Workspace permissions â â Check successful â No workspaces available Metadata scan and fetch refreshables â â Check successful Â â Source returned error For example: Permissions to scan metadata and fetch refreshables: Failed with response{\"error\":{\"code\":\"PowerBINotAuthorizedException\",\"pbi.error\":{\"code\":\"PowerBINotAuthorizedException\",\"parameters\":{},\"details\":[],\"exceptionCulprit\":1}}} STATUS:401 Tags: connectors crawl Previous What does Atlan crawl from Microsoft Power BI? Next What lineage does Atlan extract from Microsoft Power BI? Credentials scopes Workspace permissions Metadata scan and fetch refreshables"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/references/preflight-checks-for-microsoft-sql-server",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server References Preflight checks for Microsoft SQL Server On this page Preflight checks for Microsoft SQL Server Before running the Microsoft SQL Server crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Database and schema check â â Check successful â Check failed for $missingObjectName Miner â Did you know? Once you have crawled assets from Microsoft SQL Server, you can mine query history . S3 credentials â â Check successful â Check failed Tags: connectors data crawl Previous What does Atlan crawl from Microsoft SQL Server? Database and schema check Miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/references/preflight-checks-for-microstrategy",
    "text": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy References Preflight checks for MicroStrategy On this page Preflight checks for MicroStrategy Before running the MicroStrategy crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Project permission â First, the list of projects in the Include Projects and Exclude Projects fields is determined. Next,Â the Get Projects REST API is used to fetch the actual list of projects for which the user has permissions. â Check successful if all the projects from the first list are in the second one. â Check failed for $missingProjectId project Tags: connectors crawl api Previous What does Atlan crawl from MicroStrategy? Next Troubleshooting MicroStrategy connectivity Project permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/references/preflight-checks-for-mode",
    "text": "Connect data BI Tools Cloud-based BI Mode References Preflight checks for Mode On this page Preflight checks for Mode Before running the Mode crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Workspace check â â Check successful â Authenticating user does not have access to specified workspace. Please grant at least Full Members access Collections check â â Check successful â Check failed for $missingObjectName collection. Please grant at least viewer access to the collection Queries check â â User has access to <count of sources> queryable data sources â Access forbidden message or User has access to 0 queryable data sources Tags: connectors data crawl Previous What does Atlan crawl from Mode? Next Troubleshooting Mode connectivity Workspace check Collections check Queries check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/references/preflight-checks-for-monte-carlo",
    "text": "Connect data Data Quality & Observability Monte Carlo References Preflight checks for Monte Carlo On this page Preflight checks for Monte Carlo Before running the Monte Carlo crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Login check â â Check successful â Check failed for given API credential Tags: connectors crawl api Previous What does Atlan crawl from Monte Carlo? Login check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/references/preflight-checks-for-mysql",
    "text": "Connect data Databases SQL Databases MySQL References Preflight checks for MySQL On this page Preflight checks for MySQL Before running the MySQL crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Database and schema check â â Check successful â Check failed for $missingObjectName Tables check â â Check successful. Table count: <count> â Check failed. Missing some of the required grants SELECT, SHOW VIEW, EXECUTE on tables: S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors data crawl Previous What does Atlan crawl from MySQL? Next Troubleshooting MySQL connectivity Database and schema check Tables check S3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/references/preflight-checks-for-oracle",
    "text": "Connect data Databases SQL Databases Oracle References Preflight checks for Oracle On this page Preflight checks for Oracle Before running the Oracle crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors data crawl Previous What does Atlan crawl from Oracle? S3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/references/preflight-checks-for-postgresql",
    "text": "Connect data Databases SQL Databases PostgreSQL References Preflight checks for PostgreSQL On this page Preflight checks for PostgreSQL Before running the PostgreSQL crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Database and schema check â â Check successful â Check failed for $missingObjectName Tables check â â Check successful. Table count: <count> â Check failed. Missing some of the required grants SELECT, SHOW VIEW, EXECUTE on tables: S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors data crawl Previous What does Atlan crawl from PostgreSQL? Next Troubleshooting PostgreSQL connectivity Database and schema check Tables check S3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/references/preflight-checks-for-prestosql",
    "text": "Connect data Databases Query Engines PrestoSQL References Preflight checks for PrestoSQL On this page Preflight checks for PrestoSQL Before running the PrestoSQL crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Catalogs and schemas check â â Check successful â Check failed for $missingObjectName Tags: connectors data crawl Previous What does Atlan crawl from PrestoSQL? Catalogs and schemas check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/references/preflight-checks-for-qlik-sense-cloud",
    "text": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud References Preflight checks for Qlik Sense Cloud On this page Preflight checks for Qlik Sense Cloud Before running the Qlik Sense Cloud crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Spaces â â Check successful â Source returned error   -  for example, 401 authorization required Apps â â Check successful â Source returned error   -  for example, 401 authorization required Items â This check tests for access to datasets and other Qlik objects. â Check successful â Source returned error   -  for example, 401 authorization required Connections â â Check successful â Source returned error   -  for example, 401 authorization required Tags: connectors data crawl Previous What does Atlan crawl from Qlik Sense Cloud? Next Troubleshooting Qlik Sense Cloud connectivity Spaces Apps Items Connections"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/references/preflight-checks-for-redash",
    "text": "Connect data BI Tools On-premises & Enterprise BI Redash References Preflight checks for Redash On this page Preflight checks for Redash Before running the Redash crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Administrator privileges â â Check successful â Check failed. Given API key does not have admin privileges. Tags: connectors crawl api Previous What does Atlan crawl from Redash? Next Troubleshooting Redash connectivity Administrator privileges"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/references/preflight-checks-for-redpanda-kafka",
    "text": "Connect data Event/Messaging Redpanda Kafka References Preflight checks for Redpanda Kafka On this page Preflight checks for Redpanda Kafka Before running the Redpanda Kafka crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Bucket credentials â â Check successful if the bucket, region, and prefix combination is valid and the bucket credentials passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Bucket credentials: failed with error code: NoSuchBucket Tags: connectors crawl Previous What does Atlan crawl from Redpanda Kafka? Bucket credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/references/preflight-checks-for-salesforce",
    "text": "Connect data CRM Salesforce References Preflight checks for Salesforce On this page Preflight checks for Salesforce Before running the Salesforce crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Organization count check â â Organization Count check passed. Organization ID:<organization ID> Organization Name: <organization name> â Check failed or sObject type 'Organization' is not supported Folder count check â â Folder Count check passed. Total folders: <count> â Check failed Report count check â â Report Count check passed. Total reports: <count> â Check failed or MALFORMED_QUERY: Invalid SOQL query. Please check the syntax and try again. or INVALID_TYPE : sObject type 'Reportâ is not supported Dashboard count check â â Dashboard Count check passed. Total dashboards: <count> â Check failed or INVALID_TYPE : sObject type 'Dashboardâ is not supported Salesforce object count check â â Salesforce Object Count check passed. Total SObjects: <count> â Check failed Field count check â â Field Count check passed. Total organization fields: <count> â Check failed or MALFORMED_QUERY: Invalid SOQL query. Please check the syntax and try again. or INVALID_TYPE : sObject type 'EntityDefinitionâ is not supported Tags: connectors crawl salesforce Previous What does Atlan crawl from Salesforce? Next Troubleshooting Salesforce connectivity Organization count check Folder count check Report count check Dashboard count check Salesforce object count check Field count check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/references/preflight-checks-for-sap-hana",
    "text": "Connect data Databases SQL Databases SAP HANA References Preflight checks for SAP S/4HANA On this page Preflight checks for SAP S/4HANA Private Preview Before running the SAP HANA crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks must be completed: Schema check â â Check successful â Check failed for $missingObjectName Tags: connectors data crawl Previous What does Atlan crawl from SAP HANA? Schema check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/references/preflight-checks-for-sigma",
    "text": "Connect data BI Tools Cloud-based BI Sigma References Preflight checks for Sigma On this page Preflight checks for Sigma Before running the Sigma crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Workbook view permission â First, the list of workbooks in the Include Workbooks Â and Exclude Workbooks fields is determined. Next, the List Workbooks REST API is used to fetch the actual list of workbooks for which the user credentials have view permission. â Check successful if all the workbooks from the first list are in the second one. â Check failed for $missingWorkbookId workbook Tags: connectors crawl api Previous What does Atlan crawl from Sigma? Next Troubleshooting Sigma connectivity Workbook view permission"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/references/preflight-checks-for-sisense",
    "text": "Connect data BI Tools Cloud-based BI Sisense References Preflight checks for Sisense On this page Preflight checks for Sisense Before running the Sisense crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Folders API check â Atlan uses the Folders API to check if it's responding with a response status code 200. â Check successful. Folders API is working. â Check failed For example: {\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}} STATUS:403 Dashboards API check â Atlan uses the Dashboards API toÂ check if it's responding with a response status code 200. â Check successful. Dashboards API is working. â Check failed For example: {\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}} STATUS:403 Datamodels API check â Atlan uses the Datamodels API to check if it's responding with a response status code 200. The Datamodels API check may fail with a response status code 403 if the user only has Viewer permissions . â Check successful. Datamodels API is working. â Check failed For example: {\"code\":101,\"message\":\"Access denied\",\"status\":403,\"httpMessage\":\"Forbidden\"}} STATUS:403 Tags: connectors crawl api Previous What does Atlan crawl from Sisense? Next Troubleshooting Sisense connectivity Folders API check Dashboards API check Datamodels API check"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/preflight-checks-for-tableau",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau References Preflight checks for Tableau On this page Preflight checks for Tableau Before running the Tableau crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Server REST API version â The Server Info REST API is used to fetch the restApiVersion value. â Check successful if the restApiVersion is greater than or equal to 2.4. Projects view capability â First, the list of projects in the Include Projects and Exclude Projects fields is determined. Next, the Query Projects REST API is used to fetch the actual list of projects for which the user has view capability . â Check successful if all the projects from the first list are in the second one. Did you know? Atlan supports user credentials with Viewer role. Ensure that you grant View capability for all the assets you want to crawl. Metadata API â â Check successful â Cannot run the query because the Metadata API has not been enabled yet. Run the 'tsm maintenance metadata-services enable' command to enable the Metadata API or contact the Tableau administrator Tags: connectors crawl api Previous What does Atlan crawl from Tableau? Next Troubleshooting Tableau connectivity Server REST API version Projects view capability Metadata API"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/references/preflight-checks-for-teradata",
    "text": "Connect data Databases SQL Databases Teradata References Preflight checks for Teradata On this page Preflight checks for Teradata Before running the Teradata crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Databases and schemas check â â Check successful â Check failed for $missingObjectName Tables check â â Check successful â Check failed for $missingObjectName S3 â â Check successful if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. â Check failed with error code <AWS error code> - <AWS SDK ERR message> For example: Miner S3 credentials: failed with error code: NoSuchBucket Tags: connectors data crawl Previous What does Atlan crawl from Teradata? Databases and schemas check Tables check S3"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/references/preflight-checks-for-trino",
    "text": "Connect data Databases Query Engines Trino References Preflight checks for Trino On this page Preflight checks for Trino Before running the Trino crawler , you can run preflight checks to perform the necessary technical validations. The following preflight check will be completed: Catalogs and schemas check â â Check successful â Check failed for $missingObjectName Tags: connectors data crawl Previous What does Atlan crawl from Trino? Next Troubleshooting Trino connectivity Catalogs and schemas check"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/provide-credentials-to-query-data",
    "text": "Use data Insights Credentials Provide credentials to query data On this page Provide credentials to query data Who can do this? Any Atlan user with data access to the asset and their own credentials for the data store. When your admins have configured bring your own credentials (BYOC) , you must provide your own credentials to query data. Did you know? Connections that require you to provide your own user credentials have a small icon next to them. If the connection you want to query has no icon, you can query it with its default shared credentials. You only need to provide your own credentials for connections with this icon. Set up your own credentials â Atlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports SSO authentication . To set up your own credentials for a connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the drop-down to select the connection that requires user credentials. A Setup user credentials for ... dialog will appear. In the bottom right of the dialog, click Get started . The options will vary slightly depending on the connection type. Refer to the set up step for your connection for more details of what to fill in for each: Amazon Athena Amazon Redshift Databricks MySQL PostgreSQL Presto Snowflake Trino In the bottom-left of the dialog, click the Test Authentication button. Once successful, in the bottom right of the User credential setup dialog, click Done . You can then click Done again. You can now run queries using your own credentials! ð Manage your credentials â Who can do this? Any non-guest user who has previously set up their own credentials for a data store. To manage your credentials for your connections: From the upper-right of any screen, click your username and then Profile . In the sidebar that appears, click the User credentials (bottom) icon. Under User credentials : To remove a credential, hover over the row for the credential you want to delete and click the delete icon. When prompted for confirmation, click the Delete button. To change a credential, click the row for the credential you want to change and follow the instructions above. Note that your existing credentials are not shown, and can only be overwritten. When complete, click the Update button in the lower right. Tags: integration connectors Previous How to query without shared credentials Next What are the query builder actions? Set up your own credentials Manage your credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/references/recommended-provider-package-versions",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage References Provider package versions for OpenLineage On this page Provider package versions for OpenLineage This document lists the minimum required versions for the most commonly used provider packages in Atlan. Minimum required provider package versions â Atlan recommends using the latest version of each provider package, as it includes bug fixes, new features, and improvements to OpenLineage (OL) support. The minimum versions listed here guarantee that OpenLineage integration works as expected in Atlan. Did you know? Earlier versions may work but might lack recent fixes and features. Use the minimum version or later for full compatibility. Provider package Minimum version Release date PyPI link apache-airflow-providers-google 14.0.0 26 Feb 2025 PyPI apache-airflow-providers-amazon 9.4.0 26 Feb 2025 PyPI apache-airflow-providers-apache-spark 5.0.1 13 Mar 2025 PyPI apache-airflow-providers-snowflake 6.1.0 26 Feb 2025 PyPI apache-airflow-providers-dbt-cloud 4.2.0 26 Feb 2025 PyPI apache-airflow-providers-common-io 1.5.1 13 Mar 2025 PyPI apache-airflow-providers-ftp 3.12.2 26 Feb 2025 PyPI apache-airflow-providers-sftp 5.1.0 26 Feb 2025 PyPI apache-airflow-providers-apache-livy 4.2.0 26 Feb 2025 PyPI To check which operators have OpenLineage support, see the Supported classes â apache-airflow-providers-openlineage documentation . OpenLineage support can also be implemented for custom operators. Tags: lineage data-lineage impact-analysis integration connectors api rest-api graphql Previous Preflight checks for Apache Airflow Next Troubleshooting Apache Airflow/OpenLineage connectivity Minimum required provider package versions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/references/how-to-report-on-assets",
    "text": "Use data Reporting References Report on assets On this page Report on assets Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The assets dashboard in the reporting center helps you monitor your assets in Atlan. You can view a total count of all your assets as well as a summary of your assets by different categories, connectors, custom metadata , and more. Filter assets â You can use a variety of filters to customize your view of asset metrics. For example, you can even drill down to the schema level for a more granular view. To filter assets at the schema level: From the left menu in Atlan, click Reporting and then click Assets . Under Assets , for the All Asset Types filter, select an asset type   -  for this example, we'll select Table . For the All Connectors/Connections filter: Select a connector   -  for this example, we'll select Snowflake . Next, select a connection   -  for this example, we'll select the development connection. For the All Databases filter, select a database   -  for this example, we'll select the DBT_FOOD_BEVERAGE database. For the All Schemas filter, select a schema   -  for this example, we'll select the PUBLIC schema. (Optional) To further refine your search , click More filters . For example, you can use the Domains filter to filter assets by a single domain, multiple domains, or no domain   -  currently only supported for the assets dashboard. (Optional) Once the filtered assets are displayed, click any data point to view the assets in a sidebar. (Optional) In the top right of the sidebar, click the Export button to export filtered assets to a spreadsheet. You will now be able to view metrics for your filtered assets! ð Review enrichment completion â The Asset Enrichment section displays a total count of assets grouped by type of metadata enrichment. You can also view the total count of assets that need to be updated   -  for example, a total count of assets without READMEs . To view assets without metadata enrichment: From the left menu in Atlan, click Reporting and then click Assets . Under Asset Enrichment , navigate to the card you want to view   -  for this example, we'll select the With readme card. In the With readme card, click the assets remaining button to view all the assets without READMEs in a sidebar. (Optional) In the top right of the sidebar, click the Export button to export assets without enrichment to a spreadsheet. With this information in hand, you could plan a gamification drive to crowdsource documentation across your teams. Did you know? You can also view a total count of assets with resources in the Asset Enrichment section. To filter for assets without resources, use the Has resources filter listed under Properties in the more filters menu. Track lineage completion â You can track lineage completion for all your assets right from the assets dashboard.Â To track lineage completion for your assets: From the left menu in Atlan, click Reporting and then click Assets . In the Assets dashboard, scroll down to the Assets with Lineage section. Total counts for lineage completion are reported in   numbers by default. Under Assets with Lineage, click the % icon to view total counts in percentage. (Optional) Click any asset type to view the assets in a sidebar. (Optional) In the top right of the sidebar, click the Export button to export assets with lineage to a spreadsheet. Did you know? You can also view metrics for assets that have data lineage using the Has lineage filter listed under Properties in the filters menu. Customize enrichment metrics â The enrichment tracker allows you to customize your view of enrichment metrics. For example, you can filter metrics for your assets within a custom date range. To view asset enrichment metrics for a custom date range: From the left menu in Atlan, click Reporting and then click Assets . (Optional) Under Assets , click the All Connectors/Connections filter to view enrichment metrics for a specific connector or connection . In the Assets dashboard, scroll down to the Enrichment Tracker and click the date selector. From the date selector dropdown, click Custom range . To the right of the date fields, click the calendar icon and then select the dates for which you'd like to view enrichment metrics. Asset enrichment metrics will be displayed for your custom date range! ð You can also view metadata and certificate updates over time for a custom date range and filter by connector or connection. Did you know? The default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable. Tags: integration connectors glossary business-terms definitions Previous Summarize metadata Next How do I see views instead of materialized views in the reporting center? Filter assets Review enrichment completion Track lineage completion Customize enrichment metrics"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-automations",
    "text": "Use data Reporting Report Types Report on automations On this page Report on automations Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The automations dashboard in the reporting center provides you with an overview of assets enriched using Atlan's automation features. You can view a summary of updates as well as top users for each feature. Did you know? The default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable. Suggestions â You can track asset enrichment through suggestions from similar assets . You can also view top users who have accepted automated suggestions. Playbooks â You can monitor metrics related to playbooks , such as a summary of updates, top users, a playbooks-runs-over-time graph, and run activity. Total updates are reported in   counts by default. Click the % icon to view total updates in percentage form. Google Sheets â You can view a summary of asset enriched through the Atlan add-on for Google Sheets . For Updates over time , click any metadata option in the graph to view updates only for that option   -  for example, Announcements . Tags: integration connectors glossary business-terms definitions Previous Report on glossaries Next Report on queries Suggestions Playbooks Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/references/inventory-report-structure",
    "text": "Connect data Storage Amazon S3 References S3 Inventory Report Structure On this page S3 Inventory Report Structure This reference outlines the expected folder layout and file format for Amazon S3 inventory reports used by Atlanâs S3 crawler when running inventory-based ingestion . important The crawler supports a single destination bucket, with an optional prefix, to store all inventory reports from multiple source buckets. Folder structure â To enable successful inventory-based crawling , your destination bucket must follow this structure once inventory reports are generated: ð¦ <destination-bucket>/ âââ [ð <optional-prefix>/] (applies to all buckets) â   âââ ð <source-bucket-1>/ â   â   âââ ð <inventory-config-1>/ â   â   â   âââ ð <YYYY-MM-DDTHH-MM-Z>/ (timestamp folders) â   â   â   â   âââ ð manifest.json â   â   â   âââ ð data/ (CSV or gzipped files) â   â   â       âââ ð <inventory-file>.csv.gz â   â   âââ ð <inventory-config-2>/ â   âââ ð <source-bucket-2>/ âââ ... Required Component Description Example â Destination bucket Single S3 bucket to store all inventory reports atlan-inventory-reports â Prefix Folder prefix to organize reports (same for all source buckets) inventory-reports â Source bucket folder Folder named after each source bucket source-bucket-1 â Inventory config folder Folder named after the inventory configuration daily-inventory â Timestamp folder Folder with report generation timestamp 2024-01-16T00-00Z â Manifest file manifest.json containing report metadata manifest.json â Data folder Folder containing compressed inventory files data â Inventory files .csv.gz or .parquet files with actual data inventory-2024-01-16-00-00Z.csv.gz Examples â ð Basic structure (single source bucket) ð¦ atlan-inventory-reports/ âââ ð source-bucket-1/ âââ ð inventory-config-1/ âââ ð 2024-01-16T00-00Z/ â   âââ ð `manifest.json` {/* Required metadata file */} âââ ð data/ âââ ð inventory-2024-01-16-00-00Z.csv.gz ð Multiple source buckets ð¦ atlan-inventory-reports/ âââ ð source-bucket-1/ â   âââ ð inventory-config-1/ â       âââ ð 2024-01-16T00-00Z/ â       â   âââ ð `manifest.json` {/* Required metadata file */} â       âââ ð data/ â           âââ ð inventory-2024-01-16-00-00Z.csv.gz âââ ð source-bucket-2/ âââ ð inventory-config-1/ âââ ð 2024-01-16T00-00Z/ â   âââ ð `manifest.json` {/* Required metadata file */} âââ ð data/ âââ ð inventory-2024-01-16-00-00Z.csv.gz ð With optional prefix ð¦ atlan-inventory-reports/ âââ ð inventory-reports/ âââ ð source-bucket-1/ â   âââ ð inventory-config-1/ â       âââ ð 2024-01-16T00-00Z/ â       â   âââ ð `manifest.json` {/* Required metadata file */} â       âââ ð data/ â           âââ ð inventory-2024-01-16-00-00Z.csv.gz âââ ð source-bucket-2/ âââ ð inventory-config-1/ âââ ð 2024-01-16T00-00Z/ â   âââ ð `manifest.json` {/* Required metadata file */} âââ ð data/ âââ ð inventory-2024-01-16-00-00Z.csv.gz See also â Set up inventory reports for S3 : Set up inventory reports for S3 Tags: connectors s3 inventory-reports Previous Crawl S3 assets Next What does Atlan crawl from Amazon S3 Folder structure See also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/set-default-user-roles-for-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Guides Set default user roles for SSO Set default user roles for SSO Who can do this? You will need to be an admin user and configure SSO with a provider first. Admins can set default roles for new users joining the Atlan workspace via SSO. Setting the default role to admin, member, or guest will provide the appropriate permissions to users as soon as they log into Atlan. To set a default user role for SSO: In the left menu from any screen in Atlan, click Admin . Under Admin center , click SSO .Â On the SSO page for your provider, under Default Role , click the dropdown menu. From the dropdown menu, select Guest , Member , or Admin as the default role for your users. Tags: integration connectors Previous Limit SSO automatically creating users when they log in Next SSO integration with PingFederate using SAML"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/set-up-a-private-network-link-to-amazon-athena",
    "text": "Connect data Databases Query Engines Amazon Athena Get Started Set up a private network link to Amazon Athena On this page Set up a private network link to Amazon Athena Who can do this? You will need your Amazon Athena or AWS administrator involved   -  you may not have access yourself to complete these steps. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Amazon Athena and Atlan. Request Atlan's details â Before configuring the connection, you will need the following: VPC endpoint ID of the Atlan VPC endpoint in the following format   - vpce-0d90d77d1be568544 . This will be required to create the IAM policy. To enter a hostname for crawling Amazon Athena : If private DNS hostnames are enabled , enter the default Athena endpoint in the following format   - https://athena.<region>.amazonaws.com -  and it will resolve to your VPC endpoint. If privateÂ DNS hostnames are not enabled, enter the primary DNS name of the Atlan VPC endpoint in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com -  as retrieved from Atlan support. Request it from Atlan support . Create IAM policy â To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAthenaListDataCatalog\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"athena:ListDataCatalogs\" ] , \"Resource\" : \"*\" , \"Condition\" : { \"StringEquals\" : { \"aws:SourceVpce\" : [ \"<vpce-endpoint-id>\" ] } } } , { \"Sid\" : \"AllowAthenaActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"athena:StartQueryExecution\" , \"athena:GetQueryResults\" , \"athena:DeleteNamedQuery\" , \"athena:GetNamedQuery\" , \"athena:ListQueryExecutions\" , \"athena:StopQueryExecution\" , \"athena:GetQueryResultsStream\" , \"athena:ListNamedQueries\" , \"athena:CreateNamedQuery\" , \"athena:GetQueryExecution\" , \"athena:BatchGetNamedQuery\" , \"athena:BatchGetQueryExecution\" , \"athena:GetWorkGroup\" , \"athena:GetTableMetadata\" , \"athena:GetDatabase\" , \"athena:GetDataCatalog\" , \"athena:ListDatabases\" , \"athena:ListTableMetadata\" ] , \"Resource\" : [ \"arn:aws:athena:us-east-2:666568140392:datacatalog/*\" , \"arn:aws:athena:us-east-2:666568140392:workgroup/*\" ] , \"Condition\" : { \"StringEquals\" : { \"aws:SourceVpce\" : [ \"<vpce-endpoint-id>\" ] } } } , { \"Sid\" : \"AllowGlueActionsViaAthena\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"glue:GetDatabase\" , \"glue:GetDatabases\" , \"glue:CreateDatabase\" , \"glue:GetTables\" , \"glue:GetTable\" , \"glue:SearchTables\" , \"glue:GetTableVersions\" , \"glue:GetTableVersion\" , \"glue:GetPartition\" , \"glue:GetPartitions\" , \"glue:GetUserDefinedFunctions\" , \"glue:GetUserDefinedFunction\" ] , \"Resource\" : [ \"arn:aws:glue:us-east-2:666568140392:tableVersion/*/*/*\" , \"arn:aws:glue:us-east-2:666568140392:catalog\" , \"arn:aws:glue:us-east-2:666568140392:table/*/*\" , \"arn:aws:glue:us-east-2:666568140392:database/*\" ] , \"Condition\" : { \"ForAnyValue:StringEquals\" : { \"aws:CalledVia\" : [ \"athena.amazonaws.com\" ] } } } , { \"Sid\" : \"AllowS3ActionsOnDataViaAthena\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" ] , \"Resource\" : [ \"arn:aws:s3:::demo-wide-world-importers\" , \"arn:aws:s3:::demo-wide-world-importers/*\" ] , \"Condition\" : { \"ForAnyValue:StringEquals\" : { \"aws:CalledVia\" : [ \"athena.amazonaws.com\" ] } } } , { \"Sid\" : \"AllowS3ActionsOnMetadataViaAthena\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:GetObject\" , \"s3:ListBucket\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListMultipartUploadParts\" , \"s3:AbortMultipartUpload\" , \"s3:CreateBucket\" , \"s3:PutObject\" ] , \"Resource\" : [ \"arn:aws:s3:::source-curation-athena-metadata\" , \"arn:aws:s3:::source-curation-athena-metadata/*\" ] , \"Condition\" : { \"ForAnyValue:StringEquals\" : { \"aws:CalledVia\" : [ \"athena.amazonaws.com\" ] } } } ] } Replace <vpce-endpoint-id> with the VPC endpoint ID received from Atlan support . Attach this policy to the IAM user or role used for authentication. For more information, see Choose authentication mechanism or create a new IAM user by following the steps in the Create an IAM user section. Create an IAM user â Create an AWS IAM user and attach the policy created above to this user. To create an AWS IAM user: Follow the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Refer to managing access keys for IAM users to create an access key for the new user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl Amazon Athena in Atlan! ð Tags: connectors data crawl Previous Set up Amazon Athena Next Crawl Amazon Athena Request Atlan's details Create IAM policy Create an IAM user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/set-up-a-private-network-link-to-hive",
    "text": "Connect data Databases Query Engines Hive Get Started Set up a private network link to Hive On this page Set up a private network link to Hive AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Hive and Atlan. Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. Prerequisites â You should already have the following: Hive instance running in AWS (private EMR instance). Atlan hosted in the same region as the Hive instance. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Set up network to EMR instance â To set up the private network of your Hive EMR instance, from within AWS : Copy network settings â To copy the network settings of your Hive EMR instance: From the left menu, under EMR on EC2 , click Clusters . In the Clusters table, click on your Hive EMR cluster. From the cluster's Network and security tab, under Network , for Virtual Private Cloud (VPC) , click on your VPC to view more details. Under your VPC's Details tab, copy and save the value under the IPv4 CIDR column. Create inbound rule â To create an inbound rule allowing your VPC access to your Hive EMR instance: From the left menu, under EMR on EC2 , click Clusters . In the Clusters table, click on your Hive EMR cluster. From the cluster's Network and security tab, click the downward arrow for EC2 security groups (firewall) to expand this section. Under EC2 security groups (firewall) , click on a security group for the cluster. Under the Inbound rules tab, click the Edit inbound rules button. At the bottom left of the Inbound rules table, click the Add rule button. For Type ,Â select All traffic . For Port ,Â enter the port on which Hive is accessible. For Source , choose Custom and enter the CIDR range for your Hive instance (see Copy network settings ). Below the bottom right of the Inbound rules table, click the Save rules button. Repeat steps 4 to 7 for each security group in the cluster. Create internal Network Load Balancer â Start creating NLB â To create an NLB, from within AWS: Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name, Â enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where the Hive instance is located (see Copy network settings ). For Mappings , select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port , enter theÂ port value used in Created inbound rule . For Default action , click the Create target group link. This will open the target group creation in a new browser tab. Create target group â To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type , select Instances . For Target group name , enter a name. For Port , enter the port value used in Create inbound rule . For VPC , select the VPC where the Hive instance is located (see Copy network settings ). At the bottom of the form, click the Next button. From the Available instances table: Click the checkbox next to your Hive instance. Enter the port for the port value used in steps above. Click the Include as pending below button. At the bottom right of the form, click the Create target group button. Finish creating NLB â Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom right of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy â To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the row for the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service â To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name , enter a meaningful name. For Load balancer type , choose Network . For Available load balancers , select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint , enable Acceptance required . For Supported IP address types , enable IPv4 . At the bottom right of the form, click the Create button. Under the Details of the endpoint service, copy the hostname under Service name . Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN , enter the Atlan account ID. At the bottom right of the form, click the Allow principals button. Notify Atlan support â Once all the above steps are complete, provide Atlan support with the following information: The hostname for the endpoint service created above. The port number for your Hive instance. There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS: Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending . Select this row, and click the Actions button and then Accept endpoint connection request . If prompted to confirm, type accept into the field and click the Accept button. Wait for this to complete, it could take about 30 seconds. ð The connection is now established. You can now use the service endpoint provided by Atlan support as the hostname to crawl Hive in Atlan! ð Tags: integration connectors Previous Set up Hive Next Crawl Hive Prerequisites Set up network to EMR instance Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/set-up-a-private-network-link-to-trino",
    "text": "Connect data Databases Query Engines Trino Private Network Set up a private network link to Trino On this page Set up a private network link to Trino Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Trino and Atlan. Prerequisites â You should already have the following: Trino instance running in AWS (private EC2 instance). Atlan hosted in the same region as the Trino instance. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Set up network to EC2 instance â To set up the private network of your Trino EC2 instance, from within AWS : Copy network settings â To copy the network settings of yourÂ EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click your Trino EC2 instance. Under the instance's Details tab: Under VPC ID , copy the VPC identifier. Under Subnet ID , click the subnet for the instance. In the Subnets table, copy the value under the IPv4 CIDR column. Create inbound rule â To create an inbound rule allowing your private subnet access to your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click your Trino EC2 instance. Under the instance's details, change to the Security tab. Under Security groups , click the security group for the instance. Under the Inbound rules tab, click the Edit inbound rules button. At the bottom left of the Inbound rules table, click the Add rule button. For Type , select Custom TCP . For Port range , enter the port on which Trino is accessible (for example, 80 ). For Source , choose Custom and enter the CIDR range for your Trino instance (see Copy network settings ). Below the bottom right of the Inbound rules table, click the Save rules button. Create internal Network Load Balancer â Start creating NLB â To create an NLB, from within AWS: Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name , enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where the Trino instance is located (see Copy network settings ). For Mappings , select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port , enter 80 (or the non-default port value used in Created inbound rule ). For Default action , click the Create target group link. This will open the target group creation in a new browser tab. Create target group â To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type , select Instances . For Target group name , enter a name. For Port , enter 80 (or the non-default port value used in Create inbound rule ). For VPC , select the VPC where the Trino instance is located (see Copy network settings ). At the bottom of the form, click the Next button. From the Available instances table: Click the checkbox next to your Trino instance. Enter the port for the instance (80 or non-default value used in steps above). Click the Include as pending below button. At the bottom right of the form, click the Create target group button. Finish creating NLB â Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom right of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy â To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the row for the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service â To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom right of the form, click the Create button. Under the Details of the endpoint service, copy the hostname under Service name . Allow Atlan account access â To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID. At the bottom right of the form, click the Allow principals button. Notify Atlan support â Once all the above steps are complete, provide Atlan support with the following information: The hostname for the endpoint service created above. The port number for the Trino instance. There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request â To accept the consumer connection request, from within AWS: Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud , click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending . Select this row, and click the Actions button and then Accept endpoint connection request . If prompted to confirm, type accept into the field and click the Accept button. Wait for this to complete, it could take about 30 seconds. ð The connection is now established. You can now use the service endpoint provided by Atlan support as the hostname to crawl Trino in Atlan! ð Tags: integration connectors Previous Crawl Trino Next What does Atlan crawl from Trino? Prerequisites Set up network to EC2 instance Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka",
    "text": "Connect data Event/Messaging Aiven Kafka Get Started Set up Aiven Kafka On this page Set up Aiven Kafka Who can do this? You will probably need your Aiven Kafka administrator to complete these steps   -  you may not have access yourself. Atlan supports the S3 extraction method for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Create user in Aiven Kafka â To create a new user for extracting metadata from Aiven Kafka : Log in to your Aiven console and select your active cluster. From the upper right of the cluster Overview page, click the Users tab to create a new user: For Create a service user , under Username , enter a name for the new user and then click Add service user . The new user will be listed under Service users ,Â on the Users page. Copy the username and password for the new user and store them in a secure location. (Optional) If using client certificate authentication , copy the access key and access certificate and store them in a secure location. From the upper right of the cluster Overview page, click the Access Control List (ACL) tab to add a new ACL grant: Under Access Control List (ACL) , for ACL Type , click ACL For Topic . For Add access control entry , enter the following details: For Username , enter the username you created for the new user. For Topic , enter an asterisk * to include all topics. From the Permission dropdown, select Admin -  learn more about ACL permission mapping . Click Add entry to save your selections. Navigate to the Overview tab, copy or download the CA Certificate and store the details in a secure location. Did you know? Once you have extracted metadata on-premises and uploaded the results to S3 , you can crawl the metadata from Aiven Kafka into Atlan. Tags: connectors data Previous Aiven Kafka Next Crawl Aiven Kafka Create user in Aiven Kafka"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/how-tos/integrate-alteryx",
    "text": "Connect data ETL Tools Alteryx Get Started Set up Alteryx On this page Set up Alteryx Private preview Set up real-time integration between Alteryx and Atlan using OpenLineage. This integration automatically catalogs assets and creates lineage in Atlan whenever workflows run in Alteryx, providing immediate visibility into your ETL processes. Prerequisites â Before you begin, make sure you have: You have Admin or Workflow Admin permissions to create workflows and generate API tokens. You have Administrator access to configure OpenLineage integration. An API token for authentication. Create workflow in Atlan â Follow these steps to create an Alteryx listener workflow that receives OpenLineage events: Navigate to the Workflow section. In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Alteryx and then click Create Listener . Configure the connection â Important A single connection (namespace) must be used for only one Alteryx instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You only need to configure the connection once to enable Atlan to receive incoming OpenLineage events. After setup, Atlan automatically processes these events as Alteryx workflows run, enabling seamless cataloging of Alteryx assets. To configure the Alteryx connection in Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . Important If no user or group is specified, the connection can't be managed, not even by admins. Click Create connection at the bottom of the screen to save and activate it. Configure the integration in Alteryx â Did you know? You need the Atlan API token and connection name to configure the integration in Alteryx. Contact your account manager to enable Alteryx data lineage on the Alteryx side. This enables Alteryx to connect with the OpenLineage API and send events to Atlan. An Alteryx administrator configures the OpenLineage integration using: Navigate to the OpenLineage configuration settings in your Alteryx environment. Paste the API token you generated into the API Token field. Enter the connection name you used when creating the connection in Atlan (for example, production , analytics ). Add your Atlan Hostname in the format https://your-workspace.atlan.com and click Save . Verify the connection â To verify that the integration is working correctly: Run a workflow in your Alteryx instance to trigger OpenLineage events. Open Atlan and check for newly created Alteryx assets under the configured connection. Confirm that lineage information from the workflow appears in the asset view. Open the Event Logs in Atlan to review the OpenLineage events and verify successful ingestion. Atlan validates the existence of the corresponding Alteryx connection by checking that the connection name matches and the API token is valid. Once the workflows finish running in Alteryx, Alteryx workflows and lineage generated from OpenLineage events appear automatically in Atlan. See also â API authentication Tags: connectors etl-tools alteryx workflow Previous Alteryx Next What does Atlan crawl from Alteryx? Prerequisites Create workflow in Atlan Configure the connection Configure the integration in Alteryx Verify the connection See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb",
    "text": "Connect data Databases NoSQL Databases Amazon DynamoDB Get Started Set up Amazon DynamoDB On this page Set up Amazon DynamoDB warning ð¤ Who can do this? You will probably need your Amazon DynamoDB administrator to run these commands   -  you may not have access yourself. Atlan supports the following authentication methods for fetching metadata from Amazon DynamoDB: IAM user authentication -  this method uses an AWS access key, secret key, and region to fetch metadata. IAM role authentication -  this method uses an AWS role ARN and region to fetch metadata. Create IAM policy â To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:ListTables\" ] , \"Resource\" : \"*\" } , { \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:DescribeTable\" ] , \"Resource\" : \"arn:aws:dynamodb:<region>:<account_id>:table/*\" } ] } Replace <region> with the AWS region of your Amazon DynamoDB instance. Replace <account_id> with your AWS account ID. IAM permissions â Atlan requires the following permissions: dynamodb:ListTables : Fetches a list of your Amazon DynamoDB tables. This permission is used during the metadata extraction process to dynamically determine a list of tables. Note that this action does not support resource-level permissions and requires you to choose all resources, hence * for Resource . dynamodb:DescribeTable : Fetches metadata for extracted tables. This action supports resource-level permissions, so for Resource , you can either:Â Grant permission to all tables in the region for which you want to extract metadata: arn:aws:dynamodb:<region>:<account_id>:table/* Specify the table names for which you want to extract metadata: arn:aws:dynamodb:<region>:<account_id>:table/table_name_1 , arn:aws:dynamodb:<region>:<account_id>:table/table_name_2 Choose authentication mechanism â Using the policy created above , configure one of the following options for authentication. User-based authentication â To configure IAM user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On theÂ Set permissions page, attach the policy created in the previous step to this user. Refer to managing access keys for IAM users to create an access key for the new user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication â To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security, paste the external ID into the policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_external_id>\" } } } ] } Replace <atlan_external_id> with the external ID you want to use. Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: connectors data authentication Previous Amazon DynamoDB Next Crawl Amazon DynamoDB Create IAM policy IAM permissions Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/set-up-s3",
    "text": "Connect data Storage Amazon S3 Get Started Set up Amazon S3 On this page Set up Amazon S3 This guide walks you through creating IAM permissions and authentication credentials to allow Atlan to catalog your S3 buckets and objects. warning This integration catalogs only S3 buckets and objects. It doesn't support data lineage. Prerequisites â Before you begin: Set up S3 inventory reports , required only if you plan to use inventory-based ingestion . Permissions required â To complete this setup, you'll need: AWS Administrator access to create IAM policies and users/roles in AWS Management Console Atlan workflow access to configure connectors and workflows in Atlan Access to configure S3 inventory reports only if you plan to use inventory ingestion Create IAM policy â Choose the appropriate policy depending on your ingestion method. Direct ingestion Inventory ingestion In AWS, go to IAM â Policies Click Create policy Select the JSON tab and paste: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessToBuckets\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListAllMyBuckets\" , \"s3:ListBucket\" , \"s3:GetObject\" , \"s3:GetEncryptionConfiguration\" , \"s3:GetBucketVersioning\" ] , \"Resource\" : [ \"arn:aws:s3:::<s3_bucket_1>\" , \"arn:aws:s3:::<s3_bucket_1>/*\" , \"arn:aws:s3:::<s3_bucket_2>\" , \"arn:aws:s3:::<s3_bucket_2>/*\" ] } ] } Replace <s3_bucket> with your actual bucket name or pattern. Click Next , name your policy (e.g. AtlanS3CrawlerDirectPolicy ), and create it. In AWS, go to IAM â Policies Click Create policy Select the JSON tab and paste: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowInventoryAccess\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" , \"s3:GetObject\" , \"s3:SelectObjectContent\" ] , \"Resource\" : [ \"arn:aws:s3:::<s3_bucket>\" , \"arn:aws:s3:::<s3_bucket>/*\" ] } ] } Replace <s3_bucket> with your actual bucket name or pattern. Click Next , name your policy (e.g. AtlanS3CrawlerInventoryPolicy ), and create it. Set up authentication â Choose between IAM user (simpler) and IAM role (more secure and recommended for production). IAM user IAM role In AWS, go to IAM â Users Click Add users , give a name (e.g. atlan-s3-crawler ) Select Attach policies directly and choose the policy you just created Complete the steps and create an access key Save the Access Key ID and Secret Access Key â you'll need them in Atlan Contact Atlan support for the Node Instance Role ARN of your Atlan EKS cluster In AWS, go to IAM â Roles â Create role Select Trusted entity type: AWS account Enter Atlanâs AWS account ID (available via support) Attach the policy you created earlier Name the role (e.g. AtlanS3CrawlerRole ) and create it Edit the trust relationship with this policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" } ] } Share the role name and AWS account ID with Atlan support Once Atlan confirms access, copy the Role ARN (e.g. arn:aws:iam::<account-id>:role/<role-name> ) for use in the workflow warning Wait for confirmation from Atlan before proceeding to workflow configuration. Need help? â Check AWS IAM documentation for detailed reference Contact Atlan support for help with setup or integration Next steps â Crawl S3 assets : Configure your workflow and crawl S3 assets. Tags: connectors data crawl storage amazon-s3 aws Previous Amazon S3 Next Set up Inventory reports Prerequisites Permissions required Create IAM policy Set up authentication Need help? Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/how-tos/set-up-anomalo",
    "text": "Connect data Data Quality & Observability Anomalo Get Started Set up Anomalo On this page Set up Anomalo Atlan supports the API authentication method for fetching metadata from Anomalo . This method uses an API key to fetch metadata. Your Anomalo Deployment Admin Superuser must also configure an Atlan integration in your Anomalo deployment to send events to Atlan when your checks run in Anomalo. This will update the check metadata in Atlan in real time. This configuration is required only after you have completed integrating Anomalo in Atlan . You will need your Atlan hostname and an API token generated in Atlan . Generate an API key â Who can do this? You must at least have an Anomalo Viewer role to generate an API key . Atlan will require read-only access to your connected data sources in Anomalo. Did you know? Atlan does not make any API requests or queries that will update the objects in your Anomalo environment. You will need to create an API key in Anomalo for integrating with Atlan. To create an API key for crawling Anomalo : Log in to your Anomalo instance. From the left menu of your Anomalo instance, click Settings . On the Settings page, in the Account tab, change to the API keys tab. On the API keys page, to generate a new API key: If you have existing API keys, click the Add an API key button. If you do not have any API keys, click the Create an API key button. In the New API Key Â dialog, enter the following details: For Description , add a meaningful description for your API key   -  for example, Atlan connection . For Expiration , keep the default selection or select a preferred option. Click Save to finish creating the API key. From the corresponding screen, copy the API Key value and store it in a secure location. danger The API key cannot be retrieved later. You must copy the key value before closing the dialog box. Tags: connectors data integration api authentication configuration Previous Anomalo Next How to integrate Anomalo Generate an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue",
    "text": "Connect data ETL Tools AWS Glue Get Started Set up AWS Glue On this page Set up AWS Glue warning ð¤ Who can do this? You will need your AWS Glue Data Catalog administrator to run these commands   -  you may not have access yourself. Did you know? Prefixing all resources created for Atlan with atlan- will help you better identify them. You should also add AWS tags and descriptions to these resources for later reference. Atlan supports fetching metadata from AWS Glue Data Catalog . If you also want to be able to preview and query the data, you can set up an Amazon Athena connection instead. Create IAM policy â To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"glue:GetTables\" , \"glue:GetDatabases\" , \"glue:GetTable\" , \"glue:GetDatabase\" , \"glue:SearchTables\" , \"glue:GetTableVersions\" , \"glue:GetTableVersion\" , \"glue:GetPartition\" , \"glue:GetPartitions\" , \"glue:GetUserDefinedFunctions\" , \"glue:GetUserDefinedFunction\" ] , \"Resource\" : [ \"arn:aws:glue:<region>:<account_id>:tableVersion/*/*/*\" , \"arn:aws:glue:<region>:<account_id>:table/*/*\" , \"arn:aws:glue:<region>:<account_id>:catalog\" , \"arn:aws:glue:<region>:<account_id>:database/*\" ] } ] } Replace <region> with the AWS region of your Glue instance. Replace <account_id> Â with your account ID. danger If you're using AWS Lake Formation to manage access to your AWS resources, you will need to grant permissions in AWS Lake Formation as well as to the objects you want to crawl. Choose authentication mechanism â Using the policy created above, configure one of the following options for authentication. User-based authentication â To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On theÂ Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication â To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security: Generate the external ID within Atlan . Paste the external ID into the policy (replace <atlan_generated_external_id> with it): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_generated_external_id>\" } } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: connectors data crawl Previous AWS Glue Next Crawl AWS Glue Create IAM policy Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/how-tos/set-up-bigid",
    "text": "Connect data Privacy & Security BigID Get Started Set up BigID On this page Set up BigID Create a BigID system user account and generate an API token for Atlan to access BigID metadata. This guide walks through creating a custom role, system user, and API token. Permissions required â To successfully set up BigID for Atlan integration, confirm that your user role has the necessary permissions: BigID : Administrator access to create roles and system users Create custom role â Create a custom role for Atlan to access BigID metadata. Follow these steps that provide privileges only to read metadata of assets and not the actual data contained in Catalog objects. Log in to your BigID instance Navigate to Settings â Access Management â Roles Click Add New Role Give a meaningful and unique name. For example, Atlan Integration as the role name Select root as scope. Add the following permissions: Catalog : Read, Export, Get Attributes Value, View Sensitive Values, Manual Fields (Read), Business Attributes (Read) Data Sources : Read Policies : Read Security Posture : Read Click Save Create system user â Atlan uses a system user to authenticate and retrieve metadata from BigID. Follow these steps to create a system user: Navigate to Settings â Access Management â System Users Click Add New Role Fill in the required user details Click Connect Roles and select the Atlan Integration role Click Save Generate API token â Atlan uses the API token in Workflow configure to autheticate with BigID. Follow these steps to generate an API token for the system user: Select the system user you just created In the details panel, click Generate under Tokens Set the token expiry period and click Generate Copy and save the token securely for use in Atlan workflow configuration Click Save Need help â If you encounter issues during the BigID setup process: BigID documentation : Refer to the BigID documentation for detailed information about roles, system users, and API tokens Contact Atlan support : For issues related to Atlan integration, contact Atlan support Next steps â Crawl BigID Tags: connectors data crawl privacy bigid Previous BigID Next Crawl BigID Permissions required Create custom role Create system user Generate API token Need help Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-client-credentials-setup",
    "text": "Connect data CRM Salesforce Get Started Set up Salesforce Set up client credentials flow On this page Set up client credentials flow Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). Atlan supports the Salesforce OAuth 2.0 client credentials flow for server-to-server integration. This flow enables Atlan to authenticate using a dedicated integration user and an external client app in Salesforce, providing secure, non-interactive access to Salesforce metadata and data for crawling. Prerequisites â Before you begin, make sure you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Create custom profile â A custom profile defines the specific permissions and access levels for your integration user. You'll create this profile with the minimum necessary permissions for Atlan to crawl your Salesforce data securely. Create a custom profile to manage permissions for the integration user: From Setup , enter profiles in the Quick Find box and select Profiles . Click New Profile . Select Standard User from the Existing Profile dropdown to clone. Enter a name, for example AtlanIntegrationProfile . Click Save . On the new profile page, click Edit . Under Connected App Access , check the External Client App you create. Under Administrative Permissions , uncheck all except: API Enabled View All Data Run Reports Under Standard Object Permissions and Custom Object Permissions , select Read and View All for all items. Click Save . Create integration user â The integration user acts as the identity that Atlan uses to connect to Salesforce. This dedicated user ensures secure, auditable access separate from individual user accounts. Create a dedicated Salesforce user for the external client app: From Setup , expand Administration â Users and click Users . Click New User . Enter required details: First Name , Last Name , Username , Email , Nickname . Select Salesforce for User License . Assign the custom profile created in the previous step. Click Save . Create external client app â The external client app provides the OAuth infrastructure for secure server-to-server authentication. This app generates the credentials that Atlan uses to authenticate without requiring user interaction. Set up the external client app for client credentials flow: From Setup , enter external client app manager in Quick Find and select External Client App Manager . Click New External Client App . Enter: External Client App Name : for example, AtlanIntegration Contact Email : your email Distribution State : Local Expand API (Enable OAuth Settings) : Check Enable OAuth Set Callback URL : https://localhost (placeholder, unused) Move the following scopes to Selected OAuth Scopes : Manage user data via APIs (api) Perform requests at any time (refresh_token, offline_access) Access Lightning applications (lightning) Under Flow Enablement , check Enable Client Credentials Flow . Enable: Require Secret for Web Server Flow Require Secret for Refresh Token Flow Optional hardening: Require Proof Key for Code Exchange (PKCE) Enable Refresh Token Rotation Issue JSON Web Token (JWT)-based access tokens Click Create . On the app details page, copy the Consumer Key (Client ID) and Consumer Secret from OAuth Settings . Store these credentials securelyâthey're required to configure the Atlan connection. Configure policies â After creating the external client app, you need to configure its security policies. These policies control which users and profiles can access the app and define the authentication flow settings. From External Client App Manager , locate your app and click Edit . Open the Policies tab. Set Start Page to None . Move the integration custom profile to Selected Profiles . If using permission sets, move relevant sets to Selected Permission Sets . In OAuth Policies , set: Permitted Users : Admin approved users are pre-authorized OAuth Start URL : leave blank unless required In OAuth Flows and External Client App Enhancements : Check Enable Client Credentials Flow Run As (Username) : enter the integration user username Set additional policies as required (IP Relaxation, Session Timeout, Refresh Token Policy) Click Save . Troubleshooting â If you encounter issues with Client Credentials authentication, see Troubleshooting Salesforce Connectivity . Next steps â Crawl Salesforce to configure the connection in Atlan. Tags: connectors salesforce authentication Previous Set up JWT bearer flow Next Set up username-password flow Prerequisites Create custom profile Create integration user Create external client app Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry",
    "text": "Connect data Event/Messaging Confluent Schema Registry Get Started Set up Confluent Schema Registry On this page Set up Confluent Schema Registry Who can do this? You will probably need your Schema Registry administrator to complete these steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Confluent Schema Registry. This method uses an API key and secret to fetch metadata. Create an API key â Did you know? Atlan does not make any API requests or queries that will update the resources in your Confluent Cloud Schema Registry environment. To create an API key for crawling Confluent Schema Registry : Log in to your Confluent Cloud instance. From the left menu of the Cloud Console , click Environments and then select your environment. On your environment page, under Stream Governance API in the right menu, for Endpoint , click the clipboard icon to copy the endpoint and store it in a secure location. Under Credentials in the right menu, click View & manage if there are any existing API keys or click 0 keys if there are none to open the API credentials dialog. In the API credentials dialog, click + Add key if there are any existing API keys or click Create key if there are none to create a new schema registry API key. From the Create a new Schema Registry API key dialog: For Key , click the clipboard icon to copy the API key and store it in a secure location. For Secret , click the clipboard icon to copy the API secret and store it in a secure location. (Optional) For Description , enter a description for the new API key   -  for example, Atlan integration API key . Did you know? You will need the schema registry endpoint, API key, and API secret for crawling Confluent Schema Registry . Tags: connectors data crawl api authentication Previous Confluent Schema Registry Next Crawl Confluent Schema Registry Create an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/how-tos/set-up-cratedb",
    "text": "Connect data Databases SQL Databases CrateDB Get Started Set up CrateDB On this page Set up CrateDB This guide provides steps to set up a dedicated database user and configure permissions to enable Atlan to securely connect to your CrateDB cluster and extract metadata for data discovery and governance. Prerequisites â Before you begin, make sure you have: Administrative access to your CrateDB cluster Network connectivity between Atlan and your CrateDB instance Your CrateDB cluster's HTTP endpoint and port information Permission required â Before setting up the CrateDB connector, you need: Administrative access to your CrateDB cluster to create users and grant permissions To run the setup statements for each schema you want to crawl Create database user â Create a dedicated database user with the necessary permissions for Atlan to access your CrateDB cluster securely. Create a dedicated user for Atlan with basic authentication: CREATE USER atlan_user WITH ( password = '<password>' ) ; Replace <password> with a secure password for the atlan_user . Grant DQL permissions on the target schema: GRANT DQL ON SCHEMA < schema > TO atlan_user ; Replace <schema> with the schema to which the user needs access. Grant DQL permissions on system schemas for metadata access: GRANT DQL ON SCHEMA information_schema TO atlan_user ; GRANT DQL ON SCHEMA sys TO atlan_user ; Configure authentication â Choose the authentication method that best fits your security requirements and infrastructure setup. Basic authentication Certificate-based authentication To create a username and password for basic authentication: CREATE USER atlan_user WITH ( password = '<password>' ) ; GRANT DQL ON SCHEMA < schema > TO atlan_user ; Replace <password> with the password for the atlan_user user you are creating. Certificate-based authentication is available for Enterprise customers. Grant permissions â Grant the necessary permissions to enable Atlan to access metadata and data from your CrateDB cluster. Important DQL permissions grant both metadata access and data read access. CrateDB doesn't support metadata-only access. System schema permissions : Grant DQL permissions on system schemas for metadata access: GRANT DQL ON SCHEMA information_schema TO atlan_user ; GRANT DQL ON SCHEMA sys TO atlan_user ; These system schemas contain CrateDB metadata: information_schema : Standard SQL metadata tables sys : CrateDB-specific system tables for cluster and table information Data access permissions : Grant DQL permissions on user schemas for data preview and querying: GRANT DQL ON TABLE < schema > . * TO atlan_user ; Replace <schema> with the name of the schema you want Atlan to access. Configure connection â To connect Atlan to your CrateDB cluster, youâll need to provide the following details during setup: Host : The HTTP endpoint of your CrateDB cluster. For example, https://your-cluster.crate.io Username : The database username created in the Create database user section. Password : The secure password set for the database user created in the Create database user section. Database : The name of the CrateDB database to crawl. Enter the specific database for metadata extraction. Troubleshooting â If you encounter connection or authentication issues, see Connection issues . Next steps â Crawl CrateDB - Extract metadata from your CrateDB instance Tags: connectors cratedb database setup Previous CrateDB Next Crawl CrateDB Prerequisites Permission required Create database user Configure authentication Grant permissions Configure connection Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/how-tos/set-up-dagster",
    "text": "Connect data Orchestration & Workflow Dagster Get Started Set up Dagster On this page Set up Dagster Private Preview Configure Dagster to send lineage and asset metadata to Atlan. This setup enables automatic capture of Dagster assets and their lineage relationships. Prerequisites â Before you begin, make sure you have: Admin access to your Dagster instance Admin access to your Atlan workspace An Atlan API key. Learn how to create an API key Configure Dagster â Follow these steps to enable the Atlan integration in Dagster so lineage events from your Dagster assets flow into Atlan. Contact Dagster support to enable the Atlan integration. Share the following details with Dagster support: Your Atlan API key. Your Atlan tenant domain. For example, https://[your-org].atlan.com/ . The connection name you plan to create in Atlan. After the integration is enabled, Dagster assets automatically send lineage events to Atlan. Next steps â Crawl Dagster assets : Create a crawler workflow in Atlan to capture lineage from Dagster Tags: connectors lineage dagster Previous Dagster Next Crawl Dagster assets Prerequisites Configure Dagster Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud",
    "text": "Connect data ETL Tools dbt Get Started Set up dbt Cloud On this page Set up dbt Cloud Who can do this? You will probably need your dbt Cloud administrator to complete these steps   -  you may not have access yourself. If you have a dbt Cloud account, Atlan can help enrich your assets with dbt metadata. danger To enable Atlan to fetch metadata for dbt models defined in your project, you must add the dbt docs generate command to the list of commands in the job run steps. This will produce a catalog.json file containing all the relevant metadata. Alternatively, you can select the Generate docs on run checkbox to automatically generate updated project docs each time a job runs. Refer to dbt documentation to learn more. Create a token â Be sure to copy the generated token for crawling dbt . Service account token â Only dbt Cloud administrators can generate service account tokens. This is required for authenticating as a service account user and to set up granular access permissions. To generate a service account token, follow the steps in dbt documentation and configure the following permissions: Team plans: add Read-only access to all projects you want to integrate into Atlan. This permission is required to authorize requests to both the dbt Cloud Administrative API and dbt Cloud Discovery API . Enterprise plans: add Job Viewer access to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata. Learn more about dbt Cloud Enterprise permissions . Personal access token â danger User API tokens will be deprecated and replaced with account-scoped personal access tokens by October 22, 2024. If you have configured any dbt crawler workflows in Atlan with user API tokens, you may encounter errors. You must modify the configuration for any existing workflows with updated credentials   -  either a service account or personal access token. You can also create an account-scoped personal access token for crawling dbt . To generate a personal access token, follow the steps in dbt documentation and note the following: The user creating the token must have Job Viewer access to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata. Tags: connectors data crawl model Previous dbt Next Set up dbt Core Create a token"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/how-tos/set-up-domo",
    "text": "Connect data BI Tools Cloud-based BI Domo Get Started Set up Domo On this page Set up Domo Who can do this? You will need your Domo administrator to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Domo. This method uses the following to fetch metadata: Client ID Client secret Access token DomoStats dataset IDs Create client ID and secret â danger You will need your Domo Admin to create a client ID and secret for crawling Domo datasets and dataset columns . If the user creating the client credentials does not have admin privileges, only datasets will be crawled. You will need your Domo instance name to create a client ID and secret. This will be the name preceding your domo.com URL. For example, in the case of company.domo.com , the instance name will be company . To create a client ID and secret: Log in to the Domo Developer Portal with your Domo instance name   -  for example, company . Once you have entered your instance name, enter your Domo user credentials when prompted. On the Create new client page, enter the following details: For Name , enter a meaningful name for the client application   -  for example, Atlan_connection . (Optional) For Description , enter a brief description for the client application. For Application Scope , check the following boxes to assign the minimum permissions required to crawl Domo : Data -  this scope allows Atlan to access the DataSet API . Dashboard -  this scope allows Atlan to access the Page API . Click Create to complete the client application. After you have registered the client application, you will be redirected to the Manage Clients page to view your newly provisioned client ID and secret. Copy the values for Client ID and Secret and store them in a secure location. (Optional) To access your client ID and secret at a later time, navigate to the Domo Developer Portal homepage, and then from the left menu, click Manage clients to view and manage your existing client credentials. Generate access token â You will need to create an access token that will allow Atlan to generate upstream lineage for Domo datasets . Only a Domo Admin default security role or a custom role with either the Manage All Company Settings or Manage All Access Tokens grant enabled can generate access tokens. If you do not have either of these privileges, request an access token from your Domo administrator. To generate an access token: Log in to your Domo instance as a Domo administrator. From the tabs along the top of your Domo homepage, click More and then click Admin . On the Admin settings page, under Authentication , click Access tokens . In the upper right of the Manage access tokens page, click the Generate access token button. To specify the token information: For Access token description , enter a meaningful name for your token   -  for example, Atlan_connection_token . For Search users , search for and select the user to assign the token. danger Access tokens are associated with specific user accounts and grant the same access as the user who generated the token. If the user's permissions change, the access token will reflect the same. For Expire after , select an expiration date for the token. Click Generate to generate the access token. From the corresponding screen, copy the access token and store it in a secure location. The token will not be displayed again after you leave the Manage access tokens page. (Optional) To revoke the access token, follow the steps in Domo documentation . Set up DomoStats connector â The DomoStats connector allows you to import usage metadata from your Domo instance. This connector is only available to Domo administrators. Due to limitations of the Domo APIs, you will need to set up the DomoStats connector to crawl metadata for Domo cards and catalog asset relationships between cards and dashboards as well as datasets and cards in Atlan. The DomoStats connector allows you to build datasets with the required metadata. Once you have created the datasets, Atlan will require the dataset IDs to fetch the metadata from DomoStats. All three DomoStats dataset IDs are required to crawl Domo . To set up DomoStats: Log in to your Domo instance as a Domo administrator. From the tabs along the top of your Domo homepage, click Appstore . In the search bar, search for and select DomoStats . From the right panel of the DomoStats page, click the Get the Data button. In the Create DomoStats DataSet page, you will need to create three DomoStats datasets for card, card-dashboard relationship, and dataset-card relationship metadata. Except step 1 below, the remaining steps will be the same for all three datasets. Â Under Details , for Report , click the dropdown and then: Click Cards to create a dataset for card metadata. Click Card Pages to create a dataset for card-dashboard relationship metadata. Click Card Datasource to create a dataset for dataset-card relationship metadata. Click Next to proceed. Under Scheduling , you can either keep the predefined update schedule or define a custom schedule, and then click Next . Under Name & Describe Your DataSet , enter the following details: For Dataset Name (Required) , enter a meaningful name for your dataset. For Dataset Description (Optional) , enter a brief description for your dataset. For Add Dataset to Cloud , keep the default Domo selection. Click Save to finish setup. Once you have completed setting up a dataset, from the corresponding page, copy the dataset ID from the URL for all three datasets and store them in a secure location. For example, if the dataset URL is company.domo.com/datasource/ae6440eb-bef6-414a-a0e5-a5b2d58d1234/details/overview , then the dataset ID will be: ae6440eb-bef6-414a-a0e5-a5b2d58d1234 . Tags: connectors data crawl authentication Previous Domo Next Crawl Domo Create client ID and secret Generate access token Set up DomoStats connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery Get Started Set up Google BigQuery On this page Set up Google BigQuery Who can do this? You must be a Google BigQuery administrator to run these commands. For more information, see Google Cloud's Granting, changing, and revoking access to resources . Atlan extracts metadata from Google BigQuery through read-only access. Once you have crawled metadata for your Google BigQuery assets, you can mine query history to construct lineage. If you have enabled sample data preview or querying, asset previews and queries will be cost-optimized for tables only. For views and materialized views , Atlan will send you a cost nudge before you run the preview or query the data   -  learn more here . You must create a service account to enable Atlan to extract metadata from Google BigQuery . To create a service account, you can either use: Google Cloud console Google Cloud CLI Permissions â Atlan requires the following permissions to extract metadata from Google BigQuery. (Required) For metadata crawling â To configure permissions for crawling metadata, add the following permissions to the custom role: bigquery.datasets.get enables Atlan to retrieve metadata about a dataset. bigquery.datasets.getIamPolicy enables Atlan to read a dataset's IAM permissions. bigquery.jobs.create enables Atlan to run jobs (including queries) within the project. danger Without this, Atlan can't query the source. bigquery.routines.get enables Atlan to retrieve routine definitions and metadata. bigquery.routines.list enables Atlan to list routines and metadata on routines. bigquery.tables.get enables Atlan to retrieve table metadata. bigquery.tables.getIamPolicy enables Atlan to read a table's IAM policy. bigquery.tables.list enables Atlan to list tables and metadata on tables. bigquery.readsessions.create enables Atlan to create a session to stream large results. bigquery.readsessions.getData enables Atlan to retrieve data from the session. bigquery.readsessions.update enables Atlan to cancel the session. resourcemanager.projects.get enables Atlan to retrieve project names and metadata. (Optional) To add data preview and querying â To configure permissions for previewing and querying data, add the following permissions to the custom role: bigquery.tables.getData enables Atlan to retrieve table data. danger This permission is also required for retrieving metadata such as the row count and update time of a table. bigquery.jobs.get enables Atlan to retrieve data and metadata on any job, including queries. bigquery.jobs.listAll enables Atlan to list all jobs and retrieve metadata on any job submitted by any user. bigquery.jobs.update enables Atlan to cancel any job, including a running query. (Optional) To add query history mining â To configure permissions for mining query history, add the following permissions to the custom role: bigquery.jobs.listAll enables Atlan to fetch all queries for a project. bigquery.jobs.get enables Atlan to access query text for queries. (Optional) To crawl tags â To configure permissions for crawling Google BigQuery tags and policy tags , add the following permissions to the custom role: resourcemanager.tagKeys.list enables Atlan to fetch all tag keys. resourcemanager.tagValues.list enables Atlan to fetch all tag values for tag keys. datacatalog.taxonomies.list enables Atlan to fetch all policy tag taxonomies. datacatalog.taxonomies.get enables Atlan to fetch all policy tag taxonomies. Google Cloud console â Create a custom role â You will need to create a custom role in the Google Cloud console for integration with Atlan. To create a custom role: Open the Google Cloud console . From the left menu under IAM and admin , click Roles . Using the dropdown list at the top of the page, select the project in which you want to create a role. From the upper left of the Roles page, click Create Role . In the Create role page, enter the following details: For Title , enter a meaningful name for the custom role   -  for example, Atlan User Role . (Optional) For Description , enter a description for the custom role. For ID , the Google Cloud console generates a custom role ID based on the custom role name. Edit the ID if necessary   -  the ID cannot be changed later. (Optional) For Role launch stage , assign a stage for the custom role   -  for example, Alpha , Beta , or General Availability . Click Add permissions to select the permissions you want to include in the custom role. In the Add permissions dialog, click the Enter property name or value filter and add the required and any optional permissions. Click Create to finish custom role setup. Once you have created a custom role, you will need to create a service account and add your custom role to it. To create a service account: Open the Google Cloud console . From the left menu under IAM and admin , click Service accounts . Select a Google Cloud project. From the upper left of the Service accounts page, click Create Service Account . For Service account details , enter the following details: For Service account name , enter a service account name to display in the Google Cloud console. For Service account ID , the Google Cloud console generates a service account ID based on this name. Edit the ID if necessary   -  the ID cannot be changed later. (Optional) For Service account description , enter a description for the service account. Click Create and continue to proceed to the next step. For Grant this service account access to the project , enter the following details: Click the Select a role dropdown and then select the custom role you created in the previous step   -  for example, Atlan User Role . Click Continue to proceed to the next step. Click Done to finish the service account setup. Create service account key â Once you have created a service account, you will need to create a service account key for crawling Google BigQuery . To create a service account key: Open the Google Cloud console . From the left menu under IAM and admin , click Service accounts . Select the Google Cloud project for which you created the service account. On the Service accounts page, click the email address of the service account that you want to create a key for. From the upper left of your service account page, click the Keys tab. On the Keys page, click the Add Key dropdown and then click Create new key . In the Create private key dialog, for Key type , click JSON and then click Create . This will create a service account key file. Download the key file and store it in a secure location   -  you will not be able to download it again. Google Cloud CLI â Prerequisites â You will need to set up the Google Cloud CLI in any one of the following development environments: Cloud Shell -  to use an online terminal with the gcloud CLI already set up, activate Cloud Shell: To launch a Cloud Shell session from the Google Cloud console, open the Google Cloud console , and from the top right, click the Activate Cloud Shell icon. A Cloud Shell session will start and display a command-line prompt. It can take a few seconds for the session to initialize. Local shell -  to use a local development environment, install and initialize the gcloud CLI. Create a custom role â To create a custom role with the requisite and any optional permissions, run the following command: gcloud iam roles create atlanUserRole --project=<project_id> \\ --title=\"Atlan User Role\" --description=\"Atlan User Role to extract metadata\" \\ --permissions=\"bigquery.datasets.get,bigquery.datasets.getIamPolicy,bigquery.jobs.create,bigquery.readsessions.create,bigquery.readsessions.getData,bigquery.readsessions.update,bigquery.routines.get,bigquery.routines.list,bigquery.tables.get,bigquery.tables.getIamPolicy,bigquery.tables.list,resourcemanager.projects.get\" \\ --stage=ALPHA Replace <project_id> with the project ID of your Google Cloud project. Create a service account â To create a service account, run the following command: gcloud iam service-accounts create atlanUser \\ --description=\"Atlan Service Account to extract metadata\" \\ --display-name=\"Atlan User\" To add your custom role to your service account, run the following command: gcloud projects add-iam-policy-binding <project_id> \\ --member=\"serviceAccount:atlanUser@<project_id>.iam.gserviceaccount.com\" \\ --role=\"atlanUserRole\" Replace <project_id> with the project ID of your Google Cloud project. Create a service account key â To create a service account key, run the following command: gcloud iam service-accounts keys create  <key_file_path> \\ --iam-account=atlanUser@<project_id>.iam.gserviceaccount.com\" Replace <key_file_path> with path to a new output file for the private key   -  for example, ~/atlanUser-private-key.json . Replace <project_id> with the project ID of your Google Cloud project. danger Due to limitations at source, Atlan currently does not support generating lineage using the bq cp commands   -  for example, bq cp <source-table> <destination-table> . Tags: connectors data crawl Previous Google BigQuery Next How to enable SSO for Google BigQuery Permissions Google Cloud console Google Cloud CLI"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/how-tos/set-up-gcs",
    "text": "Connect data Storage Google GCS Get Started Set up Google Cloud Storage On this page Set up Google Cloud Storage This guide walks you through setting up Google Cloud Storage (GCS) to enable secure data ingestion from your GCS buckets. This guide walks you through setting up Google Cloud Storage (GCS) to enable secure data ingestion from your GCS buckets. The connector catalogs GCS buckets and objects only. Prerequisites â GCS bucket containing the data you want to ingest Appropriate permissions to create service accounts and manage IAM roles Permissions required â Make sure you (or an administrator) can assign the following IAM roles to the service account that the connector uses: Storage Bucket Viewer ( roles/storage.bucketViewer ) Storage Object Viewer ( roles/storage.objectViewer ) You also need permission to create a service account and generate its key. Create a service account â Select your project from the project dropdown. Creating a dedicated service account avoids using personal credentials and lets you manage access centrally. In the left navigation menu, go to IAM & Admin > Service accounts . Select Create service account . Enter a name for your service account (for example, atlan-gcs-connector ). Add an optional description. Select Create and continue . Assign roles and permissions â Add the following roles to your service account: These roles grant read-only access so the connector can discover buckets and objects without modifying data. Storage Bucket Viewer : Lets you read bucket details ( storage.buckets.list ). Storage Object Viewer : Lets you list objects and read object metadata ( storage.objects.list ). Select Done . Generate a service account key â In the left navigation menu, go to IAM & Admin > Service accounts . The JSON key file is used by the connector to authenticate to GCP programmatically. Select Create key . Download and store the key file securely. Select JSON as the key type. Select Create . Download the JSON file and store it securely. Configure bucket permissions â Navigate to Cloud Storage . Grant the service account access to every bucket you want Atlan to crawl. Select your bucket. Go to the Permissions tab. Select Add principal . Enter your service account email (for example, [email protected] ). Assign the Storage Object Viewer role. Select Save . Need help â If you run into issues during the GCS setup process: GCP documentation : Refer to the Google Cloud IAM documentation for detailed information about roles and permissions. Contact Atlan support : For issues related to Atlan integration, contact Atlan support . Next steps â Crawl GCS assets : Follow this guide to configure the crawler workflow and ingest metadata from your GCS buckets. Tags: connectors data crawl storage google-gcs gcp Previous Google Cloud Storage Next Crawl GCS assets Prerequisites Permissions required Create a service account Assign roles and permissions Generate a service account key Configure bucket permissions Need help Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Get Started Set up IBM Cognos Analytics On this page Set up IBM Cognos Analytics Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps   -  you may not have access yourself. Atlan supports the following authentication methods for fetching metadata from IBM Cognos Analytics: Basic authentication -  this method uses a username and password to fetch metadata. API authentication -  this method uses a username and an API key to fetch metadata. OKTA authentication -  this method uses a username and password of OKTA to fetch metadata. Create user â To create a new user for crawling IBM Cognos Analytics : Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Manage . From the corresponding menu, click People and then click Accounts . In Accounts , under Namespaces , select your Cognos namespace to open it. From the upper right of your namespace page, click the new user icon to add a new user to the selected namespace. In the New user form, enter the following details: For Given name , enter a meaningful name for the new user. For User ID , create a username for the new user. For Password , create a password for the username. For Email , you can leave this blank. Click OK to save your configuration. The new user you created is added to the list of entries in your namespace. (Optional) Create an API key â You can also use API authentication for integrating with Atlan. In addition to the username for the new user created in the Create user section, you need an API key for authenticatingÂ the connection. To create an API key for crawling IBM Cognos Analytics : Log in to your IBM Cognos Analytics instance as the new user created in the Create user section. In the top right of your homepage, click the personal menu icon and then click Profile and settings . In the Profile and settings tab, under Advanced options , next to My API keys , click Manage . From the upper right of the My API keys page, click the Generate API key button. In the Generate API key dialog, enter the following details: For Name , enter a meaningful name for the API key. (Optional) For Description , enter a brief description. Click Next to proceed. Once the encrypted key has appeared on the screen, copy and store the value in a secure location. danger IBM Cognos Analytics doesn't store the API key, you must copy and save it. Click Done . Your new API key appears in the list of keys on the My API keys page. If you experience any functionality issues with the newly created API key, you can renew your credentials. Navigate to the Profile and settings menu, and then next to the Credentials option, click the Renew button to refresh your credentials. (Optional) Create user in OKTA â If the IBM Cognos Namespace type is \"OKTA\" and OKTA is used for login, a corresponding user must be created in OKTA to enable login to IBM Cognos via OKTA. If IBM Cognos is configured to use OKTA as the authentication provider (via the OKTA namespace type), each user must have a valid account in OKTA to successfully log in. Follow these instructions to create a new user in OKTA and assign a user type for accessing IBM Cognos Analytics: Log in to your OKTA instance with Admin credentials. From the left menu on the homepage, expand Directory and select People . Click Add Person . In the New User form, fill in the following details: Select the appropriate User Type . Enter user's personal details. Assign the user to the relevant group. Click Save to complete the process. Add user to a Cognos role â To add the new user to the Cognos Reader role: Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Manage . From the corresponding menu, click Administration console . From the tabs along the top of the IBM Cognos Administration page, click Security . In the Security tab, select the Cognos namespace. From the list of standard roles, navigate to Readers . In the Actions column for Readers , click More . This role allows read-only access to IBM Cognos Analytics, refer to the standard roles documentation to learn more. In the Perform an action - Readers screen, under Available actions , click Set members . In the Members tab, click Add to add a new entry to the list. In the Select entries (Navigate) - Readers screen, from the Available entries , select the namespace where you created the new user. In the corresponding screen, under Directory , click the Show users in the list checkbox and then select the new user you created . Click the right-arrow button, and when the entry you want appears in the Selected entries box, click OK . Set permissions â All entries such as folders, reports, modules, and more already have the Readers role assigned to them by default. You will only need to set permissions for the new user to data server connections. To set access permissions for the new user to Cognos entries: Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Data server connections . On the Data server connections page, to set permissions for each data server connection, click the vertical 3-dot icon and then click Properties . From the tabs along the top of the Properties page, click the Permissions tab. In the upper right of the Permissions page, click the + icon to add a new member. In the Add member form, select the Cognos namespace and then search for and select the Readers role. Click Add . Once you have added the role, click Save to save your configuration. Find namespace â You must have the name of your namespace where you created the new user for authenticating the connection in Atlan. There are several ways to find the name of your namespace, here is one such method. To find the namespace details where you created the new user: Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Manage . From the corresponding menu, click Administration console . From the tabs along the top of the IBM Cognos Administration page, click Security . In the Security tab, select the namespace where you created the new user . Make sure that the new user is listed in the selected namespace. From the top right of your namespace page, click the Set properties chart icon. In the Set properties (namespace) page, next to Location , click the View the search path, ID and URL link. In the View the search path, ID and URL form, under Search path , next to CAMID , the name of your namespace is shown enclosed within brackets   -  for example, CAMID(<YOUR NAMESPACE NAME>) . Copy the value for <YOUR NAMESPACE NAME> and store it in a secure location. Tags: connectors data crawl api authentication Previous IBM Cognos Analytics Next Set up on-premises IBM Cognos Analytics access Create user (Optional) Create an API key (Optional) Create user in OKTA Add user to a Cognos role Set permissions Find namespace"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/how-tos/set-up-informatica-cdi",
    "text": "Connect data ETL Tools Informatica CDI Get Started Set up Informatica CDI On this page Set up Informatica CDI Configure user authentication and gather required parameter files to enable the Informatica Cloud Data Integration connector in Atlan. This guide walks you through creating a user with the Designer role and preparing the necessary parameter files for accurate lineage generation. Prerequisites â Before you begin, make sure you have: Access to Informatica Cloud as an Org Administrator Network connectivity to your Informatica Cloud instance Admin permissions in Atlan to create connections Create user â Informatica CDI connector supports Native authentication type . Perform the following steps as an Org Administrator in Informatica Cloud to set up user authentication for the connector. Log in to Informatica IICS as an Org Admin. Go to Administrator â Users . Click the Add User button (â). Follow the steps in the official document for Creating the user . Assign the Designer role to the user. For more information, see Designer role official documentation Gather parameter files â Parameter files are used to define source or target schema and table names referenced in job definitions. You need these files to generate accurate lineage. Identify the IICS CDI Projects or Folders you plan to include in the Atlan workflow. Locate the parameter files on your Informatica Cloud Secure Agent machines where the ETL jobs run. Download the parameter files for each project or folder. These are needed when configuring the crawler in Atlan. Next steps â Crawl Informatica CDI assets : Configure and run the crawler to discover and catalog your Informatica CDI assets Tags: connectors etl-tools informatica cdi setup authentication Previous Informatica CDI Next Crawl Informatica CDI assets Prerequisites Create user Gather parameter files Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/set-up-inventory-reports-for-s3",
    "text": "Connect data Storage Amazon S3 Inventory reports Set up Inventory reports On this page Set up Inventory reports Set up inventory reports for Amazon S3 to enable inventory-based ingestion through the crawler. This guide shows you how to configure inventory reports in the format required by Atlan's S3 crawler. Prerequisites â Before you begin, make sure you have: AWS permissions : Access to configure inventory reports on source buckets. Follow the official AWS documentation on inventory report configuration for permissions. Destination bucket : A dedicated S3 bucket to store inventory reports. Create destination bucket â First, create a dedicated S3 bucket to store your inventory reports. Sign in to the AWS Management Console. Navigate to S3 â Buckets . Click Create bucket . Enter a unique bucket name (for example, atlan-inventory-reports ). Make a note of the bucket name as itâs required when configuring the Atlan workflow for inventory-based ingestion. Select the appropriate region (keep this consistent for all inventory reports). Make a note of the region as itâs required when configuring the Atlan workflow for inventory-based ingestion. Configure other settings as needed. Click Create bucket . Configure inventory reports â Now configure inventory reports for each S3 bucket you want to catalog in Atlan. Navigate to S3 â Buckets . Select the source bucket you want to catalog. Go to the Management tab. Scroll down to Inventory configurations Click Create inventory configuration and configure the following settings: Inventory configuration name : Enter a meaningful name, such as atlan-inventory-config Inventory scope : Optionally choose a prefix to limit the report to specific objects. You can also use filters in your workflow. Object versions : Select Current version only (Atlan doesn't support Include all versions ). Configure the Report details : Destination bucket : Select the destination bucket you created earlier. Optionally specify a prefix to organize reports in a folder. Note : If you use a prefix, remember it for your Atlan workflow configuration and keep it consistent across all bucket reports. Report frequency : Choose daily or weekly. Report format : Select CSV or Apache Parquet (only these formats are supported). Status : Enable the inventory report by selecting Enabled . Encryption : Leave encryption disabled. Atlan's S3 crawler requires unencrypted inventory reports. Metadata fields : Select all available metadata fields. This ensures Atlan receives complete metadata information about your S3 objects. Review all settings and click Create . For multiple inventory reports, your destination bucket must follow a specific structure. See Inventory Report Structure for details. Need help? â If you run into issues while setting up inventory reports: AWS documentation : See the AWS S3 Inventory documentation for more information. Atlan support : If you have issues related to Atlan integration, contact Atlan support . Next steps â Once you've configured your inventory reports: Crawl your S3 assets : Follow steps to crawl your S3 assets. Tags: connectors data crawl storage amazon-s3 aws Previous Set up Amazon S3 Next Crawl S3 assets Prerequisites Create destination bucket Configure inventory reports Need help? Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-jwt-bearer-setup",
    "text": "Connect data CRM Salesforce Get Started Set up Salesforce Set up JWT bearer flow On this page Set up JWT bearer flow Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). Atlan recommends using OAuth 2.0 JWT bearer flow for secure server-to-server integration with Salesforce. This guide walks you through creating the connected app, uploading certificates, configuring policies, and preparing the integration user. Prerequisites â Before you begin, make sure you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Created a server key and certificate . Save the generated server.crt and server.key files securely. You need the server.crt file to upload to Salesforce and the server.key file to configure the connection in Atlan. Create custom profile â Create a custom profile with the Modify All Data permission to crawl all Salesforce objects, including custom objects. The View All Data permission isnât sufficient because it grants read-only access and can result in missing objects. From Setup , enter Profiles in Quick Find and select Profiles . Click New Profile and clone Standard User Enter Profile Name , for example, AtlanIntegrationProfile Click Save , then click Edit Under Connected App Access , check your connected app Under Administrative Permissions , select: API Enabled View All Data Run Reports Under Standard Object Permissions and Custom Object Permissions , select Read and View All Click Save Create integration user â Follow these steps to create a dedicated user account for Atlan integration and assign the custom profile. From Setup , expand Users under Administration Click Users Click New User Enter required fields: First Name , Last Name , Username , Email , Nickname Set User License : Salesforce Set Profile : custom profile created in the Create custom profile section Click Save Integration user requires Salesforce license to crawl metadata in Atlan. If license is unavailable, check allowed license limit: Salesforce user licenses Create connected app â A connected app enables Atlan to authenticate with Salesforce using OAuth 2.0. This section guides you through creating the app and configuring OAuth settings. Log in to Salesforce. Click settings icon , then click Setup . In Setup , enter App Manager in Quick Find and select App Manager . Click New Connected App . Under Basic Information , enter: Connected App Name : AtlanConnector API Name : automatically populated Contact Email : your email Under API (Enable OAuth Settings) : Check Enable OAuth Settings Enter Callback URL : your domain. For example, https://localhost Add Selected OAuth Scopes : Access Lightning applications (lightning) Manage user data via APIs (api) Perform requests at any time (refresh_token, offline_access) Check Use digital signatures Click Choose File and upload server.crt Click Save , then Continue On connected app page, click Manage Consumer Details and copy Consumer Key ( client_id ) and Consumer Secret Before proceeding, wait approximately 10 minutes for connected app activation Edit policies â Configure OAuth policies to control who can access the connected app and from where. These settings provide secure access for Atlan's integration. From Setup , enter Manage Connected Apps in Quick Find and select Manage Connected Apps . Locate your connected app and click Edit Policies . Under OAuth Policies : Set Permitted Users to Admin approved users are pre-authorized Set IP Relaxation to Relax IP restrictions If needed, set Refresh Token Policy to Refresh token is valid until revoked Click Save Add server certificate â To add the server certificate ( server.crt ) file to the connected app: From Setup , enter app manager in the Quick Find box and select App Manager . Locate your connected app, and then click the dropdown arrow and select Edit . For API Enable OAuth Settings , check Use digital signatures . Click Choose File and upload the server.crt file. Click Save . Assign profile â Assign the custom profile to the connected app so the integration user has the required permissions when accessing Salesforce. Open connected app page Scroll to Manage Profile Select the custom profile created in the Create custom profile section and click Save Troubleshooting â If you encounter issues with JWT Bearer authentication, see Troubleshooting Salesforce Connectivity . Next steps â Crawl Salesforce : Configure and run your first crawl to discover Salesforce data and metadata Tags: connectors salesforce authentication Previous Set up Salesforce Next Set up client credentials flow Prerequisites Create custom profile Create integration user Create connected app Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion",
    "text": "Connect data ETL Tools Matillion Get Started Set up Matillion On this page Set up Matillion Configure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance. This setup creates a dedicated API user with read-only access to guarantee secure metadata extraction. Important This setup guide applies only to Matillion ETL . Matillion Data Productivity Cloud (DPC) isn't supported. Prerequisites â Before you begin, make sure you have: Matillion Server Admin access : You need administrator privileges to create users and manage permissions Matillion instance access : Ability to log in to your Matillion instance Create user â To create a new user for crawling Matillion : Log in to your Matillion instance. From the top header of your Matillion instance, click Admin , and then from the dropdown, click User Configuration . On the User Configuration page, click the + button to add a new user. In the Add user dialog, enter the following details: For Username , enter a username for the new user. For Password and Repeat Password , enter a password for the new user and confirm it in the next step. For Role , click API to enable the new user to use the Matillion APIs. For Permission Groups , click Reader to enable the new user permission to only view the project and almost all parts of the instance including API profiles, credentials, OAuths, jobs, and variables - without edit access. Click OK to add the new user. On the User Configuration page, click Apply changes to confirm new user creation. Did you know? Atlan only reads metadata from Matillion and never updates or changes any objects in your instance. Set permissions â To set permissions for the new user in Matillion: Log in to your Matillion instance. From the top header of your Matillion instance, click Admin , and then from the dropdown, click Manage Permissions . In the Permissions dialog, for the Reader group, click the pencil icon to grant permissions for the group. In the group-specific Permissions - Reader dialog, for User Permissions , under State , click the dropdown and then click Granted . Read permission is now available to members of the group at the project level and can override a Forbidden Expected State . Click OK to confirm your selections. Next steps â Crawl Matillion - Use the credentials you just created to establish a connection and crawl metadata from Matillion Tags: connectors etl_tools matillion setup Previous Matillion Next Crawl Matillion Prerequisites Create user Set permissions Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs",
    "text": "Connect data Event/Messaging Microsoft Azure Event Hubs Get Started Set up Microsoft Azure Event Hubs On this page Set up Microsoft Azure Event Hubs Atlan supports the following authentication methods for Microsoft Azure Event Hubs: SAS key -  this method uses a connection string-primary key to fetch metadata. Service principal -  in addition to a connection string-primary key, this method requires a client ID, client secret, and tenant ID to fetch metadata. SAS key authentication â Who can do this? You will need your Microsoft Azure Event Hubs administrator to complete these steps   -  you may not have access yourself. Create a shared access signature policy â You will need to create a shared access signature (SAS) policy in Microsoft Azure Event Hubs for authentication in Atlan. The Manage permission is required for the following: Atlan requires read permissions of the configurations set to event hubs and the event hub namespace. Since Atlan currently only supports SAS policy-based authentication, Manage permission is required to provide this type of access. SAS policies do not support granular access control while Send or Listen permission is insufficient to crawl configuration metadata. Granular permissions will only be available once Atlan supports other authentication methods that allow for the granular access control capabilities of Microsoft Azure. To fetch the Azure Event Hub status attribute and Azure Event Hub consumer group assets through the Azure APIs. To create a SAS policy for crawling Microsoft Azure Event Hubs : Log in to the Azure portal . Open the menu and search for or click Event Hubs . On the Event Hubs page, click the namespace of your event hub. Copy your Event Hubs Namespace to use for authentication in Atlan . In the left menu of your event hub namespace, under Settings , click Shared access policies . On the _Shared access policie_s page, click + Add to add a new SAS policy. In the Add SAS policy sidebar, enter the following details: For Policy name , enter a meaningful name   -  for example, Atlan integration policy . To add the Manage permission to your SAS policy, click Manage . Click Create to finish setup. On the _Shared access policie_s page, select the newly created SAS policy. From the corresponding SAS Policy dialog, under Connection string-primary key , click the clipboard icon to copy the connection string-primary key and store it in a secure location. You will need your event hub namespace and the connection string-primary key for crawling Microsoft Azure Event Hubs . Service principal authentication â Who can do this? You will need your Microsoft Azure Event Hubs administrator to create a shared access signature policy and Cloud Application Administrator or Application Administrator to register an app with Microsoft Entra ID and add it to the Event Hubs Data Sender role -  you may not have access yourself. You need the following to authenticate the connection in Atlan: Connection string-primary key -  required to crawl Kafka assets Client ID (application ID), client secret , and tenant ID (directory ID)   -  required to crawlÂ Microsoft Azure Event Hubs assetsÂ Create a shared access signature policy â Follow the steps in Create a shared access signature policy to generate a connection string-primary key for crawling Microsoft Azure Event Hubs . Register app with Microsoft Entra ID â You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Add app to Event Hubs Data Sender role â You will need to add the service principal application created in the previous step to the Azure Event Hubs Data Sender role . To add a service principal to the Azure Event Hubs Data Sender role: Log in to the Azure portal . Open the menu and search for or click Event Hubs . On the Event Hubs page, click the namespace of your event hub. From the left menu of your event hubs namespace page, click Access Control (IAM) . In the upper right of the Access Control (IAM) page, navigate to the Add a role assignment tile and then click Add . On the Add a role assignment page, enter the following details: For Role , click the dropdown to select Azure Event Hubs Data Sender -  this allows send access to Azure Event Hubs resources . For Assign access to , click the dropdown to select Azure AD user, group, or service principal . For Select , choose the service principal application you created in the previous step. Click Save to save your role assignment. Tags: connectors data authentication Previous Microsoft Azure Event Hubs Next Crawl Microsoft Azure Event Hubs SAS key authentication Service principal authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Get Started Set up Microsoft Azure Synapse Analytics On this page Set up Microsoft Azure Synapse Analytics Atlan supports crawling the followingÂ with the Microsoft Azure Synapse Analytics package: Dedicated SQL pools (formerly SQL DW) Serverless SQL pools Atlan supports the following authentication methods for fetching metadata from Microsoft Azure Synapse Analytics: Basic authentication -  this method uses a username and password to fetch metadata. Service principal authentication -  this method requires a client ID, client secret, and tenant ID to fetch metadata. Basic authentication â Who can do this? You will need your Microsoft Azure Synapse Analytics administrator to run these commands   -  you may not have access yourself. Create a login â You must create a login within the master database for the new user. To create a login for the new user: CREATE LOGIN < login_name > WITH PASSWORD = '<password>' ; Replace <login_name> with the name of the login. Replace <password> with the password for the login. Create a user â You will need to create a new user for integrating with Atlan . To create a user for the newly created login : CREATE USER < username > FOR LOGIN < login_name > ; Replace <username> with the username to use when integrating Atlan. Replace <login_name> with the name of the login used in the previous step. Crawl assets and mine view lineage â You will need to connect to the target database that you want to crawl in Atlan . The following grant crawls all your Microsoft Azure Synapse Analytics assets and mines lineage for views. To grant the minimum permissions required to crawl assets and mine view lineage from a SQL pool: GRANT VIEW DEFINITION ON DATABASE :: < database_name > TO < username > ; Replace <database_name> with the name of the database. You must grant these permissions to all the databases you want to crawl in Atlan. Replace <username> with the username created above . Service principal authentication â Register app with Microsoft Entra ID â Who can do this? You will need your Cloud Application Administrator or Application Administrator to complete these stepsÂ   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization. You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID , and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Create a service principal user â To create a service principal user: CREATE USER < service_principal_display_name > FROM EXTERNAL PROVIDER ; Replace <service_principal_display_name> with the name of the service principal you created in the previous step. Grant SQL permissions â To grant SQL permissions to the service principal : GRANT VIEW DEFINITION ON DATABASE :: < database_name > TO < service_principal_display_name > ; Replace <database_name> with the name of the database. Replace <service_principal_display_name> with the name of the service principal you created . Assign Synapse RBAC role â Who can do this? You will need your Synapse Administrator to complete these steps   -  you may not have access yourself. To assign a Synapse role-based access control (RBAC) role to the service principal: Open Synapse Studio and log in to your Synapse workspace. From the left menu of your Synapse workspace, click the Manage tab. Then from under Security ,Â click Access control . From the options along the top of the Access control page, click + Add . In the Add role assignment tab, enter the following details: For Scope , select Workspace Â as the scope. For Role , select Synapse Artifact User as the Synapse RBAC role to assign. The Synapse Artifact User role provides read access to published code artifacts and their outputs. Although it can create new artifacts, it can neither publish changes nor run code without additional permissions. For Select user , search for and select the service principal you created . Click Apply to assign the Synapse RBAC role to the service principal. Mine query history â danger Atlan currently only supports mining query history for dedicated SQL pools with the Microsoft Azure Synapse Analytics miner . Mining query history for serverless SQL pools is currently not supported. To mine query history from Microsoft Azure Synapse Analytics, complete these steps. Enable query store â The Query Store is disabled by default for new Microsoft Azure Synapse Analytics databases. It stores 7 days of query history by default, which can be extended to 30 days. To enable the Query Store for mining query history in Atlan , run the following T-SQL command: ALTER DATABASE < database_name > SET QUERY_STORE = ON ; Replace <database_name> with the name of the database. Grant permissions â To mine query history, grant the following permissions: Basic authentication : GRANT VIEW DATABASE STATE TO < username > Replace <username> with the username you created for basic authentication. Service principal authentication : GRANT VIEW DATABASE STATE TO < service_principal_display_name > Replace <service_principal_display_name> with the name of the service principal you created for service principal authentication. Find your SQL pool server â To find the server name of your SQL pool for crawling Microsoft Azure Synapse Analytics : Open Synapse Studio . On the login page, select Synapse Workspace . From the left menu of your Synapse workspace, click the Manage tab. Then from under Analytics pools ,Â click SQL pools . On the SQL pools page, under Name , select your SQL pool. In the Properties form,Â navigate to Workspace Â SQL endpoint and copy the server name of your SQL pool and save it in a temporary location. Tags: connectors data crawl authentication Previous Microsoft Azure Synapse Analytics Next Mine Microsoft Azure Synapse Analytics Basic authentication Service principal authentication Mine query history Find your SQL pool server"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy",
    "text": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy Get Started Set up MicroStrategy On this page Set up MicroStrategy Who can do this? You will probably need your MicroStrategy administrator to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata. Create user in MicroStrategy â You will need to create a new user in your MicroStrategy Workstation and assign minimum permissions for integrating with Atlan. To create a new user for crawling MicroStrategy : Open the Workstation window with the navigation pane in smart mode. From the left navigation menu, click Users and Groups . In the upper left of the Users and Groups page, click the Select an Environment dropdown and select your environment. In the left menu of your selected environment, next to All Users , click the + button to create a new user. In the Create New User dialog: For Account and Credentials , enter the following details: For Full Name , enter a meaningful name for the new user. For Email Address , enter an email address for the new user. (Optional) For Description , enter a description. For Username (Login) , enter a username for the new user. For Password , create a password for the new user and confirm it in the next step. To disallow the new user from changing the password, check the User cannot change password box. At the bottom left of the form, check the Active User box. For User Groups , all users are automatically members of the Everyone group, which typically has read permission for most objects. To assign any permissions not inherited from the default group to the new user: In the top right of User Groups , click Manage User Group to add a new user group. Click Update to confirm your selections. To assign user privileges , in the left menu, clickÂ Privileges and check the following boxes: Use Architect Editors Â   -  for fetching attribute, fact, and table definitions Use Library Web -  for fetching project metadata Web Report SQL -  for fetching SQL statements Web use Metric Editor -  for fetching metric definitions Web run Document -  for fetching document definitions Web run Dossier -  for fetching dossier definitions Click Save to complete setup. Tags: connectors data crawl authentication Previous MicroStrategy Next Crawl MicroStrategy Create user in MicroStrategy"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/how-tos/set-up-mode",
    "text": "Connect data BI Tools Cloud-based BI Mode Get Started Set up Mode On this page Set up Mode Who can do this? You will probably need your Mode administrator to follow the below steps   -  you may not have access yourself. The Mode administrator will also need to be a connection admin for every connection you want Atlan to be able to crawl. Invite a user â To invite a user for Atlan to use when integrating with Mode : From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click Invite to Mode... . For Email Address , enter a valid email address, for example for the service account. Click the Invite button. In your service account's email, open the email from Mode and click Accept invite . For Set up your account , enter details about the service account: For Full name , enter a name for the service account, such as Atlan Crawler . For Username , enter a username for the service account, such as atlan_crawler . For Password and Confirm password , enter the same password to use for the service account. At the bottom of the form, click the Continue button. danger If you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email. Set permissions â To set the minimum permissions required to crawl Mode : Log into Mode as an administrator again. (If you just completed the steps above, you'll need to log out from the service account first.) From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then Workspace settings . Under the People heading on the left, click Members . Next to the Search box, click the dropdown and select Current members . Confirm the user you invited is listed with Member under the Status column. If not, change the Search box dropdown to Pending members and confirm the invitation has been accepted. If yes, change the Search box dropdown to Former members & requests , click the three-dots icon to the far right of the service account's row, and then Reinvite to org . Under the Data heading on the left, click Manage Connections . From the Manage Connections table, for each connection you want to access in Atlan: Click the row for that connection. Change to the Permissions tab. At the top of the Connection access table, click the Add members button. Search for and select the service account user, and change the dropdown for access type to View . Learn more about the View permission in Mode documentation . At the bottom of the form, click the Add members button. Did you know? Atlan does not make any API requests or queries that will update the workspaces, collections, reports, charts, or queries in your Mode instance. Generate API token â Atlan supports the following API tokens generated in Mode for authentication in Atlan: Workspace token Member token Personal token Workspace token â Workspace tokens allow admin access to the workspace. You will need to be an admin user in Mode to create and manage a workspace token. To generate a workspace API token for crawling Mode : Log in to Mode as an administrator. From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click Workspace settings . Under the Features heading on the left, click API Keys . On the API Keys page, under Workspace API Keys , click the Create API key button. In the Create new API key dialog, enter the following details: For Display name , enter a meaningful name   -  for example, atlan-crawler . For Key expiration , keep the default selection or set a longer expiration period. Click the Create button. From the corresponding Key secret dialog, copy the values for Key ID and Secret and store them in a secure location. You will not be able to see them again in Mode after leaving this screen. Member token â danger Before you can create a member token, you will need your Mode administrator to enable Member API key creation . Member tokens match an individual user's permissions to access workspace resources in Mode. To generate a member API token for crawling Mode : Log in to Mode as a member. From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click Workspace settings . Under the Workspace heading on the left, click Personal . Under the Personal heading on the left, click My API Keys . In the upper right of the API Keys Â page, click the Create API key button. In the Create new API key dialog, enter the following details: For Display name , enter a meaningful name   -  for example, atlan-crawler . For Key expiration , keep the default selection or set a longer expiration period. Click the Create button. From the corresponding Key secret dialog, copy the values for Key ID and Secret and store them in a secure location. You will not be able to see them again in Mode after leaving this screen. Personal token â danger Mode will deprecate personal token use on February 28, 2025. You can currently continue to use existing personal API tokens , but you will not be able to generate new personal tokens. Tags: connectors crawl Previous Mode Next Crawl Mode Invite a user Set permissions Generate API token"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/how-tos/set-up-mongodb",
    "text": "Connect data Databases NoSQL Databases MongoDB Get Started Set up MongoDB On this page Set up MongoDB Who can do this? Atlan currently only supports integration with MongoDB Atlas . You will need your MongoDB Organization Owner or Project Owner to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a username and password to fetch metadata. You will also need the following connection details from your MongoDB database deployment for integrating with Atlan : Host name of your MongoDB database Host name of the SQL (or JDBC) endpoint of your MongoDB database obtained via Data Federation Name of the default database Name of the authentication database Create database user in MongoDB â You will need to create a database user in MongoDB to allow Atlan to crawl MongoDB . A database user's access is determined by the role assigned to that user. You can either: Create a database user with a built-in role -  provides read-only access to all databases. Create a database user with a custom role -  provides restricted access to selected databases and requires allowed actions. Create database user with built-in role â To add a database user with a built-in role for crawling MongoDB : Sign in to your MongoDB database. From the left menu of the Data Services page, under the Security heading, click Database Access . In the upper right of the Database Access page, click Add New Database User . In the Add New Database User dialog, enter the following details: For Authentication Method , keep the default Password . For Password Authentication , there are two text fields: Enter a username for the new database user in the top text field   -  for example, atlan_user . Enter a password in the lower text field or click the Autogenerate Secure Password button to copy and use an auto-generated password. To assign database privileges to the new user, for Database Privileges , under Built-in Role , click the Add Built-in Role dropdown to select a built-in role : From the Select role dropdown, click Only read any database to assign read-only access to your MongoDB database(s). (Optional) By default, users can access all the clusters and federated database instances in the project. To restrict access to specific clusters and federated database instances: Toggle on Restrict Access to Specific Clusters/Federated Database Instances . For Grant Access To , check the boxes next to the clusters and federated database instances to which you want to grant access to the new database user. At the bottom of the dialog, click Add User to finish setup. Create database user with custom role â If you have a large number of databases, you can programmatically create a custom role in MongoDB using Atlas API instead   -  refer to MongoDB documentation to learn more. To add a database user with a custom role for crawling MongoDB : Sign in to your MongoDB database. From the left menu of the Data Services page, under the Security heading, click Database Access . In the Database Access page, change to the Custom Roles tab. In the upper right of the Custom Roles Â page, click Add New Custom Role . In the Add Custom Role dialog, for Custom Role Name , enter a meaningful name   -  for example, atlan_integration . For Action or Role , click Select Actions or Roles and grant the following privileges to the custom role: listDatabases , listed under Global Actions and Roles -  to list all existing databases in the cluster. sqlGetSchema , listed under Global Actions and Roles -  to retrieve collection schema generated by MongoDB Atlas Data Federation without read or find permission on the database or collection. listCollections , listed under Database Actions and Roles -  to list collections in a database. For Database , specify all the databases you want to crawl in Atlan. For Collection , you can either specify collections within selected databases or leave blank to include all. collStats , listed under Collection Actions -  to retrieve collection metadata such as average document size, document count, and more. For Database , specify all the databases you want to crawl in Atlan. For Collection , you can either specify collections within selected databases or leave blank to include all. find , listed under Collection Actions -  this action provides read permission on the data. Atlan requires this action for the MongoDB JDBC driver to validate Atlan's connection to the database. For Database , specify all the databases you want to crawl in Atlan. For Collection , you can either specify collections within selected databases, leave blank to include all, or restrict read access by specifying a nonexistent collection such as na , none , or - against a selected database. Click Add Custom Role to complete setup. In the Database Access page, change to the Database Users Â tab. In the upper right of the Database Access page, click Add New Database User . In the Add New Database User dialog, enter the following details: For Authentication Method , keep the default Password . For Password Authentication , there are two text fields: Enter a username for the new database user in the top text field   -  for example, atlan_user . Enter a password in the lower text field or click the Autogenerate Secure Password button to copy and use an auto-generated password. To assign database privileges to the new user, for Database Privileges , under Custom Roles , click the Add Custom Role dropdown. From the Select role dropdown, select the custom role you created previously. (Optional) By default, users can access all the clusters and federated database instances in the project. To restrict access to specific clusters and federated database instances: Toggle on Restrict Access to Specific Clusters/Federated Database Instances . For Grant Access To , check the boxes next to the clusters and federated database instances to which you want to grant access to the new database user. At the bottom of the dialog, click Add User to finish setup. Data Federation enables a SQL-like interface for Atlan to interact with MongoDB. It also provides schema access to collections that are either generated automatically through sampling or manual updates. This allows Atlan to fetch metadata without read access to databases or collections through the sqlGetSchema permission. Retrieve connection details â To retrieve connection details forÂ crawling MongoDB : Sign in to your MongoDB database. From the left menu of the Data Services page, under the Overview heading, click Database . On the Database Deployment page, navigate to the database deployment you want to crawl in Atlan and click Connect . From the corresponding page, under Connect to your application : Click Drivers , and then navigate to the Add your connection string into your application code section: Copy the host name of your MongoDB database from the code snippet and store it in a secure location. For example, in mongodb://myDBReader:D1fficultP% [email protected] :27017/?authSource=admin , mongodb0.example.com will be the MongoDB native host . Close the dialog box and return to the Connect to your application Â page. Click Atlas SQL , and then navigate to the Select your driver heading: From the driver dropdown, click JDBC Driver . Navigate to the Get Connection String heading, and then for URL , copy the following connection details and store them in a secure location. As an example, jdbc:mongodb://atlas-sql-64c0b504b658f37cd67dc406-xtapf.a.query.mongodb.net/atlan_db?ssl=trueauth&Source=admin : Copy the host name of the SQL (or JDBC) endpoint of your MongoDB databaseÂ atlas-sql-64c0b504b658f37cd67dc406-xtapf.a.query.mongodb.net to enter as the SQL interface host name . Copy the name of the default database atlan_db to enter as the Default database . Copy the name of the authentication databaseÂ admin to enter as the Authentication database . Tags: connectors data integration crawl authentication Previous MongoDB Next Crawl MongoDB Create database user in MongoDB Retrieve connection details"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction",
    "text": "Connect data Data Warehouses Databricks On-premises Setup Set up on-premises Databricks lineage extraction On this page Set up on-premises Databricks lineage extraction Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Databricks instance details, including credentials. In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of lineage from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract lineage from your on-premises Databricks instance, you will need to use Atlan's databricks-extractor tool. Did you know? Atlan uses exactly the same databricks-extractor behind the scenes when it connects to Databricks in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the databricks-extractor tool â To get the databricks-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to extract lineage from Databricks: sudo docker load -i /path/to/databricks-extractor-master.tar Get the compose file â Atlan provides you with a Docker compose file for the databricks-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Databricks instance. The file is docker-compose.yaml . Define Databricks connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Databricks connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises Databricks instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract-lineage environment: <<: *databricks-defaults EXTRACT_QUERY_HISTORY: true QUERY_HISTORY_START_TIME_MS: 0 volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *extract-lineage tells the databricks-extractor tool to run. environment contains all parameters for the tool. EXTRACT_QUERY_HISTORY -  specifies whether to extract query history for the Databricks connection, in addition to lineage. The query history output can then be used to calculate usage and popularity metrics . QUERY_HISTORY_START_TIME_MS -  specifies the time in epoch milliseconds from when to extract query history. If unspecified, the extractor will extract queries for the past 30 days by default. In Databricks, the query history retains query data for the past 30 days. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Databricks connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your Databricks connections, you will need to provide a Databricks configuration file. The Databricks configuration is a .ini file with the following format: [DatabricksConfig] host = <host> port = <port>   seconds to wait for a response from the server timeout = 300   Databricks authentication type. Options: personal_access_token, aws_service_principal auth_type = personal_access_token   Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] personal_access_token = <personal_access_token>   Required only if auth_type is aws_service_principal. [AWSServicePrincipalAuth] client_id = <client_id> client_secret = <client_secret> Secure credentials â Using local files â danger If you decide to keep Databricks credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: databricks_config: file: ./databricks.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a subsection of the services section. Using Docker secrets â To create and use Docker secrets: Store the Databricks configuration file: sudo docker secret create databricks_config path/to/databricks.ini At the top of your compose file, add a secrets element to access your secret: secrets: databricks_config: external: true name: databricks_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Databricks configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets: databricks_config: external: true name: databricks_config x-templates:   ... services: databricks-lineage-example: <<: *extract-lineage environment: <<: *databricks-defaults EXTRACT_QUERY_HISTORY: true QUERY_HISTORY_START_TIME_MS: 0 volumes: - ./output/databricks-lineage-example:/output secrets: - databricks_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The databricks_config refers to an external Docker secret created using the docker secret create command. The name of this service is databricks-lineage-example . You can use any meaningful name you want. The <<: *databricks-defaults sets the connection type to Databricks. The ./output/databricks-lineage-example:/output Â line tells the extractor where to store results. In this example, the extractor will store results in theÂ ./output/databricks-lineage-example directory on the local file system. We recommend you output the extracted lineage for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: lineage data-lineage impact-analysis integration connectors security access-control permissions Previous Crawl on-premises Databricks Next Set up an AWS private network link to Databricks Prerequisites Get the compose file Define Databricks connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-on-premises-microsoft-azure-synapse-analytics-miner-access",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Get Started Set up on-premises Microsoft Azure Synapse Analytics miner access On this page Set up on-premises Microsoft Azure Synapse Analytics miner access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Microsoft Azure Synapse Analytics instance details, including credentials. In some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to mine query history from the Query Store . For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the mining of query history from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Once you have mined query history on-premises and uploaded the results to S3 , you can mine query history in Atlan: How to mine Microsoft Azure Synapse Analytics Prerequisites â To mine query history from your on-premises Microsoft Azure Synapse Analytics instance, you will need to use Atlan's synapse-miner tool. Did you know? Atlan uses exactly the same synapse-miner behind the scenes when it connects to Microsoft Azure Synapse Analytics in the cloud. danger If you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the synapse-miner tool â To get the synapse-miner tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to mine Microsoft Azure Synapse Analytics: sudo docker load -i /path/to/synapse-miner-master.tar Get the compose file â Atlan provides you with a configuration file for the synapse-miner tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Microsoft Azure Synapse Analytics instance. The file is docker-compose.yml . Define database connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Microsoft Azure Synapse Analytics connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises Microsoft Azure Synapse Analytics instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *mine environment: <<: *synapsedb USERNAME: <USERNAME> PASSWORD: <PASSWORD> HOST: <HOST> PORT: <PORT> DATABASE: <DATABASE> volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *mine tells the synapse-miner tool to run. environment contains all parameters for the tool: USERNAME -  specify the database username. PASSWORD -  specify the database password. HOST -  specify the database host. PORT -  specify the database port. DATABASE -  specify the database name. volumes specifies where to store results. In this example, the miner will store results in the ./output/connection-name folder on the local file system. You can add as many Microsoft Azure Synapse Analytics connections as you want. Did you know? Docker's documentation describes the services format in more detail. Secure credentials â Using local files â danger If you decide to keep Microsoft Azure Synapse Analytics credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. Using Docker secrets â To create and use Docker secrets: Create a new Docker secret: printf \"This is a secret password\" | docker secret create my_database_password - At the top of your compose file, add a secrets element to access your secret: secrets: my-database-password: external: true Within the service section of the compose file, add a new secrets element and specify PASSWORD_SECRET_PATH to use it as a password. Example â Let's explain in detail with an example: secrets: my-database-password: external: true x-templates:   ... services: my-database: <<: *mine environment: <<: *synapsedb USERNAME: <USERNAME> PASSWORD_SECRET_PATH: \"/run/secrets/my_database_password\"   ... volumes:   ... secrets: - my-database-password volumes: jars: Tags: connectors data Previous Mine Microsoft Azure Synapse Analytics Next Crawl Microsoft Azure Synapse Analytics Prerequisites Get the compose file Define database connections Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/set-up-on-premises-teradata-miner-access",
    "text": "Connect data Databases SQL Databases Teradata Mine Set up on-premises Teradata miner access On this page Set up on-premises Teradata miner access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Teradata instance details, including credentials. In some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the mining of query history from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Once you have mined query history on-premises and uploaded the results to S3 , you can mine query history in Atlan: How to mine Teradata Prerequisites â To mine query history from your on-premises Teradata instance, you will need to use Atlan's teradata-miner tool. Did you know? Atlan uses exactly the same teradata-miner behind the scenes when it connects to Teradata in the cloud. danger If you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the teradata-miner tool â To get the teradata-miner tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to mine Teradata: sudo docker load -i /path/to/teradata-miner-master.tar Get the compose file â Atlan provides you with a configuration file for the teradata-miner tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Teradata instance. The file is docker-compose.yml . Define database connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Teradata connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises Teradata instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *mine environment: <<: *teradatadb USERNAME: <USERNAME> PASSWORD: <PASSWORD> HOST: <HOST> MARKER: \"0\" volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *mine tells the teradata-miner tool to run. environment contains all parameters for the tool: USERNAME -  specify the database username. PASSWORD -  specify the database password. HOST -  specify the database host. MARKER -  specify the timestamp from when queries should be mined. volumes specifies where to store results. In this example, the miner will store results in the ./output/connection-name folder on the local file system. You can add as many Teradata connections as you want. Did you know? Docker's documentation describes the services format in more detail. Secure credentials â Using local files â danger If you decide to keep Teradata credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. Using Docker secrets â To create and use Docker secrets: Create a new Docker secret: printf \"This is a secret password\" | docker secret create my_database_password - At the top of your compose file, add a secrets element to access your secret: secrets: my-database-password: external: true Within the service section of the compose file, add a new secrets element and specify PASSWORD_SECRET_PATH to use it as a password. Example â Let's explain in detail with an example: secrets: my-database-password: external: true x-templates:   ... services: my-database: <<: *mine environment: <<: *teradatadb USERNAME: <USERNAME> PASSWORD_SECRET_PATH: \"/run/secrets/my_database_password\"   ... volumes:   ... secrets: - my-database-password volumes: jars: Tags: connectors data Previous Mine Teradata Next What does Atlan crawl from Teradata? Prerequisites Get the compose file Define database connections Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/how-tos/set-up-prestosql",
    "text": "Connect data Databases Query Engines PrestoSQL Get Started Set up PrestoSQL On this page Set up PrestoSQL danger For Starburst Presto, we recommend using the Trino connector because the official Starburst documentation recommends using the Trino JDBC driver . Who can do this? You will probably need your Presto administrator to run these commands   -  you may not have access yourself. Atlan only supports PrestoSQL until version 349   -  PrestoDB is not supported at present. Currently, we only support basic (username and password) authentication for PrestoSQL. We recommend creating a separate user for Atlan with read-only access. Please ensure you are using frontend password authentication over HTTPS for clients. Create user in PrestoSQL â To create a new user with password file authentication follow the steps in the official Presto documentation . Grant read-only access â To grant read-only access to the user created above follow the steps in the official Presto documentation . This includes adding a list of catalogs you wish to crawl to your rules.json file, for example: { \"catalogs\" : [ { \"user\" : \"atlan\" , \"catalog\" : \"postgresql\" , \"allow\" : \"read-only\" } , { \"user\" : \"atlan\" , \"catalog\" : \"mysql\" , \"allow\" : \"read-only\" } , ... ] } Tags: connectors data authentication Previous PrestoSQL Next Crawl PrestoSQL Create user in PrestoSQL Grant read-only access"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka",
    "text": "Connect data Event/Messaging Redpanda Kafka Get Started Set up Redpanda Kafka On this page Set up Redpanda Kafka Who can do this? You will probably need your Redpanda Kafka administrator to complete these steps   -  you may not have access yourself. Atlan supports the S3 extraction method for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Create user in Redpanda Kafka â To create a new user for extracting metadata from Redpanda Kafka : Log in to your Redpanda Console and select your active cluster. From the left menu of your cluster's Overview page, click the Security tab. From the upper right of the Security page, click Create User to create a new user. In the Create User dialog, enter the following details:Â For Username , enter a name for the new user   -  for example, Atlan integration . For Password , set a password for the new user. From the Mechanism dropdown, select a SCRAM mechanism . Click Create to finish creating the new user. From the list of users on the Security page, select the newly created user to edit the associated Access Control Lists (ACLs) . In the Edit ACL dialog, enter the following details: For Topics , click Add Topic ACL to allow the following operations   - Describe and DescribeConfigs . For Consumer Groups , click Add Consumer Group ACL to allow the following operation   - Describe . For Transactional ID , click Add Transactional ID ACL to allow the following operation   - Describe . For Clusters , click Add Cluster ACL to allow the following operations   - Describe and DescribeConfigs . Once you have added all the required operations, click OK Â to finish setup. Did you know? Once you have extracted metadata on-premises and uploaded the results to S3 , you can crawl the metadata from Redpanda Kafka into Atlan. Tags: connectors data Previous Redpanda Kafka Next Crawl Redpanda Kafka Create user in Redpanda Kafka"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/set-up-salesforce",
    "text": "Connect data CRM Salesforce Get Started Set up Salesforce On this page Set up Salesforce Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). To integrate Salesforce with Atlan, you need to set up a connection between your Salesforce account and Atlan. The steps apply to both Salesforce Cloud and Financial Services Cloud (FSC). Prerequisites â Make sure the following prerequisites are met, you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Choose authentication flow â Atlan supports the following authentication flows for Salesforce, select the authentication method that best fits your use case: Recommended ð JWT Bearer Flow Secure server-to-server integration No password storage required Production-ready security Automated token refresh View Setup Guide ð Client Credentials Flow Alternative server-to-server method Client ID/Secret based No user interaction Existing credential workflows View Setup Guide ð¤ Username-Password Flow Traditional authentication method Simple setup process Widely supported No special hardware needed View Setup Guide Next steps â After completing your chosen authentication setup, proceed to crawl Salesforce to configure the connection in Atlan. Tags: connectors integration salesforce authentication Previous Salesforce Next Set up JWT bearer flow Prerequisites Choose authentication flow Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense",
    "text": "Connect data BI Tools Cloud-based BI Sisense Get Started Set up Sisense On this page Set up Sisense Who can do this? You will need your SisenseÂ administrator Â to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Note that the Sisense connector does not support Sisense for Cloud Data Teams, formerly Periscope Data. Create new user in Sisense â Did you know? Atlan does not make any API requests or queries that will update the objects in your Sisense environment. You will need to create a new user in your Sisense instance and assign the Data Admin role to the new user for integrating with Atlan. While Atlan can crawl all other Sisense asset types with the Viewer role, the Data Admin role is required to crawl and generate lineage for data model tables . Atlan uses the Datamodels API to crawl data models and data model tables from Sisense. The Viewer role does not provide access to data models. To create a new user for crawling Sisense : Log in to your Sisense instance with the Admin role. From the tabs along the top, click Admin . On the Admin page, in the System Management box, click Users . From the top right of the Users page, click + Users to add a new user. In the Add Users dialog, enter the following details: For Email , enter an email address for the new user   -  this will be the username for authenticating the connection in Atlan . (Optional) For First Name and Last Name , enter a first and last name for the new user   -  for example, Atlan_integration . For Role , click the role dropdown and then click Data Admin to assign that role to the new user. Toggle on Define Password , and for Set Password , set a password for the new user. Confirm the password in the next step. Click Save to complete new user creation. Tags: connectors data crawl api authentication Previous Sisense Next Crawl Sisense Create new user in Sisense"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/set-up-teradata",
    "text": "Connect data Databases SQL Databases Teradata Get Started Set up Teradata On this page Set up Teradata Who can do this? You will probably need your Teradata administrator to run these commands   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Teradata. This method uses a username and password to fetch metadata. To create a username and password for basic authentication for Teradata, run the following commands: Create role in Teradata â Create a role in Teradata using the following commands: CREATE role atlan_user_role Create user in Teradata â Create a new user for integrating with Atlan using the following commands: CREATE USER atlan_user FROM [ database ] AS PASSWORD = [ password ] PERM = 20000000 ; Grant access to the role or directly to the user with the following commands: GRANT SELECT ON dbc . databases TO atlan_user_role ; GRANT SELECT ON dbc . tables TO atlan_user_role ; GRANT SELECT ON dbc . TablesV TO atlan_user_role ; GRANT SELECT ON DBC . TableStatsV TO atlan_user_role ; GRANT SELECT ON dbc . columns TO atlan_user_role ; GRANT SELECT ON dbc . TableTextV TO atlan_user_role ; GRANT SELECT ON dbc . TableSizeV TO atlan_user_role ; GRANT SELECT ON DBC . ColumnsV TO atlan_user_role ; GRANT SELECT ON DBC . IndicesV TO atlan_user_role ; GRANT SELECT ON DBC . All_RI_ChildrenV TO atlan_user_role ; Grant additional permissions to mine query history with the following commands: GRANT SELECT ON dbc . dbqlogtbl TO atlan_user_role ; Grant role to user â To grant the atlan_user_role to the new user: GRANT atlan_user_role TO atlan_user ; Tags: connectors data crawl authentication Previous Teradata Next Crawl Teradata Create role in Teradata Create user in Teradata Grant role to user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot Get Started Set up ThoughtSpot On this page Set up ThoughtSpot Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps   -  you may not have access yourself. Atlan supports the following authentication methods for ThoughtSpot: Basic authentication Trusted authentication Basic authentication â You will need to create a new user in ThoughtSpot and authenticate in Atlan with username and password. Create user in ThoughtSpot â To create a new user for crawling ThoughtSpot : Log in to your ThoughtSpot instance. To navigate to the admin console, in the top header, click Admin . In the top left of the Admin page, click Add User .Â In the Add a new user dialog, enter the following details: For Username , enter a username for the new user. For Display name , add a meaningful name for the new user   -  for example, Atlan . For Sharing visibility , keep the default selection   - SHAREABLE . danger Atlan will only crawl assets that are either created by or shared with this user. If you add the user to a group in ThoughtSpot, ensure that you share all the assets you want to crawl in Atlan with that group. For New password , enter a password for the new user and confirm it in the next step. For Email , enter an email address for the new user. Click Add to add the new user. The new user will be assigned Can upload user data and Can download data permissions by default. Trusted authentication â danger You will need ThoughtSpot Everywhere to use the trusted authentication option. ThoughtSpot Analytics users, however, can get ThoughtSpot Everywhere as an add-on to use trusted authentication. Learn more here . You will need to create a secret key in ThoughtSpot and authenticate in Atlan with username and secret key. Create a secret key â To create a secret key for crawling ThoughtSpot : Log in to your ThoughtSpot instance. To navigate to the developer console, in the top header, click Develop . In the left menu under Customizations , clickÂ Security settings .Â In the top right of the Security settings page, click Edit . Scroll down to Trusted authentication and turn it on. Turning on trusted authentication will generate a secret key. Click the clipboard icon to copy the secret key and store it in a secure location. Click Save Changes to save your selections. Tags: connectors crawl authentication Previous ThoughtSpot Next Set up on-premises ThoughtSpot access Basic authentication Trusted authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-username-password-setup",
    "text": "Connect data CRM Salesforce Get Started Set up Salesforce Set up username-password flow On this page Set up username-password flow Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). Atlan supports the Salesforce OAuth 2.0 username-password flow for scenarios where server-to-server integration isn't possible. This flow enables Atlan to authenticate using the integration user's credentials along with a connected app in Salesforce. Prerequisites â Before you begin, make sure you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Create connected app â To enable Atlan to connect to your Salesforce instance, you need to create a connected app. This app provides the OAuth credentials that Atlan uses for authentication. Log in to Salesforce. In the upper right, click the settings icon and select Setup . In Setup , enter apps in the Quick Find box and select App Manager . Click New Connected App in the upper-right corner. Under Basic Information , provide: Connected App Name : for example, AtlanConnector Contact Email : your email address Under API (Enable OAuth Settings) : Check Enable OAuth Settings Check Enable for Device Flow Set Callback URL to any domain, for example, https://localhost (unused) Add the following Selected OAuth Scopes : Access Lightning applications (lightning) Manage user data via APIs (api) Perform requests at any time (refresh_token, offline_access) Check Require Secret for Web Server Flow Check Require Secret for Refresh Token Flow Click Save . On the resulting page, click Continue . Under API (Enable OAuth Settings) , click Manage Consumer Details and copy: Consumer Key (Client ID) Consumer Secret (Client Secret) Wait approximately 10 minutes for the connected app to activate. Retrieve security token â Salesforce requires a security token in addition to your password for API access. You'll need to retrieve this token from your user settings and append it to your password when configuring the connection in Atlan. In Salesforce, click the integration user's user icon in the upper-right and select Settings . Expand My Personal Information and click Reset My Security Token . Click Reset Security Token . Copy the resulting security token. Important To crawl Salesforce , enter your password + security token in the Password field (for example, password xyz and token 123 â xyz123 ). Troubleshooting â If you encounter issues with Username-password authentication, see Troubleshooting Salesforce Connectivity . Next steps â Crawl Salesforce : Configure and run your first crawl to discover Salesforce data and metadata Tags: connectors salesforce authentication Previous Set up client credentials flow Next Crawl Salesforce Prerequisites Create connected app Retrieve security token Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/troubleshooting/task-and-crawl-issues",
    "text": "Connect data ETL Tools Informatica CDI Troubleshooting Task and crawl issues On this page Task and crawl issues Resolve common connectivity and crawling issues when working with the Informatica CDI connector. Missing objects despite designer role assignment â Error Some objects are still missing after assigning the Designer role to the user. Cause â Projects/Folers/Tasks can have explicit restrictions that override role-based permissions at a parent level. Solution â Log in to Informatica CDI with the same service account configured in Atlan. Navigate to the Mapping task and export the task Repeat the same operation for the underlying Mapping If either activity fails, contact your Informatica Cloud Admin for additional permissions. Objects or folders not in workflow filter appear in assets â Error Objects or folders that aren't part of the workflow filter configuration appear in Atlan. Cause â In Informatica CDI, the Mapping Task and underlying Mappings can be present in different projects or folders. Solution â Check if unexpected object is linked to a Mapping Task that is configured as part of the filter. If so, this is designed behaviour Missing source or target lineage for mapping task â Error Source or target lineage for a particular mapping task is missing. Cause â This issue can occur when: Source or target assets referenced by the Mapping Task haven't been crawled into Atlan Parameter files used by the Mapping Task aren't uploaded or are from a different environment (DEV vs PROD) You might see debug messages like: DEBUG process - _create_process_asset_from_raw_data: Process <process_id> (Mapping Task: <mapping_task_name>): Source pattern: EDW/DEV/FINANCE_ARR_RAW, Target pattern: EDW/DEV/FINANCE_ARR_BRNZ DEBUG cache_resolver - resolve_asset_from_connection_cache: No assets found for pattern: EDW/DEV/FINANCE_ARR_BRNZ DEBUG process - _create_process_asset_from_raw_data: Skipping process <process_id> (Mapping Task: <mapping_task_name>, ID: <mapping_task_id>): Source resolved: True, Target resolved: False Solution â Verify that source or target system are supported and the corresponding crawlers are set up with the correct filters. Make sure parameter files are uploaded and match the environment/project you're crawling. Need help â If you need assistance after trying the steps, contact Atlan support: Submit a request . Tags: connectors etl-tools informatica cdi Previous What does Atlan crawl from Informatica CDI Next Tasks, transformations, and lineage Missing objects despite designer role assignment Objects or folders not in workflow filter appear in assets Missing source or target lineage for mapping task Need help"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/faq/tasks-transformations-and-lineage",
    "text": "Connect data ETL Tools Informatica CDI FAQ Tasks, transformations, and lineage On this page Tasks, transformations, and lineage Find answers to common questions about the Informatica Cloud Data Integration connector. What types of tasks does the connector support? â The connector currently supports Mapping Tasks . What transformations are explicitly processed? â The following transformations receive special handling with detailed information extraction: Source Target Expression Lookup Joiner Filter Router Aggregator What about transformations not in the above list? â Transformations that are not listed above are also processed and are taken into account for lineage calculations. How are Lookup transformations handled? â Lookup transformations work similarly to source transformations and generate lineage. For unconnected lookups, lineage is created based on the positioning of columns in the transformation. What IICS CDI API calls are executed by the connector? â The following API calls are executed: API Payload Details Description /user/login {\"@type\":\"login\",\"username\":\"***\",\"password\":\"****\"} Login to IICS Cloud /public/core/v3/objects q=type=='MTT' q=type=='PROJECT' q=type=='FOLDER' {{mappingTaskId}}/references?refType=usedBy Get listing for different objects /public/core/v3/export {\"name\":\"\",\"objects\":[{\"id\":\"{{mappingTaskId}}\",\"includeDependencies\":true}]} {\"name\":\"\",\"objects\":[{\"id\":\"{{taskfFlowId}}\",\"includeDependencies\":false}]} Run export job /public/core/v3/export/{{jobId}} â Monitor export job /public/core/v3/export/{{jobId}}/package â Download export job results /api/v2/connection/name/{{connectionName}} /api/v2/connection/ â Get connection object details Are parameter files processed for lineage? â Yes. When you upload parameter files to the Atlan Informatica workflow, the connector processes them to create accurate lineage connections. Are taskflow-level parameters processed? â Yes. The connector processes parameters defined at the Taskflow level when you upload the corresponding parameter files to the workflow. What content formats are supported for parameter files? â The connector supports parameter files that follow IICS CDI guidelines for parameterization. In what form must parameter files be uploaded? â Upload parameter files as a .zip archive with these supported MIME types: application/zip application/x-zip application/x-zip-compressed What source and target systems are supported for lineage? â Lineage is generated for sources and targets that are relational in nature, such as Snowflake and Oracle. Not supported: Files, web services, and similar non-relational systems. Is lineage generated at the field level? â Yes. The connector creates lineage at the field and column level between sources and targets. To view field-level details, expand the source or target table in the lineage diagram. If a custom query (SQL override) is used for a source transformation, is lineage generated? â Yes. The connector parses SQL queries and generates lineage based on the parsing results. What are the limitations? â Parameter file handling limitations Parameter file names defined in Taskflows aren't always applied to Mapping Tasks correctly. View supported and unsupported scenarios Supported scenarios: Parameter and directory mentioned in Assignment transformation level and type as content Parameter and directory mentioned in Data task level and type as content Unsupported scenarios: Parameter and directory mentioned in Start as input field Workaround: Create a param.meta file and upload it with your parameter files. Format the content as key-value pairs: < MappingName > = < Uploaded_Param_filename > Other limitations Mapplets: Treated as single transformations within mappings. Inner transformations aren't exposed individually. SQL parsing: Has known limitations. Incremental extraction: Not supported. The connector scans all objects in each run based on your filter configuration. Secure Agent: The connector is currently not available to run as a Secure Agent application Authentication Types: - The connector currently supports only basic authentication. Support for SAML based authentication is planned to be added. Tags: connectors etl-tools informatica cdi faq tasks transformations lineage Previous Task and crawl issues"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/concepts/transformation-logic",
    "text": "Connect data ETL Tools Informatica CDI Concepts Transformations On this page Transformations When you build mappings in Informatica Cloud Data Integration, you embed business logic in Transformation objects. These transformations determine how source data is processed to form target data. How Atlan captures transformation logic â When Atlan crawls your Informatica CDI environment, it reads key transformation logic and surfaces it for easier consumption. The following table shows the transformation types and the business logic that Atlan captures. The table also indicates where you can find this information in Atlan's interface: Transformation Type Where in Atlan Expression Transformation Field > Sidebar > Expression Aggregator - Group by columns Transformation > Sidebar > Expression Aggregator - Aggregate functions Transformation > Sidebar > Expression Router Transformation > Sidebar > Expression Joiner - Join Condition Transformation > Sidebar > Expression Joiner - Join Type Transformation > Sidebar > Expression Lookup - Lookup Condition Transformation > Sidebar > Expression Lookup - Lookup Query Transformation > Sidebar > Query Source - Source Filter Transformation > Sidebar > Query Source - Source Query Transformation > Sidebar > Query See also â What does Atlan crawl from Informatica CDI : Complete reference of all assets and metadata discovered during crawling Tags: connectors etl-tools informatica transformations logic concepts Previous Crawl Informatica CDI assets Next What does Atlan crawl from Informatica CDI How Atlan captures transformation logic See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/troubleshooting/troubleshooting-aws-glue-connectivity",
    "text": "Connect data ETL Tools AWS Glue Troubleshooting Troubleshooting AWS Glue connectivity On this page Troubleshooting AWS Glue connectivity What are the known limitations of the AWS Glue connector? â Atlan currently does not support the following: Parsing MAP type objects for columns and nested columns. What does the sizeBytes property mean in AWS Glue crawls? â In Atlan, the sizeBytes property is derived from the SizeKey parameter returned by AWS Glue. This value represents the total size of all files associated with a table, in bytes. For example, if a Glue table points to an S3 location with three files, each 1 MB in size, sizeBytes shows 3,145,728 bytes (3 Ã 1,048,576). For more information, see Parameters set on Data Catalog tables by crawler - AWS Glue . Tags: integration connectors Previous What does Atlan crawl from AWS Glue?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/troubleshooting/troubleshooting-metabase-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Metabase Troubleshooting Troubleshooting Metabase connectivity On this page Troubleshooting Metabase connectivity Why are assets missing for a collection? â Check permissions attached to the Atlan group . The group should have View permission for the collection. Why are Metabase questions missing for a dashboard? â Check the Metabase question's collection. Was the collection excluded by the metadata filters configured when crawling Metabase ? If so, you can modify the workflow and re-run it . Tags: integration connectors Previous Preflight checks for Metabase"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/troubleshooting/troubleshooting-microsoft-teams",
    "text": "Configure Atlan Integrations Collaboration Microsoft Teams Troubleshooting Troubleshooting Microsoft Teams On this page Troubleshooting Microsoft Teams Why do I get an error while adding Atlan to Microsoft Teams? â If you get an error message   -  for example, App not found -  while adding the Atlan app from the Apps page to your team in Microsoft Teams, it may be due to caching issues. In that case, youÂ can try installing the app directly from the Teams admin center. Search for the Atlan app in the Teams admin center, click the button to the left of the Atlan logo and select the Add to a team option to integrate the Atlan app with your team. Tags: integration connectors Previous Link your Microsoft Teams account Next What is included in the Microsoft Teams integration?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/troubleshooting/troubleshooting-mode-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Mode Troubleshooting Troubleshooting Mode connectivity On this page Troubleshooting Mode connectivity What are the known limitations of the Mode connector? â Atlan currently does not support the following: Lineage between Mode queries and Glue assets when connected through Trino. Tags: lineage data-lineage impact-analysis integration connectors Previous Preflight checks for Mode"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/troubleshooting/troubleshooting-redash-connectivity",
    "text": "Connect data BI Tools On-premises & Enterprise BI Redash Troubleshooting Troubleshooting Redash connectivity On this page Troubleshooting Redash connectivity Does Atlan crawl unpublished dashboards? â Atlan does not support crawling unpublished dashboards. Unlike unpublished queries, unpublished dashboards are only visible to the users who created them in Redash. Why don't I see Redash alerts? â Atlan currently does not support Redash alerts as assets. Tags: dashboards visualization analytics integration connectors alerts monitoring notifications Previous Preflight checks for Redash"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/troubleshooting/troubleshooting-scim-provisioning",
    "text": "Configure Atlan Integrations Identity Management SCIM Troubleshooting SCIM provisioning On this page Troubleshooting SCIM provisioning Atlan currently supports SCIM provisioning for the following SSO providers: Azure AD Okta What information does Atlan sync from SSO providers? â Atlan syncs the user's first name, last name, username, email ID, group information, and user status through group mapping. The username and email ID are only synced once when the user is provisioned in Atlan for the first time. Can I change the username of a provisioned user in Atlan? â No, once you have integrated SCIM in Atlan , the usernames of provisioned users will be dependent on your SCIM provider. For example, if a username has changed due to an automation at source or in the case of a migration from one provider to another, you will not be able to update usernames in Atlan. Usernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. Ensure that your username in the SCIM provider matches that in Atlan. What will happen if an SSO or Atlan group is renamed? â If SCIM provisioning is enabled and an SSO group that is mapped to Atlan is renamed, changes will sync automatically. Renaming an Atlan group does not affect SCIM functionality. What happens if an SSO group is deleted? â If an SSO group is deleted in the SSO provider, then the group mapping will also be deleted in Atlan. The corresponding group in Atlan will remain active, but all the users will be removed from that group. However, if you would like to retain the group membership for your users in Atlan, you can first delete the group mapping in Atlan and then delete your SSO group in the SSO provider. What happens if a user is deleted from the SSO provider? â If users are removed from your SSO provider, then the same users will also be deactivated in Atlan . Their status will be displayed as Disabled . To permanently delete them from Atlan, you can remove the users and transfer ownership of assets . What happens if a username already exists in Atlan? â If a user with the username user.name and email address [email protected] already exists in Atlan and another user with the same username user.name but different email address [email protected] is to be added via SSO, it will create a conflict in Atlan. The existing user will remain in Atlan while the new SSO user will not be synced. When does the SCIM token expire? â The SCIM token does not expire by default and can only be revoked if deleted. Can user removal affect the SCIM tokens that user created? â Yes, user removal will also result in the deletion of any SCIM tokens created by that user. For more guidance, see the User management FAQ . Does SCIM provisioning work only after a provisioned user has logged into Atlan? â No, SCIM provisioning works as soon as the user has been provisioned from the SSO provider. For example, even if the user is yet to log into Atlan, the user profile can be updated or the user disabled in Atlan directly from the SSO provider. If SCIM is enabled and a user has never logged into Atlan, the status of the user will be Enabled by default. Once the user has logged in, their last login activity will be displayed in the Last Active column. Can I assign SCIM provisioned users as asset owners before their first login? â Yes, you can assign asset ownership to SCIM provisioned users even if they are yet to log into Atlan for the first time. How can I manage users in Atlan? â Following are the detailed permissions for managing your users in Atlan: Permission SCIM on (SSO enforced) SCIM on (SSO not enforced) SCIM off (SSO enforced) SCIM off (SSO not enforced) Invite user from Atlan â â â â Edit user profile in Atlan â â â â Add users to Atlan groups â Only for unmapped groups â Only for unmapped groups â â Enable or disable users in Atlan â â â â Tags: integration connectors Previous How to enable Okta for SCIM provisioning Next SSO Integration"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/troubleshooting/troubleshooting-servicenow",
    "text": "Configure Atlan Integrations Project Management ServiceNow Troubleshooting Troubleshooting ServiceNow On this page Troubleshooting ServiceNow Why is the security_admin role required to complete the ServiceNow integration? â Atlan strongly recommends that a System Administrator with a security_admin role completes the integration from start to finish. However, it is not necessary and may depend on the rules or policies configured on your ServiceNow instance. For example, while a default System AdministratorÂ can create an OAuth application , they may not be permitted to modify access control lists (ACLs)Â onÂ ServiceNow. The security_admin role has elevated privileges precisely for creating or modifying ACLs.Â This is required to configure the Atlan integration in ServiceNow . What is the Atlan auto-approve workflow attached to the Atlan Data Access catalog in ServiceNow? Can it be removed? â The Atlan auto-approve workflow is a basic workflow that auto-approves the Requested Item (RITM) for requests created in the Atlan Data Access catalog. This does not mean that requests have been auto-completed. Atlan will wait for the final request state to be updated to the state selected in the governance workflow before completing the request. For example, if you have selected Closed Complete , the request state in ServiceNow must reflect that state before Atlan can mark the request as completed. Yes, you can remove the Atlan auto-approve workflow from your ServiceNow instance. You can configure the Process Engine setup for any requests in the Atlan Data Access catalog to adhere to existing processes in your organization. Why are our custom request states from ServiceNow not showing up in Atlan? â If your custom request states from ServiceNow are not showing up in Atlan, please contact Atlan support . The commit update set action is failing. How can I resolve it? â If the commit update set action is failing, please contact Atlan support as this may be due to conflicts. The Atlan team may need to generate a different update set or work with your ServiceNow System Administrator to set up the process manually. Can we modify the Atlan Data Access catalog? â Atlan strongly recommends that you refrain from modifying the Atlan Data Access catalog in ServiceNow. Removing any existing variables used by Atlan can cause errors in the request creation process. However, you can add any non-mandatory variables to the catalog. Bear in mind that Atlan will not populate such variables. Tags: integration connectors security access-control permissions Previous Link your ServiceNow account"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/troubleshooting/troubleshooting-sisense-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Sisense Troubleshooting Troubleshooting Sisense connectivity On this page Troubleshooting Sisense connectivity What are the known limitations of the Sisense connector? â Atlan currently does not support column-level lineage for Sisense assets. Tags: lineage data-lineage impact-analysis integration connectors Previous Preflight checks for Sisense"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack",
    "text": "Configure Atlan Integrations Collaboration Slack Troubleshooting Troubleshooting Slack On this page Troubleshooting Slack What do the colors in Slack notifications for modified assets mean? â These colors are based on asset certifications , which are as follows: Green = Verified Yellow = Draft Red = Deprecated Blue = No certificate How do I send messages or search assets from Slack? â Slack is integrated with Atlan . However, sending messages and searching assets from Slack are disabled. You cannot push messages from Slack to Atlan directly. You can, however, add a Slack thread to an asset profile in Atlan using the Slack icon. In the channel that you've integrated with Atlan, you can use the /search-term <term_name> command to search for glossary terms or the /search-query <saved query name> command to search for queries. Why am I getting a There was error processing the request message? â If you encounter a There was error processing the request error message while attempting to take action on a request in Slack , this means that the request is no longer active in Atlan. That may be due to one of the following reasons: The request raised was being approved by two or more approvers simultaneously. The request was deleted by the requester. Tags: integration connectors faq troubleshooting Previous Link your Slack account Next How do I send messages or search assets from Slack?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/troubleshooting/troubleshooting-spreadsheets",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets Troubleshooting Troubleshooting spreadsheets On this page Troubleshooting spreadsheets Why do I need admin consent for exporting assets to Microsoft Excel? â You are likely to require admin consent for exporting assets to Microsoft Excel in the following scenarios: If your Microsoft Entra admin has turned on the Assignment required setting for the app, each user in the organization will require admin approval to provide consent to Atlan for exporting assets. Learn more about the setting in Microsoft documentation . If the app has been uninstalled by your Microsoft Entra admin, this request will automatically reinstall the app once your admin has approved it. Why do I get a 400 status code on exporting assets? â Atlan recommends that you try again after disconnecting and then reconnecting the spreadsheet tool integration from your user profile. If the issue persists, contact Atlan support . Why does Microsoft Excel sync show that zero changes were detected? â Microsoft Excel may take longer to autosave changes on the workbook. If you notice that zero changes were detected, retry syncing after a few seconds. Does the asset export option distinguish between description and user description? â No, the Description column on the spreadsheet only shows one value, the latest description , whether you export assets from Atlan or use the Atlan extension in spreadsheets. Tags: integration connectors Previous How to update column metadata in Microsoft Excel Next Send alerts for workflow events"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/troubleshooting/troubleshooting-thoughtspot-connectivity",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot Troubleshooting Troubleshooting ThoughtSpot connectivity On this page Troubleshooting ThoughtSpot connectivity What are the known limitations of the ThoughtSpot connector? â Atlan currently does not support the following: Column-level lineage between ThoughSpot tables and views. Upstream lineage to source assets for ThoughtSpot views Tags: lineage data-lineage impact-analysis integration connectors upstream-dependencies data-sources Previous What does Atlan crawl from ThoughtSpot?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/update-column-metadata-in-microsoft-excel",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to update column metadata in Microsoft Excel On this page Update column metadata in Microsoft Excel Once you've connected Atlan with Microsoft Excel , you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel. Atlan currently supports importing and updating column metadata for the following asset types: Tables Views Materialized views Looker explores Microsoft Power BI tables Salesforce objects Tableau data sources danger You will need to be logged into your Atlan instance before you can start importing column metadata for your Atlan assets in Microsoft Excel. If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Microsoft Excel. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Import column metadata â You can import column metadata for your data assets directly into Microsoft Excel. To import column metadata for your data assets to Microsoft Excel: In the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Enrich metadata to view a list of your data assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, click the asset type   -  for example, Tableau Datasources . In the Atlan sidebar on your worksheet, you can either: Individually select the data asset(s) you want to import. To the left of the Import button, click the Select All checkbox to select all the assets that have loaded in the sidebar. (Optional) Scroll down and click Load more to load more assets in the sidebar. Click Import to import column metadata for your selected assets. The column metadata for your selected assets are now available in Microsoft Excel! ð Did you know? If any changes are made to your imported columns in Atlan, you'll need to import those columns once again to access the updated version in Microsoft Excel. Update column metadata â Once you've imported your data assets from Atlan, you can edit the metadata for your selected data asset in Microsoft Excel. You can make changes to the column metadata once all the columns have been successfully imported. You can only make changes to the metadata in the following columns: Description Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the Column Name , Data Type , Propagated Tags , and Qualified Name columns Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Bulk update tags for columns â danger You cannot make any changes to the metadata in the Propagated Tags column. Navigate to the Tags column to add tags to your column assets in Microsoft Excel: When adding tags to columns: The tag must already exist in Atlan. If the tag does not exist in Atlan, updates will not sync and you will receive an error message. Tag match is case-sensitive, ensure that the tag is formatted exactly as it exists in Atlan. For example, if the tag in Atlan is formatted as Marketing Analysis , then columns tagged with marketing analysis will not sync. You can add multiple tags in the Tags column   -  separate multiple tags with a comma , . If you have added tags that exist in Atlan as well as ones that do not, only the existing tags will be synced. The unsupported tags will not sync and you will receive an error message. Tag propagation is disabled by default in Atlan, hence tags will not be propagated. Push your changes to Atlan â Once you've made changes to the column metadata, to push your changes: In the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Sync to Atlan . The Atlan sidebar will appear once the changes have synced. (Optional) Click Open in Atlan to verify the changes. In Atlan, an Updated using Microsoft Excel stamp will appear in the activity log for updated assets. (Optional) Click theÂ Microsoft Excel Â link to view the source spreadsheet from Atlan. Did you know? You can download impacted assets for impact analysis in Microsoft Excel. Tags: connectors data integration crawl Previous How to update column metadata in Google Sheets Next Troubleshooting spreadsheets Import column metadata Update column metadata Push your changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/administration/logs/how-tos/view-event-logs",
    "text": "Configure Atlan Administration Monitoring How to view event logs On this page View event logs Who can do this? You will need to be an admin user in Atlan to view event logs. Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. You will first need to configure any of the following supported sources to receive events: Airflow/OpenLineage Amazon MWAA/OpenLineage Astronomer/OpenLineage Google Cloud Composer/OpenLineage Apache Spark/OpenLineage Anomalo Once you have configured a supported source, you can view event logs for your events from the admin center: View a list of the 20 most recently received events. For every event, you can also view the timestamp for when it was received in Atlan based on your local timezone and 24-hour time notation, name of the connector configured, and event details. Filter events by connectors â Airflow (also includes all other supported distributions, Amazon MWAA, Astronomer, and Google Cloud Composer), Apache Spark, and Anomalo. Expand any event to view the full JSON code. View event logs â You can view event logs in Atlan through two methods, depending on your role and workflow preferences: Via admin panel â To view event logs: From the left menu of any screen in Atlan, click Admin . Under the Logs heading of your admin Workspace , click Event logs . On the Event logs page, you can view a list of up to 20 most recently received events in Atlan. (Optional) Click the Connector dropdown to filter events by the connector configured: Click Airflow to view events received through Airflow , Amazon MWAA , Astronomer , or Google Cloud Composer connections. Click Spark to view events received through Apache Spark connections. Click Anomalo to view events received through Anomalo connections. (Optional) Click the refresh button to refresh event logs and view the latest events. For any event listed in the event logs, you can view the timestamp for when it was received in Atlan, name of the connector configured, and event details. (Optional) Click any event to view more details in the Event details sidebar: View the JSON code, connector name, and timestamp for the event received. When viewing the code, you can also click the brackets to collapse or expand them. Click the copy icon to copy the event details. Click the expand icon to view the JSON code in fullscreen mode. Via connection profile â caution ð¤ Who can do this? You must be an admin user in Atlan to view event logs. Navigate to Workflow > Manage . Select the Listeners tab. Click or search for the connection you want. Click the connection to open its profile. Click the Events tab. This view shows events specific to that connection. Tags: lineage data-lineage impact-analysis integration connectors Previous Restrict glossary visibility Next How to view query logs View event logs"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/references/what-does-atlan-crawl-from-aiven-kafka",
    "text": "Connect data Event/Messaging Aiven Kafka References What does Atlan crawl from Aiven Kafka? On this page What does Atlan crawl from Aiven Kafka? Atlan crawls and maps the following assets and properties from Aiven Kafka. Once you've crawled Aiven Kafka , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Topics -  Message count, size (MB), partition count, and cleanup policy filters Consumer groups -  Member count and topic name filters Topics â Atlan maps topics from Aiven Kafka to its KafkaTopic asset type. Source property Atlan property Topic name PartitionCount kafkaTopicPartitionsCount ReplicationFactor kafkaTopicReplicationFactor segment.byte kafkaTopicSegmentBytes compression.type kafkaTopicCompressionType cleanup.policy kafkaLogTopicCleanupPolicy isInternal kafkaTopicIsInternal sizeInBytes kafkaTopicSizeInBytes recordCount kafkaTopicRecordCount retention.ms kafkaTopicRetentionTimeInMs Consumer groups â Atlan maps consumer groups from Aiven Kafka to its KafkaConsumerGroup asset type. Did you know? Consumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Aiven Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset. Source property Atlan property GROUP name memberCount kafkaConsumerGroupMemberCount ReplicationFactor kafkaTopicReplicationFactor topic_names kafkaTopicNames TOPIC kafkaConsumerGroupTopicConsumptionProperties.topicName PARTITION kafkaConsumerGroupTopicConsumptionProperties.topicPartition LAG kafkaConsumerGroupTopicConsumptionProperties.topicLag CURRENT-OFFSET kafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset Tags: connectors crawl Previous Crawl Aiven Kafka Next Preflight checks for Aiven Kafka Topics Consumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/references/what-does-atlan-crawl-from-amazon-msk",
    "text": "Connect data Event/Messaging Amazon MSK References What does Atlan crawl from Amazon MSK? On this page What does Atlan crawl from Amazon MSK? Once you've crawled Amazon MSK , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Topics -  Message count, size (MB), partition count, and cleanup policy filters Consumer groups -  Member count and topic name filters Atlan crawls and maps the following assets and properties from Amazon MSK. Atlan currently only supports asset-level lineage between topics and consumer groups. Upstream, downstream, and column-level lineage are currently not supported. Topics â Atlan maps topics from Amazon MSK to its KafkaTopic asset type. Source property Atlan property Topic name PartitionCount kafkaTopicPartitionsCount ReplicationFactor kafkaTopicReplicationFactor segment.byte kafkaTopicSegmentBytes compression.type kafkaTopicCompressionType cleanup.policy kafkaLogTopicCleanupPolicy isInternal kafkaTopicIsInternal sizeInBytes kafkaTopicSizeInBytes recordCount kafkaTopicRecordCount retention.ms kafkaTopicRetentionTimeInMs Consumer groups â Atlan maps consumer groups from Amazon MSK to its KafkaConsumerGroup asset type. Did you know? Consumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Amazon MSK will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset. Source property Atlan property GROUP name memberCount kafkaConsumerGroupMemberCount ReplicationFactor kafkaTopicReplicationFactor topic_names kafkaTopicNames TOPIC kafkaConsumerGroupTopicConsumptionProperties.topicName PARTITION kafkaConsumerGroupTopicConsumptionProperties.topicPartition LAG kafkaConsumerGroupTopicConsumptionProperties.topicLag CURRENT-OFFSET kafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset Tags: connectors crawl Previous Crawl Amazon MSK Next Preflight checks for Amazon MSK Topics Consumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage/references/what-does-atlan-crawl-from-amazon-mwaa-openlineage",
    "text": "Connect data Orchestration & Workflow Amazon MWAA OpenLineage References What does Atlan crawl from Amazon MWAA/OpenLineage? On this page What does Atlan crawl from Amazon MWAA/OpenLineage? Once you have integrated Amazon MWAA/OpenLineage , you can use connector-specific filters for quick asset discovery. The following filters are currently supported: Status filter   -  last run status for an asset Duration filter   -  last run duration for an asset Atlan maps the following assets and properties from Amazon MWAA/OpenLineage. Asset lineage support depends on the list of operators supported by OpenLineage . DAGs â Atlan maps DAGs (directed acyclic graphs) from Amazon MWAA/OpenLineage to its AirflowDAG asset type. Source property Atlan property schedule_interval airflowDagSchedule timetable airflowDagScheduleDelta tags airflowTags version airflowRunVersion openlineageAdapterVersion airflowRunOpenLineageVersion runid airflowRunName run_type airflowRunType eventTime airflowRunStartTime eventTime airflowRunEndTime eventType airflowRunOpenLineageState query airflowTaskSql group_id airflowTaskGroupName Tasks â Atlan maps tasks from Amazon MWAA/OpenLineage to its AirflowTask asset type. Source property Atlan property retries airflowTaskRetryNumber pool airflowTaskPool pool_slots airflowTaskPoolSlots queue airflowTaskQueue priority_weight airflowTaskPriorityWeight trigger_rule airflowTaskTriggerRule operator_class airflowTaskOperatorClass dag_id airflowDagName conn_id `airflowTaskConnectionId` sql airflowTaskSql tags airflowTags version airflowRunVersion openlineageAdapterVersion airflowRunOpenLineageVersion runid airflowRunName run_type airflowRunType eventTime airflowRunStartTime eventTime airflowRunEndTime eventType airflowRunOpenLineageState Tags: connectors crawl Previous How to integrate Amazon MWAA/OpenLineage DAGs Tasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/references/what-does-atlan-crawl-from-anomalo",
    "text": "Connect data Data Quality & Observability Anomalo References What does Atlan crawl from Anomalo? On this page What does Atlan crawl from Anomalo? Once you have integrated Anomalo , Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check. You can use connector-specific filters for quick asset discovery. The following Anomalo filters are currently available for supported SQL assets: Has checks   -  filter SQL assets associated with Anomalo checks Check status   -  filter SQL assets by overall data quality status such as Pass or Fail Check methods   -  filter SQL assets by specific types of checks configured Number of checks   -  filter SQL assets by total count of checks configured in Anomalo Last checked   -  filter SQL assets by timestamp for when any associated checks were last scanned in Anomalo Atlan crawls and maps the following assets and properties from Anomalo. Checks â Atlan maps checks from Anomalo to its AnomaloCheck asset type. Source property Atlan property name name description description run_config._metadata.check_type anomaloCheckCategoryType run_config.check anomaloCheckType run_config._metadata.priority_level anomaloCheckPriorityLevel run_config._metadata.is_system_check anomaloCheckIsSystemAdded status anomaloCheckStatus check_status_image_url anomaloCheckStatusImageUrl completed_at anomaloCheckLastRunCompletedAt results.evaluated_message anomaloCheckLastRunEvaluatedMessage check_run_url anomaloCheckLastRunUrl results.history_message anomaloCheckHistoricRunStatus Supported sources â If you have crawled supported data sources, you can view Anomalo checks on your existing assets in Atlan: Amazon Athena Amazon Redshift Databricks Google BigQuery Microsoft Azure Synapse Analytics Microsoft SQL Server MySQL Oracle PostgreSQL PrestoSQL SAP HANA Snowflake Teradata Tags: connectors data crawl Previous How to integrate Anomalo Next Preflight checks for Anomalo Checks Supported sources"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/references/what-does-atlan-crawl-from-apache-airflow-openlineage",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage References What does Atlan crawl from Apache Airflow/OpenLineage? On this page What does Atlan crawl from Apache Airflow/OpenLineage? Once you have integrated Apache Airflow/OpenLineage , you can use connector-specific filters for quick asset discovery. The following filters are currently supported: Status filter   -  last run status for an asset Duration filter   -  last run duration for an asset Atlan maps the following assets and properties from Apache Airflow/OpenLineage. Asset lineage support depends on the list of operators supported by OpenLineage . DAGs â Atlan maps DAGs (directed acyclic graphs) from Apache Airflow/OpenLineage to its AirflowDAG asset type. Source property Atlan property Description job.name name Name of the Airflow DAG - qualifiedName Unique identifier for the DAG in Atlan description description Description of the DAG from Airflow owners sourceOwners Original owner information from Airflow - ownerUsers Validated Atlan usernames (mapped from source owners) schedule_interval airflowDagSchedule DAG's schedule interval (cron expression or preset) delta airflowDagScheduleDelta Schedule interval in seconds tags airflowTags Tags assigned to the DAG run_id airflowRunName Unique identifier for the DAG run run_type airflowRunType Type of run (scheduled, manual, backfill) eventTime (start) airflowRunStartTime Timestamp when the DAG run started eventTime (end) airflowRunEndTime Timestamp when the DAG run completed eventType airflowRunOpenLineageState Final status of the DAG run version airflowRunVersion Airflow version openlineageAdapterVersion airflowRunOpenLineageVersion OpenLineage adapter version - sourceURL Direct link to the DAG in Airflow UI - connectionName Name of the connector instance - connectionQualifiedName Unique identifier for the connector instance - connectorName Name of the connector type Did you know? If a DAG has more than 10 valid owner email addresses (comma-separated), only the first 10 will be captured and published. Tasks â Atlan maps tasks from Apache Airflow/OpenLineage to its AirflowTask asset type. Source property Atlan property Description job.name (partial) name Name of the task (extracted from full job name) - qualifiedName Unique identifier for the task in Atlan - airflowDagName Name of the parent DAG - airflowDagQualifiedName Unique identifier for the parent DAG in Atlan operator_class airflowTaskOperatorClass Type of operator used for the task conn_id airflowTaskConnectionId Connection ID used by the task sql airflowTaskSql SQL query (for SQL-based operators) owner sourceOwners Owner information from the task definition eventTime (start) airflowRunStartTime Timestamp when the task started eventTime (end) airflowRunEndTime Timestamp when the task completed eventType airflowRunOpenLineageState Final status of the task run run_id airflowRunName Unique identifier for the task run run_type airflowRunType Type of run (from parent DAG) pool airflowTaskPool Worker pool assigned to the task pool_slots airflowTaskPoolSlots Number of pool slots used by the task priority_weight airflowTaskPriorityWeight Priority weight for execution order queue Tags: connectors crawl Previous How to implement OpenLineage in Airflow operators Next Preflight checks for Apache Airflow DAGs Tasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/references/what-does-atlan-crawl-from-apache-kafka",
    "text": "Connect data Event/Messaging Apache Kafka References What does Atlan crawl from Apache Kafka? On this page What does Atlan crawl from Apache Kafka? Atlan crawls and maps the following assets and properties from Apache Kafka. Once you've crawled Apache Kafka , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Topics -  Message count, size (MB), partition count, and cleanup policy filters Consumer groups -  Member count and topic name filters Topics â Atlan maps topics from Apache Kafka to its KafkaTopic asset type. Source property Atlan property Topic name PartitionCount kafkaTopicPartitionsCount ReplicationFactor kafkaTopicReplicationFactor segment.byte kafkaTopicSegmentBytes compression.type kafkaTopicCompressionType cleanup.policy kafkaLogTopicCleanupPolicy isInternal kafkaTopicIsInternal sizeInBytes kafkaTopicSizeInBytes recordCount kafkaTopicRecordCount retention.ms kafkaTopicRetentionTimeInMs Consumer groups â Atlan maps consumer groups from Apache Kafka to its KafkaConsumerGroup asset type. Did you know? Consumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Apache Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset. Source property Atlan property GROUP name memberCount kafkaConsumerGroupMemberCount ReplicationFactor kafkaTopicReplicationFactor topic_names kafkaTopicNames TOPIC kafkaConsumerGroupTopicConsumptionProperties.topicName PARTITION kafkaConsumerGroupTopicConsumptionProperties.topicPartition LAG kafkaConsumerGroupTopicConsumptionProperties.topicLag CURRENT-OFFSET kafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset Tags: connectors crawl Previous Crawl on-premises Kafka Next Preflight checks for Apache Kafka Topics Consumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/references/what-does-atlan-crawl-from-apache-spark-openlineage",
    "text": "Connect data Orchestration & Workflow Apache Spark OpenLineage References What does Atlan crawl from Apache Spark/OpenLineage? On this page What does Atlan crawl from Apache Spark/OpenLineage? Once you have integrated Apache Spark/OpenLineage , you can use connector-specific filters for quick asset discovery. The following filters are currently supported: Status filter   -  last run status for an asset Duration filter   -  last run duration for an asset Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports. Jobs â Atlan maps jobs from Apache Spark to its SparkJob asset type. Atlan also supports column-level lineage for Spark jobs. Source property Atlan property Description job.name name Name of the Spark job - qualifiedName Unique identifier for the job in Atlan Derived from job.name sparkAppName Name of the Spark application (substring before first '.') spark.master sparkMaster Spark master URL (for example, yarn, local, and more.) - connectionQualifiedName Unique identifier for the connector instance - connectorName Name of the connector instance OpenLineage metadata â Atlan reports OpenLineage operational metadata for Spark jobs. Source Atlan property Description run.runId sparkRunId Unique run identifier run.facets.spark_version.spark-version sparkRunVersion Spark runtime version run.facets.spark_version.openlineage-spark-version sparkRunOpenLineageVersion OpenLineage library version START event timestamp sparkRunStartTime Job start time COMPLETE / ABORT / FAIL event timestamp sparkRunEndTime Job end time Final event type sparkRunOpenLineageState Status of the job (COMPLETE, FAIL, ABORT) Tags: connectors data crawl Previous How to integrate Apache Spark/OpenLineage Next Troubleshooting Apache Spark/OpenLineage connectivity Jobs OpenLineage metadata"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage/references/what-does-atlan-crawl-from-astronomer-openlineage",
    "text": "Connect data Orchestration & Workflow Astronomer OpenLineage References What does Atlan crawl from Astronomer/OpenLineage? On this page What does Atlan crawl from Astronomer/OpenLineage? Once you have integrated Astronomer/OpenLineage , you can use connector-specific filters for quick asset discovery. The following filters are currently supported: Status filter   -  last run status for an asset Duration filter   -  last run duration for an asset Atlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the list of operators supported by OpenLineage . DAGs â Atlan maps DAGs (directed acyclic graphs) from Astronomer/OpenLineage to its AirflowDAG asset type. Source property Atlan property schedule_interval airflowDagSchedule timetable airflowDagScheduleDelta tags airflowTags version airflowRunVersion openlineageAdapterVersion airflowRunOpenLineageVersion runid airflowRunName run_type airflowRunType eventTime airflowRunStartTime eventTime airflowRunEndTime eventType airflowRunOpenLineageState query airflowTaskSql group_id airflowTaskGroupName Tasks â Atlan maps tasks from Astronomer/OpenLineage to its AirflowTask asset type. Source property Atlan property retries airflowTaskRetryNumber pool airflowTaskPool pool_slots airflowTaskPoolSlots queue airflowTaskQueue priority_weight airflowTaskPriorityWeight trigger_rule airflowTaskTriggerRule operator_class airflowTaskOperatorClass dag_id airflowDagName conn_id `airflowTaskConnectionId` sql airflowTaskSql tags airflowTags version airflowRunVersion openlineageAdapterVersion airflowRunOpenLineageVersion runid airflowRunName run_type airflowRunType eventTime airflowRunStartTime eventTime airflowRunEndTime eventType airflowRunOpenLineageState Tags: connectors crawl Previous How to integrate Astronomer/OpenLineage DAGs Tasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/references/what-does-atlan-crawl-from-bigid",
    "text": "Connect data Privacy & Security BigID References What does Atlan crawl from BigID? On this page What does Atlan crawl from BigID? Once you have crawled BigID , you can use connector-specific filters for quick asset discovery. This document provides details on the metadata and assets that Atlan crawls from BigID. Attributes â Atlan maps BigID Attributes, associated with Catalog Objects as a result of scans, to the Custom Metadata Property specified in the configuration. Multiple attributes are concatenated with comma as the delimiter. Tags â Atlan sources both system-assigned and user-assigned tags on BigID Catalog Objects to source-linked Atlan Tags on the associated Atlan assets. The sourced tags can then be propagated and assigned like regular Atlan Tags. warning Assigning values for BigID-sourced Tags on Atlan and reverse-sync of those values back to BigID is currently not supported. Policy violations â Atlan maps Policy violations detected for Catalog Objects during BigID scans as Announcements on Atlan. The Announcement messages indicate the details of the Policies found violated. Once these violations are marked resolved on BigID, the corresponding Atlan Announcements are updated or removed appropriately. See also â Set up BigID Crawl BigID Tags: connectors data crawl privacy bigid Previous Crawl BigID Attributes Tags Policy violations See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/references/what-does-atlan-crawl-from-cloudera-impala",
    "text": "Connect data Databases Query Engines Cloudera Impala References What does Atlan crawl from Cloudera Impala? On this page What does Atlan crawl from Cloudera Impala? Atlan crawls and maps the following assets and properties from Cloudera Impala. This integration helps you understand and govern data stored in Impala by organizing metadata into Atlan asset types and enabling downstream visibility, including lineage. Lineage â Atlan supports the following lineage in Cloudera Impala. Asset-level and field-level lineage for Tables &rarr; Views Asset-level and field-level lineage for Views &rarr; Views Schema â Atlan maps databases from Cloudera Impala schema to its Schema asset type. Source property Atlan property owner sourceOwners comment description external_location schemaExternalLocation Tables â Atlan maps Cloudera Impala tables to its Tables asset type. Source property Atlan property rows rowCount sizeBytes sizeBytes owner sourceOwners stats.partitionCount (Only applicable for Kudu tables) partitionCount OutputFormat (Not available for Impala-managed Kudu tables for Impala 3.3+) externalLocationFormat Location externalLocation Table Type subType comment description table_query tableDefinition Views â Atlan maps tables from Cloudera Impala views to its Views asset type. Source property Atlan property rows rowCount sizeBytes sizeBytes owner sourceOwners Table Type subType KUDU (only set for Kudu tables) tableType comment description VIEW_DEFINITION definition Columns â Atlan maps columns from Cloudera Impala to its Column asset type. Source property Atlan property column_name name ENCODING (only applicable for Kudu table types) columnEncoding COMPRESSION (only applicable for Kudu table types) columnCompression PRIMARY_KEY (only applicable for Kudu tables) isPrimary ISNULLABLE isNullable TYPE dataType COMMENT description Tags: lineage data-lineage impact-analysis schema schema-drift schema-monitoring integration connectors downstream-impact dependencies Previous Crawl Cloudera Impala Next Preflight Checks for Cloudera Impala Lineage Schema Tables Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/references/what-does-atlan-crawl-from-confluent-kafka",
    "text": "Connect data Event/Messaging Confluent Kafka References What does Atlan crawl from Confluent Kafka? On this page What does Atlan crawl from Confluent Kafka? Atlan crawls and maps the following assets and properties from Confluent Kafka. Once you've crawled Confluent Kafka , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Topics -  Message count, size (MB), partition count, and cleanup policy filters Consumer groups -  Member count and topic name filters Topics â Atlan maps topics from Confluent Kafka to its KafkaTopic asset type. Source property Atlan property Topic name PartitionCount kafkaTopicPartitionsCount ReplicationFactor kafkaTopicReplicationFactor segment.byte kafkaTopicSegmentBytes compression.type kafkaTopicCompressionType cleanup.policy kafkaLogTopicCleanupPolicy isInternal kafkaTopicIsInternal sizeInBytes kafkaTopicSizeInBytes recordCount kafkaTopicRecordCount retention.ms kafkaTopicRetentionTimeInMs danger Retrieving sizeInBytes Confluent Kafka to be set up with a Cloud API key and secret . If the Cloud API key is not configured or the API key and Secret are not set , sizeInBytes will be set to 0 . Consumer groups â Atlan maps consumer groups from Confluent Kafka to its KafkaConsumerGroup asset type. Did you know? Consumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Confluent Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset. Source property Atlan property GROUP name memberCount kafkaConsumerGroupMemberCount ReplicationFactor kafkaTopicReplicationFactor topic_names kafkaTopicNames TOPIC kafkaConsumerGroupTopicConsumptionProperties.topicName PARTITION kafkaConsumerGroupTopicConsumptionProperties.topicPartition LAG kafkaConsumerGroupTopicConsumptionProperties.topicLag CURRENT-OFFSET kafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset Tags: connectors crawl Previous Crawl on-premises Kafka Topics Consumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/references/what-does-atlan-crawl-from-confluent-schema-registry",
    "text": "Connect data Event/Messaging Confluent Schema Registry References What does Atlan crawl from Confluent Schema Registry? On this page What does Atlan crawl from Confluent Schema Registry? Atlan crawls and maps the following assets and properties from Confluent Schema Registry. Subjects â Atlan maps subjects from Confluent Schema Registry to its Subject asset type. Source property Atlan property subject name schemaType schemaRegistrySchemaType id schemaRegistrySchemaId subject schemaRegistrySubjectBaseName subject schemaRegistrySubjectIsKeySchema compatibilityLevel schemaRegistrySubjectSchemaCompatibility version schemaRegistrySubjectLatestSchemaVersion schema schemaRegistrySubjectLatestSchemaDefinition Tags: schema schema-drift schema-monitoring integration connectors Previous Crawl Confluent Schema Registry Next Preflight checks for Confluent Schema Registry Subjects"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/references/what-does-atlan-crawl-from-cratedb",
    "text": "Connect data Databases SQL Databases CrateDB References What does Atlan crawl from CrateDB? On this page What does Atlan crawl from CrateDB? CrateDB uses a single-cluster architecture where all data is stored within one logical database. Unlike traditional databases that support multiple databases, CrateDB organizes data using schemas within a single cluster. This affects how Atlan maps CrateDB assets: Database : Represents the single CrateDB cluster Schemas : Organize tables, views, and other objects within the cluster Tables/Views : Store the actual data and metadata Once you have crawled CrateDB , you can use connector-specific filters for quick asset discovery. Atlan extracts and maps the following assets and properties from CrateDB during crawling. Databases â Atlan maps databases from CrateDB to its Database asset type. Source property Atlan property Description TABLE_CATALOG name Database name SCHEMA_COUNT schemaCount Number of schemas in the database Schemas â Atlan maps schemas from CrateDB to its Schema asset type. Source property Atlan property Description TABLE_SCHEMA name Schema name TABLE_COUNT tableCount Number of tables in the schema VIEW_COUNT viewsCount Number of views in the schema 'crate' (literal) databaseName Database name Tables â Atlan maps tables from CrateDB to its Table asset type. Source property Atlan property Description TABLE_NAME name Table name REMARKS description Table description COLUMN_COUNT columnCount Number of columns in the table ROW_COUNT rowCount Number of rows in the table SIZE_BYTES sizeBytes Table size in bytes IS_PARTITIONED isPartitioned Whether the table is partitioned PARTITION_STRATEGY partitionStrategy Partitioning strategy used PARTITION_COUNT partitionCount Number of partitions Table partitions â Atlan maps table partitions from CrateDB to its TablePartition asset type. Source property Atlan property Description TABLE_NAME name Partition name PARTITION_STRATEGY partitionStrategy Partitioning strategy ROW_COUNT rowCount Number of rows in the partition SIZE_BYTES sizeBytes Partition size in bytes PARTITION_COUNT partitionCount Number of sub-partitions COLUMN_COUNT columnCount Number of columns in the partition Views â Atlan maps views from CrateDB to its View asset type. Source property Atlan property Description TABLE_NAME name View name REMARKS description View description COLUMN_COUNT columnCount Number of columns in the view VIEW_DEFINITION definition SQL definition of the view Columns â Atlan maps columns from CrateDB to its Column asset type. Source property Atlan property Description COLUMN_NAME name Column name REMARKS description Column description ORDINAL_POSITION order Column position in the table DATA_TYPE dataType Data type of the column IS_NULLABLE isNullable Whether the column accepts NULL values IS_PARTITION_COLUMN isPartition Whether the column is used for partitioning PARTITION_ORDER partitionOrder Order of the column in partitioning IS_PRIMARY_KEY isPrimary Whether the column is part of the primary key COLUMN_DEFAULT defaultValue Default value for the column IS_GENERATED is_generated Whether the column is atuomatically generated Tags: connectors cratedb database metadata crawl Previous Crawl CrateDB Next Preflight checks for CrateDB Databases Schemas Tables Table partitions Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/references/what-does-atlan-crawl-from-dagster",
    "text": "Connect data Orchestration & Workflow Dagster References What does Atlan crawl from Dagster On this page What does Atlan crawl from Dagster Private Preview Dagster assets are crawled in as the FlowControlOperation Atlan type. This reference provides details about the metadata attributes that Atlan captures from Dagster and where they appear in the Atlan interface. Metadata attributes â Source Attribute Atlan Attribute Where in Atlan Asset Key name Asset profile and overview sidebar Description description Asset profile and overview sidebar Organization flowProjectName Overview sidebar Asset Group flowFolderName Asset profile and overview sidebar Latest Materialization Run Id flowRunId Overview sidebar Latest Materialization Run Timestamp flowStartedAt Overview sidebar Partition Key [ dagster/partition ] flowInputParameters.dagsterPartitionKey API only Latest Materialization Job Name flowInputParameters.dagsterJobName API only Kinds [ dagster/kind ] flowInputParameters.dagsterKinds API only Latest Materialization Error Message flowErrorMessage Overview sidebar Latest Materialization Status flowStatus Asset profile and overview sidebar Owners [ref] ownerUsers Overview sidebar Tables [ dagster/table_name ] flowDataResults Relations tab in sidebar Tags: connectors dagster lineage metadata reference Previous Crawl Dagster assets Next Dagster integration Metadata attributes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/references/what-does-atlan-crawl-from-fivetran",
    "text": "Connect data ETL Tools Fivetran References What does Atlan crawl from Fivetran? On this page What does Atlan crawl from Fivetran? Lineage â Atlan uses Fivetran's log events from the Fivetran Platform Connector to generate lineage associated with Fivetran connectors . This is particularly useful for creating lineage between different tools, such as Salesforce and Snowflake. danger The assets involved in lineage (tables, columns, objects, fields, and so on) must already be crawled by Atlan before running the Fivetran Enrichment package to enrich them. Specifically, Atlan will: Create lineage between each data asset in Atlan that is associated with a Fivetran connector. (For example, between Salesforce objects and Snowflake tables.) Atlan creates Fivetran Process objects for each data asset that is replicated. Atlan creates column-level lineage to connect the sources (inputs) and destinations (outputs) of each Process . (For example, between Salesforce fields and Snowflake columns.) Link each Process to its corresponding connector in the Fivetran UI. For any Fivetran sources or destinations that are not natively supported or are supported but have not been crawled yet, Atlan will create partial assets to provide you with a complete view of lineage. Supported sources and destinations for Fivetran Platform Connector â Sources â Atlan's Fivetran Platform Connector integration supports all sources listed in Fivetran documentation for lineage. For any sources that are not natively supported or are supported but have not been crawled yet, Atlan will create partial assets to provide you with a complete view of lineage. Note that Atlan will only catalog partial column assets and generate column-level lineage for SQL sources natively supported in Atlan . Destinations â Atlan's Fivetran Platform Connector integration supports all destinations listed in Fivetran documentation for lineage. For any destinations that are not natively supported or are supported but have not been crawled yet, Atlan will create partial assets to provide you with a complete view of lineage. Note that Atlan will only catalog partial column assets and generate column-level lineage for SQL sources natively supported in Atlan . Note that for crawler configuration , only the destinations listed here are supported. Connectors â Atlan maps the following metadata from Fivetran to its FivetranConnector asset type. This is only applicable to metadata crawled using the Fivetran Platform Connector. Source property Atlan property Where in Atlan connector.NAME fivetranConnectorConnectorName overview sidebar connector_type.ID fivetranConnectorConnectorType overview sidebar dynamically created by script fivetranConnectorConnectorURL overview sidebar destination.NAME fivetranConnectorDestinationName overview sidebar destination.TYPE fivetranConnectorDestinationType overview sidebar dynamically created by script fivetranConnectorDestinationURL overview sidebar connector.SIGNED_UP fivetranConnectorSyncSetupOn overview sidebar connector.SYNC_FREQUENCY fivetranConnectorSyncFrequency overview sidebar connector.PAUSED fivetranConnectorSyncPaused API only user.GIVEN_NAME + user.FAMILY_NAME fivetranConnectorSyncSetupUserFullName API only connector.USER_EMAIL fivetranConnectorSyncSetupUserEmail API only incremental_mar. INCREMENTAL_ROWS fivetranConnectorMonthlyActiveRowsFree overview sidebar incremental_mar. INCREMENTAL_ROWS fivetranConnectorMonthlyActiveRowsPaid overview sidebar fivetranConnectorMonthlyActiveRowsFree + fivetranConnectorMonthlyActiveRowsPaid fivetranConnectorMonthlyActiveRowsTotal API only calculated by script fivetranConnectorMonthlyActiveRowsChangePercentageFree API only calculated by script fivetranConnectorMonthlyActiveRowsChangePercentagePaid API only calculated by script fivetranConnectorMonthlyActiveRowsChangePercentageTotal overview sidebar calculated by script fivetranConnectorMonthlyActiveRowsFreePercentageOfAccount API only calculated by script fivetranConnectorMonthlyActiveRowsPaidPercentageOfAccount overview sidebar calculated by script fivetranConnectorMonthlyActiveRowsTotalPercentageOfAccount API only dynamically generated by script sourceURL overview sidebar connector_type.CREATED_AT sourceCreatedAt asset preview, profile, and filter, overview sidebar calculated from LOG table fivetranConnectorLastSyncId overview sidebar calculated from LOG table fivetranConnectorLastSyncStartedAt overview sidebar calculated from LOG table fivetranConnectorLastSyncFinishedAt overview sidebar calculated from LOG table fivetranLastSyncStatusfivetranConnectorLastSyncReason overview sidebar calculated from LOG table fivetranConnectorLastSyncTaskType overview sidebar calculated from LOG table fivetranConnectorLastSyncRescheduledAt overview sidebar calculated from LOG table fivetranConnectorLastSyncTablesSynced overview sidebar calculated from LOG table fivetranLastSyncRecordsUpdated overview sidebar calculated from LOG table fivetranConnectorLastSyncExtractTimeSeconds overview sidebar calculated from LOG table fivetranConnectorLastSyncExtractVolumeMegabytes overview sidebar calculated from LOG table fivetranConnectorLastSyncLoadTimeSeconds overview sidebar calculated from LOG table fivetranConnectorLastSyncLoadVolumeMegabytes overview sidebar calculated from LOG table fivetranConnectorLastSyncProcessTimeSeconds overview sidebar calculated from LOG table fivetranConnectorLastSyncProcessVolumeMegabytes overview sidebar calculated from LOG table fivetranConnectorLastSyncTotalTimeSeconds overview sidebar Tags: connectors data crawl api configuration Previous Crawl Fivetran Next Troubleshooting Fivetran connectivity Lineage Supported sources and destinations for Fivetran Platform Connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage/references/what-does-atlan-crawl-from-google-cloud-composer-openlineage",
    "text": "Connect data Orchestration & Workflow Google Cloud OpenLineage References What does Atlan crawl from Google Cloud Composer/OpenLineage? On this page What does Atlan crawl from Google Cloud Composer/OpenLineage? Once you have integrated Google Cloud Composer/OpenLineage , you can use connector-specific filters for quick asset discovery. The following filters are currently supported: Status filter   -  last run status for an asset Duration filter   -  last run duration for an asset Atlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the list of operators supported by OpenLineage . DAGs â Atlan maps DAGs (directed acyclic graphs) from Google Cloud Composer/OpenLineage to its AirflowDAG asset type. Source property Atlan property schedule_interval airflowDagSchedule timetable airflowDagScheduleDelta tags airflowTags version airflowRunVersion openlineageAdapterVersion airflowRunOpenLineageVersion runid airflowRunName run_type airflowRunType eventTime airflowRunStartTime eventTime airflowRunEndTime eventType airflowRunOpenLineageState query airflowTaskSql group_id airflowTaskGroupName Tasks â Atlan maps tasks from Google Cloud Composer/OpenLineage to its AirflowTask asset type. Source property Atlan property retries airflowTaskRetryNumber pool airflowTaskPool pool_slots airflowTaskPoolSlots queue airflowTaskQueue priority_weight airflowTaskPriorityWeight trigger_rule airflowTaskTriggerRule operator_class airflowTaskOperatorClass dag_id airflowDagName conn_id `airflowTaskConnectionId` sql airflowTaskSql tags airflowTags version airflowRunVersion openlineageAdapterVersion airflowRunOpenLineageVersion runid airflowRunName run_type airflowRunType eventTime airflowRunStartTime eventTime airflowRunEndTime eventType airflowRunOpenLineageState Tags: connectors crawl Previous How to integrate Google Cloud Composer/OpenLineage DAGs Tasks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/references/what-does-atlan-crawl-from-informatica-cdi",
    "text": "Connect data ETL Tools Informatica CDI References What does Atlan crawl from Informatica CDI On this page What does Atlan crawl from Informatica CDI Atlan discovers and catalogs various Informatica Cloud Data Integration (CDI) assets to provide comprehensive data lineage and metadata management. Important! The Informatica GUID (source system global unique identifier) for an object is available as part of the Atlan object Qualified Name (QN) and additionally in the flowId attribute (for SDK based consumption). Project â Source property Atlan property Where in Atlan name name name description description description updatedTime sourceUpdatedAt Last updated (on Informatica CDI) updatedBy sourceUpdatedBy Last updated (on Informatica CDI) by Folder â Source property Atlan property Where in Atlan name name name description description description updatedTime sourceUpdatedAt Last updated (on Informatica CDI) updatedBy sourceUpdatedBy Last updated (on Informatica CDI) by Mapping task â Source property Where in Atlan Atlan property Task Name name name Description description description Location flowProjectName, flowFolderName Project, Folder Mapping â Source property Where in Atlan Atlan property name name name Location flowProjectName, flowFolderName Project, Folder Number of transformations in the mapping flowDatasetCount Transformation â Source property Where in Atlan Atlan property Name name name Description description description Type flowType transformationType Number of fields in the transformation flowFieldCount Transformation field â Source property Where in Atlan Atlan property Name name name Type flowDataType Field Type Tags: connectors etl-tools informatica Previous Transformations Next Task and crawl issues Project Folder Mapping task Mapping Transformation Transformation field"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/references/what-does-atlan-crawl-from-matillion",
    "text": "Connect data ETL Tools Matillion References What does Atlan crawl from Matillion? On this page What does Atlan crawl from Matillion? Atlan crawls and maps the following assets and properties from Matillion. Once you've crawled Matillion , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these Matillion assets: Jobs -  Job type, job path, version, and job schedule filters Groups â Atlan maps groups from Matillion to its MatillionGroup asset type. Source property Atlan property Where in Atlan name name asset profile, and overview and properties sidebar dynamically generated from total projects crawled matillionProjectCount asset profile and overview sidebar Projects â Atlan maps projects from Matillion to its MatillionProject asset type. Source property Atlan property Where in Atlan projects.objects.name name asset profile, and overview and properties sidebar versionExports.objects.name matillionVersions asset profile and overview sidebar environmentExports.objects.name matillionEnvironments API only dynamically generated from number of jobs per project version matillionProjectJobCount asset profile, and overview and properties sidebar dynamically generated from group and project name sourceURL overview sidebar Jobs â Atlan maps jobs from Matillion to its MatillionJob asset type. Source property Atlan property Where in Atlan jobs.objects.info.name name asset profile, and overview and properties sidebar jobs.objects.info.description description asset profile, and overview and properties sidebar jobs.objects.info.type matillionJobType asset profile and filter, and overview sidebar jobs.objects.path matillionJobPath asset profile and filter, and overview sidebar versionExports.objects.name matillionVersion asset profile and filter, and overview sidebar scheduleExports.objects.daysOfWeek or scheduleExports.objects.daysOfMonth matillionJobSchedule asset profile and filter, and overview sidebar dynamically generated from number of components per job matillionJobComponentCount asset profile and overview sidebar dynamically generated from group, project, and job name sourceURL overview sidebar Components â Atlan maps components from Matillion to its MatillionComponent asset type. Source property Atlan property Where in Atlan name name asset profile, and overview and properties sidebar jobObject.components.id matillionComponentId asset profile and overview sidebar jobObject.components.implementationID matillionComponentImplementationId API only versionExports.objects.name matillionVersion asset profile and overview sidebar dynamically generated during metadata processing matillionComponentLinkedJob asset profile and overview sidebar tasks.state matillionComponentLastRunStatus asset profile and overview sidebar tasks.state matillionComponentLastFiveRunStatus properties sidebar output.sql matillionComponentSqls properties sidebar Tags: connectors crawl Previous Crawl Matillion Next What lineage does Atlan extract from Matillion? Groups Projects Jobs Components"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/references/what-does-atlan-crawl-from-metabase",
    "text": "Connect data BI Tools Cloud-based BI Metabase References What does Atlan crawl from Metabase? On this page What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase. danger Currently Atlan only represents the assets marked with ð in lineage. Collections â Atlan maps collections from Metabase to its MetabaseCollection asset type. Source property Atlan property name name slug metabaseSlug color metabaseColor namespace metabaseNamespace personal_owner_id metabaseIsPersonalCollection Dashboards ð â Atlan maps dashboards from Metabase to its MetabaseDashboard asset type. Source property Atlan property name name collection metabaseCollectionName created_at sourceCreatedAt updated_at sourceUpdatedAt last-edit-info sourceUpdatedBy ordered_cards metabaseQuestionCount collection ( Official ) certificateStatus (VERIFIED) Questions ð â Atlan maps questions from Metabase to its MetabaseQuestion asset type. Source property Atlan property name name collection metabaseCollectionName created_at sourceCreatedAt updated_at sourceUpdatedAt last-edit-info sourceUpdatedBy ordered_cards metabaseDashboardCount query_type metabaseQueryType query metabaseQuery collection ( Official ) certificateStatus (VERIFIED) invalid non-native queries certificateStatus (DEPRECATED) Tags: lineage data-lineage impact-analysis dashboards visualization analytics integration connectors Previous Crawl Metabase Next Preflight checks for Metabase Collections Dashboards ð Questions ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/references/what-does-atlan-crawl-from-microsoft-azure-cosmos-db",
    "text": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB References What does Atlan crawl from Microsoft Azure Cosmos DB? On this page What does Atlan crawl from Microsoft Azure Cosmos DB? Once you have crawled Microsoft Azure Cosmos DB , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for Microsoft Azure Cosmos DB assets: Accounts   -  you can use the following filters: Account Type -  filter accounts by account type Subscription ID -  filter accounts by subscription ID Account Resource Group -  filter accounts by resource group Collections   - Type filter, filtering options include Time Series , Capped , and Clustered collection types Atlan crawls and maps the following assets and properties from Microsoft Azure Cosmos DB. Atlan currently does not support lineage for Microsoft Azure Cosmos DB assets. Accounts â Atlan maps accounts from Microsoft Azure Cosmos DB to its Account asset type. vCore-based deployment â Source property Atlan property name name type cosmosMongoDBAccountType dynamically generated cosmosMongoDBDatabaseCount RU-based deployment â Source property Atlan property name name createdAt sourceCreatedAt instanceId cosmosMongoDBAccountInstanceId dynamically generated cosmosMongoDBDatabaseCount type cosmosMongoDBAccountType subscription cosmosMongoDBAccountSubscriptionId resource group cosmosMongoDBAccountResourceGroup document endpoint cosmosMongoDBAccountDocumentEndpoint mongo endpoint cosmosMongoDBAccountMongoEndpoint publicNetworkAccess cosmosMongoDBAccountPublicNetworkAccess enableAutomaticFailover cosmosMongoDBAccountEnableAutomaticFailover enableMultipleWriteLocations cosmosMongoDBAccountEnableMultipleWriteLocations enablePartitionKeyMonitor cosmosMongoDBAccountEnablePartitionKeyMonitor isVirtualNetworkFilterEnabled cosmosMongoDBAccountIsVirtualNetworkFilterEnabled locations cosmosMongoDBAccountLocations readLocations cosmosMongoDBAccountReadLocations writeLocations cosmosMongoDBAccountWriteLocations defaultConsistencyLevel cosmosMongoDBAccountConsistencyPolicy Databases â Atlan maps databases from Microsoft Azure Cosmos DB to its Database asset type. Source property Atlan property TABLE_CAT name TABLE_COUNT mongoDBDatabaseCollectionCount Collections â Atlan maps collections from Microsoft Azure Cosmos DB to its Collection asset type. Source property Atlan property TABLE_NAME name TABLE_CAT databaseName COLUMN_COUNT columnCount collStats.storageSize sizeBytes collStats.count rowCount type mongoDBCollectionSubtype schema mongoDBCollectionSchemaDefinition collStats.capped mongoDBCollectionIsCapped collStats.maxSize mongoDBCollectionMaxSize collStats.max mongoDBCollectionMaximumDocumentCount options.timeseries.timeField mongoDBCollectionTimeField options.timeseries.granularity mongoDBCollectionTimeGranularity options.expireAfterSeconds mongoDBCollectionExpireAfterSeconds collStats.numOrphanDocs mongoDBCollectionNumOrphanDocs collStats.nindexes mongoDBCollectionNumIndexes collStats.totalIndexSize mongoDBCollectionTotalIndexSize collStats.avgObjSize mongoDBCollectionAverageObjectSize Columns â To extract columns from Microsoft Azure Cosmos DB collections, Atlan analyzes document schemas , which must be enabled and configured to crawl columns. Atlan consolidates multiple document schemas into a unified view, capturing all possible fields with their nesting levels and data types. The system then performs a depth-first traversal, converting each unique field path into column entries while tracking parent-child relationships and hierarchy through qualified names. The total column count is calculated by summing up all unique paths, including nested structures, with arrays processed to identify additional column patterns. Atlan maps the following metadata from Microsoft Azure Cosmos DB to its Column asset type. Atlan supports nested columns up to level 35 for parent columns. Column-level lineage is currently not supported. Tag propagation is supported from: collections to columns parent to nested columns Atlan property Where in Atlan name asset preview and profile, overview sidebar order asset preview and profile, overview sidebar dataType asset preview and profile, overview sidebar rawdataTypeDefinition (raw schema of nested columns in a given parent column) asset preview and profile, overview sidebar path (complete hierarchy from parent column to child column) asset preview and profile, overview sidebar Tags: connectors data crawl Previous Crawl Microsoft Azure Cosmos DB Next Troubleshooting Microsoft Azure Cosmos DB connectivity Accounts Databases Collections Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/references/what-does-atlan-crawl-from-microsoft-azure-event-hubs",
    "text": "Connect data Event/Messaging Microsoft Azure Event Hubs References What does Atlan crawl from Microsoft Azure Event Hubs? On this page What does Atlan crawl from Microsoft Azure Event Hubs? Atlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs. Once you've crawled Microsoft Azure Event Hubs , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Event hubs -  Message count, size (MB), partition count, and cleanup policy filters Consumer groups -  Member count and topic name filters Event hubs â Atlan maps event hubs from Microsoft Azure Event Hubs to its AzureEventHub asset type. Source property Atlan property Topic name PartitionCount kafkaTopicPartitionsCount ReplicationFactor kafkaTopicReplicationFactor segment.byte kafkaTopicSegmentBytes compression.type kafkaTopicCompressionType cleanup.policy kafkaLogTopicCleanupPolicy isInternal kafkaTopicIsInternal sizeInBytes kafkaTopicSizeInBytes recordCount kafkaTopicRecordCount status azureEventHubStatus retention.ms kafkaTopicRetentionTimeInMs Consumer groups â Atlan maps consumer groups from Microsoft Azure Event Hubs to either of the following asset types : KafkaConsumerGroup -  managed via Kafka clients. AzureEventHubConsumerGroup -  managed via Azure portal, SDK, or Azure Resource Manager templates. Did you know? Consumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Microsoft Azure Event Hubs will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset. Source property Atlan property GROUP name memberCount kafkaConsumerGroupMemberCount ReplicationFactor kafkaTopicReplicationFactor topic_names kafkaTopicNames TOPIC kafkaConsumerGroupTopicConsumptionProperties.topicName PARTITION kafkaConsumerGroupTopicConsumptionProperties.topicPartition LAG kafkaConsumerGroupTopicConsumptionProperties.topicLag CURRENT-OFFSET kafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset Tags: connectors crawl Previous Crawl Microsoft Azure Event Hubs Event hubs Consumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/references/what-does-atlan-crawl-from-mongodb",
    "text": "Connect data Databases NoSQL Databases MongoDB References What does Atlan crawl from MongoDB? On this page What does Atlan crawl from MongoDB? Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets. Once you have crawled MongoDB , you can use connector-specific filters for quick asset discovery. The following filter is currently supported for MongoDB collections : Collection type filter   -  filtering options include Time Series , Capped , and Clustered collection types Databases â Atlan maps databases from MongoDB to its Database asset type. Source property Atlan property TABLE_CAT name TABLE_COUNT mongoDBDatabaseCollectionCount Collections â Atlan maps collections from MongoDB to its Collection asset type. Source property Atlan property TABLE_NAME name TABLE_CAT databaseName COLUMN_COUNT columnCount collStats.storageSize sizeBytes collStats.count rowCount type mongoDBCollectionSubtype schema mongoDBCollectionSchemaDefinition collStats.capped mongoDBCollectionIsCapped collStats.maxSize mongoDBCollectionMaxSize collStats.max mongoDBCollectionMaximumDocumentCount options.timeseries.timeField mongoDBCollectionTimeField options.timeseries.granularity mongoDBCollectionTimeGranularity options.expireAfterSeconds mongoDBCollectionExpireAfterSeconds collStats.numOrphanDocs mongoDBCollectionNumOrphanDocs collStats.nindexes mongoDBCollectionNumIndexes collStats.totalIndexSize mongoDBCollectionTotalIndexSize collStats.avgObjSize mongoDBCollectionAverageObjectSize Tags: connectors data crawl Previous Crawl MongoDB Next Troubleshooting MongoDB connectivity Databases Collections"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/references/what-does-atlan-crawl-from-mysql",
    "text": "Connect data Databases SQL Databases MySQL References What does Atlan crawl from MySQL? On this page What does Atlan crawl from MySQL? Atlan crawls and maps the following assets and properties from MySQL. Once you have crawled MySQL , you can use connector-specific filters for quick asset discovery. Databases â Atlan maps databases from MySQL to its Database asset type. Source property Atlan property Where in Atlan TABLE_CATALOG name asset preview and profile, overview sidebar SCHEMA_COUNT schemaCount asset filters Schemas â Atlan maps schemas from MySQL to its Schema asset type. Source property Atlan property Where in Atlan TABLE_SCHEMA name asset preview and profile, overview sidebar TABLE_COUNT tableCount asset preview and profile VIEW_COUNT viewsCount asset preview and profile TABLE_CATALOG databaseName asset preview and profile, overview sidebar Tables â Atlan maps tables from MySQL to its Table asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview and profile, overview sidebar REMARKS description asset preview and profile, overview sidebar COLUMN_COUNT columnCount asset preview and profile, overview sidebar ROW_COUNT rowCount asset preview and profile, overview sidebar BYTES sizeBytes asset preview and profile, overview sidebar TABLE_TYPE subType API only IS_PARTITION isPartitioned API only PARTITION_STRATEGY partitionStrategy API only PARTITION_COUNT partitionCount API only PARTITIONS partitionList API only CREATE_TIME sourceCreatedAt asset preview and profile, properties sidebar Views â Atlan maps views from MySQL to its View asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview and profile, overview sidebar REMARKS description asset preview and profile, overview sidebar COLUMN_COUNT columnCount asset preview and profile, overview sidebar VIEW_DEFINITION definition asset profile and overview sidebar IS_PARTITION isPartitioned API only PARTITION_COUNT partitionCount API only CREATE_TIME sourceCreatedAt asset preview and profile, properties sidebar Columns â Atlan maps columns from MySQL to its Column asset type. Source property Atlan property Where in Atlan COLUMN_NAME name asset profile and filter, overview sidebar REMARKS description asset profile and filter, overview sidebar ORDINAL_POSITION order parent asset profile TYPE_NAME dataType asset preview and filter, overview sidebar, parent asset profile CONSTRAINT_TYPE (PRIMARY KEY) isPrimary asset preview and filter, overview sidebar, parent asset profile CONSTRAINT_TYPE (FOREIGN KEY) isForeign asset preview and filter, overview sidebar, parent asset profile IS_NULLABLE isNullable API only NUMERIC_SCALE numericScale API only CHARACTER_MAXIMUM_LENGTH maxLength API only Stored procedures â Atlan maps stored procedures from MySQL to its Procedure asset type. Source property Atlan property Where in Atlan PROCEDURE_NAME name API only REMARKS description API only PROCEDURE_TYPE subType API only ROUTINE_DEFINITION definition API only CREATED sourceCreatedAt API only LAST_ALTERED sourceUpdatedAt API only Tags: connectors data crawl Previous Crawl MySQL Next Preflight checks for MySQL Databases Schemas Tables Views Columns Stored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/references/what-does-atlan-crawl-from-postgresql",
    "text": "Connect data Databases SQL Databases PostgreSQL References What does Atlan crawl from PostgreSQL? On this page What does Atlan crawl from PostgreSQL? Atlan crawls and maps the following assets and properties from PostgreSQL. Once you have crawled PostgreSQL , you can use connector-specific filters for quick asset discovery. Databases â Atlan maps databases from PostgreSQL to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from PostgreSQL to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName Tables â Atlan maps tables from PostgreSQL to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes TABLE_KIND (p) , TABLE_TYPE (PARTITIONED TABLE) isPartitioned PARTITION_STRATEGY partitionStrategy PARTITION_COUNT partitionCount IS_INSERTABLE_INTO is_insertable_into IS_TYPED is_typed SELF_REFERENCING_COL_NAME self_referencing_col_name REF_GENERATION ref_generation IS_TRANSIENT is_transient Table partitions â Atlan maps table partitions from PostgreSQL to its TablePartition asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes PARTITION_CONSTRAINT constraint TABLE_KIND (p) , TABLE_TYPE (PARTITIONED TABLE) isPartitioned PARTITION_STRATEGY partitionStrategy PARTITION_COUNT partitionCount IS_INSERTABLE_INTO is_insertable_into IS_TYPED is_typed SELF_REFERENCING_COL_NAME self_referencing_col_name REF_GENERATION ref_generation IS_TRANSIENT is_transient Views â Atlan maps views from PostgreSQL to its View asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount VIEW_DEFINITION definition IS_INSERTABLE_INTO is_insertable_into IS_TYPED is_typed SELF_REFERENCING_COL_NAME self_referencing_col_name REF_GENERATION ref_generation IS_TRANSIENT is_transient Materialized views â Atlan maps materialized views from PostgreSQL to its MaterialisedView asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes VIEW_DEFINITION definition Columns â Atlan maps columns from PostgreSQL to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description Tags: connectors data crawl Previous Crawl PostgreSQL Next Preflight checks for PostgreSQL Databases Schemas Tables Table partitions Views Materialized views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/references/what-does-atlan-crawl-from-qlik-sense-enterprise-on-windows",
    "text": "Connect data BI Tools On-premises & Enterprise BI Qlik Sense Enterprise on Windows References What does Atlan crawl from Qlik Sense Enterprise on Windows? On this page What does Atlan crawl from Qlik Sense Enterprise on Windows? Atlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows. Once you've crawled Qlik Sense Enterprise on Windows , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Apps and sheets -  Is Published filter Streams â Atlan maps streams from Qlik Sense Enterprise on Windows to its QlikStream asset type. Source property Atlan property type qlikStreamType ownerId qlikOwnerId id qlikId createdAt sourceCreatedAt updatedAt sourceUpdatedAt Apps â Atlan maps apps from Qlik Sense Enterprise on Windows to its QlikApp asset type. Source property Atlan property attributes.name name attributes.description description attributes.resourceId qlikId static_byte_size qlikAppStaticByteSize attributes.spaceId qlikSpaceId attributes.resourceCreatedAt sourceCreatedAt attributes.resourceUpdatedAt sourceUpdatedAt attributes.ownerId qlikOwnerId attributes.resourceAttributes.originAppId qlikOriginAppId attributes.resourceAttributes.hasSectionAccess qlikHasSectionAccess attributes.resourceAttributes.directQueryMode qlikIsDirectQueryMode attributes.resourceAttributes.published qlikIsPublished Sheets â Atlan maps sheets from Qlik Sense Enterprise on Windows to its QlikSheet asset type. Source property Atlan property qProperty.qMetaDef.title name qProperty.qMetaDef.description description qProperty.qInfo.qId qlikId spaceId qlikSpaceId appId qlikAppId approved qlikIsApproved published qlikIsPublished Charts â Atlan maps charts from Qlik Sense Enterprise on Windows to its QlikChart asset type. Source property Atlan property qProperty.qInfo.qId qlikId qProperty.subtitle qlikChartSubtitle qProperty.footnote qlikChartFootnote qProperty.qInfo.qType qlikChartType qProperty.options.dimensionsOrientation qlikChartOrientation Tags: connectors crawl Previous Crawl Qlik Sense Enterprise on Windows Streams Apps Sheets Charts"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/references/what-does-atlan-crawl-from-redpanda-kafka",
    "text": "Connect data Event/Messaging Redpanda Kafka References What does Atlan crawl from Redpanda Kafka? On this page What does Atlan crawl from Redpanda Kafka? Atlan crawls and maps the following assets and properties from Redpanda Kafka. Once you've crawled Redpanda Kafka , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Topics -  Message count, size (MB), partition count, and cleanup policy filters Consumer groups -  Member count and topic name filters Topics â Atlan maps topics from Redpanda Kafka to its KafkaTopic asset type. Source property Atlan property Topic name PartitionCount kafkaTopicPartitionsCount ReplicationFactor kafkaTopicReplicationFactor segment.byte kafkaTopicSegmentBytes compression.type kafkaTopicCompressionType cleanup.policy kafkaLogTopicCleanupPolicy isInternal kafkaTopicIsInternal sizeInBytes kafkaTopicSizeInBytes recordCount kafkaTopicRecordCount retention.ms kafkaTopicRetentionTimeInMs Consumer groups â Atlan maps consumer groups from Redpanda Kafka to its KafkaConsumerGroup asset type. Did you know? Consumer groups are most likely to show up only in streaming scenarios. This is because if a topic is not being consumed actively, Redpanda Kafka will purge the consumer group. So, if a consumer group is inactive while the workflow runs in Atlan, it will not be cataloged as an asset. Source property Atlan property GROUP name memberCount kafkaConsumerGroupMemberCount ReplicationFactor kafkaTopicReplicationFactor topic_names kafkaTopicNames TOPIC kafkaConsumerGroupTopicConsumptionProperties.topicName PARTITION kafkaConsumerGroupTopicConsumptionProperties.topicPartition LAG kafkaConsumerGroupTopicConsumptionProperties.topicLag CURRENT-OFFSET kafkaConsumerGroupTopicConsumptionProperties.topicCurrentOffset Tags: connectors crawl Previous Crawl on-premises Kafka Next Preflight checks for Redpanda Kafka Topics Consumer groups"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/references/what-does-atlan-crawl-from-sisense",
    "text": "Connect data BI Tools Cloud-based BI Sisense References What does Atlan crawl from Sisense? On this page What does Atlan crawl from Sisense? Once you've crawled Sisense , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these asset types: Dashboards: Widget count filter   -  filter dashboards by widget count Data models: Type filter   -  filter data models by live or extract (formerly called Elasticube) type Data model tables: Tags filter   -  filter data model tables by Sisense tags Is hidden filter   -  use this filter to find hidden data model tables Atlan crawls and maps the following assets and properties from Sisense. danger Currently, Atlan only represents the assets marked with ð in lineage. Column-level lineage is not supported at present. Dashboards ð â Atlan maps dashboards from Sisense to its SisenseDashboard asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar desc description asset profile and overview sidebar created createdAt properties sidebar lastUpdated updatedAt properties sidebar calculated by Atlan widgetCount asset filter and overview sidebar generated using the dashboard ID sourceURL View in Sisense button on asset profile Widgets ð â Atlan maps widgets from Sisense to its SisenseWidget asset type. Source property Atlan property Where in Atlan title name asset profile and overview sidebar desc description asset profile and overview sidebar created createdAt properties sidebar lastUpdated updatedAt properties sidebar calculated by Atlan columnCount overview sidebar subType subType API only size size API only Data models â Atlan maps data models from Sisense to its SisenseDataModel asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar desc description asset profile and overview sidebar created createdAt properties sidebar lastUpdated updatedAt properties sidebar calculated by Atlan tableCount API only server server API only revision revision API only lastBuildTime lastBuildTime API only lastSuccessfulBuildTime lastSuccessfulBuildTime API only type type asset filter and overview sidebar relationshipType relationshipType API only Data model tables ð â Atlan maps data model tables from Sisense to its SisenseDataModelTable asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar description description asset profile and overview sidebar created createdAt properties sidebar lastUpdated updatedAt properties sidebar calculated by Atlan columnCount overview sidebar type type API only expression expression API only isMaterialized isMaterialized API only isHidden isHidden asset filter and overview sidebar schedule schedule API only liveQuerySettings liveQuerySettings API only tags assetTags properties sidebar Folders â Atlan maps folders from Sisense to its SisenseFolder asset type. Source property Atlan property Where in Atlan name name asset profile and overview sidebar created createdAt properties sidebar lastUpdated updatedAt properties sidebar Tags: connectors data crawl model Previous Crawl Sisense Next Preflight checks for Sisense Dashboards ð Widgets ð Data models Data model tables ð Folders"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/references/what-does-atlan-crawl-from-soda",
    "text": "Connect data Data Quality & Observability Soda References What does Atlan crawl from Soda? On this page What does Atlan crawl from Soda? Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset. Once you have crawled Soda , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for Soda assets: Check status   -  filter Soda checks by status Check owner   -  filter Soda checks by email address of check owner Last scanned at   -  filter Soda checks by timestamp for last scanned in Soda The following Soda filters are currently available for supported SQL assets: Data quality status   -  filter SQL assets by overall data quality status, including Pass , Warn , Fail , and Not evaluated Check count   -  filter SQL assets by total count of associated Soda checks Scanned date   -  filter SQL assets by timestamp for last scanned in Soda Last synced (in Atlan)   -  filter SQL assets by timestamp for when any associated checks were last updated in Atlan Atlan crawls and maps the following assets and properties from Soda. Checks â Atlan maps checks from Soda to its SodaCheck asset type. Source property Atlan property name name description description id sodaCheckId evaluationStatus sodaCheckEvaluationStatus definition sodaCheckDefinition incidents sodaCheckIncidentCount lastCheckRunTime sodaCheckLastScanAt cloudUrl sourceURL lastUpdated sourceUpdatedAt owner.email sourceOwners Supported sources â If you have crawled supported data sources, you can view Soda checks on your existing assets in Atlan: Amazon Athena Amazon Redshift Databricks Google BigQuery Hive Microsoft Azure Synapse Analytics Microsoft SQL Server MySQL PostgreSQL Snowflake Trino Soda checks can also be cataloged for some data sources that are not natively supported in Atlan. These will require additional configuration at source. To ensure that the datasets are mapped to your assets in Atlan, set the value of the data_source_name field to <database>.<schema> when connecting to: Dask and Pandas Apache Spark Tags: connectors data crawl Previous Crawl Soda Next Preflight checks for Soda Checks Supported sources"
  },
  {
    "url": "https://docs.atlan.com/product/connections/concepts/what-is-the-crawler-logic-for-a-deprecated-asset",
    "text": "Connect data Connectivity Framework Connector Framework Concepts What is the crawler logic for a deprecated asset? What is the crawler logic for a deprecated asset? If an asset is deprecated at source, it will be archived (or soft-deleted) in Atlan. The status of the asset will remain unchanged during the next crawler run. Changing the certification status of an asset to Deprecated in Atlan has no impact on the crawler logic. Tags: integration connectors Previous What are preflight checks? Next Additional connectivity to data sources"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/references/what-lineage-does-atlan-extract-from-matillion",
    "text": "Connect data ETL Tools Matillion References What lineage does Atlan extract from Matillion? On this page What lineage does Atlan extract from Matillion? Atlan uses Matillion's metadata API to generate lineage associated with Matillion connectors . This is particularly useful for creating lineage between different tools. Due to limitations in metadata provisioning at source, Atlan currently only supports lineage for data transformations between Snowflake tables as both source and destination. The related Matillion processes neither have asset profiles nor are they discoverable. Atlan recommends upgrading to the latest version of Matillion ETL to allow for enhanced metadata provisioning. danger The assets involved in lineage (tables, columns, and so on) must already be crawled by Atlan before running the MatillionÂ package to enrich them. Specifically, Atlan will: Create lineage between each data asset in Atlan that is associated with a Matillion connector. (For example, between Snowflake tables and columns.) Atlan creates Matillion Process objects for each data asset that is replicated. Learn more about processes here. Supported sources and destinations â Sources â Atlan's Matillion integration supports the following sources: Snowflake Destinations â Atlan's Matillion integration supports the following destinations: Snowflake Did you know? We welcome feedback   -  have a burning need for another source or destination? Please let us know ! Tags: connectors data crawl api Previous What does Atlan crawl from Matillion? Next Troubleshooting Matillion connectivity Supported sources and destinations"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/references/what-lineage-does-atlan-extract-from-microsoft-azure-data-factory",
    "text": "Connect data ETL Tools Microsoft Azure Data Factory References What lineage does Atlan extract from Microsoft Azure Data Factory? On this page What lineage does Atlan extract from Microsoft Azure Data Factory? Atlan uses the Microsoft Azure Data Factory REST API to generate lineage associated with Microsoft Azure Data Factory connectors . This is particularly useful for creating lineage between different tools. Atlan currently only supports lineage for the following: Supported sources and sinks listed below. Data loaded through supported activities in data factory pipelines. danger The assets involved in lineage must already be crawled by Atlan before running the Microsoft Azure Data Factory package to enrich them. Specifically, Atlan will: Create lineage between each data asset in Atlan that is associated with a supported Microsoft Azure Data Factory connector. (For example, between Microsoft Azure Cosmos DB collections and ADLS objects.) Atlan creates Microsoft Azure Data Factory Process objects for each data asset that is replicated. Learn more about processes here. Supported sources and sinks â Sources â Atlan's Microsoft Azure Data Factory integration supports the following sources: Azure Data Lake Storage (ADLS) Azure Databricks Microsoft Azure Cosmos DB for MongoDB Snowflake Sinks â Atlan's Microsoft Azure Data Factory integration supports the following sinks: Azure Data Lake Storage (ADLS) Azure Databricks Microsoft Azure Cosmos DB for MongoDB Snowflake Did you know? We welcome feedback   -  have a burning need for another source or sink? Please let us know ! Tags: connectors data crawl api Previous What does Atlan crawl from Microsoft Azure Data Factory? Next Troubleshooting Microsoft Azure Data Factory connectivity Supported sources and sinks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/references/what-lineage-does-atlan-extract-from-microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics References What lineage does Atlan extract from Microsoft Azure Synapse Analytics? On this page What lineage does Atlan extract from Microsoft Azure Synapse Analytics? danger Atlan currently only supports mining query history for dedicated SQL pools with the Microsoft Azure Synapse Analytics miner . Mining query history for serverless SQL pools is currently not supported. Atlan uses the Azure Synapse Analytics REST API to generate lineage associated with Microsoft Azure Synapse AnalyticsÂ connectors . This is particularly useful for creating lineage between different tools. Atlan currently only supports lineage for the following: Supported sources and destinations listed below. Data loaded through Copy activity in Synapse pipelines. Supported sources and destinations â Sources â Atlan's Microsoft Azure Synapse Analytics integration supports the following sources: Azure Data Lake Storage (ADLS) Gen2 Microsoft Azure Synapse Analytics Destinations â Atlan's Microsoft Azure Synapse Analytics integration supports the following destinations: Microsoft Azure Synapse Analytics Microsoft Power BI Did you know? We welcome feedback   -  have a burning need for another source or destination? Please let us know ! Tags: connectors data api Previous Preflight checks for Microsoft Azure Synapse Analytics Supported sources and destinations"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-lineage-does-atlan-extract-from-microsoft-power-bi",
    "text": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI References What lineage does Atlan extract from Microsoft Power BI? On this page What lineage does Atlan extract from Microsoft Power BI? Atlan currently supports the following lineage for Microsoft Power BI : Lineage between Microsoft Power BI assets crawled in Atlan Upstream lineage to SQL warehouse assets, includes table- and column-level lineage for the following supported SQL sources: Amazon Redshift Databricks Google BigQuery Microsoft Azure Synapse Analytics Microsoft SQL Server MySQL Oracle -  Atlan generates lineage for the following methods of Oracle connectivity: connection string   -  for example, <host_name>:<port>/<service_name> connect descriptor   -  for example, (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=<host_name>)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=<service_name>))) Lineage generation for TNS name connectivity is currently not supported. SAP HANA Snowflake Teradata Salesforce This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation. Lineage generation â Atlan generates lineage for your Microsoft Power BI assets as follows: You connect to a SQL data source such as Snowflake and extract relevant SQL tables to create a table in Microsoft Power BI for analysis. Once the data has been loaded, you can perform Microsoft Power BI native operations as required. Each table created in Microsoft Power BI and part of a dataset has a Power Query expression associated with it. For example: let Source = Snowflake.Databases( \"example.snowflakecomputing.com\" , \"EXAMPLE_WAREHOUSE\" , [ Role= \"EXAMPLE_ROLE\" ] ) , EXAMPLE_DB = Source { [ Name= \"EXAMPLE_DATABASE_NAME\" , Kind= \"Database\" ] } [ Data ] , EXAMPLE_Sch = EXAMPLE_DB { [ Name= \"EXAMPLE_SCHEMA_NAME\" , Kind= \"Schema\" ] } [ Data ] , EXAMPLE_Table_Var = EXAMPLE_Sch { [ Name= \"EXAMPLE_TABLE_NAME\" , Kind= \"Table\" ] } [ Data ] in EXAMPLE_Table_Var Atlan retrieves the Power Query expression as a plain string from the Microsoft Power BI API response. Atlan's custom query parser then parses the Power Query expression to derive lineage between the upstream SQL tables and Microsoft Power BI table asset. However, note that the Power Query expression can be modified in the Power Query Editor of the Power BI Desktop application. These modifications may involve using parameter substitutes and variable naming patterns in the Power Query expression. These modifications may lead to inconsistent behavior in Atlan's query parser. This is because the latter is built on the standard format of a Power Query expression, without any modifications. Limitations of query parser â To create seamless lineage generation, Atlan recommends the following when building tables in Microsoft Power BI. Using parameters â The Power Query expression associated with a table can be manually modified to serve different use cases. For example, if you're creating multiple tables using data from the same database and schema at source, you may want to use dynamic M query parameters to substitute common values in Power Query expressions. Atlan recommends the following: Avoid using the following words to define your parameter names: Database Schema Table View Warehouse Role Avoid including any spaces in your parameter names   -  for example, ( Example : Example DB ) For example, Atlan's query parser doesn't support the following: let Source = Snowflake.Databases( \"example.snowflakecomputing.com\" , WarehouseName , [ Role= \"EXAMPLE_ROLE\" ] ) , DatabaseName = Source { [ Name=DatabaseName , Kind= \"Database\" ] } [ Data ] , EXAMPLE_Sch = DatabaseName { [ Name=SchemaName , Kind= \"Schema\" ] } [ Data ] , EXAMPLE_Table_Var = EXAMPLE_Sch { [ Name=TableName , Kind= \"Table\" ] } [ Data ] in EXAMPLE_Table_Var This example includes WarehouseName , DatabaseName , SchemaName , and TableName as parameters, which aren't supported in the query parser. Parameter syntax â There are different formats for the syntax used in parameter names for Power Query expressions. For example, param_name ,  âparam_nameâ , or  \"param nameâ . Atlan recommends the following for parameter names: Use plain text format Avoid any special characters   -  for example,   , \" , and more For example, Atlan's query parser doesn't support the following: let Source = Snowflake.Databases( \"example.snowflakecomputing.com\" , \"EXAMPLE_WAREHOUSE\" , [ Role= \"EXAMPLE_ROLE\" ] ) , DatabaseName = Source { [ Name=  \"DatabaseName\" , Kind= \"Database\" ] } [ Data ] , EXAMPLE_Sch = DatabaseName { [ Name= \"EXAMPLE_SCHEMA_NAME\" , Kind= \"Schema\" ] } [ Data ] , EXAMPLE_Table_Var = EXAMPLE_Sch { [ Name= \"EXAMPLE_TABLE_NAME\" , Kind= \"Table\" ] } [ Data ] in EXAMPLE_Table_Var This example includes  \"DatabaseName\" as parameter name, which isn't supported in the query parser. Variable names â While using parameters in Power Query expressions, make sure that the variable names don't match the parameter names. For example, Atlan's query parser doesn't support the following: let Source = Snowflake.Databases( \"example.snowflakecomputing.com\" , \"EXAMPLE_WAREHOUSE\" , [ Role= \"EXAMPLE_ROLE\" ] ) , DatabaseName = Source { [ Name=DatabaseName , Kind= \"Database\" ] } [ Data ] , EXAMPLE_Sch = DatabaseName { [ Name= \"EXAMPLE_SCHEMA_NAME\" , Kind= \"Schema\" ] } [ Data ] , EXAMPLE_Table_Var = EXAMPLE_Sch { [ Name= \"EXAMPLE_TABLE_NAME\" , Kind= \"Table\" ] } [ Data ] in EXAMPLE_Table_Var In this example, DatabaseName is used as both a parameter name and variable name, which isn't supported in the query parser. User-defined expressions â Parts of a Power Query expression can be parameterized and cross-referenced in other Power Query expressions. Atlan's query parser currently only parses standard forms of Power Query expressions, hence these user-defined expressions aren't supported. Example of a supported Power Query expression: let Source = Snowflake.Databases( \"example.snowflakecomputing.com\" , \"EXAMPLE_WAREHOUSE\" , [ Role= \"EXAMPLE_ROLE\" ] ) , EXAMPLE_DB = Source { [ Name= \"EXAMPLE_DATABASE_NAME\" , Kind= \"Database\" ] } [ Data ] , EXAMPLE_Sch = EXAMPLE_DB { [ Name= \"EXAMPLE_SCHEMA_NAME\" , Kind= \"Schema\" ] } [ Data ] , EXAMPLE_Table_Var = EXAMPLE_Sch { [ Name= \"TBL_AGG_SALES_HT_POS_BEER\" , Kind= \"Table\" ] } [ Data ] in EXAMPLE_Table_Var Example of an unsupported Power Query expression: let Source = db_source , EXAMPLE_Sch = db_source { [ Name= \"EXAMPLE_SCHEMA_NAME\" , Kind= \"Schema\" ] } [ Data ] , EXAMPLE_Table_Var = EXAMPLE_Sch { [ Name= \"EXAMPLE_TABLE_NAME\" , Kind= \"Table\" ] } [ Data ] in EXAMPLE_Table_Var Example of a reference expression, parameterized as db_source : let Source = Snowflake.Databases( \"example.snowflakecomputing.com\" , \"EXAMPLE_WAREHOUSE\" , [ Role= \"EXAMPLE_ROLE\" ] ) , EXAMPLE_DB = Source { [ Name= \"EXAMPLE_DATABASE_NAME\" , Kind= \"Database\" ] } [ Data ] in EXAMPLE_DB Table functions â For column-level lineage generation, Atlan's custom query parser currently supports parsing expressions with the following Table Functions : Table.RenameColumns Table.TransformColumnNames Table.TransformColumns Power query functions â Upstream lineage isn't supported when the data source expression involves the use of certain built-in Power Query functions. The following functions aren't supported: Csv.Document DateTime.LocalNow Excel.Workbook Folder.Files Json.Document List.Dates SharePoint.Files SharePoint.Tables UsageMetricsDataConnector.GetMetricsData Xml.Tables Tags: connectors data crawl Previous Preflight checks for Microsoft Power BI Next Troubleshooting Microsoft Power BI connectivity Lineage generation Limitations of query parser"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/sso-user-provisioning",
    "text": "Configure Atlan Integrations Identity Management SSO FAQ What type of user provisioning does Atlan support for SSO integrations? What type of user provisioning does Atlan support for SSO integrations? Atlan currently supports System for Cross-domain Identity Management (SCIM) capabilities for user provisioning for: Azure AD Okta Atlan also supports just-in-time (JIT) user provisioning and automated assignment of user groups during every login for several SSO providers . Once a user is deprovisioned in your SSO provider, the user will not be able to log into Atlan. Since the user profile in Atlan is not deleted automatically, you will also need to disable the user . Tags: integration connectors faq-integrations Previous Can we use a Microsoft SSO login? Next When does Atlan become a personal data processor or subprocessor?"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/athena-vs-glue",
    "text": "Connect data Connectivity Framework Connector Framework FAQ What's the difference between connecting to Athena and Glue? What's the difference between connecting to Athena and Glue? Atlan recommends setting up AWS Glue or Amazon Athena based on your intended outcome: To only crawl metadata, you can set up an AWS Glue connection . To also preview and query the data, you can set up an Amazon Athena connection . Note that Amazon Athena also sources asset metadata directly from AWS Glue . Tags: integration connectors faq-connections Previous What column keys does Atlan crawl?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/invite-email",
    "text": "Configure Atlan Integrations Identity Management SSO FAQ Why did my users not receive an invite email from Atlan? Why did my users not receive an invite email from Atlan? If you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following: Check if the email has been routed to the user's spam mailbox. Add users to the Atlan app in your SSO provider. When SSO is integrated , you do not need to invite users via the Atlan UI (using their email). Instead, the first time a user logs into Atlan via SSO, their user profile will be automatically created. Tags: integration connectors faq-integrations Previous When does Atlan become a personal data processor or subprocessor? Next Project Management Integrations"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/faq/browser-extension-error",
    "text": "Configure Atlan Integrations Automation Browser Extension FAQ Why do I get an error message when I click on Atlan's browser extension? Why do I get an error message when I click on Atlan's browser extension? Refer to Troubleshooting the Atlan browser extension . Tags: integration connectors Previous Can I add Atlan's browser extension for everyone in my organization? Next Why is Atlan's browser extension not loading?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/faq/salesforce-description-not-showing",
    "text": "Connect data CRM Salesforce FAQ Why does the description from Salesforce not show up in Atlan? Why does the description from Salesforce not show up in Atlan? Atlan supports extracting and displaying description metadata for your Salesforce objects . Tags: connectors data crawl salesforce Previous Does Atlan require an admin user in Salesforce?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/faq/browser-extension-not-loading",
    "text": "Configure Atlan Integrations Automation Browser Extension FAQ Why is Atlan's browser extension not loading? Why is Atlan's browser extension not loading? Refer to Troubleshooting the Atlan browser extension . Tags: integration connectors Previous Why do I get an error message when I click on Atlan's browser extension? Next Connections Integration"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-custom-metadata",
    "text": "Use data Discovery Configuration Add custom metadata Add custom metadata Who can do this? Any user with permission to edit custom metadata can add or change custom metadata values on assets. (An admin user must first create the custom metadata structures for those values, though.) To add custom metadata to an asset: Navigate to the asset. You could do this by searching, through discovery, or even through our Chrome extension . In the right sidebar, click the icon for the metadata structure you want to enrich. If no values yet exist, click the Start editing button. Otherwise, in the upper-right of the custom metadata panel in the sidebar, click the Edit button. Enter values for any properties you want to enrich. (You do not need to fill in every property.) In the upper-right of the custom metadata panel in the sidebar, click the Update button. That's it, the asset is now enriched with your organization's own custom metadata! ð Tags: data integration asset-profile Previous How to use the filters menu Next How do I use the filters menu?"
  },
  {
    "url": "https://docs.atlan.com/faq/ai-and-automation-features",
    "text": "Configure Atlan Frequently Asked Questions AI and Automation Features On this page AI and Automation Features Guide to Atlan's AI capabilities and automation features for enhanced data governance and productivity. Do you provide any machine learning or AI assistance? â Atlan makes intelligent recommendations to support your documentation initiatives. For example: Suggestions to populate context and metadata Automating asset tag propagation Use Atlan AI for documentation and other use cases Where can third-party developers contribute? â The Atlan marketplace is a foundational part of our connector and package ecosystem. We provide out-of-the-box connectors to popular data sources and tools. We also have packages that can power active metadata use cases . Our vision is to open this up to the developer community, who can help build more packages to achieve complex value streams. Our intention is to support a package manager (similar to npm or pip) for building, managing, monitoring, and orchestrating container-native workflows on Kubernetes, powered by Argo . Why can't I see the Atlan logo in my data tool? â Refer to the Troubleshooting the Atlan browser extension document. Is the PII tagging of data or metadata automated? â Atlan propagates tags based on hierarchy and lineage, which means tags can be propagated to connected assets. For example, if you tag a table as PII and that table has downstream columns, Atlan will tag those downstream columns as PII as well â only if propagation is enabled. Atlan does not auto-detect PII data. Atlan will only propagate the PII tag to downstream assets if you have enabled tag propagation manually or automated the task using playbooks . Can I integrate Atlan with any web application? â Atlan provides API-backed SDKs for integration. You can use these to: Extract and manipulate any metadata curated in Atlan, Ingest or enrich any metadata into Atlan, And ultimately power active metadata use cases . We're always happy to learn more about what folks are hoping to accomplish, so feel free to reach out if you want to discuss specific ideas or possibilities in more detail. Tags: data integration api faq-automation Previous Administration and Configuration Next Data Connections and Integration"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/sql-dump-lineage-support",
    "text": "Use data Lineage FAQ Can Atlan read a dump of SQL statements to create lineage? Can Atlan read a dump of SQL statements to create lineage? Atlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan mines SQL queries from query history at source. These are the SQL statements that Atlan parses: CREATE TABLE CREATE VIEW CREATE TABLE AS SELECT MERGE INSERT INTO UPDATE CLONE Tags: data faq faq-lineage Previous Can Atlan integrate with Airflow to generate lineage? Next Can I be notified if there is a change in downstream dashboards or a schema drift?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/lineage-change-notifications",
    "text": "Use data Lineage FAQ Can I be notified if there is a change in downstream dashboards or a schema drift? Can I be notified if there is a change in downstream dashboards or a schema drift? You can create webhooks in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events. Tags: data integration faq faq-lineage Previous Can Atlan read a dump of SQL statements to create lineage? Next Does Atlan support field-level lineage for BI tools?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/query-any-dw-dl",
    "text": "Use data Insights FAQ Can I query any DW/DL? Can I query any DW/DL? You can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's supported sources . Once integrated, you will be able to query the underlying data using the Insights feature. Tags: data integration faq faq-insights Previous Are there any limits on concurrent queries? Next Can I turn off sample data preview for the entire organization?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/concepts/what-are-data-models",
    "text": "Configure Atlan Data Models Concepts What are data models? On this page Data Models Data models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network. Atlan enables you to ingest your entityârelationship (ER) models and associate them with existing data assets in Atlan. Cataloging your ER model metadata in Atlan can help you: Foster collaboration   -  business and technical users work best when they share a common understanding of the data landscape without tool boundaries. Handle change management through impact analysis   -  data models enable visualization of an asset's lifecycle within an organization, helping users assess business impact due to technical changes with accuracy and vice versa. Implement data governance   -  define access control mechanisms, data retention policies, and data governance rules spanning different systems by understanding relationships between data assets. When business-approved data models are coupled with technical objects, trust and accountability are established between key stakeholders. Ingest ER models â You can ingest your ER models in Atlan using the following methods: Data model ingestion -  Atlan recommends using this custom package to ingest your ER models via an Excel template. Atlan SDK Atlan REST API Entityârelationship models â Entityârelationship (ER) models focus on entities (objects/concepts) and the attributes (characteristics) and relationships (associations) between those entities. In the context of entityârelationship modeling, a model encompasses the entities, attributes, and relationships that define how data is organized and interactions between different elements within a specific domain. Data models can be used to represent information at different levels of abstraction: Conceptual   -  overall structure of content without specific details. This acts as a starting point for new data initiatives and is the most abstract form of the model. Logical   -  implementation-agnostic breakdown of data into specific objects and interactions between these objects. Physical   -  a refined adaptation of data concepts conforming to a particular software application or data storage system. This level takes into account finer nuances like naming conventions, optimizations, partitioning, and more. Entity-relationship diagrams â An entity-relationship diagram (ERD) is a visual representation of data that illustrates the entities (objects or concepts) within a system, relationships between those entities, and their attributes. Entity -  in an ERD, an entity is a fundamental component that represents a real-world object or concept within a database. For example, entities are typically nouns, such as Customer , Order , or Product and data can be stored about them. Attribute -  an entity has attributes, which are the properties or characteristics of the entity. For example, a Customer entity may have attributes like CustomerID , Name , Email , and Phone Number . Relationship -  a relationship determines how two entities interact with each other. For example, a Customer places an Order . A relationship encompasses several elements, like: Cardinality   -  defines the quantitative aspect of a relationship. For example, a Quote provides pricing for many related Orders (one-to-many). Optionality   -  defines whether a relationship is mandatory in an entity. For example, an Order must have an associated Customer . Cardinality and optionality can be combined to define business rules. For example, in a Library system, a Member can borrow 0-n book(s). Types of relationships: Association   -  refers to a peer-to-peer relationship between two entities. Generalization   -  refers to a parent-child relationship between two entities. For example, a Loan entity can be of type Home Loan , Auto Loan , Business Loan , and so on. Model -  in the context of ER modeling, a model encompasses the entities, attributes, and relationships that define how data is organized and how different elements interact within a specific domain. Models can be of different types   -  conceptual, logical, and physical. Mapping   -  entities within a model can be mapped to entities within another model of a different type. For example, a logical entity Order can be mapped to your assets in Atlan, such as an Order table in Snowflake. Tags: data model Previous How to view data models Next Troubleshooting data models Ingest ER models Entityârelationship models Entity-relationship diagrams"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/download-impacted-assets-in-microsoft-excel",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Download impacted assets in Microsoft Excel On this page Download impacted assets in Microsoft Excel Once you've connected Atlan with Microsoft Excel , you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for impact analysis . danger You need to be logged into your Atlan instance before you can download impacted assets from Atlan in Microsoft Excel. If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Microsoft Excel. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Download impacted assets in Microsoft Excel â To import impacted assets in Microsoft Excel: In the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, clickÂ Import Lineage to open a list of your assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, select the asset type. In the Atlan sidebar on your worksheet, you can either: Individually select the data asset(s) you want to import. To the left of the Import button, click the Select All checkbox to select all the assets that have loaded in the sidebar. (Optional) Scroll down and click Load more to load more assets in the sidebar. To select the type of impacted assets you'd like to download, in the Atlan sidebar, from the Direction dropdown: To download downstream assets for impact analysis , click Downstream . To download upstream assets for root cause analysis , click Upstream . To download all impacted assets, click Both . To download the impacted assets in Microsoft Excel: Click Import Lineage to download all the impacted assets in one sheet.Â Click the vertical three dots and then click Import to new sheet to download the assets in separate sheet tabs. (Optional) Once download is successful, click the asset links in your spreadsheet to view the assets in Atlan. The impacted assets are now available in Microsoft Excel! ð Update column metadata for impacted assets â Once you've imported your impacted assets from Atlan, you can edit the column metadata for impacted assets in Microsoft Excel. You can make changes to the column metadata once all the columns have been imported successfully. You can only make changes to the metadata in the following columns: Description Owner Users Owner Groups Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the following columns: Source Asset Source Asset Connector Source Asset Type Impacted Asset Impacted Asset Connector Impacted Asset Type Direction Terms Propagated Tags Source URL Qualified Name Source Asset GUID Impacted Asset GUID Immediate Upstream and Immediate Downstream Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Did you know? When adding tags or owners for impacted assets in Microsoft Excel, the tag or user name for the owner user or group must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags or owners as comma-separated values. Push changes to Atlan â Once you've made changes to the column metadata, to push your changes: In the menu bar of your Microsoft Excel workbook, click the Atlan tab. From the top left of the Atlan tab, click Atlan and then click Sync to Atlan . A dialog box will appear once the changes have synced Tags: data integration Previous Download impacted assets in Google Sheets Next How to export assets Download impacted assets in Microsoft Excel Update column metadata for impacted assets Push changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/update-personas-glossary-terms",
    "text": "Build governance Glossary FAQ Use personas to update a term in a glossary How can I use personas to update a term in a glossary? By default, any user in Atlan can view all glossaries and nested categories and terms in the Glossary section. Users who do not have the permission to update assets like a glossary term   -  for example, a member user who has not been granted access to update terms via a glossary policy -  can raise a request to suggest changes to the term. Even though the user will be able to view the request they made on a specific glossary, they will neither be able to approve nor reject the request. Tags: data faq faq-governance Previous What is the default permission for a glossary? Next Can I create backups of glossaries?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/how-tos/integrate-apache-spark-openlineage",
    "text": "Connect data Orchestration & Workflow Apache Spark OpenLineage Get Started How to integrate Apache Spark/OpenLineage On this page Integrate Apache Spark/OpenLineage Atlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to OpenLineage configuration and facets . To integrate Apache Spark/OpenLineage with Atlan, review the order of operations and then complete the following steps. Create an API token in Atlan â Before running the workflow, you will need to create an API token in Atlan. Select the source in Atlan â To select Apache Spark/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the list of packages, select Spark Assets Â and then click Setup Workflow . Configure the integration in Atlan â You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your jobs run to catalog your assets. To configure the Apache Spark/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. To run the workflow, at the bottom of the screen, click the Run button. Configure the integration in Apache Spark â Did you know? You will need the Atlan API token and connection name to configure the integration in Apache Spark/OpenLineage. This will allow Apache Spark to connect with the OpenLineage API and send events to Atlan. Spark has a default SparkListener interface that OpenLineage leverages to collect information about Spark jobs. To configure Apache Spark to send OpenLineage events to Atlan, you can either: To activate the listener, add the following properties to your Spark configuration:   Initialize Spark session spark = ( SparkSession . builder . master ( 'local' ) . appName ( \"SparkJobs\" ) . config ( 'spark.jars.packages' , \"io.openlineage:openlineage-spark:<latest OpenLineage version>\" ) . config ( 'spark.extraListeners' , 'io.openlineage.spark.agent.OpenLineageSparkListener' ) . config ( 'spark.openlineage.transport.type' , 'http' ) . config ( 'spark.openlineage.transport.url' , 'https://<instance>.atlan.com' ) . config ( 'spark.openlineage.transport.endpoint' , '/events/openlineage/spark/api/v1/lineage' ) . config ( 'spark.openlineage.namespace' , '<connection-name>' ) . config ( 'spark.openlineage.transport.auth.type' , 'api_key' ) . config ( 'spark.openlineage.transport.auth.apiKey' , '<Atlan_api_key>' ) . getOrCreate ( ) ) Atlan recommends using the latest available version of the OpenLineage package for the Apache Spark integration. Replace <latest OpenLineage version> with the latest version of OpenLineage . url : set the URL of your Atlan instance   -  for example, https://<instance>.atlan.com . endpoint : points to the service that will consume OpenLineage events   -  for example, /events/openlineage/spark/api/v1/lineage . namespace : set the connection name as exactly configured in Atlan. apiKey : set the API token generated in Atlan. Add the above configuration to your cluster's spark-defaults.conf file or specific jobs on submission via the spark-submit command. Once the data processing tool has completed running, you will see Spark jobs along with lineage from OpenLineage events in Atlan! ð You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: data api authentication configuration Previous Apache Spark OpenLineage Next What does Atlan crawl from Apache Spark/OpenLineage? Create an API token in Atlan Select the source in Atlan Configure the integration in Atlan Configure the integration in Apache Spark"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to integrate Atlan with Microsoft Excel On this page Integrate Atlan with Microsoft Excel The Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel. Integrating Atlan with Microsoft Excel will allow you to: Import column metadata for your data assets to Microsoft Excel Update column metadata for your imported assets directly in Microsoft Excel Download impacted assets in Microsoft Excel To integrate Atlan with Microsoft Excel: If your Atlan tenant is hosted on the standard domain https://<your-tenant-name>.atlan.com , you can either: Install and connect the Atlan add-in as an individual user Deploy and publish the Atlan add-in for your organization as a Microsoft 365 admin Â If your Atlan tenant is hosted on a custom domain https://<your-tenant-name>.mycompany.com , your Microsoft 365 admin will need to configure the Atlan add-in using PowerShell Set up the add-in as a user â Who can do this? Any individual in your organization with access to Atlan can install the Atlan add-in for Microsoft Excel. Your Atlan tenant must be hosted on the standard domain atlan.com to set up the add-in as a user. Install Atlan in Microsoft Excel â To install the Atlan add-in directly in Microsoft Excel: Open a new Microsoft Excel workbook. From the upper right of the Home tab, click the Add-ins button, and then from the dropdown, click More Add-ins . In the Office add-ins dialog, click Store . In the search bar of your Office Store , type Atlan and press enter. Select the Atlan add-in and click Add . If you see a dialog asking for permissions, click Continue to proceed. Connect Atlan to Microsoft Excel â To connect Atlan with your Microsoft Excel workbook: In the menu bar of your Microsoft Excel workbook, click the Atlan tab. From the top left of the Atlan tab, click Setup to set up Atlan in your Microsoft Excel workbook. In the Atlan sidebar on the right, enter your Atlan tenant name   -  for example, https://<your-tenant-name>.atlan.com . If you have a custom domain,Â additional configuration will be required. Click Login to connect Atlan to Microsoft Excel. If you haven't logged into Atlan, you will be prompted to enter your credentials   -  including SSO, if enforced in your organization. Once connected, you can either enrich column metadata or download impacted assets for lineage analysis . Congrats on connecting Atlan with Microsoft Excel! ð danger For every new Microsoft Excel workbook that you create, you will need to follow the steps outlined above to connect Atlan to that workbook. The Atlan add-in will remain connected for all worksheets within an already connected workbook. Deploy and publish the add-in as an admin â Who can do this? You will need your Microsoft 365 administrator to complete these steps   -  you may not have access yourself. Before you begin, you may need to Determine if Centralized Deployment of add-ins works for your organization . The Atlan add-in can be installed at the workspace level for: Standard domains -  your Atlan tenant must be hosted as a subdomain of atlan.com to deploy the add-in using the steps below. Custom domains -  if your Atlan tenant is hosted under a custom domain belonging to your organization, you will need to configure the Atlan add-in using PowerShell . To install the Atlan add-in directly in Microsoft Excel: Sign in atÂ admin.microsoft.com . From the left menu of the admin center, click the Settings dropdown and then click Integrated apps . On the Integrated apps page, under Deployed apps , click Get apps . In the top right of the Microsoft 365 Apps published apps page, navigate to the search bar, type Atlan and press enter. Select the Atlan add-in for Microsoft Excel and click Get it now . If you see a dialog asking for permissions, click Get it now to proceed. In the Deploy New App dialog, enter the following details: In the Add users page, for Assign users , you can either: Click Entire organization to deploy the add-in to all users in your organization. Click Specific users/groups to deploy the add-in to a subset of users in your organization. Use the search box to find specific users or groups. Click Next to continue. In the Accept permissions requests page, the app capabilities and permissions of the apps are listed. If the app needs consent, select Accept permissions . Otherwise, click Next to continue. In the Review and finish deployment page, review the deployment and click Finish deployment . Once deployment is completed, click Done to finish setup. Note that it can take up to 24 hours for an add-in to show up for all your users. All your users will need to do next is connect Atlan to Microsoft Excel ! ð Tags: data integration Previous How to integrate Atlan with Google Sheets Next Link your account Set up the add-in as a user Deploy and publish the add-in as an admin"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/integrate-jira-data-center",
    "text": "Configure Atlan Integrations Project Management Jira Get Started How to integrate Jira Data Center On this page Integrate Jira Data Center Who can do this? You will need to be an admin in Atlan to configure the Jira Data Center integration. You will also need inputs and approval from an administrator of your Jira workspace. danger If your Jira Data Center instance is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you are not using the IP allowlist, you can skip this step.) To integrate Jira Data Center and Atlan, follow th ese steps. Configure an incoming app link in Jira Data Center â You will need to configure an incoming link with an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider. Atlan requires the minimum scope of WRITE to create issues in Jira Data Center. However, actual permissions are capped at what the authorizing user can do. For example, if the authorizing user lacks the permission to delete issues or projects, then Atlan will not have the permission to do so even with the WRITE scope. To configure an incoming link for Atlan, from within Jira Data Center: Log in to your Jira instance. Copy the Jira site URL from your browser's address bar and store it in a secure location. If you're viewing the dashboard, the site URL is everything that comes before /secure/Dashboard.jspa . From the top right of your Jira instance, click the settings icon, and from the dropdown, click Applications . In the left menu of the Applications page, under Integrations , click Application links . From the top right of the Application links page, click Create link to create a new application link for Atlan. In the Create link dialog, enter the following details: For Application type , click External application to link to an external application using OAuth 2.0. For Direction , click Incoming to allow Atlan to access data from Jira. In the corresponding Configure an incoming link page, enter the following details: For Name , enter a meaningful name for your application   -  for example, Atlan_integration . Under Application details , for Redirect URL , enter the redirect URL in the following format   - https://${client-domain}.atlan.com/oauth-callback . Under Application permissions , for Permissions , click the dropdown and then click Write . The WRITE scope will allow Atlan to: View projects and issues Create, update, and delete projects and issues Click Save Â to save your selections. From the corresponding Credentials page, click Copy to copy the Client ID and Client secret and store them in a secure location. Connect Atlan to Jira Data Center â Once you have retrieved the Jira instance URL and client ID and client secret from Jira, you can proceed to connecting Atlan to Jira Data Center. danger You must have at least one issue already created in Jira before integrating it with Atlan. To connect Atlan to Jira Data Center, from within Atlan: From the left menu, clickÂ Admin . UnderÂ Workspace , clickÂ Integrations . In theÂ Jira tile, to the right of the Connect button, click the downward arrow and then click Connect with Jira Data Center . A new Jira Data Center window will open and you'll be asked to install the Atlan app and create an application link in Jira. Click Next to proceed. In the corresponding Jira Data Center dialog, for Add credentials , enter the following details: For Instance URL , enter the URL of your Jira instance . For Client ID , enter the client ID you copied in Jira. For Client secret , enter the client secret you copied in Jira. In the OAuth 2.0 Authorization Consent popup, click Allow to complete the connection. Click the Add to Jira button to install the Atlan app in Jira Data Center. Install Atlan app in Jira Data Center â Before you can install the Atlan app in Jira Data Center, navigate to the Atlan for Jira Data Center app URL and click Get it now to download the app. To install the Atlan app, from within Jira Data Center: Log in to your Jira instance. Under Administration , from the tabs along the top, click Manage apps . From the left menu under Atlassian Marketplace , click Manage apps . From the upper right of the Manage apps page, click Upload app . In the Upload app dialog, for Upload the .jar or .obr file for a custom or third-party app here. , select the app file you downloaded . Click Upload to complete the installation. Changes to the apps in your instance will affect Jira search index. After you make changes to the app, you'll get the following message in the Administration view: We recommend that you perform a re-index, as configuration changes were made to SECTION by USER at TIME . If you have other changes to make, complete them first so that you don't perform multiple re-indexes . You will need to perform a full re-index for the integration to succeed, follow the steps in the official Jira documentation to do so. Configure integration from Atlan to Jira Data Center â To configure the Jira Data Center integration from Atlan, from the Integrations sub-menu: Expand theÂ Jira tile. (You may need to refresh the page before the following options appear.) Under theÂ Configurations tab, for Projects , select the Jira project to use as your default project from the dropdown and click Update . (Optional) At any future time, you can review theÂ Overview tab to see the number of linked issues between Jira Data Center and Atlan. Atlan is now connected to Jira Data Center! ð Did you know? The default project is preselected when creating or linking issues to an asset in Atlan. You can change the project while creating or linking issues as needed. Tags: data integration Previous How to integrate Jira Cloud Next Link your Jira account Configure an incoming app link in Jira Data Center Connect Atlan to Jira Data Center Install Atlan app in Jira Data Center Configure integration from Atlan to Jira Data Center"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/how-tos/integrate-servicenow",
    "text": "Configure Atlan Integrations Project Management ServiceNow Get Started How to integrate ServiceNow On this page Integrate ServiceNow Who can do this? You will need to be an admin in Atlan to configure the ServiceNow integration. You will also need inputs and approval from a System Administrator of your ServiceNow instance with a security_admin role . danger If your ServiceNow instance is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan or submit a request . (If you are not using the IP allowlist, you can skip this step.) If your Atlan admin has enabled the governance workflows and inbox module in your Atlan workspace, you can create a ServiceNow integration to allow your users to grant or revoke data access for governed assets in Atlan or any other data source. This is only applicable if you: Enable governance workflows Want to use the data access approval workflow To integrate ServiceNow and Atlan, follow these st eps. Create an OAuth application in ServiceNow â You will need to create an OAuth application endpoint for Atlan to access your ServiceNow instance. To create an OAuth application , from within ServiceNow: Log in to your ServiceNow instance as a System Administrator with a security_admin role. From the address bar at the top of your browser window, copy the ServiceNow instance URL   -  for example, https://<instance_name>.service-now.com . This will be required to connect Atlan to your ServiceNow instance . From the top header of your ServiceNow instance, click All . From the dropdown, search for and select System OAuth and then click Application Registry . In the top-right corner of the Application Registries page, click New to create a new OAuth application. In the corresponding screen, for What kind of OAuth application? , click Create an OAuth API endpoint for external clients . In the Application Registries New record form, enter the following details: For Name , enter Atlan OAuth App . For Redirect URL , enter the redirect URL in the following format   - https://<atlan_instance_name>/oauth-callback . Replace <atlan_instance_name> with the name of your Atlan instance. For Logo URL , copy and paste https://assets.atlan.com/assets/atlan-primary-logo-png.png . Click Submit to create the OAuth application in ServiceNow. From the Application Registries page, select the OAuth application you created above. Copy the values for Client ID and Client Secret and store them in a secure location. Connect Atlan to ServiceNow â To connect Atlan to ServiceNow, you will need the following: ServiceNow instance URL   -  for example, https://<instance_name>.service-now.com Client ID and client secret of the OAuth application you created in ServiceNow To connect Atlan to ServiceNow, from within Atlan: Log in to your Atlan instance as an admin user. From the left menu, click Admin . Under Workspace , click Integrations . In the ServiceNow tile, click the Connect button. In the corresponding Add to ServiceNow dialog, for ServiceNow URL , enter the URL of your ServiceNow instance -  for example, https://<instance_name>.service-now.com . Click Next to proceed. This step requires the creation of an OAuth application in ServiceNow, follow the steps to do so . If you have already created it, in the Create OAuth app section, for Copy the Client ID and Secret from the new OAuth app and paste below , enter the following: For Client ID , enter the client ID you copied from ServiceNow . For Client Secret , enter the client secret you copied from ServiceNow . Click Next to proceed. In the Commit update set section, click Atlan Update Set.xml to download the update set XML file from Atlan to import and commit in ServiceNow . Configure the Atlan integration in ServiceNow â To configure the Atlan integration in ServiceNow, your ServiceNow System Administrator with a security_admin role will need to complete the following two steps: Import and commit the update set XML file downloaded from Atlan to create an Atlan data access catalog and business rule in ServiceNow. Create a new user in ServiceNow for the Atlan integration. Import and commit the update set XML file â To import and commit the update set XML file , from within ServiceNow: Log in to your ServiceNow instance as a System Administrator with a security_admin role. From the top header of your ServiceNow instance, click All . From the dropdown, search for and select System Update Sets and then click Retrieved Update Sets . On the Retrieved Update Sets page, under Related Links , click the Import Update Set from XML link. On the Import XML page, to upload the update set XML file downloaded from Atlan : For Step 1: Choose file to upload , click Choose file to upload the Atlan Update Set.xml file. For Step 2: Upload the file , click the Upload button. In the top-left corner of your screen, click the back button to return to the Retrieved Update Sets page. The Atlan update set will appear on the Retrieved Update Set list in a Loaded state. Once the XML file has successfully loaded, select the Atlan Update Set . Click Preview Update Set to preview the update set and address any issues. The update set includes the following: Atlan Data Access catalog   -  Atlan will create data access requests in this catalog. Atlan Business Rule -  this is required for Atlan to receive events from your ServiceNow instance to detect any changes in the status of data access requests created in Atlan and automatically update governance workflow requests. Atlan service role and access control list (ACL) updates   -  the Atlan service account requires a role with write access on the sc_request table to update specific fields such as description , short_description , and more. This operation especially requires the security_admin role to commit the update set from Atlan in ServiceNow. Scripted REST API -  this is initially required to retrieve the username and sys_id of the Atlan user completing the ServiceNow integration. Atlan creates a Scripted REST API /api/snc/oauth_userinfo that returns the username and sys_id for an authenticated user. Once the integration has been completed, Atlan will have the access token required for the integration to continue working. Click Commit Update Set . If the commit action fails, contact Atlan support . Create a new user â To create a new user , from within ServiceNow: Log in to your ServiceNow instance as a System Administrator with a security_admin role. From the top header of your ServiceNow instance, click All . From the dropdown, search for and select User Administration and then click Users . In the top-right corner of the Users page, click New to create a new user. In the User New record form, enter the following details: For User ID , enter atlan.service . For First name and Last name , enter Atlan and Service , respectively. Click Submit to create the new user. From the Users page, search for and select the atlan.service user you created. On the User atlan.service page, scroll down to the table at the bottom of the screen. In the table, change to the Roles tab and then click the Edit button. On the Edit Members page, configure the following: In the Collection list, search for and select atlan_service_account_role . Click the greater than icon to add the role to the atlan.service user you created. Click Save to save your role assignment for the new user. In the User atlan.service page, click the Set Password button to create a password for the new user. In the Set Password dialog, click the Generate button to generate a password. Once a password has been generated, click the clipboard icon to copy the value and store it in a secure location. Click Save Password . This password will be required to configure the ServiceNow integration in Atlan . Configure the ServiceNow integration in Atlan â To configure the ServiceNow integration in Atlan, from within Atlan: Log in to your Atlan instance as an admin user. From the left menu, click Admin . Under Workspace , click Integrations . Expand the ServiceNow tile. In the Commit update set section, for Password , enter the password you copied from ServiceNow for the new user . Click the Connect and test button to test the ServiceNow integration. This ensures that the update set was committed successfully in ServiceNow and Atlan can receive webhook events. To test the latter, Atlan will create a sample request in ServiceNow, wait for a few seconds to receive a webhook event, and then display the status of the connection as CONNECTED . (Optional) Under the Configurations tab, for Test Connection , click the Run test button to verify that the ServiceNow integration is working properly in Atlan at any time. (Optional) At any future time, you can review the Overview tab to see the number of linked issues between ServiceNow and Atlan. Atlan is now connected to ServiceNow! ð Tags: data integration Previous ServiceNow Next Link your ServiceNow account Create an OAuth application in ServiceNow Connect Atlan to ServiceNow Configure the Atlan integration in ServiceNow Configure the ServiceNow integration in Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/link-your-account",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Link your account On this page Link your account To export assets to and bulk enrich metadata from a supported spreadsheet tool, you may first need to link your Google or Microsoft online account. This is done automatically for the admin user that integrated the spreadsheet tool to enable asset export, but not for other users. Although you will be prompted to sign in with your Google or Microsoft account while exporting assets, Atlan also provides an additional option to connect your account from the user profile. Atlan uses the same set of permissions to connect to your organizational Google or Microsoft online account as specified here . Link your account â To link your spreadsheet tool account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. From the Integrations tab, you can either: For Google Sheets , click theÂ Connect link. For Microsoft Excel ,Â click theÂ Connect link. In the resulting popup, scroll to the bottom and clickÂ Allow . Unlink your account â To unlink your spreadsheet tool account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. From the Integrations tab, you can either: For Google Sheets , click the Disconnect Â link. For Microsoft Excel , click the Disconnect Â link. In the confirmation dialog, clickÂ Confirm . Tags: data integration Previous How to integrate Atlan with Microsoft Excel Next How to update column metadata in Google Sheets Link your account Unlink your account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/how-tos/link-your-servicenow-account",
    "text": "Configure Atlan Integrations Project Management ServiceNow Get Started Link your ServiceNow account On this page Link your ServiceNow account To request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that set up the ServiceNow integration , but not for other users. Link your ServiceNow account â To link your ServiceNow account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under ServiceNow , click the Connect link. In the resulting popup, scroll to the bottom and click Allow . Unlink your ServiceNow account â To unlink your ServiceNow account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under ServiceNow , click the Disconnect link. In the confirmation dialog, click Confirm . Tags: data integration Previous How to integrate ServiceNow Next Troubleshooting ServiceNow Link your ServiceNow account Unlink your ServiceNow account"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/references/preflight-checks-for-soda",
    "text": "Connect data Data Quality & Observability Soda References Preflight checks for Soda Preflight checks for Soda Before running the Soda crawler , you can run preflight checks to perform the necessary technical validations. The following preflight checks are completed: Verify datasources have datasets: â Check successful â No datasets found for datasources: datasource1, datasource2 Tags: data crawl Previous What does Atlan crawl from Soda?"
  },
  {
    "url": "https://docs.atlan.com/faq/security-and-compliance",
    "text": "Configure Atlan Frequently Asked Questions Security and Compliance On this page Security and Compliance Complete guide to Atlan's security features, compliance certifications, and data protection capabilities. Do you support HIPAA? â Yes, Atlan is HIPAA compliant. Visit Atlan's security portal to view the attestation letter from an external auditor. Do you support SOC 2? â Yes, Atlan maintains SOC 2 Type II compliance certification. Visit Atlan's security portal to access compliance documentation and reports. How does Atlan protect the data at rest? â Atlan implements comprehensive data protection measures. For detailed information, see Encryption and key management . How's security enforced in Atlan? â Atlan uses Kubernetes in an Atlan-managed VPC (virtual private cloud). The data in Atlan is secured in the following ways: Infrastructure security : Restrict network access to the control planes as well as nodes. Access policies : Administrators can restrict user access to certain assets. Bring your own credentials (BYOC) : Users can provide their own data store credentials to query data. What data is Atlan actually bringing in? â Atlan enables you to search and discover metadata, not the data itself. As a data catalog of all your data assets, Atlan enables you to: Extract metadata from source systems via pushdown queries or API requests. Process data with the sample data and query features, both of which can be turned off. Push down queries when sample data or query functionality is used, so, the results are neither cached nor stored in Atlan. Integrate with your supported data sources via a service account with read-only permissions to the data source and complete control over these permissions. What Atlan IP address can I add to my organization's firewall? â If your organization's firewall only allows access from whitelisted IP locations, you can contact Atlan support to provide you with your Atlan IP address. Tags: data authentication faq-security Previous Data Connections and Integration Next Tags and Metadata Management"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/set-up-amazon-msk",
    "text": "Connect data Event/Messaging Amazon MSK Get Started Set up Amazon MSK On this page Set up Amazon MSK warning ð¤ Who can do this? You will need your Amazon MSK administrator to run these commands   -  you may not have access yourself. Atlan supports IAM role authentication for fetching metadata from Amazon Managed Streaming for Apache Kafka (Amazon MSK). This method uses an AWS role ARN and region to fetch metadata. For IAM role authentication, Atlan supports TLS encryption to ensure secure and encrypted communication between Atlan and your Amazon MSK cluster. Additionally, Atlan currently only supports the following: Apache Kafka 2.7.1 or higher for Amazon MSK Provisioned deployment Create IAM policy â To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"kafka-cluster:Connect\" \"kafka-cluster:DescribeCluster\" , \"kafka-cluster:DescribeGroup\" , \"kafka-cluster:DescribeTopic\" , \"kafka-cluster:DescribeTopicDynamicConfiguration\" , \"kafka-cluster:DescribeClusterDynamicConfiguration\" ] , \"Resource\" : [ \"arn:aws:kafka:<region>:<account_id>:cluster/<cluster_name>/<cluster_uuid>\" , \"arn:aws:kafka:<region>:<account_id>:group/<cluster_name>/<cluster_uuid>/*\" , \"arn:aws:kafka:<region>:<account_id>:topic/<cluster_name>/<cluster_uuid>/*\" ] , } ] } Replace <region> with the AWS region of your Amazon MSK cluster. Replace <account_id> with your AWS account ID. Replace <cluster_name> with the name of your Amazon MSK cluster. Replace <cluster_uuid> with the universally unique identifier (UUID) of your Amazon MSK cluster. IAM permissions â Atlan requires the following permissions: kafka-cluster:Connect -  grants permission to connect to the Amazon MSK cluster as a Kafka client, allowing the user or service to interact with Kafka brokers for producing and consuming messages. kafka-cluster:DescribeCluster -  grants permission to extract metadata about the Amazon MSK cluster, such as its configuration, status, and associated brokers. kafka-cluster:DescribeGroup -  grants permission to describe consumer groups in the Kafka cluster. This includes metadata such as consumer group, members, and their assigned partitions. kafka-cluster:DescribeTopic -  grants permission to describe Kafka topics, including metadata such as partitions and replication factor for a topic. kafka-cluster:DescribeTopicDynamicConfiguration -  allows access to view the dynamic configurations of Kafka topics. This includes topic-level overrides for configurations like retention periods, which can be changed without requiring a cluster restart. kafka-cluster:DescribeClusterDynamicConfiguration -  allows access to view the dynamic configuration settings of a Kafka cluster. These configurations can change without restarting the cluster and include parameters like replication settings, broker properties, and more. Role delegation-based authentication â Using the policy created above, configure IAM role delegation-based authentication. To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: data authentication Previous Amazon MSK Next Set up a private network link to Amazon MSK Create IAM policy IAM permissions Role delegation-based authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight",
    "text": "Connect data BI Tools Cloud-based BI Amazon QuickSight Get Started Set up Amazon QuickSight On this page Set up Amazon QuickSight warning ð¤ Who can do this? You will probably need your Amazon QuickSight administrator to run these commands   -  you may not have access yourself. Atlan currently only supports IAM user authentication for Amazon QuickSight. Create IAM policy â To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"quicksight:ListAnalyses\" , \"quicksight:ListDataSets\" , \"quicksight:ListDashboards\" , \"quicksight:ListFolders\" , \"quicksight:ListDataSources\" , \"quicksight:DescribeAnalysis\" , \"quicksight:DescribeDashboard\" , \"quicksight:DescribeDataSet\" , \"quicksight:DescribeFolder\" , \"quicksight:ListFolderMembers\" ] , \"Resource\" : [ \"arn:aws:quicksight:<region>:<account_id>:*\" ] } ] } Replace <region> with the AWS region of your Amazon QuickSight instance. Replace <account_id> with your AWS account ID. Configure user-based authentication â Using the IAM policy created above, configure user-based authentication. To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On theÂ Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Tags: data authentication Previous Amazon QuickSight Next Crawl Amazon QuickSight Create IAM policy Configure user-based authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift Get Started Set up Amazon Redshift On this page Set up Amazon Redshift Who can do this? You will need your Amazon Redshift administrator to run these commands   -  you may not have access yourself. Atlan supports fetching metadata from Amazon Redshift for the following types of deployment: Provisioned RA3 DC2 Serverless danger If you're using the DC2 node type, Redshift restricts cross-database joins and metadata access to a single database. For more information, see Considerations - Amazon Redshift . Because of this restriction, you must set up a separate workflow for each database you want to crawl. Grant permissions â For all supported authentication mechanisms except IAM role authentication on serverless deployment , you must first grant the following permissions on Amazon Redshift. For IAM role authentication on serverless deployment only, skip to this step . Create a group and user â To create a group and user, run the following commands: CREATE GROUP atlan_users ; CREATE USER atlan_user password '<pass>' IN GROUP atlan_users ; Replace <pass> with the password for the atlan_user being created. To crawl Amazon Redshift , for Username , you must enter the username you configured for the database user. For example, atlan_user . Grant required permissions to group â To grant the minimum required permissions, run the following commands: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; GRANT SELECT ON pg_catalog.svv_table_info TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the above commands for all the databases in your schema( <schema_name> ). The permissions are used for the following: SVV_TABLE_INFO is used to obtain information on the table ID to table/schema/database relation. External schema â If your Redshift instance setup external schemas, you must grant permissions for each schema. Grant USAGE permissions â For external schemas, use the following command to grant USAGE permission: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat this command for all external schemas. Did you know? If your external tables are sourced from Amazon S3 and AWS Glue Catalog, granting only the USAGE permission is sufficient, provided that the IAM role associated with the Redshift cluster has read access to the data. (Optional) Grant SELECT permissions â For Redshift-based external schemas, you must explicitly grant SELECT along with USAGE permissions to allow metadata crawling. Use the following command to grant this permission: GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the command for all the Redshift-based external schemas. Verify external schema permissions â Follow these steps to verify permissions granted to your external schema: Log in to the system using the IAM role created earlier. Run the following command using any database viewer tool: SELECT * FROM SVV_EXTERNAL_TABLES WHERE schema_name = '<external_schema_name>' Replace <external_schema_name> with the name of your external schema. If the tables appear in the results, the permissions are correctly configured. If you can't provide SELECT or USAGE access, create a cloned schema and grant access to the atlan_users group. For more information, see Cloned schema for restricted access section. Cloned schema for restricted access â If you can't grant USAGE or SELECT permissions to the atlan_users group, you must create a cloned schema containing the necessary metadata tables. Then, grant permissions to the cloned schema. Follow these steps to create a cloned schema and provide required permissions: Log in as dbadmin . Create a new schema and give it a meaningful name. For example, atlan . Clone the following views as tables from the pg_catalog schema into the cloned schema: pg_views SVV_TABLES SVV_EXTERNAL_TABLES SVV_COLUMNS Clone the following views as tables from the information_schema into the cloned schema: key_column_usage as information_schema_key_column_usage table_constraints as information_schema_table_constraints Grant USAGE and SELECT access to the atlan_users group on the cloned schema: GRANT USAGE ON SCHEMA <cloned_schema_name> TO GROUP atlan_users; GRANT SELECT ON ALL TABLES IN SCHEMA <cloned_schema_name> TO GROUP atlan_users; Replace <cloned_schema_name> with the name of your cloned schema. Since Atlan relies on these tables to crawl metadata, schedule a cron job to refresh the cloned tables periodically. Did you know? You can reach out to Atlan support if you need assistance with setting up a Cloned Schema. (Optional) Grant permissions for role-based authentication on serverless â For IAM role-based authentication on Amazon Redshift serverless deployment only, you must first grant the following permissions on Amazon Redshift. Create a role â To create a role, run the following commands: CREATE ROLE atlan_role ; Grant required permissions to role â To grant the minimum required permissions, run the following commands: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; GRANT SELECT ON pg_catalog.svv_table_info TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the above commands for all the databases in your schema( <schema_name> ). The permissions are used for the following: SVV_TABLE_INFO is used to obtain information on the table ID to table/schema/database relation. External schema â If your Redshift setup uses external schemas, you must grant permissions for each schema. You can do this in one of the following ways: Grant USAGE permissions â For external schemas, use the following command to grant USAGE permission: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat this command for all external schemas. Did you know? If your external tables are sourced from Amazon S3 and AWS Glue Catalog, granting only the USAGE permission is sufficient, provided that the IAM role associated with the Redshift cluster has read access to the data. (Optional) Grant SELECT permissions â For Redshift-based external schemas, you must explicitly grant SELECT along with USAGE permissions to allow metadata crawling. Use the following command to grant this permission: GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the command for all the Redshift-based external schemas. Verify external schema permissions â Follow these steps to verify permissions granted to your external schema: Log in to the system using the IAM role created earlier. Run the following command using the Amazon Redshift Data API : SELECT * FROM SVV_EXTERNAL_TABLES WHERE schema_name = '<external_schema_name>' Replace <external_schema_name> with the name of your external schema. If the tables appear in the results, the permissions are correctly configured. If you can't provide SELECT or USAGE access, create a cloned schema and grant access to the atlan_users group. For more information, see Cloned schema for restricted access section. Cloned schema for restricted access â If you can't grant USAGE or SELECT permissions to the atlan_users group, you must create a cloned schema containing the necessary metadata tables. Then, grant permissions to the cloned schema. Follow these steps to create a cloned schema and provide required permissions: Log in as dbadmin . Create a new schema and give it a meaningful name. For example, atlan . Clone the following views as tables from the pg_catalog schema into the cloned schema: pg_views SVV_TABLES SVV_EXTERNAL_TABLES SVV_COLUMNS Clone the following views as tables from the information_schema into the cloned schema: key_column_usage as information_schema_key_column_usage table_constraints as information_schema_table_constraints Grant USAGE and SELECT access to the atlan_users group on the cloned schema: GRANT USAGE ON SCHEMA <cloned_schema_name> TO GROUP atlan_users; GRANT SELECT ON ALL TABLES IN SCHEMA <cloned_schema_name> TO GROUP atlan_users; Replace <cloned_schema_name> with the name of your cloned schema. Since Atlan relies on these tables to crawl metadata, schedule a cron job to refresh the cloned tables periodically. Did you know? You can reach out to Atlan support if you need assistance with setting up a Cloned Schema. Grant additional permissions for mining query history â Did you know? For mining query history from Redshift Serverless, permissions on STL and SVL views are not required as they do not exist in serverless deployment. To grant the additional permissions needed to mine query history, run the following commands: GRANT SELECT on pg_catalog . stl_ddltext to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_query to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_connection_log to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_undone to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_insert to GROUP atlan_users ; GRANT SELECT on pg_catalog . svl_statementtext to GROUP atlan_users ; ALTER USER atlan_user SYSLOG ACCESS UNRESTRICTED ; The additional permissions are used for the following: STL_DDLTEXT is used for DDL queries. STL_QUERY is used for DML and regular queries. STL_CONNECTION_LOG is used to obtain the session ID that a query is part of. STL_UNDONE is used to obtain information about transactions that have been undone or rolled back. STL_INSERT is used to obtain the table ID used in the insert queries. SVL_STATEMENTTEXT is used to obtain the query text for all queries. SYSLOG ACCESS UNRESTRICTED is used to access all queries performed by any user in the system tables above. To use basic authentication, your setup is now complete. To configure IAM-based authentication, you need to continue with the following steps. (Optional) Create IAM policy â All IAM-based authentication mechanisms require an IAM policy to be created. For all supported authentication mechanisms except IAM role authentication on serverless deployment , create the following IAM policy. For IAM role authentication on serverless deployment only, skip to this step . To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"redshift:GetClusterCredentials\" ] , \"Resource\" : [ \"arn:aws:redshift:<region>:<account_id>:dbuser:<redshift_cluster_identifier>/atlan_user\" , \"arn:aws:redshift:<region>:<account_id>:dbname:<redshift_cluster_identifier>/<database>\" ] } ] } Replace <region> with the AWS region of your Redshift instance. Replace <account_id> with your account ID. Replace <redshift_cluster_identifier> with your Redshift cluster identifier. Replace <database> with the name of the Redshift database. (Optional) Create IAM policy for role-based authentication on serverless â For IAM role-based authentication on Amazon Redshift serverless deployment only, create an IAM policy with the necessary permissions. Follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"redshift-serverless:GetCredentials\" ] , \"Resource\" : [ \"arn:aws:redshift-serverless:<region>:<account_id>:workgroup/<workgroup_identifier>\" , ] } ] } Replace <region> with the AWS region of your Amazon Redshift instance. Replace <account_id> with your AWS account ID. Replace <workgroup_identifier> with your Amazon Redshift serverless workgroup identifier. Configure tag for IAM role: { RedshiftDbRoles : <role> } Replace <role> with the role you created . (Optional) Choose IAM-based authentication mechanism â Using the policy created above, configure one of the following options for authentication. User-based authentication â To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication â To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security, paste the external ID into the policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_external_id>\" } } } ] } Replace <atlan_external_id> with the external ID you want to use. Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: data crawl authentication Previous Amazon Redshift Next How to enable SSO for Amazon Redshift Grant permissions (Optional) Grant permissions for role-based authentication on serverless Grant additional permissions for mining query history (Optional) Create IAM policy (Optional) Create IAM policy for role-based authentication on serverless (Optional) Choose IAM-based authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-an-azure-private-network-link-to-databricks",
    "text": "Connect data Data Warehouses Databricks Private Network Setup Set up an Azure private network link to Databricks On this page Set up an Azure private network link to Databricks Azure Private Link creates a secure, private connection between services running in Azure. This document describes the steps to set this up between Databricks and Atlan. Who can do this? You will need Databricks support, and probably your Databricks administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites â Your Databricks instance must be Azure-managed and created from the Azure marketplace. For all details, see Databricks documentation . Notify Atlan support â Provide Atlan support with the following information: Resource ID of your Azure-managed Databricks instance   -  the resource ID will be in this format: /subscriptions/<subscriptionID>/resourceGroups/azure-databricks/providers/Microsoft.Databricks/workspaces/<databricks-workspace> . There are additional steps that Atlan will need to complete. Once the Atlan team has confirmed that the configuration is ready, please continue with the remaining steps. Approve the endpoint connection request â To approve the endpoint connection request from Atlan: Open your Azure-managed Databricks workspace. In the left menu, click Networking and then click the Private endpoint connections tab. From the list of endpoints, search for the endpoint connection request from Atlan. In the Connection state column for the Atlan endpoint connection, click the Approve button to approve the request . Once the endpoint connection from Atlan has been approved, the status of the private endpoint in Atlan will change to Approved . When you use this endpoint in the configuration for crawling Databricks , Atlan will connect to Databricks over Azure Private Link. Tags: data configuration Previous Set up an AWS private network link to Databricks Next How to extract lineage and usage from Databricks Prerequisites Notify Atlan support Approve the endpoint connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/how-tos/set-up-confluent-kafka",
    "text": "Connect data Event/Messaging Confluent Kafka Get Started Set up Confluent Kafka On this page Set up Confluent Kafka Who can do this? You will need your OrganizationAdmin , EnvironmentAdmin , or CloudClusterAdmin role to complete these steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata. Create an API Key â This section provides steps for creating an API key to access metadata from your Confluent Kafka environment. info ðª Note: Atlan does not perform any API requests or queries that modify the resources in your Confluent Kafka environment. Resource-Specific API Key â To create a resource-specific API key for crawling Confluent Kafka , follow these steps: Log in to your Confluent Cloud instance with a OrganizationAdmin , EnvironmentAdmin , or CloudClusterAdmin role. From the Cloud Console , select your active cluster. Under Cluster Overview in the left menu for your active cluster, click API Keys . In the upper right of the API Keys page, click + Add key . On the Create key page, enter the following details: For Access Control , under Select Scope for API key , select Granular access to define a specific set of access rules, then click Next to proceed. For Service Account , click Create a new one and enter the following details: For New service account name , enter a meaningful name for your API key. For example, Atlan . (Optional) For Description , add a description for your API key. For example, Atlan crawler connection . Click Next to proceed. For Add ACLs to service account , click + Add ACLs to add and allow the following minimum permissions required for your Confluent Kafka resources: Cluster: DescribeConfigs Group: Describe Topic: Describe , DescribeConfigs Once you've added all the permissions, click Next to proceed. For Create key , under Get your API key , copy or download the API key and secret. Make sure to store them securely, as the secret can't be retrieved later. (Optional) Cloud API Key â To access Kafka metrics( sizeInBytes ), you need a Cloud API key. Follow these steps to generate one: Click the hamburger menu (â°) icon in the top right corner to open the menu. In the menu, click API Keys . On the API Key listing screen, click the Add API Key button to add a new API key. On the Select an account for API key screen, select the account appropriate for your service or user account, then click Next . On the Select resource scope for API key screen, choose Cloud resource management , then click Next . On the API details screen , enter the required details: Name : Provide a unique and meaningful name for your API key. For example, Atlan Kafka Metrics Key. Description : Add a description that illustrates the purpose of the key. For example, Atlan Kafka metrics API key with read-only permissions. Click Create API Key to generate the key. On the API key download screen , copy the API key and secret or click Download API Key to save them as a file. Make sure to store them securely, as the secret can't be retrieved later. Click Complete to finish the API key creation process. You will be able to see the generated API key on the API key listing screen. Tags: data api authentication Previous Confluent Kafka Next Crawl Confluent Kafka Create an API Key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/how-tos/set-up-datastax-enterprise",
    "text": "Connect data Databases NoSQL Databases Datastax Enterprise Get Started Set up DataStax Enterprise On this page Set up DataStax Enterprise ð¤ Who can do this? You need your DataStax Enterprise administrator or a user who has create role permissions to run these commands; you may not have access yourself. For more details, refer to the Apache Cassandra documentation on role-based access control . This guide outlines the steps to configure your DataStax Enterprise instance so Atlan can crawl its metadata. Prerequisites â Before you begin, ensure you have the following: Access to a DataStax Enterprise instance - You need the necessary credentials to connect to your Cassandra instance. cqlsh installed - The Cassandra Query Language Shell (cqlsh) is required to execute commands for user and permission management. If cqlsh is not installed, refer to the Apache Cassandra documentation for cqlsh . Create an Atlan user role â This section guides you through creating a dedicated role in DataStax Enterprise for Atlan. Connect to the DataStax Enterprise instance : Use cqlsh to connect to your DataStax Enterprise instance. Create a dedicated role for Atlan : Run the following command in cqlsh to create a role. CREATE ROLE <username> WITH SUPERUSER = false AND LOGIN = true AND PASSWORD = '<password>'; Replace the placeholders <username> : The username for the Atlan role. <password> : A strong and unique password. Grant permissions to the Atlan user role â Atlan needs specific permissions to access metadata in DataStax Enterprise Follow these steps to grant the required permissions: Grant DESCRIBE on all keyspaces with: GRANT DESCRIBE ON ALL KEYSPACES TO atlan-admin; You may also choose to grant DESCRIBE permission on specific keyspaces by running the following command: GRANT DESCRIBE ON KEYSPACE <keyspace_name> TO <username>; Replace placeholders: <keyspace_name> : The name of the keyspace. <username> : The username for the Atlan role. Run this command for each keyspace you want Atlan to access. Tags: data crawl Previous DataStax Enterprise Next Crawl DataStax Enterprise Prerequisites Create an Atlan user role Grant permissions to the Atlan user role"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/how-tos/set-up-fivetran",
    "text": "Connect data ETL Tools Fivetran Get Started Set up Fivetran On this page Set up Fivetran Fivetran Platform Connector â Who can do this? You need your Fivetran Account Administrator (who can create, view, edit, and delete destinations and connectors) to complete the steps below   -  you may not have access yourself. Note : You also need Fivetran Enterprise (or above) to use this integration. If you're not on such a plan yet, Atlan may be able to help you access a trial period from Fivetran. Just reach out to your Atlan contact! Create a destination â The Fivetran Platform Connector delivers your logs and account or destination metadata to a schema in your destination. Fivetran automatically adds this connector to every new destination you create. You need to set up at least one destination to receive the log events. Atlan currently supports the following destinations, refer to Fivetran documentation linked below to configure them in Fivetran: Amazon Redshift Databricks Google BigQuery PostgreSQL Snowflake If you have already configured a destination in Fivetran, skip to the next step. Configure Fivetran Platform Connector â Once you have set up a supported destination, follow the steps in this setup guide from Fivetran to set up your Fivetran Platform Connector account-wide. You must configure the Fivetran Platform Connector on an account level. Atlan recommends not excluding any columns , as this can impact the successful execution of the Fivetran enrichment package. The warehouse credentials or role configured in Atlan must have the necessary permissions to query tables in <Fivetran_destination_database>.<FPC_schema_name> . Refer to the Fivetran documentation for available FPC_schema_name values. The role must have the required permissions to access these tables. Refer to the relevant guide below for setting up permissions: Set up Snowflake Set up Google BigQuery Set up Databricks Set up Amazon Redshift Set up PostgreSQL This enables you to sync all the metadata and logs for all the connectors in your account to a single destination. Tags: data integration crawl api configuration Previous Fivetran Next Crawl Fivetran Fivetran Platform Connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/set-up-hive",
    "text": "Connect data Databases Query Engines Hive Get Started Set up Hive On this page Set up Hive Who can do this? You will need your Hadoop administrator to run these commands   -  you may not have access yourself. Currently, we only support basic username and password authentication for Hive. Complete the following steps to configure it. Set the right permissions â To configure basic authentication for Hive, enter the following details: For Host Name , enter theÂ Atlan-accessible Hive instance URL. For Port , enter the port number where your Hive instance is accessible. For Default Schema , enter the default schema name in your Hive instance for connection. Atlan will crawl other schemas too   -  not just the default one. Grant read permission on objects â Grant read permission on objects with the following commands: GRANT SELECT ON DATABASE < database_name > TO USER < username > ; Atlan requires read permission for all the objects you want to crawl in Hive. Did you know? Available users and access control may also be controlled or affected by HDFS ACL, LDAP, and any other policy engine that is in effect. Overall, Atlan requires the authenticating user to have read permission at a minimum. Tags: data crawl authentication Previous Hive Next Set up a private network link to Hive Set the right permissions Grant read permission on objects"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db",
    "text": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB Get Started Set up Microsoft Azure Cosmos DB On this page Set up Microsoft Azure Cosmos DB Did you know? Atlan currently only supports crawling Microsoft Azure Cosmos DB for MongoDB with the Microsoft Azure Cosmos DB connector. Atlan supports the following deployment types for fetching metadata from Microsoft Azure Cosmos DB: vCore-based deployment   -  you can use SCRAM-SHA authentication for vCore-based accounts. You will need to authenticate the connection in Atlan with a primary connection string to fetch metadata from vCore-based accounts. Atlan provides multi-account support for fetching metadata. RU-based deployment   -  you can use service principal authentication for request unit (RU)-based accounts. You will need to authenticate the connection in Atlan with a client ID, client secret, and tenant ID to fetch metadata from RU-based accounts. Atlan provides multi-account support for fetching metadata. If your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the vCore and RU deployment option to crawl your Microsoft Azure Cosmos DB assets . vCore deployment â Who can do this? You will need your Microsoft Azure Cosmos DB administrator to complete these steps   -  you may not have access yourself. For vCore-based accounts, you will need the primary connection string of your Microsoft Azure Cosmos DB deployment to use SCRAM-SHA authentication for integrating with Atlan . To retrieve the primary connection string for vCore-based accounts: Log in to the Azure portal as an admin. In the portal, search for and select Azure Cosmos DB . On the Azure Cosmos DB page, select your Azure Cosmos DB for MongoDB (vCore) account. From the Overview page, copy the value of the Admin username . For password, you will need the password that was set up during your Microsoft Azure Cosmos DB deployment. In the left menu of the account page, under Settings , click Connection strings . Copy the value of the Primary Connection String and store it in a secure location. You will need to add the values of the admin username and password to the placeholder values in the primary connection string you copied. Repeat steps 1 to 6 for all the vCore-based accounts you want to crawl in Atlan. RU-based deployment â For request Unit (RU)-based accounts, you will need a client ID, client secret, and tenant ID for service principal authentication. Microsoft Azure Cosmos DB for MongoDB deployment currently does not support service principal authentication for vCore-based accounts. Register app with Microsoft Entra ID â Who can do this? You will need your Cloud Application Administrator or Application Administrator to complete these stepsÂ   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization. You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID , and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Set permissions â Who can do this? You will need your Microsoft Azure Cosmos DB administrator to complete these steps   -  you may not have access yourself. You will need to add the service principal to the Cosmos DB Account Reader Role . This will allow the service principal read-only access to your Azure Cosmos DB account data. To add the service principal to the Cosmos DB Account Reader Role : Log in to the Azure portal . Open the menu and search for or select Azure Cosmos DB . On the Azure Cosmos DB page, select your Azure Cosmos DB for MongoDB (RU) account. From the left menu of your Azure Cosmos DB for MongoDB (RU) account page, click Access control (IAM) . From the tabs along the top of the Access control (IAM) page, click Add and then click Add role assignment . On the Add role assignment page, configure the following: In the Roles tab, from the list of roles under Job function roles ,Â select Cosmos DB Account Reader Role Â   -  this allows read-only access to Azure Cosmos DB account data -  and then click Next . You will need to assign this role to all the RU-based accounts you want to crawl in Atlan. In the Members tab, enter the following details: For Assign access to , click User, group, or service principal . For Members , click + Select members and then select the service principal you created. Click Next to proceed to the next step. In the Review + assign tab, click Review + assign to add role assignment. (Optional) Whitelist Atlan IP range â You may need to whitelist Atlan's IP range to allow Atlan to crawl Microsoft Azure Cosmos DB . To whitelist the Atlan IP range: Log in to the Azure portal . Open the menu and search for or select Azure Cosmos DB . On the Azure Cosmos DB page, select your Azure Cosmos DB for MongoDB account. From the left menu of your Azure Cosmos DB for MongoDB account page, click Networking . On the Networking page, under Public network access , check the following: If All networks is enabled, no further action required. If Select networks is enabled, raise an Atlan support request to obtain Atlan's IP range. Once received from Atlan support, for IP (Single IPv4 or CIDR range) , enter Atlan's IP range and click the Save button. Tags: data crawl authentication Previous Microsoft Azure Cosmos DB Next Crawl Microsoft Azure Cosmos DB vCore deployment RU-based deployment (Optional) Whitelist Atlan IP range"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-azure-data-factory",
    "text": "Connect data ETL Tools Microsoft Azure Data Factory Get Started Set up Microsoft Azure Data Factory On this page Set up Microsoft Azure Data Factory Atlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata. Register app with Microsoft Entra ID â Who can do this? You will need your Cloud Application Administrator or Application Administrator to complete these stepsÂ   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization. You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID , and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Set permissions â Who can do this? You will need your Microsoft Azure Data Factory administrator to complete these steps   -  you may not have access yourself. You will need to add the service principal to the Reader role . This will allow the service principal read-only access to your Microsoft Azure Data Factory account. To add the service principal to the Reader role: Log in to the Azure portal . Open the menu and search for or select Data factories . On the Data factories page, select the data factory you want to crawl in Atlan. From the left menu of your data factory page, click Access control (IAM) . From the tabs along the top of the Access control (IAM) page, click Add and then click Add role assignment . On the Add role assignment page, configure the following: In the Roles tab, from the list of roles under Job function roles , select Reader Â   -  this allows read-only access to your data factory -  and then click Next . You will need to assign this role to all the data factories you want to crawl in Atlan. In the Members tab, enter the following details: For Assign access to , click User, group, or service principal . For Members , click + Select members and then select the service principal you created. Click Next to proceed to the next step. In the Review + assign tab, click Review + assign to add role assignment. Atlan will extract metadata from all the data factories you specified in your Microsoft Azure Data Factory account with Reader access. Tags: data api authentication Previous Microsoft Azure Data Factory Next Crawl Microsoft Azure Data Factory Register app with Microsoft Entra ID Set permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server Get Started Set up Microsoft SQL Server On this page Set up Microsoft SQL Server Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Microsoft SQL Server. This method uses a username and password to fetch metadata. Create a login â To create a login with a specific password to integrate into Atlan: CREATE LOGIN < login_name > WITH PASSWORD = '<password>' ; Replace <login_name> with the name of the login. Replace <password> with the password for the login. Create a user â To create a user for that login: CREATE USER < username > FOR LOGIN < login_name > ; Replace <username> with the username to use when integrating Atlan. Replace <login_name> with the name of the login used in the previous step. Grant permissions â Crawl assets and mine view lineage â The following grant crawls all your Microsoft SQL Server assets and mines lineage for views. As the option of least privilege access, it avoids accessing any raw data. To grant the minimum permissions required to crawl assets and mine view lineage from Microsoft SQL Server: GRANT VIEW DEFINITION ON DATABASE :: < database_name > TO < username > ; Replace <database_name> with the name of the database. Replace <username> with the username created above. (Optional) Preview and query assets â To grant the minimum permissions required to also preview sample data and query assets from Microsoft SQL Server: GRANT SELECT ON DATABASE :: < database_name > TO < username > ; Replace <database_name> with the name of the database. Replace <username> with the username created above. danger You must grant permissions to the user for all the databases you want to crawl in Atlan except the system databases ( master , tempdb , msdb , model ). The Microsoft SQL Server crawler will only fetch database and schema names without these permissions and no other metadata for other asset types. Tags: data crawl authentication Previous Microsoft SQL Server Next Crawl Microsoft SQL Server Create a login Create a user Grant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-mysql",
    "text": "Connect data Databases SQL Databases MySQL Get Started Set up MySQL On this page Set up MySQL Who can do this? You will probably need your MySQL administrator to run these commands   -  you may not have access yourself. Did you know? Atlan supports both of the following AWS database engines   -  RDS MySQL and Aurora MySQL. Currently we support the following authentication mechanisms. You will need to choose one and configure it according to the steps below. Basic authentication Identity and Access Management (IAM) authentication Basic authentication â To configure basic authentication for MySQL, run the following commands: CREATE USER '{{db-username}}' @'%' IDENTIFIED BY '{{password}}' ; GRANT SELECT , SHOW VIEW , EXECUTE ON * . * TO '{{db-username}}' @'%' ; FLUSH PRIVILEGES ; Replace {{db-username}} with the username you want to create. Replace {{password}} with the password to be used for that username. Atlan requires the following privileges to: SELECT : Fetch the technical metadata persisted in the INFORMATION_SCHEMA . *.* is required because INFORMATION_SCHEMA tables cannot be granted access directly. Metadata is inferred from the access that the querying user has on the underlying tables. Enable users to preview or query the underlying tables and views   -  this functionality can also be turned off. SHOW VIEW enables the use of the SHOW CREATE VIEW statement to fetch view definitions for generating lineage. EXECUTE is only required if using MySQL 5.7 and any earlier versions. Identity and Access Management (IAM) authentication â To configure IAM authentication for MySQL follow each of these steps. Enable IAM authentication â To enable IAM authentication for your database instance: For Amazon RDS, follow the steps in the Amazon RDS documentation . For Aurora, follow the steps in the User Guide for Aurora documentation . When given the option, apply the changes immediately and wait until they are complete. Create database user â To create a database user with the necessary permissions run the following commands: CREATE USER '{{db-username}}' @'%' IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS' ; GRANT SELECT , SHOW VIEW , EXECUTE ON * . * TO '{{db-username}}' @'%' ; FLUSH PRIVILEGES ; Replace {{db-username}} with the username you want to create. These permissions will allow you to crawl metadata, preview and query data from within Atlan. Create IAM policy â To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"rds-db:connect\" ] , \"Resource\" : [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } Replace {{aws-region}} with the AWS region of your database instance. Replace {{account-id}} with your account ID. Replace {{resource-id}} with the resource ID. Replace {{db-username}} with the username created in the previous step. Attach IAM policy â To attach the IAM policy for Atlan's use, you have two options: IAM role : Attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster. Please raise a support ticket to use this option. IAM user : Create an AWS IAM user and attach the policy to this user. To create an AWS IAM user: Follow the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Tags: data authentication Previous MySQL Next Set up a private network link to MySQL Basic authentication Identity and Access Management (IAM) authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Get Started Set up on-premises IBM Cognos Analytics access On this page Set up on-premises IBM Cognos Analytics access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details, including credentials. In some cases you will not be able to expose your IBM Cognos Analytics instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your on-premises IBM Cognos Analytics instance, you will need to use Atlan's cognos-extractor tool. Did you know? Atlan uses exactly the same cognos-extractor behind the scenes when it connects to IBM Cognos Analytics in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the cognos-extractor tool â To get the cognos-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl IBM Cognos Analytics: sudo docker load -i /path/to/cognos-extractor-master.tar Get the compose file â Atlan provides you with a Docker compose file for the cognos-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises IBM Cognos Analytics instance. The file is docker-compose.yaml . Define IBM Cognos Analytics connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your IBM Cognos Analytics connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises IBM Cognos Analytics instance, define an entry under services in the compose file. Each entry will have the following structure: services: cognos-example: <<: *extract environment: <<: *cognos-defaults EXCLUDE_FILTER: '{}' INCLUDE_FILTER: '{}' volumes: - ./output/cognos-example:/output/process Replace cognos-example with the name of your connection. <<: *extract tells the cognos-extractor tool to run. environment contains all parameters for the tool. EXCLUDE_FILTER and INCLUDE_FILTER -  specify a regular expression to filter assets to exclude or include, respectively. For example, to exclude a folder with the ID 76471ff1e0f02c7d3349 in team_content , configure the EXCLUDE_FILTER as follows   - '{\"team_content\": {\"76471ff1e0f02c7d3349\": {}}' . volumes specifies where to store results. In this example, the extractor will store results in the ./output/cognos-example Â folder on the local file system. You can add as many IBM Cognos Analytics connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your IBM Cognos Analytics connections, you will need to provide an IBM Cognos Analytics configuration file. The IBM Cognos Analytics configuration is a .ini file with the following format: [CognosConfig] host=http://cognos-application-host-example.us-east-2.compute.amazonaws.com port=9300 namespace=CognosEx   possible values are \"basic_auth\", \"okta_auth\" and \"api_key\" auth_type=basic_auth   Only required when auth_type = basic_auth [BasicAuth] [email protected] password=<password>   Only required when auth_type = okta_auth [OKTAAuth] [email protected] password=<password>   Only required when auth_type = api_key [APIKeyAuth] key=<yourAPIkey> Secure credentials â Using local files â danger If you decide to keep IBM Cognos Analytics credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: cognos_config: file: ./cognos.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets â To create and use Docker secrets: Store the IBM Cognos Analytics configuration file: sudo docker secret create cognos_config path/to/cognos.ini At the top of your compose file, add a secrets element to access your secret: secrets: cognos_config: external: true name: cognos_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local IBM Cognos Analytics configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets: cognos_config: external: true name: cognos_config x-templates:   ... services: cognos-example: <<: *extract environment: <<: *cognos-defaults EXCLUDE_FILTER: '{}' INCLUDE_FILTER: '{}' volumes: - ./output/cognos-example:/output/process secrets: - cognos_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The cognos_config refers to an external Docker secret created using the docker secret create command. The name of this service is cognos-example . You can use any meaningful name you want. The <<: *cognos-defaults sets the connection type to IBM Cognos Analytics. The EXCLUDE_FILTER and INCLUDE_FILTER tells the extractor to filter folders. The ./output/cognos-example:/output/process line tells the extractor where to store results. In this example, the extractor will store results in the ./output/cognos-example directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up IBM Cognos Analytics Next Crawl IBM Cognos Analytics Prerequisites Get the compose file Define IBM Cognos Analytics connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access",
    "text": "Connect data Event/Messaging Redpanda Kafka Guides Set up on-premises Kafka access On this page Set up on-premises Kafka access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Kafka instance details, including credentials. In some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your Kafka instance, you will need to use Atlan's kafka-extractor tool. Did you know? Atlan uses exactly the same kafka-extractor behind the scenes when it connects to Kafka in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the kafka-extractor tool â To get the kafka-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Kafka: sudo docker load -i /path/to/kafka-extractor-master.tar Get the compose file â Atlan provides you with a Docker compose file for the kafka-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Kafka instance. The file is docker-compose.yaml . Define Kafka connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. Keep the default settings; no changes are required. services is where you will define your Kafka connections. volumes contains mount information. Keep the default settings; no changes are required. Define services for Apache Kafka â For each Apache Kafka instance, define an entry under services in the compose file. Each entry will have the following structure:   Example Apache Kafka connection connection-name : << : *extract environment : << : *kafka-defaults   Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\"   Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes :   You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract tells the kafka-extractor tool to run. environment contains all parameters for the tool. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Apache Kafka connections as you want. Define services for Confluent Kafka â For each Confluent Kafka instance, define an entry under services in the compose file. Each entry will have the following structure:   Example Confluent Kafka connection connection-name : << : *extract environment : << : *kafka-defaults   Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\"   Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" KAFKA_FLAVOUR : \"CONFLUENT_CLOUD\" CONFLUENT_AUTH : \"<cloud_api_key>:<cloud_api_secret>\" CONFLUENT_CLUSTER_ID : \"<lkc-xxxx>\" volumes :   You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract : Tells the kafka-extractor tool to run. environment : Contains all parameters for the tool. KAFKA_FLAVOUR : Defines the Kafka distribution being used. Use CONFLUENT_CLOUD when working with Confluent Cloud. CONFLUENT_AUTH : Configures authentication using a Cloud API key and secret. Replace cloud_api_key with the Cloud API key retrieved during setup. For more information, see How to set up Confluent Kafka Cloud API Key . Replace cloud_api_secret with the corresponding secret for your Cloud API key. For more information, see How to set up Confluent Kafka Cloud API Key . CONFLUENT_CLUSTER_ID : The unique ID of your Kafka cluster. You can find this in the cluster overview page of the Confluent Cloud console. The cluster ID follows the lkc-xxxx format. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Confluent Kafka connections as you want. Define services for Aiven Kafka â For each Aiven Kafka instance, define an entry under services in the compose file. Each entry will have the following structure:   Example Aiven Kafka connection connection-name : << : *extract secrets : - kafka_client_config - kafka_ca_cert   Uncomment the following lines if you are using Aiven Kafka with Client Certificate Authentication        - kafka_access_cert        - kafka_access_key environment : << : *kafka-defaults   Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\"   Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes :   You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract tells the kafka-extractor tool to run. environment contains all parameters for the tool. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Aiven Kafka connections as you want. Define services for Redpanda Kafka â For each Redpanda Kafka instance, define an entry under services in the compose file. Each entry will have the following structure:   Example Redpanda Kafka connection connection-name : << : *extract environment : << : *kafka-defaults   Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\"   Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes :   You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract tells the kafka-extractor tool to run. environment contains all parameters for the tool. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Redpanda Kafka connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your Kafka connections, you will need to provide a Kafka client configuration file. For managed Kafka instances such as Confluent Cloud and Aiven , this configuration can be copied directly from the console.Â Here is an example that would be compatible with all Kafka variants: Apache Kafka, Confluent Cloud, and Aiven Kafka. This is just an example, your cluster configuration may vary:   Required connection configs for Kafka producer, consumer, and admin   If SSL enabled, use SASL_SSL, otherwise use SASL_PLAINTEXT (when using with basic auth) security.protocol=SASL_SSL   If basic auth is enabled sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='{{ USERNAME or CLUSTER_API_KEY }}' password='{{ PASSWORD or CLUSTER_API_SECRET }}'; sasl.mechanism=PLAIN   Best practice for higher availability in Apache Kafka clients prior to 3.0 session.timeout.ms=45000 Redpanda Kafka only supports the SCRAM authentication method . Here is an example configuration: sasl.mechanism=<SCRAM - SHA - 256 or SCRAM - SHA - 512 depending on your config > security.protocol=SASL_SSL sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"<username > \" password=\"<password > \";   Best practice for higher availability in Apache Kafka clients prior to 3.0 session.timeout.ms=45000 Secure credentials â Using local files â danger If you decide to keep Kafka credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets : kafka_client_config :   Change it to the actual location of your kafka config file (MANDATORY) file : ./kafka.client.config kafka_ca_cert :   Change it to the actual location of your kafka CA cert file (OPTIONAL - only use if using custom CA) file : ./ca.pem kafka_access_cert :   Change it to the actual location of your kafka access cert file (OPTIONAL - only use if using Client Certificate auth) file : ./service.cert kafka_access_key :   Change it to the actual location of your kafka access key file (OPTIONAL - only use if using Client Certificate auth) file : ./service.key danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets â To create and use Docker secrets: Store the Kafka configuration file: sudo docker secret create kafka_client_config path/to/kafka.client.config   Optional sudo docker secret create kafka_ca_cert path/to/ca.pem sudo docker secret create kafka_access_cert path/to/service.cert sudo docker secret create kafka_access_key path/to/service.key At the top of your compose file, add a secrets element to access your secret: secrets : kafka_client_config : external : true name : kafka_client_config kafka_ca_cert : external : true name : kafka_ca_cert kafka_access_cert : external : true name : kafka_access_cert kafka_access_key : external : true name : kafka_access_key The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Kafka configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets : kafka_client_config : external : true name : kafka_client_config x-templates :   ... services :   Example Apache Kafka connection apache-kafka-example : << : *extract environment : << : *kafka-defaults   Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\"   Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes :   You can change './output/apache-kafka-example' to any output location you want - ./output/apache - kafka - example : /output In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The kafka_client_config refers to an external Docker secret created using the docker secret create command. The name of this service is apache-kafka-example . You can use any meaningful name you want. The <<: *kafka-defaults sets the connection type to Kafka. KAFKA_BOOTSTRAP_SERVERS tells the extractor about the Kafka hosts or brokers. SKIP_INTERNAL_TOPICS tells the extractor whether to extract internal topics or skip them. TheÂ ./output/apache-kafka-example:/output Â line tells the extractor where to store results. In this example, the extractor will store results in theÂ ./output/apache-kafka-example Â directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Crawl Redpanda Kafka Next Crawl on-premises Kafka Prerequisites Get the compose file Define Kafka connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access",
    "text": "Connect data BI Tools Cloud-based BI Looker Get Started Set up on-premises Looker access On this page Set up on-premises Looker access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Looker access details, including credentials. In some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your on-premises Looker instance you will need to use Atlan's looker-extractor tool. Did you know? Atlan uses exactly the same looker-extractor behind the scenes when it connects to Looker in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the looker-extractor tool â To get the looker-extractor tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Looker: sudo docker load -i /path/to/looker-extractor-master.tar Get the compose file â Atlan provides you with a configuration file for the looker-extractor tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises databases. The file is docker-compose.yaml . Define Looker connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Looker connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises Looker instance, define an entry under services in the compose file. Each entry will have the following structure: services: CONNECTION-NAME: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project1,project2\" USE_FIELD_LEVEL_LINEAGE: \"true\" volumes: - ./output/looker-example:/output/process Replace CONNECTION-NAME with the name of your connection. <<: *extract tells the looker-extractor tool to run. environment contains all parameters for the tool. Replaces the values given for INCLUDE_PROJECTS with the names of your own Looker projects you want to extract. Separate each project name by a comma. volumes specifies where to store results. In this example, the extractor will store results in the ./output/looker-example folder on the local file system. You can add as many Looker connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your Looker connections you will need to provide: A Looker SDK configuration file A private key to access your git repository via ssh (to extract field-level lineage) A passphrase to decipher the private key (to extract field-level lineage) The Looker metadata includes the git repo locations. The Looker SDK configuration is a .ini file with the following format: [Looker]   Base URL for your looker instance API. Do not include /api/* in the URL. base_url=https://<host>:<port>   API 3 client id client_id=YourClientID   API 3 client secret client_secret=YourClientSecret verify_ssl=True Secure credentials â Using local files â danger If you decide to keep Looker credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: looker_config: file: ./looker.ini looker_git_private_key: file: ./id_ed25519 looker_git_private_key_passphrase: file: ./passphrase.txt danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets â To create and use Docker secrets: Store the Looker SDK configuration file: sudo docker secret create looker_config path/to/looker.ini At the top of your compose file, add a secrets element to access your secret: secrets: looker_config: external: true name: looker_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Looker SDK configuration file. info ðª Did you know? You can use the same steps to create Docker secrets for your git details, as well. Replace the name ( looker_config ) and path to the file, but otherwise run the same command. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets: looker_config: external: true name: looker_config looker_git_private_key: file: ./id_ed25519 looker_git_private_key_passphrase: external: true name: looker_git_private_key_passphrase x-templates:   ... services: my-looker: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project1,project2\" USE_FIELD_LEVEL_LINEAGE: \"true\" volumes: - ./output/looker-example:/output/process secrets: - looker_config - looker_git_private_key - looker_git_private_key_passphrase volumes: jars: In this example we've defined the secrets at the top of the file (you could also define them at the bottom): looker_config refers to an external Docker secret created using the docker secret create command. looker_git_private_key refers to a local file. looker_git_private_key_passphrase refers to an external Docker secret created using the docker secret create command. The name of this service is my-looker . You can use any meaningful name you want. The <<: *looker-defaults sets the connection type to Looker. INCLUDE_PROJECTS tells the extractor to only extract project1 and project2 from Looker. USE_FIELD_LEVEL_LINEAGE tells the extractor to extract field-level lineage. This means the git private key information is also required. The ./output/looker-example:/output/process line tells the extractor where to store results. In this example, the extractor will store results in the ./output/looker-example directory on the local file system. We recommend you output metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up Looker Next Crawl Looker Prerequisites Get the compose file Define Looker connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau Get Started Set up on-premises Tableau access On this page Set up on-premises Tableau access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Tableau instance details, including credentials. In some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your on-premises Tableau instance, you will need to use Atlan's tableau-extractor tool. Did you know? Atlan uses exactly the same tableau-extractor behind the scenes when it connects to Tableau in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the tableau-extractor tool â To get the tableau-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Tableau: sudo docker load -i /path/to/tableau-extractor-master.tar Get the compose file â Atlan provides you with a Docker compose file for the tableau-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Tableau instance. The file is docker-compose.yaml . Define Tableau connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Tableau connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises Tableau instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"\" volumes: - ./output/connection-name:/output/process Replace connection-name with the name of your connection. <<: *extract tells the tableau-extractor tool to run. environment contains all parameters for the tool. CERT_PATH -  if applicable, specify the SSL certificate path and store it as a new volume. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Tableau connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your Tableau connections, you will need to provide a Tableau configuration file. The Tableau configuration is a .ini file with the following format: [TableauConfig]   Tableau instance URL. Do not include /api/* in the URL. server_url=https://:<hostname>:<port>   Tableau site name. Leaving this empty will select the default site. site_name=YourTableauSite   Tableau authentication type. Options: basic, personal_access_token. auth_type=basic   Required only if auth_type is basic. [BasicAuth] username=YourTableauUsername password=YourTableauPassword   Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] token_name=YourTableauTokenName token_value=YourTableauTokenValue danger For basic authentication, ensure that your password does not contain the special character % . If the percent sign is included in your password, add another % to escape it. Secure credentials â Using local files â danger If you decide to keep Tableau credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: tableau_config: file: ./tableau.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets â To create and use Docker secrets: Store the Tableau configuration file: sudo docker secret create tableau_config path/to/tableau.ini At the top of your compose file, add a secrets element to access your secret: secrets: tableau_config: external: true name: tableau_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Tableau configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets: tableau_config: external: true name: tableau_config x-templates:   ... services: my-tableau: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"/tmp/tab-cert.pem\" volumes: - ./output/my-tableau:/output/process - ./tab-cert.pem:/tmp/tab-cert.pem secrets: - tableau_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The tableau_config refers to an external Docker secret created using the docker secret create command. The name of this service is my-tableau . You can use any meaningful name you want. The <<: *tableau-defaults sets the connection type to Tableau. EXCLUDE_PROJECTS_REGEX tells the extractor to filter out all the projects whose names match the Test1.* and Test2.* regex patterns in the extracted metadata. CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS tells the extractor to include all hidden or unpublished worksheets and dashboards that are part of a Tableau workbook in the extracted metadata. CRAWL_EMBEDDED_DASHBOARDS tells the extractor to create relationships between Tableau dashboards used within another dashboard as a Web Page item. The CERT_PATH tells the extractor where to store the SSL certificate , if applicable. In this example, the extractor will store results in the ./tab-cert.pem directory on the local file system. If the SSL certificate is not stored in the same folder as the compose file, you will need to specify the full path. The ./output/my-tableau:/output/process line tells the extractor where to store results. In this example, the extractor will store results in the ./output/my-tableau directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up Tableau Next Set up a private network link to Tableau server Prerequisites Get the compose file Define Tableau connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot Get Started Set up on-premises ThoughtSpot access On this page Set up on-premises ThoughtSpot access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your ThoughtSpot instance details, including credentials. In some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites â To extract metadata from your on-premises ThoughtSpot instance, you will need to use Atlan's thoughtspot-extractor tool. Did you know? Atlan uses exactly the same thoughtspot-extractor behind the scenes when it connects to ThoughtSpot in the cloud. Install Docker Compose â Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? ð) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the thoughtspot-extractor tool â To get the thoughtspot-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl ThoughtSpot: sudo docker load -i /path/to/thoughtspot-extractor-master.tar Get the compose file â Atlan provides you with a Docker compose file for the thoughtspot-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises ThoughtSpot instance. The file is docker-compose.yaml . Define ThoughtSpot connections â The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your ThoughtSpot connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services â For each on-premises ThoughtSpot instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract environment: <<: *thoughtspot-defaults EXCLUDE_TAGS_REGEX: \"Test1.*|Test2.*\" WITHOUT_TAGS: \"true\" volumes: - ./output/connection-name/filter:/output/filter Replace connection-name with the name of your connection. <<: *extract tells the thoughtspot-extractor tool to run. environment contains all parameters for the tool. EXCLUDE_TAGS_REGEX -  specify a regular expression to exclude ThoughtSpot assets based on ThoughtSpot tags. WITHOUT_TAGS -  specify a Boolean configuration to determine whether to crawl ThoughtSpot assets without any ThoughtSpot tags. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name/filter folder on the local file system. You can add as many ThoughtSpot connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials â To define the credentials for your ThoughtSpot connections, you will need to provide a ThoughtSpot configuration file. The ThoughtSpot configuration is a .ini file with the following format: [ThoughtSpotConfig] host=atlan.thoughtspot.cloud port=443 auth_type=basic_auth; This will use BasicAuth; auth_type=trusted_auth; This will use TruestedAuth; auth_type=oauth_access_token; This will use OAuth; [BasicAuth] username={{username}} password={{password}} [TrustedAuth] username={{username}} secret_key={{secret_key}} [OAuth] token={{oauth_access_token}} [ExtractionConfig] offset=1 limit=10 Secure credentials â Using local files â danger If you decide to keep ThoughtSpot credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: thoughtspot_config: file: ./thoughtspot.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets â To create and use Docker secrets: Store the ThoughtSpot configuration file: sudo docker secret create thoughtspot_config path/to/thoughtspot.ini At the top of your compose file, add a secrets element to access your secret: secrets: thoughtspot_config: external: true name: thoughtspot_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local ThoughtSpot configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example â Let's explain in detail with an example: secrets: thoughtspot_config: external: true name: thoughtspot_config x-templates:   ... services: thoughtspot-example: <<: *extract environment: <<: *thoughtspot-defaults EXCLUDE_TAGS_REGEX: \"Test1.*|Test2.*\" WITHOUT_TAGS: \"true\" volumes: - ./output/connection-name/filter:/output/filter In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The thoughtspot_config refers to an external Docker secret created using the docker secret create command. The name of this service is thoughtspot-example . You can use any meaningful name you want. The <<: *thoughtspot-defaults sets the connection type to ThoughtSpot. The ./output/thoughtspot_example/filter:/output/filter line tells the extractor where to store results. In this example, the extractor will store results in the ./output/thoughtspot_example/filter directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up ThoughtSpot Next Crawl ThoughtSpot Prerequisites Get the compose file Define ThoughtSpot connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/how-tos/set-up-oracle",
    "text": "Connect data Databases SQL Databases Oracle Get Started Set up Oracle On this page Set up Oracle Who can do this? You need your Oracle database administrator or a similar role to run these commands  - you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Oracle. This method uses a username and password to fetch metadata. Create user in Oracle â To create a username and password for basic authentication for Oracle, run the following commands: CREATE USER <username> IDENTIFIED BY <password>; GRANT CREATE SESSION TO <username>; Replace <username> with the username you want to create. Replace <password> with the password to use for that username. Grant permissions â Atlan requires specific privileges to crawl assets and fetch technical metadata from Oracle. Grant permissions for metadata extraction â Run the following commands to grant permissions for metadata extraction: GRANT SELECT_CATALOG_ROLE TO <username>; GRANT SELECT ON DBA_TABLES TO <username>; GRANT SELECT ON DBA_VIEWS TO <username>; GRANT SELECT ON DBA_TAB_COLUMNS TO <username>; GRANT SELECT ON DBA_SYNONYMS TO <username>; Replace <username> with the username you created. If these permissions arenât sufficient in your environment, use the optional approach below. Before proceeding, revoke the previously granted DBA permissions. (Optional) Grant permissions to query and preview data â Grant permissions on specific tables â To grant permissions to query and preview data for specific tables, run the following command for each table you want to provide access to. GRANT SELECT ON <schema_name>.<table_name> TO <username>; Replace <schema_name> with the name of the schema you want to crawl. Replace <table_name> with the name of the table (or view) you want to crawl. Replace <username> with the username you created. Grant permissions on any table â To grant permissions on specific tables, run the following command for each table you want to provide access to. GRANT SELECT ANY TABLE TO <username>; Replace <username> with the username you created. This permission allows the new user to query tables or views in any schema except SYS and AUDSYS . danger Oracle recommends granting ANY privileges only to trusted users. These permissions allow you to crawl metadata, preview data, and run queries in Atlan, depending on the privileges granted. Tags: data crawl authentication Previous Oracle Next Crawl Oracle Create user in Oracle Grant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-postgresql",
    "text": "Connect data Databases SQL Databases PostgreSQL Get Started Set up PostgreSQL On this page Set up PostgreSQL Who can do this? You will probably need your PostgreSQL administrator to run these commands   -  you may not have access yourself. Create a database role â To configure a database role for PostgreSQL, run the following commands: CREATE role atlan_user_role ; GRANT USAGE ON SCHEMA < schema > TO atlan_user_role ; Replace <schema> with the schema to which the user should have access. danger You (or your administrator) will need to run these statements for each database and schema you want to crawl. Atlan requires the following privileges: USAGE : Access a schema and fetch metadata. By default, users cannot access any objects in schemas that they do not own. The owner of a schema must grant the USAGE privilege on the schema to allow access. Fetch the technical metadata persisted in the INFORMATION_SCHEMA . These permissions enables Atlan to crawl metadat from PostgreSQL. (Optional) Grant permissions to query and preview data â To grant permissions to query data and preview sample data: GRANT SELECT , REFERENCES ON ALL TABLES IN SCHEMA schema_name TO atlan_user_role ; Replace schema_name : Name of the schema you want Atlan to access. Replace atlan_user_role : Role assigned to Atlan in your database. The SELECT privilege is required to preview and query data from within Atlan. Choose authentication mechanism â Atlan currently supports the following authentication mechanisms. You will need to choose one and configure it according to the steps below. Basic authentication Identity and Access Management (IAM) authentication Basic authentication â To create a username and password for basic authentication for PostgreSQL run the following commands: CREATE USER atlan_user password '<pass>' ; GRANT atlan_user_role TO atlan_user ; Replace <pass> with the password for the atlan_user user you are creating. Identity and Access Management (IAM) authentication â To configure IAM authentication for PostgreSQL follow each of these steps. Enable IAM authentication â To enable IAM authentication for your database instance follow the steps in the Amazon RDS documentation . When given the option, apply the changes immediately and wait until they are complete. Create database user â To create a database user with the necessary permissions run the following commands: Connect to the database: psql -h {{endpoint}} -U {{username}} -d {{database}} Replace {{endpoint}} with the database or cluster endpoint. Replace {{username}} with the master username (admin account) for the database. Replace {{database}} with the name of the database. Create a database user: CREATE USER {{db - username}} WITH LOGIN ; GRANT atlan_user_role , rds_iam TO {{db - username}} ; Replace {{db-username}} with the name for the database user to create. Create IAM policy â To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"rds-db:connect\" ] , \"Resource\" : [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } Replace {{aws-region}} with the AWS region of your database instance. Replace {{account-id}} with your account ID. Replace {{resource-id}} with the resource ID. Replace {{db-username}} with the username created in the previous step. Attach IAM policy â To attach the IAM policy for Atlan's use, you have two options: IAM role : Create a new role in your AWS account and attach the policy to this role. To create an AWS IAM role: Follow the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. Raise a support ticket to provide the AWS IAM role ARN to Atlan and get the ARN of the Node Instance Role for your Atlan EKS cluster from Atlan. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , } ] } IAM user : Create an AWS IAM user and attach the policy to this user. To create an AWS IAM user: Follow the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user'sÂ access key ID andÂ secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Tags: data crawl Previous PostgreSQL Next Crawl PostgreSQL Create a database role (Optional) Grant permissions to query and preview data Choose authentication mechanism Basic authentication Identity and Access Management (IAM) authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/how-tos/set-up-redash",
    "text": "Connect data BI Tools On-premises & Enterprise BI Redash Get Started Set up Redash On this page Set up Redash Who can do this? You will probably need your Redash administrator to complete the following steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Redash. This method uses an API key to fetch metadata. Create user in Redash â To create a new user for Atlan to use when integrating with Redash: Log in to your Redash instance. In the top right of your Redash instance, click your profile name, and from the dropdown, click Users . On the Settings page, under the Users tab, click the New User button. In the Create a New User dialog, enter the following details: For Name , add a meaningful name for the new user   -  for example, Atlan . For Email address , enter the email address for the new user. Click Create to create the new user. Configure new user â Once the new user has accepted the invitation, the new user will be added to the list of users in your Redash instance. You will need to configure the new user for integration with Atlan. To configure the new user for crawling Redash : Log in to your Redash instance. In the top right of your Redash instance, click your profile name, and from the dropdown, click Users . On the Settings page, under the Users tab, select the new user you created . From the new user screen, complete the following steps: Each new user is added to the Default group automatically in Redash. To configure group permissions , for Groups , click the dropdown and select Admin to add the new user to the admin group for full access . For API Key , click the clipboard icon to copy the API key for the new user and save it in a secure location. Tags: data api authentication Previous Redash Next Crawl Redash Create user in Redash Configure new user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana",
    "text": "Connect data Databases SQL Databases SAP HANA Get Started Set up SAP HANA On this page Set up SAP HANA Who can do this? You will probably need your SAP HANA administrator to run these commands   -  you may not have access yourself. Did you know? This connector supports both SAP HANA on-premise as well as SAP HANA Cloud and SAP HANA Platform database deployments. Atlan currently only supports basic username and password authentication for fetching metadata from SAP HANA. Complete the following steps to configure it: Create a database user â Create a database user with the following commands: CREATE USER < username > PASSWORD < password > NO FORCE_FIRST_PASSWORD_CHANGE ; Replace <username> with the username you want to create. Replace <password> with the password for that username. Grant read permission on schema â Grant read permission on schema with the following commands. To crawl metadata as well as preview and query data in Atlan: GRANT SELECT , SELECT METADATA ON SCHEMA < schema > TO < username > ; To only crawl metadata in Atlan: GRANT SELECT METADATA ON SCHEMA < schema > TO < username > ; Replace <schema> with the name of the schema you want to crawl. To crawl calculation views in Atlan: GRANT SELECT ON _SYS_REPO . ACTIVE_OBJECT TO < username > ; GRANT SELECT ON _SYS_BI . BIMC_PROPERTIES TO < username > ; danger Your SAP HANA administrator will need to run these statements for each schema you want to crawl. Tags: data crawl authentication Previous SAP HANA Next Crawl SAP HANA Create a database user Grant read permission on schema"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/how-tos/set-up-soda",
    "text": "Connect data Data Quality & Observability Soda Get Started Set up Soda On this page Set up Soda Who can do this? You will need your Soda Cloud administrator to complete these steps   -  you may not have access yourself. You will also need to scan your datasets using the latest version of Soda Library or migrate from Soda Core to Soda Library to ensure the best possible experience in Atlan. Associated checks for datasets scanned using an older version of Soda Library may be unavailable or missing the relationship with datasets in Atlan. Atlan supports the API authentication method for fetching metadata from Soda. This method uses an API key ID and API secret to fetch metadata. Create an API key â Did you know? Atlan does not make any API requests or queries that will update the objects in your Soda instance. You will need to create an API key in Soda for integration with Atlan. To create an API key for crawling Soda : Log in to your Soda Cloud instance as an Admin . In the top right of your Soda Cloud account, click on your avatar, and from the dropdown, click Profile . Under your profile name, click the API Keys tab. On the API Keys page, click the + button to generate a new API key. In the API Keys dialog, enter the following details: For Description , enter a meaningful description. For Organization , enter the name of your organization. Click Create to finish setup. From the corresponding screen, copy the API Key ID and API Key Secret and store them in a secure location. danger The API secret cannot be retrieved later. Tags: data api authentication Previous Soda Next Crawl Soda Create an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-tableau",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau Get Started Set up Tableau On this page Set up Tableau Who can do this? You will probably need your Tableau administrator to run these commands   -  you may not have access yourself. Enable the Tableau Metadata API â To enable the Tableau Metadata API, follow the steps in Tableau documentation . danger Atlan needs the Tableau Metadata API to crawl metadata. Please ensure you are running the latest version of Tableau Server or Tableau Online (2022.x with REST API version 3.14+). Learn more about the permissions used to access metadata through the Tableau Metadata API. Publish the worksheets you want to crawl â Ensure you publish the worksheets in Tableau that you want to crawl in Atlan. To publish Tableau worksheets, follow the steps in Tableau documentation . Choose authentication mechanism â Atlan supports the following authentication methods for fetching metadata from Tableau: Basic -  this method uses a username and password. Personal access token -  this method uses a personal access token. JWT bearer -  this method uses a username and JWT client ID, secret ID, and secret value. Basic authentication â Did you know? To crawl assets and extract asset lineage from Tableau, the user must have the Site Administrator Explorer role . Atlan requires the Site Administrator Explorer role in Tableau to extract data source fields and calculated fields and create field-level assets and lineage. It is not possible to fetch either with the Viewer role in the current version of the Tableau Metadata API. Add a user â Ensure you add a user with the role Site Administrator Explorer to the site you want to crawl. To add such a user, follow the steps in Tableau documentation . Grant user permissions â Ensure you grant the View capability for all the assets you want to crawl. To grant the permission, follow the steps in Tableau documentation . Personal access token authentication â If you want to access Tableau using an access token, you can generate a personal access token. To generate a personal access token, follow the steps in Tableau documentation . JWT bearer authentication â danger To access the Tableau Metadata API using JWT bearer authentication , you must have Tableau Cloud October 2023 or Tableau Server 2023.3 version. In addition, JWT authorization currently does not support all REST API capabilities . Due to these limitations at source, Atlan will not be able to crawl Tableau flows if you use the JWT bearer authentication method. Configure a connected app â If you want to access Tableau using a JSON web token (JWT), you can configure a Tableau connected app. There are two types of connected apps that you can configure   -  direct trust or OAuth 2.0 trust. To authenticate the Tableau connection in Atlan using this method, you will need the following: Username   -  your Tableau Server username or Tableau Online email address, the user must have a Site Administrator Explorer role Connected app ID   -  client ID generated for the connected app Secret ID   -  secret ID linked to the client ID of the connected app Secret value   -  secret value used to sign the token To configure a connected app, follow the steps in Tableau documentation: Direct trust OAuth 2.0 trust Access scopes for connected apps â For JWT authorization, scopes define access permissions granted to the token holder. Scopes control the specific actions that an application or user can perform in Tableau while accessing content through a connected app. The Tableau connector in Atlan uses two read scopes to extract metadata from Tableau. Note that the Tableau connector is preconfigured to use these scopes, no action required. Atlan uses the following scopes for JWT authentication: tableau:content:read -  allows read access to your assets in Tableau, including: Workbooks   -  can list, access, and retrieve metadata for workbooks. Views   -  can fetch specific views or dashboards within workbooks. Data sources   -  can access published data sources and associated metadata. Projects   -  can retrieve project metadata. Metrics   -  can read metrics associated with workbooks or dashboards. Tables and databases   -  can access metadata for tables and databases connected to Tableau. tableau:users:read -  allows read access to user details. This enables Atlan to display the source owner property for supported Tableau assets, including in the impact analysis report . Optional) tableau:workbooks:download â allows downloading a workbook ( .twb or .twbx ), enabling Atlan to display relationships for embedded Tableau dashboards. Tags: data crawl api Previous Tableau Next Set up on-premises Tableau access Enable the Tableau Metadata API Publish the worksheets you want to crawl Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/references/integration-with-pingfederate-using-saml",
    "text": "Configure Atlan Integrations Identity Management SSO References SSO integration with PingFederate using SAML SSO integration with PingFederate using SAML If you have PingFederate as your IdP and are trying to integrate the SAML-based IdP using the metadata supplied from the page, you can use the following SAML assertion URL: For identity provider initiated (IdP-initiated) SSO: https://{{instance}}/auth/realms/default/broker/{{alias}}/endpoint/clients/atlan-saml For service provider initiated (SP-initiated) SSO: https://{{instance}}/auth/realms/default/broker/{{alias}}/endpoint To use both IdP- and SP-initiated SSO, add both the URLs mentioned above. If you encounter an Invalid signature error, you must ensure that the certificate in the XML metadata file is of the SHA-256 or SHA-512 type. Tags: data integration Previous Set default user roles for SSO Next Troubleshooting SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/troubleshooting/troubleshooting-data-models",
    "text": "Configure Atlan Data Models Troubleshooting Troubleshooting data models On this page Troubleshooting data models What are the known limitations of data models in Atlan? â Following are the known limitations of data models in Atlan: Generalizations are not part of the entity diagram. These types of relationships are shown in a separate section of the Relations tab in the entity sidebar in a tree-like representation. Although attributes can be mapped to other attributes or columns, these mappings are currently not displayed on the Atlan UI. Atlan currently only displays entity mappings in the Layers section of the entity profile and sidebar. There is no asset filter or indicator on lineage graphs to help you identify crawled database assets linked to ER assets. Atlan does not distinguish between the optionality ends of an association. For example, an Account can place multiple Orders Â (one-to-many). Atlan stores this information and visually represents it in the entity diagram. However, the below two variations that depict more details about the relationship are currently not supported: An Account should place at least one Order . An Account may or may not place an Order . Inverse relationships need to be defined independently and are shown distinctly in the entity diagram. For example, a Customer (entity) places (relationship) an Order (entity). The inverse of the relationship is Order (entity) is placed by a Customer (entity). The entity diagram currently represents these two relationships distinctively and does not club them in a unified way. Can a single model contain entities of different types? â No, the entity type (conceptual, logical, or physical) is defined for a model and all associated objects such as entities and attributes within the model inherit the same type. You can create two different models with the same name but with different types and use them to populate entities. Can fine-grain mapping between two layers be done at an attribute level? â The backend supports attribute-level mapping and information can be stored via the Data Model Ingestion package as well as retrieved via API/SDK. However, attribute-to-attribute mapping is currently not displayed on the Atlan UI. What is the relation between a data model connection in Atlan and data models present elsewhere? â There are no strict rules on how many data models or if a subset of a data model should form part of a single Atlan Data Model connection. For example, you may choose to include ER models belonging to the same domain/business function to be part of a single connection. Atlan functionalities like access control, asset deletion, and more operate at the connection level. These factors need to be taken into consideration when deciding what to ingest. Some guidelines: Corresponding models representing different levels of abstraction are clubbed into one connection. Change frequency and refresh requirement (schedules) can drive this decision, too. What is the difference between mapping and relation? â While entities can be related to each other in two ways, mapping and relation, there is a fundamental difference between the two: Entity mapping   -  ties an entity across different layers of abstraction. For example, Customer logical entity can be mapped to CUST_DETAILS physical entity. Entity relations   -  representation of a peer-to-peer relationship between entities. For example, Order generates Invoice . Related entities are of the same type (physical/logical/conceptual). Generally materialized as a primary key-foreign key relationship at the database level. What ER modeling tools does Atlan support? â Atlan currently does not support native integration with any specific ER modeling tool. Object information from an ER modeling tool can be exported to and transformed with the Data Model Ingestion package and then ingested into Atlan. What ER assets can be linked to database assets crawled by Atlan? â Entities can currently be linked to database tables or views. Although not required, entities of the physical type are generally mapped to database assets. Is it mandatory to create and map all three types of entities? â No, you can choose to create assets of any one, two, or all three types. The Atlan UI is optimized for entity mapping in the following order: Conceptual to logical entity Logical to physical entity Physical entity to database assets You can skip a certain level of abstraction, as needed. Tags: data crawl model Previous What are data models?"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira",
    "text": "Configure Atlan Integrations Project Management Jira Troubleshooting Troubleshooting Jira On this page Troubleshooting Jira What fields are supported when creating tickets or requesting access? â Atlan currently only supports standard fields such as project, issue type, title, and description. Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? â Atlan's Jira Cloud and Jira Data Center integrations currently do not support assigning owners by default or configuring additional fields while creating an issue . However, you can assign an owner or add any basic or required fields within Jira once the ticket has been created from Atlan. Can site renaming affect the Jira integration? â Atlan currently stores and uses the organization URL to help you access your Jira workspace from Atlan, if required. Since Jira automatically redirects the old URL to the new one, site renaming will not impact the Jira integration . Does Atlan support multiple Jira accounts and boards? â While Atlan's Jira integration supports multiple Jira projects, it currently does not support multiple Jira accounts. How long are Jira tickets valid in Atlan? â Jira issues will remain in Atlan unless they are unlinked. To unlink a Jira issue, you can either: Manually unlink the Jira issue from the asset in Atlan. Delete the issue in Jira. Does Atlan support Jira Service Management? â No, Atlan currently does not support Jira Service Management. You can use Atlan's Jira Cloud and Jira Data Center integrations. Tags: data integration Previous Link your Jira account Next What is included in the Jira integration?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/how-tos/view-data-models",
    "text": "Configure Atlan Data Models Get Started How to view data models On this page view data models Once you have ingested your ER model assets in Atlan , you can: Search, discover, and filter models, entities, attributes, relationships, and mappings, cataloged as native assets in Atlan. Link ER entities at the conceptual, logical, or physical layer to your crawled data assets. Note that the Atlan UI is optimized for the following linking design: Conceptual entity â Logical entity â Physical entity â Database table or view. Trace the object lifecycle across various layers of abstraction or implementation: Business glossary ER models Crawled data assets   -  tables and views Models â To view an entityârelationship (ER) model: From the left menu of any screen in Atlan, click Assets . In the Filters menu on the left, click Source . Click Choose connection to filter for assets in a Data Model connection. Under the search bar on the Assets page, click the Asset type dropdown. From the Asset type dropdown, select a Model to search by a specific asset type. Select a model asset to view the asset sidebar or open the asset profile. Asset preview â The model asset preview includes basic information about the asset, including technical name, alias , model type, description, and total count of associated entities. Asset sidebar â The sidebar to the right of the asset preview provides high-level information about the asset. Here's what you can view specific to model assets: Overview offers a preview of the key characteristics of the asset, including model name, type, and description. You can add an alias , enrich metadata, link domains to your data models, and more from the asset sidebar. Entities displays a list of entities in a model, along with entity type and description. This tab also provides you with a search bar to search and sort entities. Asset profile â The model profile summarizes important details about the asset. Overview tab displays details such as technical name, alias , entity count, model type, description, certification status, and owners. The Entities section offers a snapshot of all the entities in a model, listing the entity name and description. Entities tab allows you to update metadata such as tags and terms for your entities directly from the model profile. Entities â To view an entity: From the left menu of any screen in Atlan, click Assets . In the Filters menu on the left, click Source . Click Choose connection to filter for assets in a Data Model connection. Under the search bar on the Assets page, click the Asset type dropdown. From the Asset type dropdown, select a Entity to search by a specific asset type. Select an entity asset to view the asset sidebar or open the asset profile. Asset preview â The entity asset preview includes basic information about the asset, including technical name, alias , entity type, description, and total count of associated attributes. Asset sidebar â The sidebar to the right of the asset preview provides high-level information about the asset. Here's what you can view specific to entity assets: Overview offers a preview of the key characteristics of the asset, including entity name, type, and description. You can add an alias , enrich metadata, link domains to your entities, and more from the asset sidebar. Attributes displays a list of attributes in an entity, along with data type, description, and primary key indicator, if any. This tab also provides you with a search bar to search and sort attributes. Relations shows a list of entity relationships: Associations displays relation name, associated entity, and cardinality. Generalization displays parent-child relationships between entities. Layers organizes different levels of abstraction in a tree-like representation: Conceptual entity â Logical entity â Physical entity â Database table or view. Asset profile â The entity profile displays important details about the asset. Overview tab summarizes details such as technical name, alias , attribute count, entity type, associated model, description, certification status, and owners. The Layers section organizes different levels of abstraction in a tree-like representation: Conceptual entity â Logical entity â Physical entity â Database table or view. Click an entity name to view more details in the asset sidebar. The Attributes section offers a snapshot of all the attributes of an entity, including the attribute name, primary key indicator, data type, nullability, and description. Attributes tab allows you to update metadata such as tags and terms for your attributes directly from the entity profile. Entity Diagram provides a visual representation of entity relationships . Entity diagram â An entity diagram is a visual representation of entity relationships. It also allows you to navigate to the next set of related entities as you explore the graph. Note that the entity diagram in Atlan is not a replacement for ER modeling tools, which are optimized for editing objects and displaying an entire model. The entity diagram in Atlan: Is entity-focused only Uses crowâs foot notation to represent one-to-many relationships Lists attributes that are part of an entity: Attributes that form part of a relationship are shown at the top of the list. Clicking on one highlights the attribute in the related entity that forms the basis of the association. The association itself can be highlighted and has a sidebar with detailed information. Layers â The Layers section of an entity or database table helps you navigate across different abstraction layers of entities. A layer is a mechanism to have multiple abstractions for a model or an entity. Conceptual   -  most abstract Logical   -  more defined Physical   -  well-defined and conformant to a target database system Tags: data crawl model Previous Data Models Next What are data models? Models Entities Entity diagram Layers"
  },
  {
    "url": "https://docs.atlan.com/product/administration/logs/how-tos/view-query-logs",
    "text": "Configure Atlan Administration Monitoring How to view query logs View query logs Who can do this? You will need to be an admin user in Atlan to view query logs. The query log helps you track all queries run in Atlan, including: saved and unsaved queries in the Insights query editor queries run through both the Atlan UI and API sample data previews from asset profiles You can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization. To view query logs: From the left menu of any screen in Atlan, click Admin . Under the Logs heading of your admin Workspace , click Query logs . On the Query logs page, you can view all the queries that your users have run or are running in Atlan. (Optional) Click the funnel icon to filter queries and then: Click Status to filter queries by run status â Succeeded , Failed , or Aborted . Click Users to filter queries by Atlan users. (Optional) Use the search bar to search for queries using specific keywords. The default date range is set to 30 days. Use the date filter to view query logs for the last 7 days, past 3 or 6 months, or a custom date range of your choice. For any query listed in the query logs, you can view the query name, connection, execution details, user that run the query, and timestamp for when the query was run. (Optional) Click any query to view more details in the Query details sidebar: In the Query details sidebar, you can view the full query, connection, database, schema, and asset name, query status, and query run time. Click the copy icon to copy the query and use it as a template for writing your own queries. Click the expand icon to see the full query. For Query Source , click Copy ID to copy the query ID. Tags: data api Previous How to view event logs Next Create README templates"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/power-bi-lineage-processes",
    "text": "Use data Lineage FAQ What are Power BI processes on the lineage graph? What are Power BI processes on the lineage graph? Processes in general represent the movement and transformation of assets in Atlan. By default, process assets are hidden on the assets page and reporting center. Note that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets. These can be especially useful for SQL-first sources like Snowflake, in which case the process entities show the transformation SQL query that was parsed to generate lineage. This allows users to easily discover source code or scripts. Tags: data faq faq-lineage Previous Is there a way to build lineage from NetSuite to Snowflake? Next What do the numbers in lineage view mean?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/references/what-does-atlan-crawl-from-amazon-athena",
    "text": "Connect data Databases Query Engines Amazon Athena References What does Atlan crawl from Amazon Athena? On this page What does Atlan crawl from Amazon Athena? Atlan crawls and maps the following assets and properties from Amazon Athena. Databases â Atlan maps databases from Amazon Athena to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from Amazon Athena to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName CreateTime (via Glue) sourceCreatedAt Tables â Atlan maps tables from Amazon Athena to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount recordCount (via Glue) rowCount sizeKey (via Glue) sizeBytes TABLE_TYPE subType StorageDescriptor Location (via Glue) externalLocation StorageDescriptor typeOfData (via Glue) externalLocationFormat StorageDescriptor Columns and COLUMN_COUNT certificateStatus (DEPRECATED) if Athena JDBC and Glue API do not have the same column count PartitionKeys (via Glue) isPartitioned PartitionData (via Glue) partitionCount PartitionData (via Glue) partitionList CreatedBy (via Glue) sourceCreatedBy CreateTime or CreationTime (via Glue) sourceCreatedAt UpdateTime or LastAccessTime (via Glue) sourceUpdatedAt Views â Atlan maps views from Amazon Athena to its View asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount VIEW_DEFINITION definition StorageDescriptor Columns and COLUMN_COUNT certificateStatus (DEPRECATED) if Athena JDBC and Glue API do not have the same column count recordCount (via Glue) rowCount PartitionKeys (via Glue) isPartitioned PartitionData (via Glue) partitionCount PartitionData (via Glue) partitionList CreatedBy (via Glue) sourceCreatedBy CreateTime or CreationTime (via Glue) sourceCreatedAt UpdateTime or LastAccessTime (via Glue) sourceUpdatedAt Columns â Atlan maps columns from Amazon Athena to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description ORDINAL_POSITION order TYPE_NAME dataType NULLABLE isNullable IS_PARTITION isPartition PARTITION_ORDER partitionOrder PRIMARY_KEY isPrimary DECIMAL_DIGITS precision Tags: data crawl Previous Crawl Amazon Athena Databases Schemas Tables Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/references/what-does-atlan-crawl-from-amazon-dynamodb",
    "text": "Connect data Databases NoSQL Databases Amazon DynamoDB References What does Atlan crawl from Amazon DynamoDB? On this page What does Atlan crawl from Amazon DynamoDB? Atlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran. Tables â Atlan maps tables from Amazon DynamoDB to its DynamoDBTable asset type. Source property Atlan property Where in Atlan TableName name asset profile and overview sidebar ItemCount rowCount asset preview and profile, overview sidebar TableSizeBytes sizeBytes asset filter and overview sidebar AttributeDefinitions noSQLSchemaDefinition asset profile HASH key in KeySchema dynamoDBPartitionKey asset profile and overview sidebar RANGE key in KeySchema dynamoDBSortKey asset profile and overview sidebar TableStatus dynamoDBStatus asset profile and overview sidebar CreationDateTime sourceCreatedAt asset profile and overview sidebar ProvisionedThroughput.ReadCapacityUnits dynamoDBReadCapacityUnits asset profile and overview sidebar ProvisionedThroughput.WriteCapacityUnits dynamoDBWriteCapacityUnits asset profile and overview sidebar Global secondary indexes â Atlan maps global secondary indexes (GSI) from Amazon Dynamo DB to its DynamoDBGlobalSecondaryIndex asset type. Source property Atlan property Where in Atlan ItemName name asset profile and overview sidebar IndexCount rowCount asset preview and profile, overview sidebar IndexSizeBytes sizeBytes asset filter and overview sidebar IndexStatus dynamoDBStatus asset profile and overview sidebar HASH key in KeySchema dynamoDBPartitionKey asset profile and overview sidebar RANGE key in KeySchema dynamoDBSortKey asset profile and overview sidebar Projection.ProjectionType dynamoDBSecondaryIndexProjectionType asset profile and overview sidebar ProvisionedThroughput.ReadCapacityUnits dynamoDBReadCapacityUnits asset profile and overview sidebar ProvisionedThroughput.WriteCapacityUnits dynamoDBWriteCapacityUnits asset profile and overview sidebar Local secondary indexes â Atlan maps local secondary indexes (LSI) from Amazon Dynamo DB to its DynamoDBLocalSecondaryIndex asset type. Source property Atlan property Where in Atlan IndexName name asset profile and overview sidebar ItemCount rowCount asset preview and profile, overview sidebar IndexSizeBytes sizeBytes asset filter and overview sidebar HASH key in KeySchema dynamoDBPartitionKey asset profile and overview sidebar RANGE key in KeySchema dynamoDBSortKey asset profile and overview sidebar Projection.ProjectionType dynamoDBSecondaryIndexProjectionType asset profile and overview sidebar Tags: data crawl Previous Crawl Amazon DynamoDB Next Troubleshooting Amazon DynamoDB connectivity Tables Global secondary indexes Local secondary indexes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/references/what-does-atlan-crawl-from-amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift References What does Atlan crawl from Amazon Redshift? On this page What does Atlan crawl from Amazon Redshift? Atlan crawls and maps the following assets and properties from Amazon Redshift. Databases â Atlan maps databases from Amazon Redshift to its Database asset type. Source property Atlan property DATABASE_NAME name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from Amazon Redshift to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount DATABASE_NAME databaseName OWNER sourceCreatedBy SCHEMA_TYPE subType Tables â Atlan maps tables from Amazon Redshift to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes TABLE_TYPE (EXTERNAL TABLE) subType LOCATION externalLocation INPUT_FORMAT externalLocationFormat TABLE_OWNER sourceCreatedBy CREATED sourceCreatedAt Views â Atlan maps views from Amazon Redshift to its View and MaterialisedView asset types. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount VIEW_DEFINITION definition TABLE_OWNER sourceCreatedBy Columns â Atlan maps columns from Amazon Redshift to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description ORDINAL_POSITION order TYPE_NAME dataType NOTNULL isNullable CHARACTER_MAXIMUM_LENGTH maxLength DECIMAL_DIGITS precision CONSTRAINT_TYPE (PRIMARY KEY) isPrimary CONSTRAINT_TYPE (FOREIGN KEY) isForeign SORTKEY isSort DISKEY isDist Tags: data crawl Previous Mine Amazon Redshift Next Preflight checks for Amazon Redshift Databases Schemas Tables Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/references/what-does-atlan-crawl-from-aws-glue",
    "text": "Connect data ETL Tools AWS Glue References What does Atlan crawl from AWS Glue? On this page What does Atlan crawl from AWS Glue? Atlan crawls and maps the following assets and properties from AWS Glue. Databases â Atlan maps databases from AWS Glue to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from AWS Glue to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName COMMENTS description CreateTime sourceCreatedAt Tables â Atlan maps tables from AWS Glue to its Table asset type. Source property Atlan property TABLE_NAME name COMMENTS description COLUMN_COUNT columnCount ROW_COUNT rowCount objectCount tableObjectCount BYTES sizeBytes Parameters (recordCount) rowCount Parameters (sizeKey) sizeBytes TABLE_TYPE subType StorageDescriptor (Location) externalLocation Parameters (typeOfData, classification) externalLocationFormat PartitionKeys isPartitioned PartitionData partitionCount , partitionList CreatedBy sourceCreatedBy CreateTime , CreationTime sourceCreatedAt UpdateTime , LastAccessTime sourceUpdatedAt Views â Atlan maps views from AWS Glue to its View asset type. Source property Atlan property TABLE_NAME name COMMENTS description COLUMN_COUNT columnCount VIEW_DEFINITION definition Parameters (recordCount) rowCount PartitionKeys isPartitioned PartitionData partitionCount , partitionList CreatedBy sourceCreatedBy CreateTime , CreationTime sourceCreatedAt UpdateTime , LastAccessTime sourceUpdatedAt Columns â Atlan maps columns from AWS Glue to its Column asset type. Atlan also supports nested columns up to level 15 for AWS Glue to help you enrich your semi-structured data types: Atlan retrieves raw STRUCT and ARRAY type objects for nested columns up to 15 levels. View nested columns in the column preview and overview sidebar for your table assets. Column-level lineage is supported. Search, enrich metadata, and view lineage for nested columns. Tag propagation is currently only supported from parent to nested columns. Atlan currently doesn't parse MAP type objects for columns and nested columns. Source property Atlan property COLUMN_NAME name COMMENTS description ORDINAL_POSITION , COLUMN_ID order TYPE_NAME , DATA_TYPE dataType IS_PARTITION isPartition PARTITION_ORDER partitionOrder Tags: data crawl Previous Crawl AWS Glue Next Troubleshooting AWS Glue connectivity Databases Schemas Tables Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/references/what-does-atlan-crawl-from-datastax-enterprise",
    "text": "Connect data Databases NoSQL Databases Datastax Enterprise References What does Atlan crawl from DataStax Enterprise? On this page What does Atlan crawl from DataStax Enterprise? Atlan integrates with DataStax Enterprise to crawl and map various asset types, helping you discover and understand your distributed data. This page outlines the DataStax Enterprise components that Atlan supports and how their properties are mapped. Lineage support â Atlan also supports the following lineage: Asset-level lineage for Tables andÂ Materialised Views . Column-level lineage for Tables and Materialised Views . Assets â Atlan crawls and maps the following assets and properties from DataStax Enterprise Cassandra. Keyspaces â Atlan maps keyspaces from DataStax Enterprise Cassandra to its Keyspace asset type. Source property Atlan property keyspace_name name durable_writes schemaCount replication cassandraKeyspaceReplication virtual cassandraKeyspaceVirtual query cassandraKeyspaceQuery Tables â Atlan maps tables from DataStax Enterprise Cassandra to its Table asset type. Source property Atlan property table_name name bloom_filter_fp_chance cassandraTableBloomFilterFpChance caching cassandraTableCaching compaction cassandraTableCompaction compression cassandraTableCompression crc_check_chance cassandraTableCrcCheckChance dclocal_read_repair_chance cassandraTableDclocalReadRepairChance default_time_to_live cassandraTableDefaultTimeToLive extensions cassandraTableExtensions flags cassandraTableFlags comment cassandraTableComment gc_grace_seconds cassandraTableGcGraceSeconds id cassandraTableId max_index_interval cassandraTableMaxIndexInterval read_repair_chance cassandraTableReadRepairChance Materialised Views â Atlan maps tables from DataStax Enterprise Cassandra to its MaterialisedViews asset type. Source property Atlan property table_name name bloom_filter_fp_chance cassandraViewBloomFilterFPChance caching cassandraViewCaching compaction cassandraViewCompaction compression cassandraViewCompression crc_check_chance cassandraViewCRCCheckChance dclocal_read_repair_chance cassandraViewDCLocalReadRepairChance default_time_to_live cassandraViewDefaultTTL gc_grace_seconds cassandraViewGCGraceSeconds include_all_columns cassandraViewIncludeAllColumns comment description gc_grace_seconds cassandraTableGcGraceSeconds base_table_id cassandraViewTableId max_index_interval cassandrViewMaxIndexInterval read_repair_chance cassandraViewReadRepairInterval speculative_retry cassandraViewSpeculativeRetry base_table_name cassandraTableName query cassandraViewQuery memtable_flush_period_in_ms cassandraViewMembtableFlushPeriodInMS min_index_interval cassandraViewMinIndexInterval keyspace_name cassandraKeyspaceName Indexes â Atlan maps views from DataStax Enterprise Cassandra to its Indexes asset type. Source property Atlan property index_name name kind cassandraIndexKind options cassandraIndexOptions table_name cassandraTableName keyspace_name cassandraKeyspaceName query cassandraIndexQuery Columns â Atlan maps columns from DataStax Enterprise Cassandra to its Column asset type. Source property Atlan property column_name name table_name (if a view) cassandraViewName table_name (if a table) cassandraTableName clustering_order cassandraColumnClusteringOrder kind cassandraColumnKind position cassandraColumnPosition type cassandraColumnType keyspace_name cassandraColumnIsStatic Tags: data crawl Previous Crawl DataStax Enterprise Next Preflight checks for DataStax Enterprise Lineage support Assets Keyspaces Tables Materialised Views Indexes Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/references/what-does-atlan-crawl-from-domo",
    "text": "Connect data BI Tools Cloud-based BI Domo References What does Atlan crawl from Domo? On this page What does Atlan crawl from Domo? Atlan supports lineage for the following asset types: Datasets -  upstream lineage to Google BigQuery and Snowflake data sources. Domo currently only supports upstream lineage for the following dataset types: Google BigQuery High Bandwidth Service connector Snowflake connector with Query Type as Enter Query Google BigQuery Service Connector with report type as query. Note that this is currently only supported when queryParameter is blank and queryType is standardSQL . Snowflake Federated Data with basic authentication and Snowflake OAuth. To configure Snowflake OAuth-based access from Domo, please reach out to Domo support . Cards -  upstream lineage to Domo datasets Dashboards -  upstream lineage to Domo cards Atlan crawls and maps the following assets and properties from Domo. danger Currently, Atlan only represents the assets marked with ð in lineage. Datasets ð â Atlan maps datasets from Domo to its DomoDataset asset type. Source property Atlan property Where in Atlan name name asset preview and profile, overview sidebar description description asset preview and profile, overview sidebar id DomoId API only owner.name sourceCreatedBy overview sidebar owner.id domoOwnerId API only rows domoDatasetRowCount asset profile and overview sidebar columns domoDatasetColumnCount asset preview and profile, overview sidebar dataCurrentAt domoDatasetLastRun API only createdAt sourceCreatedAt asset preview and profile, properties sidebar updatedAt sourceUpdatedAt asset preview and profile, properties sidebar calculated using dataset-card relationship API domoDatasetCardCount asset preview and profile, overview sidebar Dataset columns â Atlan maps dataset columns from Domo to its DomoDatasetColumn asset type. Source property Atlan property Where in Atlan name name asset preview and profile, overview sidebar type domoDatasetColumnType overview sidebar Dashboards ð â Atlan maps dashboards from Domo to its DomoDashboard asset type. Source property Atlan property Where in Atlan name name asset preview and profile, overview sidebar id domoId API only calculated using card-dashboard relationship API domoDashboardCardCount asset preview and profile, overview sidebar Cards ð â Atlan maps cards from Domo to its DomoCard asset type. Source property Atlan property Where in Atlan cardTitle name asset preview and profile, overview sidebar id domoId API only type domoCardType overview sidebar lastModified sourceUpdatedAt asset preview and profile, properties sidebar calculated using card-dashboard relationship API domoCardDashboardCount asset preview and profile, overview sidebar Tags: data crawl authentication Previous Crawl Domo Next Preflight checks for Domo Datasets ð Dataset columns Dashboards ð Cards ð"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/references/what-does-atlan-crawl-from-hive",
    "text": "Connect data Databases Query Engines Hive References What does Atlan crawl from Hive? On this page What does Atlan crawl from Hive? Atlan crawls and maps the following assets and properties from Hive. Database â Atlan always creates a Database asset called Hive to maintain the three-level hierarchy. Schemas â Atlan maps schemas from Hive to its Schema asset type. Source property Atlan property DATABASE_NAME name COMMENT description Tables â Atlan maps tables from Hive to its Table asset type. Source property Atlan property tableName name COMMENT description column_count columnCount numRows rowCount totalSize sizeBytes numPartitions isPartitioned numPartitions partitionCount location externalLocation inputFormat externalLocationFormat Views â Atlan maps views from Hive to its View asset type. Source property Atlan property tableName name COMMENT description column_count columnCount viewExpandedText definition Materialized views â Atlan maps materialized views in Hive to its Materialised View asset type. Source property Atlan property tableName name COMMENT description column_count columnCount viewExpandedText definition staleSinceDate materializationTime numRows rowCount totalSize sizeBytes Columns â Atlan maps columns from Hive to its Column asset type. Source property Atlan property COL_NAME name DATA_TYPE dataType COMMENT description Detailed Table Information order Constraints (Primary Key) isPrimary Constraints (Foreign Key) isForeign Tags: data crawl Previous Crawl Hive Next Preflight checks for Hive Database Schemas Tables Views Materialized views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/references/what-does-atlan-crawl-from-microsoft-azure-data-factory",
    "text": "Connect data ETL Tools Microsoft Azure Data Factory References What does Atlan crawl from Microsoft Azure Data Factory? On this page What does Atlan crawl from Microsoft Azure Data Factory? Atlan crawls and maps the following assets and properties from Microsoft Azure Data Factory. For any currently unsupported linked services, datasets, and activities from Microsoft Azure Data Factory not listed below, Atlan will map them to the relevant asset type and only display asset name, type, and description. Linked services â Atlan maps linked services from Microsoft Azure Data Factory to its AdfLinkedservice asset type. Microsoft Azure Cosmos DB for MongoDB â Source property Atlan property name name type adfLinkedserviceType description description annotations adfLinkedserviceAnnotations typeProperties.database adfLinkedserviceDatabaseName typeProperties.isServerVersionAbove32 adfLinkedserviceVersionAbove version adfLinkedserviceVersion Azure Data Lake Storage (ADLS) â Source property Atlan property name name type adfLinkedserviceType description description typeProperties.azureCloudType adfLinkedserviceAzureCloudType typeProperties.servicePrincipalCredentialType adfLinkedserviceCredentialType typeProperties.tenant adfLinkedserviceTenant typeProperties.url adfLinkedserviceDomainEndpoint Azure Databricks â Source property Atlan property name name type adfLinkedserviceType description description annotations adfLinkedserviceAnnotations typeProperties.clusterId adfLinkedserviceClusterId typeProperties.domain adfLinkedserviceDomainEndpoint typeProperties.workspaceResourceId adfLinkedserviceResourceId Snowflake â Source property Atlan property name name type adfLinkedserviceType description description annotations adfLinkedserviceAnnotations typeProperties.accountIdentifier adfLinkedserviceDomainEndpoint typeProperties.authenticationType adfLinkedserviceCredentialType typeProperties.database adfLinkedserviceDatabaseName typeProperties.user adfLinkedserviceUserName typeProperties.warehouse adfLinkedserviceWarehouseName Microsoft Azure SQL Database â Source property Atlan property name name type adfLinkedserviceType description description annotations adfLinkedserviceAnnotations typeProperties.authenticationType adfLinkedserviceCredentialType typeProperties.database adfLinkedserviceDatabaseName typeProperties.server adfLinkedserviceDomainEndpoint Datasets â Atlan maps databases from Microsoft Azure Cosmos DB to its AdfDataset asset type. Microsoft Azure Cosmos DB for MongoDB â Source property Atlan property name name type adfDatasetType description description annotations adfDatasetAnnotations folder adfDatasetFolderPath linkedServiceName adfDatasetLinkedService typeProperties.collection adfDatasetCollectionName Azure Data Lake Storage (ADLS) â Source property Atlan property name name type adfDatasetType description description annotations adfDatasetAnnotations folder adfDatasetFolderPath linkedServiceName adfDatasetLinkedService typeProperties.fileName adfDatasetFileName typeProperties.folderPath adfDatasetFileFolderPath typeProperties.format adfDatasetStorageType typeProperties.container adfDatasetContainerName Azure Databricks â Source property Atlan property name name type adfDatasetType description description annotations adfDatasetAnnotations folder adfDatasetFolderPath linkedServiceName adfDatasetLinkedService typeProperties.database adfDatasetDatabaseName typeProperties.table adfDatasetTableName Snowflake â Source property Atlan property name name type adfDatasetType description description annotations adfDatasetAnnotations folder adfDatasetFolderPath linkedServiceName adfDatasetLinkedService typeProperties.schema adfDatasetSchemaName typeProperties.table adfDatasetTableName Microsoft Azure SQL Database â Source property Atlan property name name type adfDatasetType description description annotations adfDatasetAnnotations folder adfDatasetFolderPath linkedServiceName adfDatasetLinkedService typeProperties.schema adfDatasetSchemaName typeProperties.table adfDatasetTableName Data flows â Atlan maps data flows from Microsoft Azure Data Factory to its AdfDataflow asset type. Source property Atlan property name name description description folder adfDataflowFolderPath typeProperties.sources adfDataflowSources typeProperties.sinks adfDataflowSinks typeProperties.scriptLines adfDataflowScript Activities â Atlan maps activities from Microsoft Azure Data Factory to its AdfActivity asset type. Copy activity â Source property Atlan property name name type adfActivityType description description dependsOn adfActivityPrecedingDependency policyTimeout adfActivityPolicyTimeout policyRetryInterval adfActivityPolictRetryInterval state adfActivityState activityRuns adfActivityRuns typeProperties.sink adfActivitySinks typeProperties.source adfActivitySources typeProperties.sinkType adfActivitySinkType typeProperties.sourceType adfActivitySourceType Databricks notebooks â Source property Atlan property name name type adfActivityType description description dependsOn adfActivityPrecedingDependency policyTimeout adfActivityPolicyTimeout policyRetryInterval adfActivityPolictRetryInterval state adfActivityState activityRuns adfActivityRuns typeProperties.source adfActivitySources typeProperties.sourceType adfActivitySources typeProperties.notebookPath adfActivityNotebookPath DatabricksSparkJar activity â Source property Atlan property name name type adfActivityType description description dependsOn adfActivityPrecedingDependency policyTimeout adfActivityPolicyTimeout policyRetryInterval adfActivityPolictRetryInterval state adfActivityState activityRuns adfActivityRuns typeProperties.source adfActivitySources typeProperties.sourceType adfActivitySources typeProperties.mainClassName adfActivityMainClassName DatabricksSparkPython activity â Source property Atlan property name name type adfActivityType description description dependsOn adfActivityPrecedingDependency policyTimeout adfActivityPolicyTimeout policyRetryInterval adfActivityPolictRetryInterval state adfActivityState activityRuns adfActivityRuns typeProperties.source adfActivitySources typeProperties.sourceType adfActivitySources typeProperties.pythonFile adfActivityPythonFilePath Lookup activity â Source property Atlan property name name type adfActivityType description description dependsOn adfActivityPrecedingDependency policyTimeout adfActivityPolicyTimeout policyRetryInterval adfActivityPolictRetryInterval state adfActivityState activityRuns adfActivityRuns typeProperties.firstRowOnly adfActivityFirstRowOnly typeProperties.source adfActivitySources ForEach activity â Source property Atlan property name name type adfActivityType description description dependsOn adfActivityPrecedingDependency policyTimeout adfActivityPolicyTimeout policyRetryInterval adfActivityPolictRetryInterval state adfActivityState activityRuns adfActivityRuns typeProperties.activities adfActivitySubActivities typeProperties.batchCount adfActivityBatchCount typeProperties.isSequential adfActivityIsSequential Pipelines â Atlan maps pipelines from Microsoft Azure Data Factory to its AdfPipeline asset type. Source property Atlan property name name type adfActivityType description description activityCount adfPipelineActivityCount pipelineRun adfPipelineRuns typeProperties.folder adfPipelineFolderPath Tags: data crawl Previous Crawl Microsoft Azure Data Factory Next What lineage does Atlan extract from Microsoft Azure Data Factory? Linked services Datasets Data flows Activities Pipelines"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/references/what-does-atlan-crawl-from-microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics References What does Atlan crawl from Microsoft Azure Synapse Analytics? On this page What does Atlan crawl from Microsoft Azure Synapse Analytics? Atlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources. Databases â Atlan maps databases from Microsoft Azure Synapse Analytics to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from Microsoft Azure Synapse Analytics to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName Tables â Atlan maps tables from Microsoft Azure Synapse Analytics to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes PARTITIONS isPartitioned PARTITION_COUNT partitionCount Views â Atlan maps views from Microsoft Azure Synapse Analytics to its View asset type. Source property Atlan property VIEW_NAME name REMARKS description COLUMN_COUNT columnCount VIEW_DEFINITION (WITH SCHEMABINDING) isClustered VIEW_DEFINITION definition Columns â Atlan maps columns from Microsoft Azure Synapse Analytics to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description ORDINAL_POSITION order TYPE_NAME dataType CONSTRAINT_TYPE (PRIMARY KEY) isPrimary CONSTRAINT_TYPE (FOREIGN KEY) isForeign NULLABLE isNullable NUMERIC_SCALE numericScale NUMERIC_PRECISION precision Routines â Atlan maps routines in Microsoft Azure Synapse Analytics to its Procedure asset type. Source property Atlan property ROUTINE_NAME name REMARKS description PROCEDURE_TYPE subType ROUTINE_DEFINITION definition Tags: data crawl Previous Crawl Microsoft Azure Synapse Analytics Next Preflight checks for Microsoft Azure Synapse Analytics Databases Schemas Tables Views Columns Routines"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/references/what-does-atlan-crawl-from-microsoft-sql-server",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server References What does Atlan crawl from Microsoft SQL Server? On this page What does Atlan crawl from Microsoft SQL Server? Atlan crawls and maps the following assets and properties from Microsoft SQL Server. Databases â Atlan maps databases from Microsoft SQL Server to its Database asset type. Source property Atlan property Where in Atlan TABLE_CATALOG name asset preview, profile, and filter, overview sidebar SCHEMA_COUNT schemaCount API only Schemas â Atlan maps schemas from Microsoft SQL Server to its Schema asset type. Source property Atlan property Where in Atlan TABLE_SCHEMA name asset preview, profile, and filter, overview sidebar TABLE_COUNT tableCount asset preview and profile VIEW_COUNT viewsCount asset preview and profile TABLE_CATALOG databaseName asset preview and profile Tables â Atlan maps tables from Microsoft SQL Server to its Table asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview, profile, and filter, overview sidebar REMARKS description asset preview, profile, and filter, overview sidebar COLUMN_COUNT columnCount asset preview, profile, and filter, overview sidebar ROW_COUNT rowCount asset preview, profile, and filter, overview sidebar BYTES sizeBytes asset preview, profile, and filter, overview sidebar PARTITIONS isPartitioned API only PARTITION_COUNT partitionCount API only Views â Atlan maps views from Microsoft SQL Server to its View asset type. Source property Atlan property Where in Atlan TABLE_NAME name asset preview, profile, and filter, overview sidebar REMARKS description asset preview, profile, and filter, overview sidebar COLUMN_COUNT columnCount asset preview, profile, and filter, overview sidebar VIEW_DEFINITION (WITH SCHEMABINDING) isClustered API only VIEW_DEFINITION definition asset profile and overview sidebar Columns â Atlan maps columns from Microsoft SQL Server to its Column asset type. Source property Atlan property Where in Atlan COLUMN_NAME name asset preview, profile, and filter, overview sidebar REMARKS description asset preview, profile, and filter, overview sidebar ORDINAL_POSITION order asset profile TYPE_NAME dataType asset preview, profile, and filter, overview sidebar CONSTRAINT_TYPE (PRIMARY KEY) isPrimary asset preview, profile, and filter CONSTRAINT_TYPE (FOREIGN KEY) isForeign asset preview, profile, and filter NULLABLE isNullable asset profile and overview sidebar NUMERIC_SCALE numericScale asset profile and overview sidebar NUMERIC_PRECISION precision asset profile and overview sidebar Routines â Atlan maps routines in Microsoft SQL Server to its Procedure asset type. These assets are currently neither published nor discoverable in Atlan. Source property Atlan property Where in Atlan ROUTINE_NAME name API only REMARKS description API only PROCEDURE_TYPE subType API only ROUTINE_DEFINITION definition API only Tags: data integration crawl api Previous Set up a private network link to Microsoft SQL Server on Amazon RDS Next Preflight checks for Microsoft SQL Server Databases Schemas Tables Views Columns Routines"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/references/what-does-atlan-crawl-from-oracle",
    "text": "Connect data Databases SQL Databases Oracle References What does Atlan crawl from Oracle? On this page What does Atlan crawl from Oracle? Atlan crawls and maps the following assets and properties from Oracle. Databases â Atlan maps databases from Oracle to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from Oracle to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName Tables â Atlan maps tables from Oracle to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes TABLE_TYPE subType HAS_PARTITIONS isPartitioned PARTITION_STRATEGY partitionStrategy PARTITION_COUNT partitionCount TEMPORARY isTemporary ALIAS displayName Views â Atlan maps views from Oracle to its View asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount VIEW_DEFINITION definition HAS_PARTITIONS isPartitioned PARTITION_COUNT partitionCount TEMPORARY isTemporary ALIAS displayName Columns â Atlan maps columns from Oracle to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description ORDINAL_POSITION order TYPE_NAME dataType CONSTRAINT_TYPES (PRIMARY KEY) isPrimary CONSTRAINT_TYPES (FOREIGN KEY) isForeign IS_NULLABLE isNullable NUMERIC_SCALE numericScale CHARACTER_MAXIMUM_LENGTH maxLength Tags: data crawl Previous Crawl Oracle Next Preflight checks for Oracle Databases Schemas Tables Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/references/what-does-atlan-crawl-from-prestosql",
    "text": "Connect data Databases Query Engines PrestoSQL References What does Atlan crawl from PrestoSQL? On this page What does Atlan crawl from PrestoSQL? Atlan crawls and maps the following assets and properties from PrestoSQL. Did you know? Atlan currently only supports PrestoSQL until version 349. PrestoDB is not supported at present. Databases â Atlan maps databases from PrestoSQL to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from PrestoSQL to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName Tables â Atlan maps tables from PrestoSQL to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes Views â Atlan maps views from PrestoSQL to its View asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount EXTRA_INFO (CREATE VIEW) definition Columns â Atlan maps columns from PrestoSQL to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description ORDINAL_POSITION order TYPE_NAME dataType NULLABLE isNullable DECIMAL_DIGITS precision NUMERIC_SCALE numericScale Stored procedures â Atlan maps stored procedures in PrestoSQL to its Procedure asset type. Source property Atlan property PROCEDURE_NAME name REMARKS description PROCEDURE_TYPE subType ROUTINE_DEFINITION definition Tags: data crawl Previous Crawl PrestoSQL Next Preflight checks for PrestoSQL Databases Schemas Tables Views Columns Stored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/references/what-does-atlan-crawl-from-sap-ecc",
    "text": "Connect data ERP SAP ECC References What does Atlan crawl from SAP ECC? On this page What does Atlan crawl from SAP ECC? Private Preview Atlan integrates with SAP ECC to crawl and map various asset types, helping you gain insights into structured business data. This page outlines the SAP ECC components that Atlan supports and how their properties are mapped. Lineage â Atlan supports lineage for the following lineage: Asset Lineage - Table to View Column Level Lineage - Table Columns to View Columns Assets Assets â Atlan extracts metadata from SAP ECC across different asset types, including: Components : Software modules providing specific functionalities. Tables : Structured storage for master data, transactions, and configurations. Views : Logical representations of data for efficient access. Columns : Data attributes within tables and views. ABAP programs : Custom scripts written for automation and processing. Function modules : Reusable logic blocks for ABAP programs and remote function calls. Transaction Codes : Shortcuts for executing SAP functions. The following sections detail how each asset type is mapped in Atlan. Components â SAP ECC components are modular software units that deliver specific business functions. These components form the foundation of SAP's enterprise applications, enabling functionalities such as finance, logistics, and human resources. Atlan maps Components from SAP ECC to its SapErpComponent asset type. Source property Atlan property TEXT name NAME sapComponentName Tables â Tables in SAP ECC store structured business data, including master records, transactional details, and configuration settings. These tables form the foundation of SAP's data storage and retrieval system. Atlan maps Table from SAP ECC to its SapErpTable asset type. Source property Atlan property TABNAME name DDTEXT description TABCLASS sapErpTableType DEVCLASS sapPackageName Column Count sapFieldCount CONTFLAG sapErpTableDeliveryClass AS4USER sourceUpdatedBy AS4DATE sourceUpdatedAt Views â Views provide a logical representation of data by combining information from one or more tables. They simplify data access and reporting by allowing users to work with pre-defined, structured datasets. Atlan maps View from SAP ECC to its SapErpView asset type. Source property Atlan property VIEWNAME name DDTEXT description VIEWCLASS sapErpViewType DEVCLASS sapPackageName Column Count sapFieldCount AS4USER sourceUpdatedBy AS4DATE sourceUpdatedAt Columns â Columns define individual data attributes within tables and views. Each column has a specific data type, length, and constraints, ensuring accurate data representation and integrity. Atlan maps Column from SAP ECC to its SapErpColumn asset type. Source property Atlan property FIELDNAME name TABNAME sapErpTableName or sapErpViewName ROLLNAME sapErpColumnDataElement DATATYPE sapDataType INTTYPE sapErpColumnLogicalDataType LENG sapErpColumnLength DECIMALS sapErpColumnDecimals KEYFLAG sapErpColumnIsPrimary CHECKTABLE sapErpColumnIsForeign NOTNULL sapErpColumnIsMandatory DEVCLASS sapPackageName POSITION sapFieldOrder ABAP programs â Advanced Business Application Programming (ABAP) programs are scripts used to automate processes, manipulate data, and extend SAP functionalities. These programs are written in SAP's proprietary programming language. Atlan maps ABAP Programs from SAP ECC to its SapErpAbapProgram asset type. Source property Atlan property PROGNAME name TEXT description SUBC sapErpAbapProgramType DEVCLASS sapPackageName CNAM sourceCreatedBy CDAT sourceCreatedAt UNAM sourceUpdatedBy UDAT sourceUpdatedAt Function modules â Function modules are reusable code blocks that perform predefined operations in SAP ECC. They can be called within ABAP programs or accessed remotely to execute business logic efficiently. Atlan maps Function Modules from SAP ECC to its SapErpFunctionModule asset type. Source property Atlan property FUNCNAME name STEXT description FUNC_GROUP sapErpFunctionModuleGroup Import Parameters sapErpFunctionModuleImportParams Import Parameters Count sapErpFunctionModuleImportParamsCount Export Parameters sapErpFunctionModuleExportParams Export Parameters Count sapErpFunctionModuleExportParamsCount Exception List sapErpFunctionExceptionList Exception List Count sapErpFunctionExceptionListCount DEVCLASS sapPackageName Transaction Codes â Transaction codes (T-codes) provide quick access to specific SAP functions or screens. Users enter T-codes in the SAP command field to navigate directly to related operations, improving workflow efficiency. Atlan maps Transaction Code from SAP ECC to its SapErpTransactionCode asset type. Source property Atlan property FUNCNAME name STEXT description DEVCLASS sapPackageName Tags: data crawl configuration Previous Crawl SAP ECC Lineage Assets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/references/what-does-atlan-crawl-from-sap-s4hana",
    "text": "Connect data ERP SAP S/4HANA References What does Atlan crawl from SAP S/4HANA? On this page What does Atlan crawl from SAP S/4HANA? Private Preview Atlan integrates with SAP S/4HANA to crawl and map various asset types, helping you gain insights into structured business data. This page outlines the SAP S/4HANA components that Atlan supports and how their properties are mapped. Lineage â Atlan supports the following lineage in SAP S/4HANA: Asset lineage: Tracks relationships from Tables to Views. Tracks relationships from Tables, Views and CDS Views to CDS Views. Column-level lineage: Tracks mappings from Table Columns to View Columns. Assets â Atlan extracts metadata from SAP S/4HANA across different asset types, including: Components : Software modules providing specific functionalities. Tables : Structured storage for master data, transactions, and configurations. Views : Logical representations of data for efficient access. CDS views : Virtual data models in SAP S/4HANA that define and consume structured data efficiently. Columns : Data attributes within tables and views. ABAP programs : Custom scripts written for automation and processing. Function modules : Reusable logic blocks for ABAP programs and remote function calls. Transaction Codes : Shortcuts for executing SAP functions. The following sections detail how each asset type is mapped in Atlan. Components â SAP S/4HANA components are modular software units that deliver specific business functions. These components form the foundation of SAP's enterprise applications, enabling functionalities such as finance, logistics, and human resources. Atlan maps Components from SAP S/4HANA to its SapErpComponent asset type. Source property Atlan property TEXT name NAME sapComponentName Tables â Tables in SAP S/4HANA store structured business data, including master records, transactional details, and configuration settings. These tables form the foundation of SAP's data storage and retrieval system. Atlan maps Table from SAP S/4HANA to its SapErpTable asset type. Source property Atlan property TABNAME name DDTEXT description TABCLASS sapErpTableType DEVCLASS sapPackageName Column Count sapFieldCount CONTFLAG sapErpTableDeliveryClass AS4USER sourceUpdatedBy AS4DATE sourceUpdatedAt Views â Views provide a logical representation of data by combining information from one or more tables. They simplify data access and reporting by allowing users to work with pre-defined, structured datasets. Atlan maps View from SAP S/4HANA to its SapErpView asset type. Source property Atlan property VIEWNAME name DDTEXT description VIEWCLASS sapErpViewType DEVCLASS sapPackageName Column Count sapFieldCount AS4USER sourceUpdatedBy AS4DATE sourceUpdatedAt CDS views â CDS (Core Data Services) views in SAP S/4HANA are virtual data models that define and consume structured data efficiently. Atlan maps CDS View from SAP S/4HANA to its SapErpCdsView asset type. Source property Atlan property DDLNAME name DDTEXT description DEVCLASS sapPackageName Column Count sapFieldCount PS_POSID sapComponentName AS4DATE sourceUpdatedAt AS4USER sourceUpdatedBy Columns â Columns define individual data attributes within tables and views. Each column has a specific data type, length, and constraints, ensuring accurate data representation and integrity. Atlan maps Column from SAP S/4HANA to its SapErpColumn asset type. Source property Atlan property FIELDNAME name TABNAME sapErpTableName or sapErpViewName ROLLNAME sapErpColumnDataElement DATATYPE sapDataType INTTYPE sapErpColumnLogicalDataType LENG sapErpColumnLength DECIMALS sapErpColumnDecimals KEYFLAG sapErpColumnIsPrimary CHECKTABLE sapErpColumnIsForeign NOTNULL sapErpColumnIsMandatory DEVCLASS sapPackageName POSITION sapFieldOrder ABAP programs â Advanced Business Application Programming (ABAP) programs are scripts used to automate processes, manipulate data, and extend SAP functionalities. These programs are written in SAP's proprietary programming language. Atlan maps ABAP Programs from SAP S/4HANA to its SapErpAbapProgram asset type. Source property Atlan property PROGNAME name TEXT description SUBC sapErpAbapProgramType DEVCLASS sapPackageName CNAM sourceCreatedBy CDAT sourceCreatedAt UNAM sourceUpdatedBy UDAT sourceUpdatedAt Function modules â Function modules are reusable code blocks that perform predefined operations in SAP S/4HANA. They can be called within ABAP programs or accessed remotely to execute business logic efficiently. Atlan maps Function Modules from SAP S/4HANA to its SapErpFunctionModule asset type. Source property Atlan property FUNCNAME name STEXT description FUNC_GROUP sapErpFunctionModuleGroup Import Parameters sapErpFunctionModuleImportParams Import Parameters Count sapErpFunctionModuleImportParamsCount Export Parameters sapErpFunctionModuleExportParams Export Parameters Count sapErpFunctionModuleExportParamsCount Exception List sapErpFunctionExceptionList Exception List Count sapErpFunctionExceptionListCount DEVCLASS sapPackageName Transaction codes â Transaction codes (T-codes) provide quick access to specific SAP functions or screens. Users enter T-codes in the SAP command field to navigate directly to related operations, improving workflow efficiency. Atlan maps Transaction Code from SAP S/4HANA to its SapErpTransactionCode asset type. Source property Atlan property FUNCNAME name STEXT description DEVCLASS sapPackageName Tags: data crawl configuration Previous Crawl SAP S/4HANA Lineage Assets"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/references/what-does-atlan-crawl-from-teradata",
    "text": "Connect data Databases SQL Databases Teradata References What does Atlan crawl from Teradata? On this page What does Atlan crawl from Teradata? Atlan crawls and maps the following assets and properties from Teradata. Schemas â Atlan maps schemas from Teradata to its Schema asset type. Source property Atlan property dbc.databases.DatabaseName name TABLE_COUNT tableCount VIEW_COUNT viewsCount dbc.databases.CreatorName sourceCreatedBy dbc.databases.CreateTimeStamp sourceCreatedAt dbc.databases.LastAlterName sourceUpdatedBy dbc.databases.LastAlterTimeStamp sourceUpdatedAt dbc.databases.CommentString description Tables â Atlan maps tables from Teradata to its Table asset type. Source property Atlan property dbc.Tables.TableName name dbc.TablesV.CommentString description COLUMN_COUNT columnCount ROW_COUNT rowCount CurrentPerm sizeBytes dbc.TablesV.CreatorName sourceCreatedBy dbc.TablesV.CreateTimeStamp sourceCreatedAt dbc.TablesV.LastAlterName sourceUpdatedBy dbc.TablesV.LastAlterTimeStamp sourceUpdatedAt dbc.Tables.DataBaseName schema name Views â Atlan maps views from Teradata to its View asset type. Source property Atlan property dbc.Tables.TableName name dbc.TablesV.CommentString description COLUMN_COUNT columnCount ROW_COUNT rowCount CurrentPerm sizeBytes dbc.TablesV.CreatorName sourceCreatedBy dbc.TablesV.CreateTimeStamp sourceCreatedAt dbc.TablesV.LastAlterName sourceUpdatedBy dbc.TablesV.LastAlterTimeStamp sourceUpdatedAt dbc.Tables.DataBaseName schema name Columns â Atlan maps columns from Teradata to its Column asset type. Source property Atlan property dbc.ColumnsV.ColumnName name REMARKS description ORDINAL_POSITION order dbc.ColumnsV.ColumnType dataType dbc.ColumnsV.Nullable isNullable dbc.IndicesV.TableName isPrimary dbc.IndicesV.ChildDB isForeign dbc.IndicesV.DecimalTotalDigits precision dbc.IndicesV.DecimalFractionalDigits numericScale dbc.ColumnsV.DataBaseName schema name Tags: data crawl Previous Set up on-premises Teradata miner access Next Preflight checks for Teradata Schemas Tables Views Columns"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/references/what-does-atlan-crawl-from-trino",
    "text": "Connect data Databases Query Engines Trino References What does Atlan crawl from Trino? On this page What does Atlan crawl from Trino? Atlan crawls and maps the following assets and properties from Trino. Databases â Atlan maps databases from Trino to its Database asset type. Source property Atlan property TABLE_CATALOG name SCHEMA_COUNT schemaCount Schemas â Atlan maps schemas from Trino to its Schema asset type. Source property Atlan property TABLE_SCHEMA name TABLE_COUNT tableCount VIEW_COUNT viewsCount TABLE_CATALOG databaseName Tables â Atlan maps tables from Trino to its Table asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount ROW_COUNT rowCount BYTES sizeBytes Views â Atlan maps views from Trino to its View asset type. Source property Atlan property TABLE_NAME name REMARKS description COLUMN_COUNT columnCount EXTRA_INFO (CREATE VIEW) definition Columns â Atlan maps columns from Trino to its Column asset type. Source property Atlan property COLUMN_NAME name REMARKS description ORDINAL_POSITION order TYPE_NAME dataType NULLABLE isNullable DECIMAL_DIGITS precision NUMERIC_SCALE numericScale Stored procedures â Atlan maps stored procedures in Trino to its Procedure asset type. Source property Atlan property PROCEDURE_NAME name REMARKS description PROCEDURE_TYPE subType ROUTINE_DEFINITION definition Tags: data crawl Previous Set up a private network link to Trino Next Preflight checks for Trino Databases Schemas Tables Views Columns Stored procedures"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/faq/jira-integration",
    "text": "Configure Atlan Integrations Project Management Jira FAQ What is included in the Jira integration? On this page What is included in the Jira integration? With two of your most important workspaces connected, you can save time and improve the way you track issues for your data. Once you integrate Jira with Atlan you can do all of the following   -  all without leaving Atlan! Create new Jira issues on assets Link existing Jira issues to assets See the complete history of Jira issues for each asset Did you know? To use all of the features outlined, each user needs to connect their individual Jira account to their Atlan profile. Create new Jira issues on assets â danger You will first need to add a labels field to your Create issue form in Jira to be able to create Jira issues on your assets in Atlan. To create issues in Jira, without leaving Atlan: From any asset, on the right of the screen, click the Jira sidebar icon. Click theÂ Add Issue button, and then Create Issue . From the Creating issue dialog: (Optional) ForÂ Project select the project for the issue. (By default, the project configured during integration will be selected.) (Optional) ForÂ Issue type select the type of issue being created in Jira. ForÂ Title enter a brief summary of the issue. ForÂ Description enter the detailed explanation of the issue. Click theÂ Create button to submit your issue to Jira. Your issue is now on Jira and linked to the asset for future reference! ð Link existing Jira issues to assets â Ever determine that an issue in Jira is related to multiple data assets? It's important to bring that context back to your assets. To add existing issues in Jira into Atlan: From the asset, on the right of the screen, click the Jira sidebar icon. Click theÂ Add Issue button, and then Link Issue . From theÂ Link Issues dialog: (Optional) Search for any issues in Jira to link. Check each issue that should be added to the asset. At the top of the dialog, click the Link button. Your existing issue on Jira is now linked to the asset for future reference! ð Tags: data integration faq faq-integrations Previous Troubleshooting Jira Next Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Create new Jira issues on assets Link existing Jira issues to assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/glossary-default-permissions",
    "text": "Build governance Glossary FAQ What is the default permission for a glossary? What is the default permission for a glossary? By default, users can search and discover glossaries in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a glossary policy to control what users can do with glossary metadata and create a persona to curate edit access. In addition, admins can turn off the default behavior , so that your member and guest users will only have access to the glossaries curated through glossary policies in their personas. Tags: data faq faq-governance Previous Can I add duplicate glossary terms? Next Use personas to update a term in a glossary"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/personal-data-processor",
    "text": "Configure Atlan Integrations Identity Management SSO FAQ When does Atlan become a personal data processor or subprocessor? When does Atlan become a personal data processor or subprocessor? Atlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively. For this purpose, where Atlan customers intend to crawl any personal data into the platform, Atlan enters into a data protection agreement with its customers, and as applicable: a Standard Contractual Clauses (SCC) under EU GDPR, an International data transfer agreement (IDTA) under UK GDPR, or a HIPAA Business Associate Agreement (BAA) when dealing with personal health information governed by HIPAA regulations. Tags: data crawl faq-integrations Previous What type of user provisioning does Atlan support for SSO integrations? Next Why did my users not receive an invite email from Atlan?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/faq/join-options-in-visual-query",
    "text": "Use data Insights FAQ Why do I only see tables from the same schema to join from in a visual query? Why do I only see tables from the same schema to join from in a visual query? When creating a visual query , Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder . Tags: data faq faq-insights Previous What controls the frequency of queries? Next Troubleshooting bring your own credentials"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/table-level-lineage-only",
    "text": "Use data Lineage FAQ Why is lineage available for table level but not column level? Why is lineage available for table level but not column level? The home icon on top of any asset on the lineage graph indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for supported sources , click view columns and then select a column to view data flows for that particular asset. Tags: data faq faq-lineage Previous What lineage do you support? Next Why is the SQL query visible only in Snowflake process and not in dbt process nodes?"
  },
  {
    "url": "https://docs.atlan.com/faq/workflows-and-data-processing",
    "text": "Configure Atlan Frequently Asked Questions Workflows and Data Processing On this page Workflows and Data Processing Everything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan. How do I configure custom cron schedules? â You can use cron expressions to create custom cron schedules for your workflows in Atlan. A cron expression helps you specify the date and time for when a scheduled task must be executed. Cron expressions consist of five date and time fields separated by white space - there shouldn't be any white spaces within a field value. Cron expressions in Atlan include the following five fields and corresponding values: Field name Allowed values Allowed special characters Minutes 0-59 , - * / Hours 0-23 , - * / Day of the month 1-31 , - * / Month 0-12 , - * / Day of the week 0-7 , - * / , - comma specifies a list of values - - dash specifies a range of values * - asterisk specifies all possible values for a field / - slash is used to skip a given number of values Examples of cron expressions and their respective meanings: 0 0 1 * * - Run at midnight on day 1 of every month 0 0 * * * - Run once a day at midnight 0 */3 * * * - Run at minute 0 past every 3rd hour or run every 3 hours 0 0 1,15 * * - Run at midnight on day 1 and 15 of every month 30 14 * * 1,3 - Run at 14:30 on Monday and Wednesday 0 6,18 * * * - Run at minute 0 past hours 6 and 18 or run at 6 AM and then 6 PM. 0 0 1 3,6,9,12 * - Run at midnight on day 1 of March, June, September, and December Why is the first miner run taking so long to finish? â Typically, the first run of the miner takes longer than usual. This is likely because it's parsing through queries beginning from the date chosen during setup. It's recommended that the start date be no further back than a week. As long as the miner is scheduled to run and running, it continuously picks up and builds lineage as data flows run. Subsequent runs must be much quicker in relation to the first run - especially if the miner is set up to run daily. In that case, it only parses through new queries as opposed to historic ones. Keep in mind that the number of queries or transformations running daily can also be a factor in the time it takes for the miner to run. To learn more about miner logic, see here . Are there any extra steps required for rerunning the miner? â The miner can rerun without any additional steps involved. However, it's possible that the miner errors out when running after a few weeks. If this happens, consider changing the start date of the miner config to be no further back than a week. Why do some workflows take longer? â You can take the following reasons into consideration when accounting for longer workflow runtimes: Extracting a high volume of assets from the source may increase the time a workflow takes to complete. Atlan workflows run differential crawls , bringing in only delta changes from the source. This speeds things up, and further parallelisation helps optimise runtime. On some days the runtime can exceed the usual duration: There may be more transformations to process, so more delta changes need to be synced to Atlan. If one of those transformations includes a delete operation, Atlan archives the removed assets. Archival can take longer and therefore extend the overall runtime. For general guidelines, see How to order workflows . Why is the workflow config or new workflow button not working? â If the workflow config page is blank or the New workflow button doesn't proceed to the next step, try these checks: Open Atlan in an incognito / private-browsing window and see whether the page loads. If it loads, verify whether your browser has any ad-blockers enabled. Either disable the ad-blocker or add *.atlan.com to the allowlist. Workflow is failing with: Delete percentage is more than 80.0. Exiting. â Atlan has added guardrails to the workflow execution to make sure that assets don't get archived accidentally. A circuit breaker logic is triggered when 80% of existing assets are missing in the current workflow run. This logic aborts the workflow and prevents it from committing any changes. This situation commonly arises if: Permissions for the Atlan integration user are revoked or updated in the source system. Include and exclude metadata filters in the workflow configuration are modified . Assets are removed from the source system. In case the mass deletion is intentional, please reach out to Atlan support to disable the circuit breaker. The next run of the workflow proceeds with archiving assets that aren't part of the run. However, note that any metadata updates (tags, descriptions, and more) on these assets are lost. How's lineage from procedures deduced in Atlan? â Lineage from procedures is inferred indirectly from the query history generated when the procedure runs. Can offline extraction fail if there are spaces in the path? â Atlan currently doesn't support spaces in folder names for S3. The offline extraction workflow fails if you include any spaces in the folder name in S3. To follow documented guidelines for safe characters, refer to Amazon S3 documentation . Is the existing file in the bucket overwritten when uploading the JSON files? â Yes. For dbt Core workflows the recommended approach is to replace the folder with the new manifest.json and run_results.json files. The workflow uses file names to locate its inputs and doesn't check timestamps. (The catalog.json file is no longer required.) Can I configure a Snowflake workflow using account usage and then switch to information schema? â When you modify an existing Snowflake connection and change the extraction method, Atlan deletes and recreates all assets in that connection. If you need to switch from Account usage to Information schema , please contact Atlan support so the change can be applied safely. Can I receive notifications when workflows fail? â Atlan can send failure alerts to Slack and Microsoft Teams . In Admin â Integrations , open the Slack (or Microsoft Teams) tile and enable Receive failure alerts only to get a notification whenever a workflow fails. Can I create a multi-step approval workflow in Atlan? â Yes. Atlan integrates with tools such as Jira , enabling you to build multi-step approval workflows that match your organisation's processes. Is the PII tagging of data or metadata automated? â Atlan propagates tags based on hierarchy and lineage. For example, if you attach a tag named PII to a table and tag propagation is enabled, the tag is copied to downstream columns. Atlan doesn't automatically detect PII. Propagation only occurs if you enable it manually or automate it using playbooks. For details, see Why does tag propagation take time to apply? . Are there any dbt assets that cannot be viewed in dbt? â Atlan shows the View in dbt link only for dbt models, sources, and tests that include a valid target_url . Assets without a target URL won't display the link. Can I follow the background processes of workflows in Argo? â If you have cluster-level access, you can open the built-in Argo UI at https://your-atlan-domain/argo to watch each workflow's DAG, pod logs, and retry status in real time. Otherwise, use the History tab inside the workflow sidebar in Atlan or the run-level logs downloadable from the Runs table. How does Atlan work with dbt single-tenant vs multi-tenant? â For dbt Cloud single-tenant projects, Atlan authenticates with the project-scoped API key you provide. For multi-tenant workspaces, Atlan uses your account-level service token and the project ID to pull lineage and documentation. Behaviour in Atlan is identical; the difference is only in where the credentials are scoped in dbt Cloud. Cloud logging and monitoring â Atlan sends application and access logs to your cloud provider's native logging service: CloudWatch (AWS), Stackdriver (GCP), or Azure Monitor. You can ingest these logs into your SIEM for central monitoring. Contact Atlan support to enable log shipping for your tenant. Tags: data configuration faq-automation Previous User Management and Access Control"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/how-tos/crawl-gcs",
    "text": "Connect data Storage Google GCS Crawl GCS Assets Crawl GCS assets On this page Crawl GCS assets The Google Cloud Storage crawler fetches assets from Google Cloud Storage and publishes them to Atlan for discovery. The crawler catalogs buckets and objects from your GCS environment. Prerequisites â Before you begin, make sure you have: Completed the GCS setup guide . GCS credentials (Project ID and Service Account JSON key) ready. Determined which GCS buckets and prefixes you want to catalog. Create crawler workflow â Start by creating a new GCS Assets workflow: Navigate to the bottom right of any screen and select Workflow . Select Marketplace from the top if you are creating a new workflow, or select Manage if you want to use an existing workflow. Select Google Cloud Storage from the package list. Select Setup Workflow . Configure extraction method â Choose how to connect to your GCS environment. Connect directly to GCS using Atlan's credential store: Add the Project ID (Google Cloud project ID that contains the buckets). Add the Service Account JSON key that you created in the GCS setup guide . Select Test Authentication to verify connectivity. Select Next . Configure metadata filters â Set up filters to control which buckets and objects get cataloged: Bucket prefix : Publish to Atlan only the buckets that start with the specified prefix. Leave empty if you need all buckets. Object prefix : Publish to Atlan only the objects that start with the specified prefix. Leave empty if you need all objects. Object delimiter (applicable only if inventory report isn't selected): Use this to list all blobs in a \"folder,\" for example \"public/.\" The delimiter argument restricts results to only the \"files\" in the given \"folder.\" Without the delimiter, the entire tree under the prefix is returned. For example, given these blobs: a/1.txt a/b/2.txt If prefix = 'a/', without a delimiter, the following blobs are published to Atlan: a/1.txt a/b/2.txt However, if prefix = 'a/' and delimiter = '/', only the file directly under 'a/' is published to Atlan: a/1.txt Bucket exclusion list : List of buckets (comma separated) to be excluded. Configure ingestion method â Choose how to ingest data from GCS: Direct crawling Inventory report Configure direct crawling options: Build abstraction layer : Whether to build abstraction layer on top of files (default: No). Publish as-is patterns : List of comma-separated patterns to be published as-is (without abstraction layer). Applicable only if Build abstraction layer = Yes. Regex to match characters to replace : Regular expression to match characters to replace. It acts on the file full name (without bucket prefix). Regex with replacement characters : Regular expression with replacement characters. It acts on the file full name (without bucket prefix). Configure inventory report options: Inventory bucket name : Bucket where the inventory is stored. Inventory prefix : Prefix within the inventory bucket where the inventory is located. Inventory file format : File format used to generate the inventory report (CSV or Parquet). The following permissions must be granted to the role assigned to the Service Account: storage.buckets.list , storage.objects.list , and roles/storage.objectViewer . Build abstraction layer : Whether to build abstraction layer on top of files (default: No). Publish as-is patterns : List of comma-separated patterns to be published as-is (without abstraction layer). Applicable only if Build abstraction layer = Yes. Regex to match characters to replace : Regular expression to match characters to replace. It acts on the file full name (without bucket prefix). Regex with replacement characters : Regular expression with replacement characters. It acts on the file full name (without bucket prefix). Configure asset handling â Control how assets are created and updated: Input handling : How to handle assets in the CSV file that don't exist in Atlan: Create full : Create a full-fledged asset that can be discovered and maintained like other assets in Atlan. Create partial : Create a \"partial\" asset. These are only shown in lineage and can't be discovered through search. These are useful when you want to represent a placeholder for an asset that you lack full context about, but also don't want to ignore completely. Update only : Only update assets that already exist in Atlan, and don't create any asset of any kind. Note: READMEs and links in Atlan are technically separate assetsâthese are still created, even in Update only mode. Delta handling : Whether to treat the input file as an initial load, full replacement (deleting any existing assets not in the file), or only incremental (no deletion of existing assets). Remove attributes : How to delete any assets not found in the latest file. Reload which assets : Which assets to reload from the latest input CSV file. Changed assets only calculates which assets have changed between the files and only attempts to reload those changes. Configure connection â Set up the connection details: Connection : Name of the connection that's created in Atlan. The connection name must be unique across all Google Cloud Storage connections. Need help â If you run into issues during the GCS crawling process: GCP documentation : Refer to the Google Cloud Storage documentation for detailed information about buckets and objects. Contact Atlan support : For issues related to Atlan integration, contact Atlan support . See also â What does Atlan crawl from GCS : Learn about the GCS metadata that Atlan discovers and catalogs. Tags: gcs google-gcs crawl data-catalog storage Previous Set up Google Cloud Storage Next What does Atlan crawl from Google GCS Prerequisites Create crawler workflow Need help See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/crawl-s3",
    "text": "Connect data Storage Amazon S3 Crawl S3 Assets Crawl S3 assets On this page Crawl S3 assets Catalog your Amazon S3 buckets and objects in Atlan using the S3 Assets workflow. This guide walks you through setting up authentication and running your first crawl. Prerequisites â Before you begin, make sure the you have: Completed S3 setup with IAM credentials. Your AWS credentials (Access Key ID and Secret Access Key, or Role ARN) ready. Information about S3 buckets and prefixes you want to catalog. Verified that the AWS account is allowlisted to assume the role when using IAM role-based authentication Set up the destination bucket structure , required only if you plan to use inventory-based ingestion . Set up workflow â Create a new S3 Assets workflow: In the top right, select New > New Workflow . From the package list, select S3 Assets . Select Setup Workflow . Configure extraction method â Choose how to connect to your S3 environment: Direct extraction Agent extraction Select Direct for the extraction method. Choose your authentication type: IAM User : Enter your Access Key ID and Secret Access Key. IAM Role : Enter your Role ARN. Select the AWS Region where your buckets are located. Select Test Authentication to verify the connection. Select Next . Select Agent for the extraction method. Add the secret keys for your secret store configuration. Follow the Secure Agent configuration guide . Select Next . Choose ingestion method â Select your ingestion method: Direct ingestion : Recommended for fewer than 1 million objects. This method crawls S3 buckets and objects directly. Inventory ingestion : Recommended for large-scale use (more than 1 million objects). Uses inventory reports for efficiency. For inventory ingestion, provide: S3 Bucket Name : Bucket holding the inventory reports (without the s3:// prefix). S3 Bucket Prefix : Prefix used in the report configuration. Include a trailing slash ( / ). Leave empty if no prefix was used. note The region for the inventory report is picked from the credentials used in the extraction method . Configure bucket filters â Choose which buckets and prefixes to include or exclude. Exclude filters override include filters if both match. For a single bucket : Include Bucket : Exact bucket name (e.g., my-data-bucket ) Include Prefix : Specific prefix to crawl (e.g., processed/2024/ ) Leave all other filters empty. For multiple buckets : Include Bucket : Regex pattern (e.g., prod-.* | analytics-.* ) Exclude Bucket : Regex pattern (e.g., .*-temp | .*-backup ) Include Prefix : Prefixes to include (e.g., data/ | reports/ ) Exclude Prefix : Prefixes to exclude (e.g., archive/ | tmp/ ) Configure connection details â Enter a Connection Name to identify your S3 environment. Examples: production-s3 , analytics-lake , raw-data-store Assign Connection Admins to manage access. At least one admin is required. Run crawler â You can now start cataloging your assets: Run now : Select Run to start a one-time crawl. Schedule runs : Select Schedule & Run to automate recurring crawls. Monitor crawl progress in the activity log. Once complete, your S3 buckets and objects will appear in Atlan. Troubleshooting â Permissions : Confirm all required IAM permissions are set. See the S3 setup guide for details. Need help â Contact Atlan support for integration issues or assistance. See also â What Atlan crawls from S3 : Full list of assets and metadata included in the crawl. Tags: s3 amazon-s3 crawl data-catalog storage Previous Set up Inventory reports Next S3 Inventory Report Structure Prerequisites Set up workflow Troubleshooting Need help See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/how-tos/crawl-sap-ecc",
    "text": "Connect data ERP SAP ECC Crawl SAP ECC Assets Crawl SAP ECC On this page Crawl SAP ECC Once you have configured the SAP ECC access permissions , you can establish a connection between Atlan and your SAP ECC system. To crawl metadata from your SAP ECC system, review the order of operations and then complete the following steps. Select the source â To select SAP ECC as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select SAP ECC Assets , and click Setup Workflow . Provide credentials â Atlan supports using a Secure Agent for fetching metadata from SAP ECC. To use a Secure Agent, follow these steps: Select the Agent tab. Add secret keys for your SAP ECC credentials in the linked secret store. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. The SAP ECC agent configuration requires the following parameters: SAP Host Name : The hostname or IP address of your SAP ECC system SAP User Name : The dedicated user account for metadata extraction SAP Password : The password for the user account Connection Type : Choose between Application Server or Message Server. If you have chosen Message server, provide below details: Port : Port number (required if using Message Server) Group : Group number (required if using Message Server) SAP System Number : Two-digit system identifier (00-99) SAP Client Number : Three-digit client identifier (001-999) Use SAP Router : Choose Yes , if using a SAP Router, and provide the SAP Router String . Click Next after completing the configuration. Configure the connection â To complete the SAP ECC connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. Use Secure Network Connection : Choose Yes to use secure communication. When using a secure connection, prvoide below details: SNC Name : Your SNC name SNC Partner : Partner SNC name SNC Security Level : Security level SNC Library Path : Path to SNC library At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the SAP ECC crawler, you can configure which components to include or exclude: To select the components you want to include in crawling, click Include Components and use the dropdown to select specific SAP components and their subcomponents. You can search for components using the search bar or use the advanced search option. To select the components you want to exclude from crawling, click Exclude Components and use the dropdown to select specific SAP components and their subcomponents to exclude from the crawl. The component selection supports hierarchical browsing, allowing you to expand and collapse component categories to select specific subcomponents. You may choose to configure the crawler to use advance settings by providing below details: SAP Language : Language code (default: EN) SAP Codepage : Character encoding (default: 0) SAP Unicode System : Choose Yes to use unicode Unicode Support : Choose Yes to enable Unicode support SAP Trace : Choose Yes to enable trace logging for debugging Connection Pool Size : Maximum number of connections (default: 5) Run the crawler â Follow these steps to run the SAP ECC crawler: You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you see the assets in Atlan's asset page! ð Tags: erp crawl setup Previous Set up SAP ECC Next What does Atlan crawl from SAP ECC? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/how-tos/crawl-sap-s4hana",
    "text": "Connect data ERP SAP S/4HANA Crawl SAP S/4HANA Assets Crawl SAP S/4HANA On this page Crawl SAP S/4HANA Once you have configured the SAP S/4HANA access permissions , you can establish a connection between Atlan and your SAP S/4HANA system. To crawl metadata from your SAP S/4HANA system, review the order of operations and then complete the following steps. Select the source â To select SAP S/4HANA as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select SAP S/4HANA Assets , and click Setup Workflow . Provide credentials â Atlan supports using a Secure Agent for fetching metadata from SAP S/4HANA. To use a Secure Agent, follow these steps: Select the Agent tab. Add secret keys for your SAP S/4HANA credentials in the linked secret store. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. The SAP S/4HANA agent configuration requires the following parameters: SAP Host Name : The hostname or IP address of your SAP S/4HANA system SAP User Name : The dedicated user account for metadata extraction SAP Password : The password for the user account Connection Type : Choose between Application Server or Message Server. If you have chosen Message server, provide below details: Port : Port number (required if using Message Server) Group : Group number (required if using Message Server) SAP System Number : Two-digit system identifier (00-99) SAP Client Number : Three-digit client identifier (001-999) Use SAP Router : Choose Yes , if using a SAP Router, and provide the SAP Router String . Click Next after completing the configuration. Configure the connection â To complete the SAP S/4HANA connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. Use Secure Network Connection : Choose Yes to use secure communication. When using a secure connection, prvoide below details: SNC Name : Your SNC name SNC Partner : Partner SNC name SNC Security Level : Security level SNC Library Path : Path to SNC library At the bottom of the screen, click the Next button to proceed. Configure the crawler â Before running the SAP S/4HANA crawler, you can configure which components to include or exclude: To select the components you want to include in crawling, click Include Components and use the dropdown to select specific SAP components and their subcomponents. You can search for components using the search bar or use the advanced search option. To select the components you want to exclude from crawling, click Exclude Components and use the dropdown to select specific SAP components and their subcomponents to exclude from the crawl. The component selection supports hierarchical browsing, allowing you to expand and collapse component categories to select specific subcomponents. You may choose to configure the crawler to use advance settings by providing below details: SAP Language : Language code (default: EN) SAP Codepage : Character encoding (default: 0) SAP Unicode System : Choose Yes to use unicode Unicode Support : Choose Yes to enable Unicode support SAP Trace : Choose Yes to enable trace logging for debugging Connection Pool Size : Maximum number of connections (default: 5) Run the crawler â Follow these steps to run the SAP S/4HANA crawler: You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you see the assets in Atlan's asset page! ð Tags: erp crawl setup Previous Set up SAP S/4HANA Next What does Atlan crawl from SAP S/4HANA? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/set-up-looker",
    "text": "Connect data BI Tools Cloud-based BI Looker Get Started Set up Looker On this page Set up Looker Who can do this? You will probably need your Looker administrator to run these commands   -  you may not have access yourself. Atlan supports two options for user permissions in Looker. You should choose one of these methods to set up Looker: Admin role â This role is required for Atlan to automatically generate lineage across Looker objects. When using this role, the crawler can access all folders in Looker including personal folders. To set up this role: Log in to your Looker instance and ensure that you are an Admin user. From the menu in the upper left, click the Admin item. Under the Users section, click the Users item. In the table, find the user you're logged in as. Click the Edit button to the right of your user's row. Next to API3 Keys , click the Edit Keys button. On the resulting Edit User API3 Keys page, click the New API3 Key button. Save the generated credentials for crawling Looker . Custom role â danger When using this approach, Atlan will not automatically generate lineage across Looker objects. You will need to individually allow access to each folder to be included in lineage. Create role â To create a custom role for Atlan to access Looker: Log in to your Looker instance. From the menu in the upper left, click the Admin item. Under the Users section, click the Roles item. At the top of the page, click the New Permission Set button. Enter a name for the new permission set. For the permissions, select the following: access_data allows access to the other permissions below. see_lookml_dashboards allows Atlan to crawl LookML dashboards. see_looks allows Atlan to crawl Looks. see_user_dashboards allows Atlan to crawl user-defined dashboards. explore allows Atlan to fetch from the Explore page. see_sql allows Atlan to fetch the SQL of a query or Look, to generate lineage. see_lookml allows Atlan to fetch model information from LookML. develop allows Atlan to fetch connection names from models, to generate lineage. see_datagroups allows Atlan to fetch all connection names, to generate lineage. At the bottom of the permissions list, click the New Permission Set button. Back on the Roles page, at the top click the New Role button. Enter a name for the new role. For Permission Set , select the permission set you created in the previous step. For Model Set , select the models that you want to give access to. At the bottom of the page click the New Role button. Create user â To create a user through which Atlan can access Looker: Open the Admin menu in Looker. Under the Users section, click the Users item. At the top of the page, click the Add Users button. For Email addresses enter the email address for the user. For Send setup emails uncheck the setting. For Roles check the box next to the role you created above. At the bottom of the page, click the Add Users button. On the resulting page, click the Done button. Generate API key for user â To generate an API key for the user: Open the Admin menu in Looker. Under the Users section, click the Users item. In the table, find the user created above. Click the Edit button to the right of that user's row. (Optional) Consider entering a First Name and Last Name for the user to make it easier to recognize and find in the future. Next to API3 Keys , click the Edit Keys button. On the resulting Edit User API3 Keys page, click the New API3 Key button. Save the generated credentials for crawling Looker . Include folders for lineage â To include folders when using a custom role, give permission using the following steps: From the Looker menu in the upper left, click the Admin item. Under the Users section, click the Content Access item. In the resulting page next to Folders select the folder and then click on the Manage Access... button. In the blank box at the bottom of the table, select the user created above from the list. To allow Atlan to crawl only dashboards, enable the View permission for this user. To allow Atlan to crawl tiles and queries for dashboards, enable the Manage Access, Edit permission for this user. To the right of the row for that user, click the Add button. At the lower-right of the dialog, click the Save button. danger You will need to repeat these steps for every folder you want Atlan to be able to access. SSH key for lineage â Who can do this? Any user with access to the Looker project files in GitHub can set up this part. You will need to share the generated private key with whoever sets up the Looker crawler in Atlan. If your organization uses single sign-on (SSO) on GitHub, you must first authorize the SSH key for use with SSO. Refer to Authorizing an SSH key for use with SAML single sign-on to complete the process. In addition to the role, you also need to set up access to your project files in GitHub for the following: Generate field-level and cross-project lineage from Looker Crawl Looker views and build upstream lineage for views and explores To configure an SSH key for access to GitHub project files: Create a new SSH key on your local computer . For example, run the following command and enter a passphrase when prompted (or leave blank for no passphrase): ssh-keygen -t ed25519 -C \" [email protected] \" -f ~/.ssh/atlan_looker_lineage Copy the generated keys from your local computer . For example: To copy the public key, run this command and copy the output: cat ~/.ssh/atlan_looker_lineage.pub To copy the private key, run this command and copy the output: cat ~/.ssh/atlan_looker_lineage In the upper-right corner of any GitHub page, click your profile photo, then click Settings . Under the Access section of the left sidebar, clickÂ SSH and GPG keys . In the upper-right, click the New SSH key button: For Title enter a descriptive label for the new key. For example, Atlan Lineage . For Key paste in the public key you copied above. At the bottom of the form, click theÂ Add SSH key button. If prompted, enter your GitHub password and clickÂ Confirm password . Tags: crawl api Previous Looker Next Set up on-premises Looker access Admin role Custom role SSH key for lineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/set-up-trino",
    "text": "Connect data Databases Query Engines Trino Get Started Set up Trino On this page Set up Trino Who can do this? You will probably need your Trino administrator to run these commands   -  you may not have access yourself. Currently we only support basic (username and password) authentication for Trino. We recommend creating a separate user for Atlan with read-only access. Please ensure you are using frontend password authentication over HTTPS for clients. Create user in Trino â To create a new user with password file authentication follow the steps in the official Trino documentation . Grant read-only access â To grant read-only access to the user created above follow the steps in the official Trino documentation . This includes adding a list of catalogs you wish to crawl to your rules.json file, for example: { \"catalogs\" : [ { \"user\" : \"atlan\" , \"catalog\" : \"postgresql\" , \"allow\" : \"read-only\" } , { \"user\" : \"atlan\" , \"catalog\" : \"mysql\" , \"allow\" : \"read-only\" } , ... ] } Tags: crawl authentication Previous Trino Next Crawl Trino Create user in Trino Grant read-only access"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/references/what-does-atlan-crawl-from-s3",
    "text": "Connect data Storage Amazon S3 References What does Atlan crawl from Amazon S3 On this page What does Atlan crawl from Amazon S3 This document provides complete details on the S3 assets and properties that Atlan crawls and maps during the S3 cataloging process. Atlan supports two ingestion modes with different property coverage. Buckets â Atlan maps buckets from S3 to its S3Bucket asset type. Source Property Atlan Property Ingestion Mode Bucket Name name Direct, Inventory Object Count s3ObjectCount Direct, Inventory Console URL sourceURL Direct Region awsRegion Direct ServerSideEncryptionConfiguration s3Encryption Direct Versioning Status s3BucketVersioningEnabled Direct Objects â Atlan maps objects from S3 to its S3Object asset type. Source Property Atlan Property Ingestion Mode Object Name name Direct, Inventory Object Key s3ObjectKey Direct, Inventory Bucket Name s3BucketName Direct, Inventory Object Size (bytes) s3ObjectSize Direct, Inventory LastModifiedDate s3ObjectLastModifiedTime Direct, Inventory StorageClass s3ObjectStorageClass Direct, Inventory ETag s3ETag Direct, Inventory Console URL sourceURL Direct Region awsRegion Direct Content Type s3ObjectContentType Direct Content Disposition s3ObjectContentDisposition Direct Version ID s3ObjectVersionId Direct See also â Set up S3 : Configure AWS permissions and environment Crawl S3 assets : Step-by-step guide for setting up S3 crawling Tags: s3 amazon-s3 crawl data-catalog reference assets properties Previous S3 Inventory Report Structure Buckets Objects See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/references/what-does-atlan-crawl-from-gcs",
    "text": "Connect data Storage Google GCS References What does Atlan crawl from Google GCS On this page What does Atlan crawl from Google GCS This document provides complete details on the GCS assets and properties that Atlan crawls and maps during the GCS cataloging process. Atlan supports two ingestion modes with different property coverage. Buckets â Atlan maps buckets from GCS to its GCSBucket asset type. Source Property Atlan Property Bucket Name name Object Count gcsObjectCount Console URL sourceURL Objects â Atlan maps objects from GCS to its GCSObject asset type. Source Property Atlan Property Object Name name Object Key gcsObjectKey Object Size (bytes) gcsObjectSize Object updated sourceUpdatedAt Object created sourceCreatedAt Gcs object content type gcsObjectContentType Object URL sourceURL See also â Set up GCS : Configure GCS permissions and environment Crawl GCS assets : Step-by-step guide for setting up GCS crawling Tags: gcs google-gcs crawl data-catalog reference assets properties Previous Crawl GCS assets Buckets Objects See also"
  },
  {
    "url": "https://docs.atlan.com/tags/databricks",
    "text": "6 docs tagged with \"databricks\" View all tags Databricks Integrate, catalog, and govern Databricks assets in Atlan. Databricks Data Quality Studio Set up and configure Databricks for data quality monitoring through Atlan. Enable data quality on connection Enable and configure data quality for your Databricks connection in Atlan. Set up cross-workspace extraction Configure a single service principal to crawl metadata from all workspaces within a Databricks metastore using system tables Set up Databricks Configure Databricks to enable data quality monitoring through Atlan. Setup and configuration Common questions about Databricks data quality setup and configuration."
  },
  {
    "url": "https://docs.atlan.com/tags/connector",
    "text": "63 docs tagged with \"connector\" View all tags Aiven Kafka Integrate, catalog, and govern Aiven Kafka assets in Atlan. Alteryx Integrate, catalog, and govern Alteryx assets in Atlan using OpenLineage. Amazon Athena Integrate, catalog, and govern Amazon Athena assets in Atlan. Amazon DynamoDB Integrate, catalog, and govern Amazon DynamoDB assets in Atlan. Amazon MSK Integrate, catalog, and govern Amazon MSK assets in Atlan. Amazon MWAA OpenLineage Integrate, catalog, and visualize Amazon MWAA lineage in Atlan. Amazon QuickSight Integrate, catalog, and govern Amazon QuickSight assets in Atlan. Amazon Redshift Integrate, catalog, and govern Amazon Redshift assets in Atlan. Anomalo Integrate, catalog, and govern Anomalo assets in Atlan. Apache Airflow OpenLineage Integrate, catalog, and visualize Apache Airflow lineage in Atlan. Apache Kafka Integrate, catalog, and govern Apache Kafka assets in Atlan. Apache Spark OpenLineage Integrate, catalog, and visualize Apache Spark lineage in Atlan. Astronomer OpenLineage Integrate, catalog, and visualize Astronomer lineage in Atlan. AWS Glue Integrate, catalog, and govern AWS Glue assets in Atlan. BigID Integrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata. Cloudera Impala Integrate, catalog, and govern Cloudera Impala assets in Atlan. Confluent Kafka Integrate, catalog, and govern Confluent Kafka assets in Atlan. Confluent Schema Registry Integrate, catalog, and govern Confluent Schema Registry assets in Atlan. CrateDB Integrate, catalog, and govern CrateDB assets in Atlan. Dagster Integrate, catalog, and visualize Dagster lineage in Atlan. Databricks Integrate, catalog, and govern Databricks assets in Atlan. DataStax Enterprise Catalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data. dbt Integrate, catalog, and govern dbt assets in Atlan. Domo Integrate, catalog, and govern Domo assets in Atlan. Fivetran Integrate, catalog, and govern Fivetran assets in Atlan. Google BigQuery Integrate, catalog, and govern Google BigQuery assets in Atlan. Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan. Google Cloud Storage Integrate, catalog, and govern Google Cloud Storage assets in Atlan. Hive Catalog and govern Hive assets in Atlan for discovery and governance. IBM Cognos Analytics Integrate, catalog, and govern IBM Cognos Analytics assets in Atlan. Informatica CDI Integrate, catalog, and govern Informatica Cloud Data Integration assets in Atlan. Looker Integrate, catalog, and govern Looker assets in Atlan. Matillion Integrate, catalog, and govern Matillion assets in Atlan. Metabase Integrate, catalog, and govern Metabase assets in Atlan. Microsoft Azure Cosmos DB Catalog and govern Cosmos DB assets in Atlan for discovery and governance. Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan. Microsoft Azure Event Hubs Integrate, catalog, and govern Azure Event Hubs assets in Atlan. Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan. Microsoft Power BI Integrate, catalog, and govern Power BI assets in Atlan. Microsoft SQL Server Integrate, catalog, and govern Microsoft SQL Server assets in Atlan. MicroStrategy Integrate, catalog, and govern MicroStrategy assets in Atlan. Mode Integrate, catalog, and govern Mode assets in Atlan. MongoDB Catalog and govern MongoDB assets in Atlan for discovery and governance. Monte Carlo Integrate, catalog, and govern Monte Carlo assets in Atlan. MySQL Integrate, catalog, and govern MySQL assets in Atlan. Oracle Integrate, catalog, and govern Oracle assets in Atlan. PostgreSQL Integrate, catalog, and govern PostgreSQL assets in Atlan. PrestoSQL Integrate, catalog, and govern PrestoSQL assets in Atlan. Qlik Sense Cloud Integrate, catalog, and govern Qlik Sense Cloud assets in Atlan. Qlik Sense Enterprise (Windows) Integrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan. Redash Integrate, catalog, and govern Redash assets in Atlan. Redpanda Kafka Integrate, catalog, and govern Redpanda Kafka assets in Atlan. Salesforce Integrate, catalog, and govern Salesforce assets in Atlan. SAP ECC Integrate, catalog, and govern SAP ECC assets in Atlan. SAP HANA Catalog and govern SAP HANA assets in Atlan for discovery and governance. SAP S/4HANA Integrate, catalog, and govern SAP S/4HANA assets in Atlan. Sigma Integrate, catalog, and govern Sigma assets in Atlan. Sisense Integrate, catalog, and govern Sisense assets in Atlan. Soda Integrate, catalog, and govern Soda assets in Atlan. Tableau Integrate, catalog, and govern Tableau assets in Atlan. Teradata Catalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage. ThoughtSpot Integrate, catalog, and govern ThoughtSpot assets in Atlan. Trino Integrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-warehouse",
    "text": "4 docs tagged with \"data warehouse\" View all tags Amazon Redshift Integrate, catalog, and govern Amazon Redshift assets in Atlan. Databricks Integrate, catalog, and govern Databricks assets in Atlan. Google BigQuery Integrate, catalog, and govern Google BigQuery assets in Atlan. Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/cross-workspace-extraction",
    "text": "One doc tagged with \"cross-workspace-extraction\" View all tags Set up cross-workspace extraction Configure a single service principal to crawl metadata from all workspaces within a Databricks metastore using system tables"
  },
  {
    "url": "https://docs.atlan.com/tags/rest-api",
    "text": "25 docs tagged with \"rest-api\" View all tags API authentication Learn about api authentication. Atlan's open API Learn about atlan's open api. Connectors and capabilities Learn about connectors and capabilities. Generate HAR files and console logs Atlan is built on [REST APIs](https://apidocs.atlan.com/), so you can see the requests being sent by the UI to the API gateway through your browser's developer console. How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Set up an AWS private network link to Databricks For all details, see [Databricks documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html). Set up Qlik Sense Cloud :::warning Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps - you may not have access yourself. Set up Qlik Sense Enterprise on Windows :::warning Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps - you may not have access yourself. Set up Sigma :::warning Who can do this? You will probably need your Sigma administrator to complete these steps - you may not have access yourself. Software development kits (SDKs) Learn about software development kits (sdks). Support and Technical Help Complete guide to getting support, understanding API limits, and accessing technical assistance for Atlan. Supported sources Learn about supported sources. Troubleshooting Databricks connectivity Learn about troubleshooting databricks connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Matillion connectivity Learn about troubleshooting matillion connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting Sigma connectivity Learn about troubleshooting sigma connectivity. Troubleshooting Trino connectivity Learn about troubleshooting trino connectivity. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/graphql",
    "text": "25 docs tagged with \"graphql\" View all tags API authentication Learn about api authentication. Atlan's open API Learn about atlan's open api. Connectors and capabilities Learn about connectors and capabilities. Generate HAR files and console logs Atlan is built on [REST APIs](https://apidocs.atlan.com/), so you can see the requests being sent by the UI to the API gateway through your browser's developer console. How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets). Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. Provider package versions for OpenLineage Learn about provider package versions for openlineage. Set up an AWS private network link to Databricks For all details, see [Databricks documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html). Set up Qlik Sense Cloud :::warning Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps - you may not have access yourself. Set up Qlik Sense Enterprise on Windows :::warning Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps - you may not have access yourself. Set up Sigma :::warning Who can do this? You will probably need your Sigma administrator to complete these steps - you may not have access yourself. Software development kits (SDKs) Learn about software development kits (sdks). Support and Technical Help Complete guide to getting support, understanding API limits, and accessing technical assistance for Atlan. Supported sources Learn about supported sources. Troubleshooting Databricks connectivity Learn about troubleshooting databricks connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Matillion connectivity Learn about troubleshooting matillion connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting Sigma connectivity Learn about troubleshooting sigma connectivity. Troubleshooting Trino connectivity Learn about troubleshooting trino connectivity. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/troubleshooting/troubleshooting-usage-and-popularity-metrics",
    "text": "Use data Usage & Popularity Troubleshooting Troubleshooting usage and popularity metrics On this page Troubleshooting usage and popularity metrics Atlan currently supports usage and popularity metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Microsoft Power BI -  reports and dashboards Snowflake -  tables, views, and columns Can any user set this up? â There is no separate setup required. It is bundled with the Amazon Redshift , Databricks , Google BigQuery , Microsoft Power BI , and Snowflake miner packages. As long as the miner is set up, popularity will be calculated. Which editions of Snowflake are supported? â The following Snowflake editions are supported: Standard Edition Enterprise Edition Business Critical Edition Virtual Private Snowflake (VPS) Do account-level permissions need to be modified for setup? â No extra permissions are required. For enterprise customers, Atlan will use the ACCESS_HISTORY table to determine which objects have been accessed   -  keeping the data source as the source of truth. For other customers, Atlan will use internal logic to determine the same. How can I set up popularity on my instance? â First-time setup: Head over to the Atlan marketplace and set up the miner package for a supported connector with a daily frequency (recommended). On the first run, popularity will be calculated from the start date of the miner with a default window of 14 days. For all subsequent runs, the popularity window will be increased to a maximum limit of 30 days.Â For preconfigured miners: The next run of the miner will migrate the last 30 days worth of query data and calculate popularity for 30 days (if available). Subsequent runs will work as expected. How is the popularity score calculated? â The computation of popularity score is based on the number of read queries that used the data asset and the number of distinct users executing the read queries. Values are collected over a period of 30 days. Popularity score = number of distinct readers * log (total number of read queries) What asset types have popularity scores? â Popularity scores are currently available for all Amazon Redshift , Databricks , Google BigQuery , and Snowflake tables, views, and columns, and Microsoft Power BI reports and dashboards. How is the compute cost estimated per asset? â Amazon Redshift â Compute cost for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks â Compute cost for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery â If the Google BigQuery miner is configured to use: On-demand pricing   -  the bytes field will indicate the total bytes billed for the query. Flat-rate pricing   -  the slot-ms (slot-milliseconds) field will indicate the total slot utilization for the query. For example, if a 20-second query is continuously consuming 4 slots, then the query will have utilized 80,000 slot-ms. In Atlan, the compute cost of a Google BigQuery asset is estimated based on each individual query. For example:Â Query A ran on asset x = 10 slot-ms or bytes Query B ran on asset x = 15 slot-ms or bytes Estimated compute cost for asset x = 10 + 15 = 25 slot-ms or bytes Learn more about Google BigQuery pricing . Snowflake â In Atlan, the compute cost of a Snowflake asset is estimated based on each individual query. For example, if warehouse X-small costs 1 credit per hour:Â Query A ran from 1 p.m. to 5 p.m. on warehouse X-small = 4 credits Query B ran from 11 a.m. to 3 p.m. on warehouse X-small = 4 credits Estimated compute cost: 4 credits + 4 credits = 8 credits Can we exclude queries run by ETL users? â Atlan supports excluding queries by users. You can exclude users while configuring the miner behavior for all supported connectors   - Amazon Redshift , Databricks , Google BigQuery , Microsoft Power BI , and Snowflake . Why are metrics missing even after running the miner? â Usage and popularity metrics are computed using both the number of queries and the number of users who have queried that asset in the last 30 days. If an asset has not been queried in the last 30 days, there will be no usage and popularity metrics to report for that asset. If an asset has been queried and you've run the miner but metrics are still missing, then it may be due to the miner logic at work. When setting up the miner for the first time, you will need to provide a start date   -  ranging from the last two days up to past two weeks of query history. If an asset has not been queried during the selected time period, usage and popularity metrics will be unavailable. For subsequent runs, the miner will fetch query history based on the following logic: START_TIME â¤ CURRENT_DATE - INTERVAL '1 DAY' For example, the miner logic for January 23 will be: Jan 22 5 p.m. â¤ Jan 23 00:00 - 1 day Jan 22 5 p.m. â¤ Jan 22 00:00 The miner will not fetch the data for the previous day (January 22) on the current day (January 23). Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a query session. What type of queries do you show for popular, slow, or expensive queries? â Only read queries or SELECT statements are shown for popular, slow, and expensive queries. DDL and DML statements are not supported. Tags: lineage data-lineage impact-analysis Previous How to interpret usage metrics"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity",
    "text": "Use data Usage & Popularity On this page Usage and Popularity Overview: Usage and popularity metrics in Atlan help you understand how your data assets are being used. Track asset usage, identify popular assets, and make data-driven decisions about your data landscape. Get started â How to interpret usage metrics Guides â Usage analysis â How to find assets by usage : Discover assets based on their usage patterns. Troubleshooting â Troubleshooting usage metrics : Solutions for common usage tracking issues. Tags: usage popularity metrics capabilities Next How to find assets by usage Get started Guides Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/authentication",
    "text": "Secure Agent 2.0 Concepts Authentication On this page Authentication Secure Agent 2.0 applications use OAuth 2.0 client credentials flow for communicating with the Atlan SaaS deployment. Each deployed application uses an independent Client ID and Client Secret combination. The Client ID and Secret is used to generate short-lived tokens which are used for Secure Agent to Atlan SaaS interactions. How authentication works â The authentication process follows a continuous token refresh cycle: Application uses Client ID and Client Secret to request a JWT token from Atlan tenant's authentication service The JWT token is valid for 15 minutes and is used for all Atlan service communications Before token expiration, application requests a new token automatically and uses it for further interactions. Without a valid token, application can't communicate with the Atlan SaaS deployment What tokens enable â The authentication system provides these capabilities: Uses OAuth 2.0 client credentials flow per OAuth 2.0 Security - RFC 9700 Client credentials can be rotated on-demand Application registration with Atlan SaaS tenant Connecting to Atlan's hosted Temporal orchestrator service Transferring application outcome (for example metadata) to Atlan See also â Secret management : How OAuth 2.0 credentials are stored and retrieved from enterprise secret stores. Security : Overall security architecture including network isolation and container hardening. Tags: secure-agent authentication oauth tokens Previous Configure network security Next Data transfer and observability How authentication works What tokens enable See also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda/how-tos/create-an-aws-lambda-trigger",
    "text": "Configure Atlan Integrations Automation AWS Lambda Create an AWS Lambda trigger On this page Create an AWS Lambda trigger Once you have configured the AWS Lambda permissions , you can run an AWS Lambda function. To run a Lambda function, complete the following s teps. Select the utility â To select the AWS Lambda trigger utility: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Utility . From the list of packages, select AWS Lambda Trigger and click on Setup Workflow . Provide credentials â To enter your AWS credentials: For Authentication choose the method to authenticate with AWS: For IAM User authentication , enter the AWS Access Key , AWS Secret Key , and Region of AWS to use. For IAM Role authentication , enter theÂ Region of AWS and (optional) AWS Role ARN to use. Click Test Authentication to confirm connectivity to AWS using these details. When successful, at the bottom of the screen click Next . Configure the Lambda function â To configure the Lambda function: Under Function ARN enter the ARN for the Lambda function to call. (Optional) Under Qualifier enter a specific version of the Lambda function to call. (Or leave this as $LATEST to always run the latest version of the function.) (Optional) Under Payload enter a minimized (compact) form of any JSON payload to pass to the Lambda function. (Leave this as an empty JSON object {} if you have nothing to pass to the Lambda function.) Under Invocation Type select how you would like call the Lambda function: Use Synchronously to use synchronous invocation . With this approach, the response body and headers include details about the response, including errors. Use Asynchronously to use asynchronous invocation . With this approach, AWS queues events and they could be skipped or processed more than once. Run the Lambda function â You can now run the Lambda function. At the bottom of the screen, click Run to run the function once, immediately. Click Schedule & Run to scheduled the function to run hourly, daily, weekly, or monthly. Tags: integration authentication setup Previous Set up AWS Lambda Next Always On Select the utility Provide credentials Configure the Lambda function Run the Lambda function"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/faq/pingfederate-404-error",
    "text": "Configure Atlan Integrations Identity Management SSO Troubleshooting PingFederate SSO 404 error On this page PingFederate SSO 404 error Why do I get a 404 error when using PingFederate SSO? â If you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion. To reconfigure the signature policy : Navigate to the Signature Policy tab in PingFederate and uncheck the Always Sign Assertion box. By disabling the Always Sign Assertion setting, the authentication request will be able to proceed without requiring a signed assertion. If you encounter any further issues or have questions, refer to the PingFederate documentation or contact Atlan support for assistance. Tags: integration authentication faq-integrations Previous Troubleshooting connector-specific SSO authentication Next Okta first-time login authentication error"
  },
  {
    "url": "https://docs.atlan.com/tags/power-bi",
    "text": "One doc tagged with \"power bi\" View all tags Microsoft Power BI Integrate, catalog, and govern Power BI assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/business-intelligence",
    "text": "15 docs tagged with \"business intelligence\" View all tags Amazon QuickSight Integrate, catalog, and govern Amazon QuickSight assets in Atlan. Domo Integrate, catalog, and govern Domo assets in Atlan. IBM Cognos Analytics Integrate, catalog, and govern IBM Cognos Analytics assets in Atlan. Looker Integrate, catalog, and govern Looker assets in Atlan. Metabase Integrate, catalog, and govern Metabase assets in Atlan. Microsoft Power BI Integrate, catalog, and govern Power BI assets in Atlan. MicroStrategy Integrate, catalog, and govern MicroStrategy assets in Atlan. Mode Integrate, catalog, and govern Mode assets in Atlan. Qlik Sense Cloud Integrate, catalog, and govern Qlik Sense Cloud assets in Atlan. Qlik Sense Enterprise (Windows) Integrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan. Redash Integrate, catalog, and govern Redash assets in Atlan. Sigma Integrate, catalog, and govern Sigma assets in Atlan. Sisense Integrate, catalog, and govern Sisense assets in Atlan. Tableau Integrate, catalog, and govern Tableau assets in Atlan. ThoughtSpot Integrate, catalog, and govern ThoughtSpot assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/upstream-dependencies",
    "text": "13 docs tagged with \"upstream-dependencies\" View all tags Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Data and metadata persistence Learn about data and metadata persistence. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Lineage Generator (no transformations) Learn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting Microsoft Power BI connectivity Learn about troubleshooting microsoft power bi connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?."
  },
  {
    "url": "https://docs.atlan.com/tags/data-sources",
    "text": "13 docs tagged with \"data-sources\" View all tags Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Connectors Learn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management. Data and metadata persistence Learn about data and metadata persistence. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). How can Atlan generate upstream lineage from the data warehouse layer? Learn about how can atlan generate upstream lineage from the data warehouse layer?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting Microsoft Power BI connectivity Learn about troubleshooting microsoft power bi connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. Troubleshooting Qlik Sense Cloud connectivity Learn about troubleshooting qlik sense cloud connectivity. Troubleshooting ThoughtSpot connectivity Learn about troubleshooting thoughtspot connectivity. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?."
  },
  {
    "url": "https://docs.atlan.com/tags/model",
    "text": "13 docs tagged with \"model\" View all tags Add impact analysis in GitHub Learn about add impact analysis in github. Add impact analysis in GitLab Learn about add impact analysis in gitlab. Atlan AI security Atlan uses [Azure OpenAI Service](https://azure.microsoft.com/en-in/products/cognitive-services/openai-service) to power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model. Data Models Data models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network. Migrate from dbt to Atlan action The dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/. Set up dbt Cloud :::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself. Set up Local MCP Server The Atlan Model Context Protocol (MCP) server enables seamless interaction between external tools and Atlan services. It defines the data structures and conventions used to prepare and transmit context for model execution, making it easier to integrate Atlan into your existing workflows. Troubleshooting data models What are the known limitations of data models in Atlan? view data models Once you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:. What does Atlan crawl from Looker? Atlan crawls and maps the following assets and properties from Looker. What does Atlan crawl from Sisense? Atlan crawls and maps the following assets and properties from Sisense. What is the difference between a Power BI data source and dataflow? Learn about what is the difference between a power bi data source and dataflow?. Why is the SQL query visible only in Snowflake process and not in dbt process nodes? Atlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/releases",
    "text": "One doc tagged with \"releases\" View all tags Product release stages Learn about product release stages."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-integrations",
    "text": "17 docs tagged with \"faq-integrations\" View all tags Can Atlan integrate with multiple Azure AD tenants within a single instance? Learn about can atlan integrate with multiple azure ad tenants within a single instance?. Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Refer to our [troubleshooting Jira documentation](/product/integrations/project-management/jira/troubleshooting/troubleshooting-jira) to learn more. Can site renaming affect the Jira integration? Learn about can site renaming affect the jira integration?. Can we use a Microsoft SSO login? Learn about can we use a microsoft sso login?. Google Dashboard login error Learn about why do i get an error while logging in via google dashboard?. How are product updates deployed? Learn about how are product updates deployed?. How do I send messages or search assets from Slack? Sending messages and searching assets from Slack are disabled. Refer to [Troubleshooting Slack](/product/integrations/collaboration/slack/troubleshooting/troubleshooting-slack) to learn more. Okta first-time login authentication error Learn about why do i get an authentication error when logging in via okta for the first time?. PingFederate SSO 404 error If you're encountering a 404 error while using PingFederate single sign-on (SSO), it is possible that the authentication request requires a signed assertion. What does Atlan do with each Slack permission? Learn about what does atlan do with each slack permission?. What is included in the Jira integration? With two of your most important workspaces connected, you can save time and improve the way you track issues for your data. What is included in the Microsoft Teams integration? With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team. What is included in the Slack integration? Learn about the features and capabilities of the Slack integration with Atlan. What is the difference between Copy Link and Share on Slack or Teams? Learn about what is the difference between copy link and share on slack or teams?. What type of user provisioning does Atlan support for SSO integrations? Atlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:. When does Atlan become a personal data processor or subprocessor? Atlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively. Why did my users not receive an invite email from Atlan? If you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid",
    "text": "Connect data Privacy & Security BigID On this page BigID Overview: Crawl BigID for a specified set of data sources and import classifications and policy-violation metadata for assets in Atlan in the form of tags, custom metadata and announcements. Get started â Follow these steps to connect and catalog BigID metadata in Atlan: Set up the connector Crawl BigID metadata References â What does Atlan crawl from BigID : Learn about the BigID metadata that Atlan discovers and catalogs. Tags: bigid connector privacy security connectivity Next Set up BigID Get started References"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/how-tos/configure-network-security",
    "text": "Secure Agent 2.0 Manage agent Configure network security On this page Configure network security Configure network security for Secure Agent 2.0 to permit only required encrypted traffic between the agent and Atlan services. This ensures secure communication while blocking unauthorized access. Prerequisites â Before you begin, make sure you have: Administrative access to your firewall or network security groups Configure firewall rules â Access your firewall management interface : Log into your firewall management system (AWS Security Groups, Azure NSGs, enterprise firewall console, or iptables for Linux). Permit outbound connections : Configure your firewall to permit the following outbound connections from your Secure Agent deployment: firewall_rules : outbound_allowed : - destination : \"*.atlan.com\" port : 443 protocol : HTTPS - destination : \"*.atlan.com\" port : 443 protocol : gRPC/TLS Block all inbound traffic : Configure your firewall to deny all inbound connections to the Secure Agent: firewall_rules : inbound_blocked : - all_traffic : DENY   No inbound connections to agent Need help â If you are still facing issues and need help, contact [email protected] for assistance. See also â Security : Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0. Verify container images : Confirm image authenticity and integrity with Cosign before deployment. Tags: secure-agent network security firewall Previous Verify container images Next Authentication Prerequisites Configure firewall rules Need help See also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/best-practices/customer-environment-security",
    "text": "Secure Agent 2.0 Best practices Customer environment security On this page Customer environment security This document outlines customer environment security best practices and minimum security baselines for deploying and operating Secure Agent 2.0 in your environment. Security assessment process â Before deployment, customers must complete a security assessment that includes: Network architecture review : Validate network security controls and configurations Credential management validation : Verify secret management practices and policies This assessment ensures your environment meets the minimum security baselines required for secure deployment. Infrastructure security â Your infrastructure must meet these minimum security baselines: Container runtime security : Kubernetes 1.24+ or Docker 20.10+ with security features enabled Network segmentation : Agent isolated from production systems Endpoint protection : Anti-malware and endpoint detection solutions deployed Vulnerability management : Regular patching and vulnerability scanning implemented Network security controls â Secure Agent 2.0 uses outbound-only communication and never accepts inbound connections. Your network must implement these controls: Outbound-only communication : Agent never accepts inbound connections TLS 1.2 minimum : All external communication uses TLS 1.2 or higher Firewall requirements : Customer firewall rules for agent communication Required network configuration â firewall_rules : outbound_allowed : - destination : \"*.atlan.com\" port : 443 protocol : HTTPS - destination : \"*.atlan.com\" port : 443 protocol : gRPC/TLS inbound_blocked : - all_traffic : DENY   No inbound connections to agent For detailed configuration steps, see Configure network security . Identity and access management â Apply the principle of least privilege with these requirements: Principle of least privilege : Agent credentials limited to required data sources only Regular credential rotation : Scheduled rotation of agent access credentials Audit logging : Complete audit trail for agent-related activities Ongoing security requirements â Maintain security with these ongoing requirements: Vulnerability management : Prompt application of security patches Incident response coordination : Participation in security incident response Need help â If you need help, contact [email protected] for assistance. Tags: secure-agent security deployment Previous Deployment and security Security assessment process Infrastructure security Network security controls Identity and access management Ongoing security requirements Need help"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/faq/deployment-and-security-faq",
    "text": "Secure Agent 2.0 FAQ Deployment and security On this page Deployment and security Find answers to common questions about supported deployment platforms, available methods, and security incident handling for Secure Agent 2.0. What platforms are supported for running OCI images? â Secure Agent 2.0 images run on any OCI-compatible runtime, including Podman, Docker, ECS, and Kubernetes. Docker Compose and Helm charts are provided for easier deployment. How does Secure Agent 2.0 handle security incidents? â If you suspect a security incident, contact [email protected] . The Atlan security team coordinates the response and investigation process. How do I raise a security incident response? â If you suspect a security incident, follow this process: Immediate isolation : Stop agent processing immediately Preserve evidence : Capture logs and system state Contact security : Email [email protected] with incident details Document timeline : Record all incident-related activities and timestamps Coordinate response : Work with Atlan security team for investigation See also â Customer environment security best practices : Security baselines and requirements for deploying Secure Agent 2.0 in your environment. Security : Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0. Tags: secure-agent faq deployment troubleshooting security Previous Secret management Next Customer environment security See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/manage-user-authentication",
    "text": "Configure Atlan Access control Manage users and groups Manage user authentication On this page Manage user authentication Who can do this? You will need to be an admin user in Atlan to configure user authentication settings. When users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication. Atlan currently only supports configuring session settings up to a maximum value of 30 days. You must also enter a minimum value of 1 minute. You can configure the following parameters: Session idle timeout -  the total length of time that a session is allowed to be idle before it expires. Session max timeout -  the maximum amount of time before a session expires. Remember me session idle timeout -  the total length of time that a Remember Me session is allowed to be idle before it expires. If you choose not to set this parameter, Atlan will use the standard SSO session idle value, 7 days. Remember me session max timeout -  the maximum amount of time before a session expires if a user has enabled the Remember Me session option. If you choose not to set this parameter, Atlan will use the standard SSO session max value, 4,745 days. Note that tokens and browser sessions become invalid when a session expires. Configure session settings â To configure user authentication settings in Atlan: From the left menu in Atlan , click Admin . Under Workspace , click Authentication . Under Authentication , to the right of Session settings , click the Edit button. Atlan currently only supports setting a maximum value of up to 30 days. In the Change session settings dialog, you can configure the following: Set a numeric value for the following fields (You must enter a minimum value of 1 minute.): Session idle timeout Session max timeout _  - _ you must enter a value same as or higher than Session idle timeout . Remember me session idle timeout Remember me session max timeout _  - _ you must enter a value same as or higher than Remember me session idle timeout . Click the Days dropdown to specify the validity period. You can define parameters in the form of Minutes , Hours , or Days . Click Save changes to save your configuration. Tags: security access-control permissions Previous Add users to groups Next Delegate administration Configure session settings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/troubleshooting/microsoft-defender-sso-error",
    "text": "Configure Atlan Integrations Identity Management SSO Troubleshooting Microsoft Defender SSO error Microsoft Defender SSO error If you are getting an error message from Microsoft Defender that the Atlan login page cannot be loaded, it's possible that the wrapping URL prefix   -  for example, _ https://nam02.safelinks.protection.outlook.com/ - _ is causing the error. To get around this, you should add a Safe Links policy to whitelist Atlan's invitation emails and URLs. Otherwise, the Safe Links function will need to be turned off. Tags: security access-control permissions Previous Google Dashboard login error Next Can Atlan integrate with multiple Azure AD tenants within a single instance?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/faq/roles-and-permissions",
    "text": "Build governance Data Quality Studio Snowflake Data Quality FAQ Roles and permissions On this page Roles and permissions This document answers common questions about the roles Atlan creates or requires in Snowflake, why elevated permissions such as dq_admin are necessary, and how Snowflakeâs built-in controls keep your data safe. Why does the dq_admin role need table owner privileges? â Snowflake's security model restricts data metric function management to table owners only. According to Snowflake's documentation , only the role that owns a table can schedule and manage data metric functions on that table. To support data quality operations across your tables, the dq_admin role must be granted access to the table owner roles. This permission lets it manage data metric functions on your behalf. How does Atlan access these elevated privileges? â Atlan maintains security through a controlled access pattern: Atlan never receives the dq_admin role or table ownership directly All operations execute through the MANAGE_DMF stored procedure This procedure runs with dq_admin privileges but only exposes specific, predefined data quality operations Every operation remains within Snowflake's secure execution context Tags: snowflake data-quality security permissions faq Previous Operations Why does the dq_admin role need table owner privileges? How does Atlan access these elevated privileges?"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/references/security",
    "text": "Secure Agent 2.0 Architecture & Security Security On this page Security Secure Agent follows a zero-trust model where every component, connection, and operation is verified, minimized, and observable. This model starts with hardened containers, extends through verified supply chains, and encompasses all network communications and secret management. Critically, agents run entirely within your environment where you control the encryption keys and security policies. Each layer of security builds upon the previous, creating defense in depth. Container security â The foundation of Secure Agent's security begins with how containers are built and run. Every container starts from a minimal base image and runs with strict limitations that prevent exploitation even if compromised. Hardened execution environment â All Secure Agent containers run with a strict security context to reduce attack surface: securityContext : runAsNonRoot : true   Never run as root user runAsUser : 1000   Dedicated non-privileged user allowPrivilegeEscalation : false capabilities : drop : [ \"ALL\" ] readOnlyRootFilesystem : true This configuration creates the first security boundaryâeven if an attacker compromises the application code, they can't escalate privileges, modify the filesystem, or access system capabilities. The use of distroless base images further limits what's available to exploit, containing only the application and its runtime dependencies. Runtime protections â Building on the hardened base, additional runtime controls keep containers within strict boundaries: No shell access in containers eliminates common attack vectors Minimal base images (distroless) reduce the attack surface to essential components only Resource limits prevent any single container from exhausting system resources Network policies restrict each container to communicate only with required endpoints These protections work togetherâa compromised container can't spawn shells, consume unlimited resources, or communicate with unauthorized systems. Supply chain integrity â Before any container reaches your environment, it passes through multiple verification stages that verify both security and authenticity. Continuous vulnerability scanning â The security of container images is continuously validated through automated scanning: All images are scanned with Snyk and Trivy on every change and at regular intervals. These scans detect: Known vulnerabilities (CVEs) and outdated dependencies that may be exploited Security misconfigurations that deviate from best practices Accidentally embedded secrets that may provide unauthorized access License compliance to meet legal requirements Only images free of Critical and High severity vulnerabilities are released, establishing a baseline security standard before deployment. Image repository options â Container images required by the Secure Agent are hosted on Atlan using the Harbor open source registry. Harbor provides enterprise-grade security features including vulnerability scanning, image signing verification, and access control. If required, these images can be mirrored or pulled to your private container registry to meet organizational compliance and security requirements. This flexibility ensures the Secure Agent can operate within your existing container management and security infrastructure. Image signing and verification â Beyond scanning, every container is cryptographically signed to verify authenticity: All containers are signed with Cosign, creating entries in Sigstore's transparency log. This immutable record lets you verify that images haven't been tampered with since they left Atlan's build pipeline. The signing process uses keyless signing via GitHub Actions, ensuring traceability and security without requiring long-lived private keys. Verification capabilities: Image signature validation with Sigstore's transparency log GitHub workflow identity verification Certificate chain validation through GitHub's OpenID Connect (OIDC) provider For complete verification steps and examples, see Verify container images guide. Network security â With containers secured and verified, the next layer protects how they communicate. Secure Agent's network model ensures that even if other controls fail, unauthorized network access remains impossible. Outbound-only communication â Secure Agent operates with an outbound-only communication modelâit initiates all connections and never accepts inbound connections. This fundamental design choice eliminates entire categories of attacks: No exposed ports that can be scanned or exploited No services that can be overwhelmed with requests No authentication challenges from external sources All traffic to Atlan uses TLS 1.2 or higher, which means the content remains protected even if network traffic is intercepted. Firewall and routing controls â The outbound-only model is enforced through firewall rules that explicitly block all inbound traffic while permitting only specific outbound connections: Use firewall enforcement to permit authorized outbound traffic to Atlan endpoints and block all inbound traffic to the agent. For configuration steps, see Configure network security . Recommended network configuration (example) firewall_rules : outbound_allowed : - destination : \"*.atlan.com\" port : 443 protocol : HTTPS - destination : \"*.atlan.com\" port : 443 protocol : gRPC/TLS inbound_blocked : - all_traffic : DENY Advanced networking practices â For environments requiring additional security, these practices layer on top of the basic network controls: Egress-only endpoints restrict outbound connections to an approved list of external endpoints Private routing through VPC/VNet peering or private endpoints keeps traffic within cloud provider networks Proxy support routes all outbound connections through enterprise proxies for inspection Zero-trust implementation limits communications to required services using IP allowlists or service-specific endpoints Rate limiting on egress prevents data exfiltration if other controls are bypassed TLS certificate pinning prevents man-in-the-middle attacks even with compromised CAs Segregated workloads isolate agent instances handling different sensitivity levels Network monitoring â Continuous monitoring provides visibility into all network activity: Network logs capture all traffic for anomaly detection and audit trails Connection health checks continuously verify that outbound connections to Atlan endpoints remain secure and functional This monitoring feeds into the broader observability system, creating a complete picture of agent behavior. Authentication and secret management â With secure containers communicating over protected networks, the next critical layer manages how the agent authenticates and handles secrets. This system ensures credentials never leave your control. Authentication model â Secure Agent uses OAuth 2.0 client credentials flow for all authentication to Atlan services. Each deployed application receives its own unique Client ID and Client Secret, ensuring credential isolation between applications. This per-application credential model provides critical security benefits: if one application's credentials are compromised, the impact is limited to that specific application rather than affecting all Secure Agent deployments. This follows OAuth 2.0 best practices as defined in RFC 9700 . Key security features: Isolated credentials : Each application has unique credentials Rotation support : Client credentials can be rotated on-demand Secure storage : Credentials retrieved from your secret manager, never hardcoded Audit trail : All authentication events logged for security monitoring This authentication integrates with your existing identity infrastructure, enabling you to apply your standard rotation and governance policies. Secret management â Rather than storing any credentials itself, Secure Agent integrates with enterprise secret managers through Dapr's secret store abstraction: Just-in-time retrieval means secrets are fetched only when needed for a specific operation Memory-only storage ensures secrets exist only in application memory during use No transmission to Atlan keeps all credentials within your security perimeter Comprehensive audit logging tracks every secret access for compliance Multi-vault support works with AWS Secrets Manager, Azure Key Vault, HashiCorp Vault, and others This design ensures that even if an agent container is compromised, no credentials can be extracted from disk or logs. All secret access follows your existing security policies and appears in your vault's audit logs. Logging, monitoring, and incident response â All these security controls generate observable events, creating a comprehensive audit trail and enabling rapid incident response. Observability â Every security-relevant event is captured and made available through multiple channels: Structured logs capture all operations with correlation IDs for tracing Metrics expose performance and security indicators through Prometheus format Health endpoints via the FastAPI server provide real-time status Trace data shows the complete flow of operations across components These observability streams integrate with your existing monitoring infrastructure, whether that's Splunk, Datadog, CloudWatch, or other platforms. Incident response â When security events occur, the comprehensive logging enables rapid response: Emergency contact : [email protected] provides 24/7 security team access Log preservation automatically retains all incident-related logs Coordinated notification ensures customer teams are informed of any impacts Automated responses can revoke credentials or terminate connections based on detected anomalies The incident response process leverages all the security controlsânetwork isolation limits blast radius, credential rotation stops unauthorized access, and comprehensive logs enable thorough investigation. Security alerts â Customers receive automatic notifications for: Critical/High vulnerabilities discovered in deployed images Security patches available for immediate deployment Unusual security events detected in agent behavior Compliance violations or configuration drift These proactive alerts enable rapid remediation before issues can be exploited. Compliance and audit â All these security controls align with enterprise compliance requirements and provide the evidence needed for audits. Regulatory alignment â The security architecture supports compliance with major frameworks: SOC 2 Type II validates security controls and their operating effectiveness ISO 27001 aligns with information security management system requirements GDPR ensures appropriate data protection and privacy controls HIPAA provides necessary safeguards for healthcare data (when applicable) PCI DSS meets requirements for payment card data security (when applicable) Each control maps to specific requirements within these frameworks, simplifying compliance documentation. Audit capabilities â The layered security model generates comprehensive audit evidence: Immutable audit logs with cryptographic timestamps prove when events occurred Log integration with SIEM and centralized logging solutions enables enterprise-wide visibility Configurable retention from 30â365 days aligns with your policy requirements Chain of custody tracks data from extraction through delivery to Atlan This audit trail spans all security layersâfrom container startup through network connections, authentication events, secret access, and data operations. See also â Secret management : How credentials are protected and never leave your environment. Customer environment security best practices : Security baselines and requirements for deploying Secure Agent 2.0 in your environment. Architecture : Component interactions and how security controls map to the system design Tags: secure-agent security Previous Architecture Next Verify container images Container security Supply chain integrity Network security Authentication and secret management Logging, monitoring, and incident response Compliance and audit See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity",
    "text": "Connect data CRM Salesforce Troubleshooting Troubleshooting Salesforce connectivity On this page Troubleshooting Salesforce connectivity Why does Atlan require an admin user in Salesforce? â Atlan recommends a Salesforce administrator for setting up a connection between Atlan and Salesforce . Once connected, Atlan can extract all Salesforce objects   -  including corresponding fields, folders and child folders, along with dashboards and reports   -  without having to enable object-level permissions and field-level security (FLS) for each object addition in the profile or permission sets. Although it's possible to enable these permissions for non-admin users, only admins have the special permissions to oversee all newly added custom objects. This is regardless of which non-admin users created those custom objects or the permissions that were imposed on them. Why is Atlan unable to crawl some system-generated objects in Salesforce? â Atlan operates in read-only mode and doesn't make any API calls that modify your Salesforce instance. With the limited permissions typically granted during setup, Atlan may not be able to crawl certain system-generated objects, such as: ContentDocumentSubscription ContentNotification ContentTagSubscription ContentUserSubscription ContentVersionComment ContentVersionRating ContentWorkspaceSubscription CorsWhitelistEntry EmailCapture FeedPollChoice FeedPollVote OrgDeleteRequest PlatformStatusAlertEvent PromptError PromptAction SetupAssistantStep TopicUserEvent Does Atlan collect formula fields from Salesforce? â Yes, Atlan collects formula fields from Salesforce. However, if the formula fields are brought into your data warehouse via Fivetran,Â they aren't reflected in Atlan as assets from your data warehouse. This is because Fivetran does not sync formula fields from Salesforce. Why do I get an \"sObject type 'Organization' isn't supported\" error message? â To pass the preflight check for organization count , make sure that you've added the Modify All Data Â permission while setting up Salesforce . This object permission enables the user to access all shared and public folders, regardless of sharing settings. Tags: dashboards visualization analytics security access-control permissions Previous Preflight checks for Salesforce Next Does Atlan require an admin user in Salesforce?"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/how-tos/verify-container-images",
    "text": "Secure Agent 2.0 Manage agent Verify container images On this page Verify container images Container image verification prevents malicious or modified images from entering your environment. Before you deploy a Secure Agent container, verify its signature so you can be confident the image was built by Atlanâs official CI/CD pipeline, hasn't been modified since signing, and is traceable to a specific GitHub workflow. This guide provides step-by-step instructions to verify images using Cosign . Prerequisites â Cosign is installed on your system. If not, follow the official Cosign installation guide . You can access the container image you want to verify. You know the image name and tag (for example: public.ecr.aws/atlanhq/redshift-app:2.0 ). Verify container image â Run the verification command : Run the following command, replacing <image-name> and <full-image-path> with your values. For example, public.ecr.aws/atlanhq/redshift-app:2.0 : COSIGN_EXPERIMENTAL=1 cosign verify \\ --allow-insecure-registry \\ --certificate-identity=\"https://github.com/atlanhq/<image-name>/.github/workflows/test-image-sign.yaml@refs/heads/image-signing-test\" \\ --certificate-oidc-issuer=\"https://token.actions.githubusercontent.com\" \\ <full-image-path> Example: verify Redshift connector image COSIGN_EXPERIMENTAL=1 cosign verify \\ --allow-insecure-registry \\ --certificate-identity=\"https://github.com/atlanhq/connector-auth/.github/workflows/test-image-sign.yaml@refs/heads/image-signing-test\" \\ --certificate-oidc-issuer=\"https://token.actions.githubusercontent.com\" \\ public.ecr.aws/atlanhq/redshift-app:2.0 Interpret the results : Verify the results produced by the command: If the verification is successful , Cosign returns a verified signature along with signer details. Example: verification successful Verification successful! - Image Digest (SHA256): abcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890 - Signed by: atlanhq/<image-name>/.github/workflows/image-sign.yaml - OIDC Issuer: https://token.actions.githubusercontent.com If the verification fails , it means the image is either unsigned or has been modified. Don't proceed with deployment until you obtain a valid signed image. Example: verification failed Verification failed! - Error: Signature verification failed - Reason: The signature doesn't match the expected digest. - Suggested Action: Check the image signature and ensure it was signed correctly. Troubleshooting â If verification fails: Make sure you are using the correct image path and tag . Verify the certificate identity (the --certificate-identity value) matches the repository/workflow that signed the image. Confirm network connectivity to Sigstore (Cosign uses transparency and registry services). Need help â If you are still facing issues and need help, contact [email protected] for assistance. See also â Security : Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0. Configure network security : Set firewall rules, proxies, and Kubernetes policies to control agent traffic. Tags: secure-agent security container-images Previous Security Next Configure network security Prerequisites Verify container image Troubleshooting Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/auto-assign-roles-by-group",
    "text": "Configure Atlan Access control Manage users and groups Automatically assign roles On this page Automatically assign roles based on group names App You can automatically assign roles in Atlan based on user group memberships. This helps streamline onboarding, enforce consistent access control, and reduce manual effort by mapping groups to roles such as Admin, Member, or Guest. Prerequisites â Before you begin, make sure you have: Access to the User Role Sync app. If you don't have access, contact Atlan support or your Atlan customer team to request it. Admin permissions in Atlan to configure workflows and assign roles. Learn more about admin roles . A personal API token generated from your Atlan profile for workflow authentication. All required user groups created and available in Atlan, as described in Create groups . Setup workflow â In your Atlan workspace, go to the homepage and click New workflow in the top navigation bar. Search for User Role Sync , and then select Set up workflow . In the Workflow name field, enter a descriptive name that clearly identifies the purpose of this workflow. In the Authentication section, provide your API token . This token is required for the workflow to authenticate with Atlan and perform user role updates. Define role mapping rules â After authentication is configured and tested, set up how group memberships correspond to specific roles in Atlan. This configuration also determines the order of priority when a user belongs to multiple groups and can optionally include sub-role assignments for more granular control. In the Selection mode, select List to define a fixed set of Atlan group names that are explicitly mapped to roles, such as assigning the admin role to data-admins and data-leads (both part of the existing Atlan groups in your workspace). If you need to match multiple groups that follow a naming pattern, such as all groups ending in -admins , use Regex mode instead. For more details, see Selection mode . In the Role hierarchy , choose how Atlan resolves conflicts when a user belongs to multiple groups mapped to different roles. You can select the default Guest â Member â Admin , which prioritizes the most restrictive role. If a user belongs to both the guests and data-admins groups, the guest role is assigned. To view other hierarchy options, see Role hierarchy options . In the Admin group field, enter a comma-separated list of group names whose members are assigned the admin role. Use names that are identical to the display names of the groups in Atlan. data-admins,data-leads To match group names using patterns, see Regex matching . To assign more granular responsibilities under the admin role, select Static under the Admin Sub Role Option. This assigns fixed sub-roles such as workflow-admin and governance-admin to specific groups and sets their order of precedence. In the Workflow Admin (sub-role) Group field, enter the group names whose members receive the workflow-admin sub-role. Use comma-separated values, such as engineering-ops-admins . In the Governance Admin (sub-role) Group field, enter the group names whose members receive the governance-admin sub-role. Use comma-separated values, such as data-governance-leads . In the Admin Sub Role hierarchy field, define the order of precedence between sub-roles . If a user qualifies for multiple sub-roles, the one listed first is assigned. workflow-admin,governance-admin If you need to define flexible, custom admin sub-roles instead of fixed ones, see Configure dynamic sub-roles . In the Member group field, add group names to assign the member role for users who work with metadata, glossaries, or queries but donât need admin access. In the Guest group field, enter group names for users who only need limited or read-only access to Atlan, such as viewers or contractors . Schedule and run the workflow. Run the workflow manually or set a recurring schedule to keep role assignments up to date. Need help? â If you have any issues related to configuring the app, contact Atlan support . See also â User Role Sync: Reference : Detailed explanation of each configuration property, including valid values, examples, and behavior. Tags: access control roles user groups automation app governance Previous Delegate administration Next What are personas? Prerequisites Setup workflow Define role mapping rules Need help? See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/requests",
    "text": "Use data Requests On this page Requests Overview: Use Atlan's requests capabilities to suggest changes to assets that you cannot directly modify. Enable collaboration and governance by allowing users to request updates while maintaining proper access control. Get started â How to manage requests Guides â How to manage requests : Handle incoming change requests and manage the request workflow. Concepts â What are requests : Learn about requests and their role in governance. Policy-based permissions : Understand how policies affect request capabilities. Overridden permissions : Learn about permission overrides. Tags: requests governance access-control capabilities Next Manage requests Get started Guides Concepts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/user-role",
    "text": "Configure Atlan Access control References User Role Sync On this page User role sync App The User Role Sync app enables automated role and sub-role assignment in Atlan by mapping user groups to roles like Admin, Member, or Guest. It supports both exact and pattern-based matching and helps scale access control by reducing manual configuration. This reference provides complete configuration details for the User Role Sync app. Use this page to look up specific property definitions, valid values, and configuration formats when setting up automated role assignments based on group memberships. Access â The User Role Sync app isn't enabled by default. To use this app, contact Atlan support and request it be added to your tenant. Credentials â This section defines the fields required for workflow authentication and identification. Workflow name â Specifies a unique and descriptive name to identify the workflow configuration in the Atlan interface. This name appears in the workflow list and helps distinguish it from other workflows, especially in environments with multiple automated role management setups. Example: atlan - prod - user - role - sync Atlan API token â Specifies the API token used to authenticate this workflow with your Atlan tenant. Generate a personal API token from your user profile in Atlan. The token must belong to an admin user with permission to view groups and manage user roles. You can click Test authentication in the workflow UI to validate the token before proceeding. Selection mode â Determines how group names are matched to roles during role assignment. Use this setting to choose whether to provide exact group names or define a pattern that matches multiple group names. The value selected here affects how group inputs are interpreted for all role and sub-role mappings in the workflow. List : Use this option when you have a known set of group names. Each group must be entered explicitly. Example: To assign the admin role to the data-admins and analytics-leads groups, enter: data-admins,analytics-leads Regex : Use this option when group names follow a naming pattern. All groups matching the provided regular expression are included. Example: To assign the member role to all groups that start with team- , use: ^ team - . * Example: To target groups ending in -admins (for example, data-admins , cloud-admins , security-admins ), use: . * - admins$ Example: To include only groups with exactly three characters followed by -ops (for example, eng-ops , dev-ops ), use: ^ [ a - z ] { 3 } - ops$ Role hierarchy â Controls how role precedence is resolved when a user belongs to multiple groups that are mapped to different roles. The selected hierarchy determines which role is ultimately assigned based on predefined priority logic. This setting is especially useful in large organizations where users may be part of multiple functional teams with overlapping access needs. Available options: Guest â Member â Admin (default) Assigns the most restrictive role in cases of conflict. Recommended when security and access limitation are the primary concerns. Example : A user belongs to both guests (mapped to guest role) and data-admins (mapped to admin role). The assigned role is guest . Guest â Admin â Member Prioritizes the guest role first, followed by admin , then member . This ensures that users in temporary, external, or read-only groups don't receive unintended contributor access. Example : A user belongs to guests (mapped to guest role), data-admins (mapped to admin role), and reporting-team (mapped to member role). The assigned role is guest . Member â Guest â Admin Assigns the member role over others. Use this when contributor-level access must take priority, even if the user is also in limited-access or admin groups. Example : A user belongs to team-product (mapped to member role) and guests (mapped to guest role). The assigned role is member . Member â Admin â Guest Prefers the member role first, then admin , followed by guest . This is useful when users who actively work with data must always retain contributor access, even if they're admins or in guest groups. Example : A user belongs to finance-team (mapped to member role), data-admins (mapped to admin role), and contractors (mapped to guest role). The assigned role is member . Admin â Guest â Member Assigns the admin role when present. Suitable for teams where users with admin responsibilities must retain their elevated access regardless of membership in guest or contributor groups. Example : A user belongs to data-admins (mapped to admin role), guests (mapped to guest role), and team-marketing (mapped to member role). The assigned role is admin . Admin â Member â Guest The most permissive hierarchy. Always assigns the admin role if available. Use this when users who have administrative responsibilities must never be downgraded, even if they belong to other groups. Example : A user belongs to data-admins (mapped to admin role), team-analytics (mapped to member role), and viewers (mapped to guest role). The assigned role is admin . Admin â This section defines how users are assigned the admin role in Atlan. The admin role is intended for users who require elevated privileges to manage system configurations, workflows, users, and platform-level settings. Admin group â Specifies the groups whose members are assigned the admin role in Atlan. The values for this field depend on the selection made in Selection mode . If List mode is selected: Provide exact, comma-separated names of Atlan groups. Example : data-admins,analytics-leads If Regex mode is selected: Provide a valid regular expression to match group names dynamically. Example : .*admins Users who belong to any of the specified groups are assigned the admin role, subject to the logic defined in the Role hierarchy setting. Admin sub-role option â Determines how sub-roles under the admin role are assigned to users. This property supports two modes: Static :  Choose this when sub-roles and their corresponding groups are predefined and don't change often. You can configure fixed sub-roles such as workflow-admin or governance-admin and assign them to specific groups. Use the Admin sub-role hierarchy field to define precedence. Dynamic : Use this mode when sub-roles are more varied or need to be configured on a per-workflow basis. You can define up to five sub-roles and map each one to one or more groups. The order in which the sub-roles are configured determines their precedence (first = highest). Choose the mode that best aligns with your organization's administrative structure. Workflow admin (sub-role) group â Defines the Atlan groups whose members can be assigned the workflow-admin sub-role. This field is available only when Static is selected under the Admin Sub Role option. The expected input depends on the configured Selection mode : If List mode is selected: Enter one or more exact group names, separated by commas. Example: engineering-admins,project-ops-leads If Regex mode is selected: Provide a valid regular expression pattern to match group names dynamically. Example: .*workflow-admins Governance admin (sub role) group â Specifies the groups whose members can be assigned the governance-admin sub-role. This field appears only when Static is selected for the Admin sub-role option. If List mode is selected: Enter one or more exact group names, separated by commas. Example: data-governance-leads,compliance-admins If Regex mode is selected: Enter a regular expression to dynamically match group names. Example: governance-.* Admin sub role hierarchy â Sets the precedence between admin sub-roles when users are mapped to multiple sub-role groups. This field is displayed only when Static is selected for the Admin sub-role option. List the sub-role names in the desired order of priority. The first sub-role listed is considered the highest in precedence. Admin sub-role 1-5 â Available only when Dynamic is selected for the Admin sub-role option. These fields enable you to define up to five custom admin sub-roles and map them to corresponding Atlan user groups. Each sub-role entry contains: Sub-role name : A unique label to identify the custom admin sub-role (for example, data-platform-admin ). Groups : One or more group names to which this sub-role are applied. Sub-role precedence is determined by the order in which the sub-roles are configured. The first sub-role listed is considered the highest in priority. Examples: Sub-role 1: Sub-role name: data-governance-lead Groups : governance-team , data-leads Sub-role 2: Sub-role name: platform-admin Groups : infra-core , platform-eng Member â This section defines how users are assigned the member role in Atlan. The member role is intended for users who contribute to data work, such as exploring metadata, managing glossaries, or running queries, without requiring administrative privileges. Member group â Defines the group or groups whose members can be assigned the member role in Atlan. Depending on the Selection mode , group names can be matched directly or via pattern. In List mode: Enter exact, comma-separated group names. Example: data-analysts,reporting-team In Regex mode: Enter a valid regular expression to match one or more group names dynamically. Example: .*-members If a user belongs to multiple groups with different role assignments, the final role is determined based on the Role hierarchy setting. Guest â This section defines how users are assigned the guest role in Atlan. The guest role is suited for individuals who require read-only access or limited interaction with metadata, such as external users, contractors, or stakeholders. Guest role â Specifies the group or groups whose members can be assigned the guest role in Atlan. The input format depends on the Selection mode : In List mode: Provide exact, comma-separated group names. Example: viewers,contractors In Regex mode: Provide a valid regular expression to dynamically match group names. Example: .*-guests If users belong to multiple groups, the final assigned role depends on the configuration in the Role hierarchy property. See also â Automatically assign roles based on group names Create groups Tags: access control roles user groups automation app governance reference Previous What are the sidebar tabs? Access Credentials Admin Member Guest See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/references/data-quality-permissions",
    "text": "Build governance Data Quality Studio What is Data Quality Studio References Data quality permissions On this page Data quality permissions To grant users in your organization access to set up data quality rules, you must assign the necessary permissions via metadata policies in personas. Permission scopes â Scope Description Create Rule Grants users permission to create new rules on tables and add or update schedules Update Rule Grants users permission to modify existing rules Delete Rule Grants users permission to delete rules Read Rule Grants users permission to view the run values for a rule. Without this permission, users can still see the rule's run status (whether it passed or failed) and other metadata, such as the test's expected outcome, but the actual values remain hidden Configuration â Assign permissions via metadata policies in personas using the Data Quality (DQ) scopes. See also â Set up Databricks - Configure Databricks for data quality monitoring Set up Snowflake - Configure Snowflake for data quality monitoring Tags: snowflake data-quality permissions scopes reference Previous What's Data Quality Studio Next Rules and dimensions Permission scopes Configuration See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/how-tos/set-up-sap-ecc",
    "text": "Connect data ERP SAP ECC Get Started Set up SAP ECC On this page Set up SAP ECC This guide explains how to create a dedicated service user in SAP ECC and grant the necessary permissions for Atlan to extract metadata. Prerequisites â Before you begin, make sure you have: Administrative access to SAP ECC. SAP system details, including: Host System number Client number Create communication user for metadata extraction â In the SAP GUI command field, enter SU01 and press Enter to open User Maintenance . In the User field, enter a name for the new service user and click Create . On the Address tab, provide the required contact information. Open the Logon Data tab and: Set an initial password (enter it twice). Set User Type to C (Communications Data) . Switch to the Roles tab and assign roles that enable: Remote function call (RFC) execution Table-level read access for metadata tables Access to system-level information Verify that the assigned roles enable execution of these RFC modules: STFC_CONNECTION : Verifies connectivity between Atlan and SAP ECC. RFC_SYSTEM_INFO : Retrieves system metadata such as SYSID, operating system, and release version. RFC_READ_TABLE : Enables table-level reads for metadata extraction. Click Save to confirm the changes. note The user must change the password on first login. Next steps â Crawl SAP ECC : Follow the instructions to extract metadata from SAP ECC using the configured service user. Tags: erp setup permissions sap-ecc Previous SAP ECC Next Crawl SAP ECC Prerequisites Create communication user for metadata extraction Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/how-tos/set-up-sap-s4hana",
    "text": "Connect data ERP SAP S/4HANA Get Started Set up SAP S/4HANA On this page Set up SAP S/4HANA This guide explains how to create a dedicated service user in SAP S/4HANA and grant the necessary permissions for Atlan to extract metadata. Prerequisites â Before you begin, make sure you have: Administrative access to SAP S/4HANA. SAP system details, including: Host System number Client number Create communication user for metadata extraction â In the SAP GUI command field, enter SU01 and press Enter to open User Maintenance . In the User field, enter a name for the new service user and click Create . On the Address tab, provide the required contact information. Open the Logon Data tab and: Set an initial password (enter it twice). Set User Type to C (Communications Data) . Switch to the Roles tab and assign roles that enable: Remote function call (RFC) execution Table-level read access for metadata tables Access to system-level information Verify that the assigned roles enable execution of these RFC modules: STFC_CONNECTION : Verifies connectivity between Atlan and SAP S/4HANA. RFC_SYSTEM_INFO : Retrieves system metadata such as SYSID, operating system, and release version. RFC_READ_TABLE : Enables table-level reads for metadata extraction. DD_DDL_DEPENDENCY_GET : Fetches dependencies used in CDS views and table lineage. Click Save to confirm the changes. note The user must change the password on first login. Next steps â Crawl SAP S/4HANA : Follow the instructions to extract metadata using the configured service user. Tags: erp setup permissions sap-s4hana Previous SAP S/4HANA Next Crawl SAP S/4HANA Prerequisites Create communication user for metadata extraction Next steps"
  },
  {
    "url": "https://docs.atlan.com/tags/lambda",
    "text": "One doc tagged with \"lambda\" View all tags Automation Integrations Integrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On."
  },
  {
    "url": "https://docs.atlan.com/tags/webhooks",
    "text": "4 docs tagged with \"webhooks\" View all tags Automation Integrations Integrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On. Connections Integration Integrate Atlan with Connections to create webhooks and automate notifications. Create webhooks If your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or [submit a request](/support/submit-request). Webhooks Integration Integrate Atlan with Webhooks to automate actions and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/browser-extension",
    "text": "3 docs tagged with \"browser-extension\" View all tags Automation Integrations Integrate Atlan with automation tools like AWS Lambda, Connections, Webhooks, Browser Extension, and Always On. Browser Extension Integrate Atlan with the Browser Extension to enhance your data catalog experience. Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/aws-lambda",
    "text": "One doc tagged with \"aws lambda\" View all tags AWS Lambda Integrate Atlan with AWS Lambda to automate workflows and triggers."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/always-on/references/tag-propagation",
    "text": "Configure Atlan Integrations Automation Always On Tag propagation On this page Tag propagation Tag propagation is disabled by default in Atlan. When tagging an asset , you can enable tag propagation to child assets. To downstream assets â Downstream assets are derived through the data transformations of an asset. Atlan can propagate tags through lineage. For example: You tag a column as PII . That column has downstream columns (in lineage). Atlan tags the downstream columns as PII , too, if propagation is enabled. This is particularly useful when you want to tag data for protection reasons. When combined with purposes , Atlan protects all downstream copies of the data automatically. To child assets â Child assets are linked to parent assets through a parent-child hierarchical relationship. Atlan can propagate tags from parent to child assets. For example: You tag a database table as Marketing Analysis . Atlan tags all the columns in that table as Marketing Analysis , too, if propagation is enabled. You can also configure the propagation of your tags: Hierarchy & lineage enables tag propagation to child and downstream assets. Hierarchy only (no lineage) enables tag propagation to child assets only. Lineage only (no hierarchy) enables tag propagation to downstream assets only. No propagation disables any tag propagation. This is particularly useful when you want to tag data for a business domain or project use. Atlan includes all assets under the level you tag in that business domain or project. To linked assets â Atlan can propagate tags to all linked assets for terms . For example: You tag a glossary term as Public . Atlan tags all the linked assets for that term as Public , too, if propagation is enabled. You can also configure the propagation of your tags: Hierarchy & lineage enables tag propagation to child and downstream assets. Hierarchy only (no lineage) enables tag propogation to child assets only. Lineage only (no hierarchy) enables tag propogation to downstream assets only. No propagation disables any tag propagation. This is particularly useful when you want to tag data en masse for use across all teams. If propagation is enabled, Atlan includes all linked assets for the tagged term. Tags: lineage data-lineage impact-analysis Previous Suggestions from similar assets Next Browser Extension To downstream assets To child assets To linked assets"
  },
  {
    "url": "https://docs.atlan.com/tags/always-on",
    "text": "One doc tagged with \"always on\" View all tags Always On Integrate Atlan with Always On to enable continuous automation and suggestions."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau On this page Tableau Overview: Catalog dashboards, workbooks, and data sources in Atlan. Gain visibility into lineage, usage, and governance for your BI assets. Get started â Follow these steps to connect and catalog Tableau assets in Atlan: Set up the connector Crawl Tableau assets Guides â Set up on-premises access : Configure Atlan to connect to Tableau environments that are isolated from the public internet. Crawl on-premises Tableau : Extract metadata from on-premises Tableau instances. Set up a private network link to Tableau Server : Establish a secure, private network connection to Tableau Server for metadata extraction. References â What does Atlan crawl from Tableau : Learn about the Tableau assets and metadata that Atlan discovers and catalogs. Preflight checks for Tableau : Verify prerequisites before setting up the Tableau connector. Troubleshooting â Troubleshooting connectivity : Resolve common Tableau connection issues and errors. Tags: tableau connector business intelligence connectivity Next Set up Tableau Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/tags/tableau",
    "text": "2 docs tagged with \"tableau\" View all tags Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance. Tableau Integrate, catalog, and govern Tableau assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/embedded",
    "text": "One doc tagged with \"embedded\" View all tags Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/how-to",
    "text": "One doc tagged with \"how-to\" View all tags Enable embedded metadata in Tableau Learn how to enable embedded Atlan metadata in Tableau dashboards for seamless data context and governance."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/webhooks/how-tos/create-webhooks",
    "text": "Configure Atlan Integrations Automation Webhooks Create webhooks On this page Create webhooks Who can do this? You will need to be an admin user in Atlan to create webhooks. danger If your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or submit a request . Webhooks allow you to monitor events happening in Atlan, receive notifications for these events to a URL of your choice, and take action right away. For example, you can create a webhook to send notifications to your email address or messaging app when a term is updated or an asset is tagged . Webhooks send the payload in a specific format that cannot be customized. This is meant for consumption by a programmatic entity down the line   -  for example, AWS Lambda or a microservice. For a webhook to be consumed directly, Atlan will need to customize the payload, which is currently not supported. Alternatively, you can explore out-of-the-box integrations such as Slack and Microsoft Teams . Atlan currently supports creating webhooks for the following event types: Asset creation, deletion, and metadata update Custom metadata update for assets Tag attachment or removal from assets Create a webhook â To create a webhook: From the left menu in Atlan, click Admin . Under Workspace , click Webhooks . On the Webhooks page, click + New Webhook to create a new webhook. In the New Webhook dialog, enter the following details: For Name , enter a meaningful name for your webhook. For Webhook URL , enter the URL for where you want to receive event notifications. For Asset type , select the asset types for which you'd like to receive notifications. (This will default to all asset types, if none are specified.) For Event type , under Assets , select all the event types for which you'd like to receive notifications: Create -  to process notifications for asset creation . Update -  to process notifications for when assets are updated . Delete -  to process notifications for asset deletion . Update Custom Metadata -  to process notifications for when custom metadata is updated for assets. Add Tags -  to process notifications for when tags areÂ attached to an asset. Delete Tags -  to process notifications when tags areÂ removed from an asset. To validate the URL you've entered, in the upper right, click the Validate button. danger Atlan will send a sample payload to test if the webhook URL is correct. You will need to respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. Click Save to finish creating your webhook. From the Webhook successfully created dialog, under Secret Key , click the clipboard icon to copy the secret key and store it in a secure location to verify requests from Atlan . Click Done to complete setup. Verify requests from Atlan â Atlan signs its webhooks using a secret that is unique to your app. With the help of signing secrets, you can verify the authenticity of such requests with confidence. Each HTTP request sent from Atlan will include an x-atlan-signing-secret HTTP header. You can use the secret key for your webhook to validate requests from Atlan. Tags: webhooks automation notifications Previous Webhooks Integration Next Collaboration Integrations Create a webhook Verify requests from Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/connections",
    "text": "One doc tagged with \"connections\" View all tags Connections Integration Integrate Atlan with Connections to create webhooks and automate notifications."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams",
    "text": "Configure Atlan Integrations Collaboration Microsoft Teams On this page Microsoft Teams Overview: Connect Atlan with Microsoft Teams to enable seamless collaboration, notifications, and sharing of data assets within your organization. Get started â Follow these steps to connect and integrate Microsoft Teams with Atlan: Integrate Microsoft Teams Link your Microsoft Teams account Guides â Integrate Microsoft Teams : Step-by-step instructions to connect Atlan with Microsoft Teams. Link your Microsoft Teams account : How to link your Microsoft Teams account to Atlan for seamless collaboration. Troubleshooting â Troubleshooting Microsoft Teams integration : Solutions for common issues encountered when integrating Atlan with Microsoft Teams. FAQ â What is included in the Microsoft Teams integration? : Learn about the features and capabilities of the Microsoft Teams integration in Atlan. Tags: microsoft teams integration collaboration Previous Collaboration Integrations Next How to integrate Microsoft Teams Get started Guides Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack",
    "text": "Configure Atlan Integrations Collaboration Slack On this page Slack Overview: Connect Atlan with Slack to enable seamless collaboration, notifications, and sharing of data assets within your organization. Get started â Follow these steps to connect and integrate Slack with Atlan: Integrate Slack Link your Slack account Guides â Integrate Slack : Step-by-step instructions to connect Atlan with Slack. Link your Slack account : How to link your Slack account to Atlan for seamless collaboration. Troubleshooting â Troubleshooting Slack integration : Solutions for common issues encountered when integrating Atlan with Slack. FAQ â How do I send messages or search assets from Slack? What does Atlan do with each Slack permission? What is included in the Slack integration? What is the difference between Copy Link and Share on Slack or Teams? Tags: slack integration collaboration Previous What is included in the Microsoft Teams integration? Next How to integrate Slack Get started Guides Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/setup-workflow-alerting",
    "text": "Configure Atlan Integrations Collaboration Send alerts for workflow events On this page Send alerts for workflow events App You can send workflow alerts to Email and Google Chat to notify your team when workflows in Atlan complete or fail. You can also send alerts to Slack and Microsoft Teams using Atlanâs built-in notification capabilities. Prerequisites â Before you begin, make sure you have: Access to the Workflow Alerting app. You can verify this by searching for Workflow Alerting in the Atlan marketplace. If you don't have access, contact Atlan support or your Atlan customer team to request it. Setup workflow â In your Atlan workspace, go to the homepage and click New workflow in the top navigation bar. Search for Workflow Alerting , and then select Set up workflow . In the Mode option, select the alert delivery method: Email Google Chat Sends workflow alerts as a tabular report to one or more email addresses, including details of the selected workflow runs. Enter one or more email addresses in the Email IDs field, separated by commas. Use valid addresses for recipients who need to receive workflow alerts. [email protected] , [email protected] In the Email subject field, enter a clear, concise subject line for the alert email so recipients can quickly identify the notification. Atlan Workflow Failure Alert Sends workflow alerts to a Google Chat space using a webhook URL. Alerts appear as structured Google Chat cards, which include the workflow name, status (for example, Failed or Completed ), and other run details. Enter the Google Chat webhook URL in the Webhook URL field. You can generate a webhook URL by navigating to your Google Chat space, selecting Settings â Apps & integrations , and adding an incoming webhook. For more information, see the Google Chat documentation . https://chat.googleapis.com/v1/spaces/AAAA123456/messages?key=abc123&token=xyz456 In Workflows Created By , select the users whose workflows trigger alerts. Leave blank to monitor workflows from all users. Select Failed to receive alerts only for failed runs, or All Workflows to receive alerts for every workflow execution. info Workflows that are still running are ignored. The package only captures workflows that have finished, either successfully or with a failure. In Scheduling Status , choose one of the following options to specify which workflows to monitor: Scheduled : Includes only workflows with a defined run schedule. Unscheduled : Includes only workflows without a set schedule. All : Includes both scheduled and unscheduled workflows. In Workflow Type , select the categories of workflows you want to monitor. Multiple categories can be selected if needed. In Monitoring interval for workflow completions , select the time range to track workflow completions. Atlan evaluates workflows completed within the chosen interval and triggers alerts if they meet the configured criteria, enabling timely detection of issues and faster response. If Last 24 Hours is selected, the alert includes all workflows that completed or failed within the past day, listed together in a single notification. Schedule and run the workflow. Run the workflow manually or set a recurring schedule to send alerts at regular intervals. Need help? â If you have any issues related to configuring the app, contact Atlan support . Tags: automation alerts workflows app Previous Troubleshooting spreadsheets Next Communication Integrations Prerequisites Setup workflow Need help?"
  },
  {
    "url": "https://docs.atlan.com/tags/collaboration",
    "text": "4 docs tagged with \"collaboration\" View all tags Collaboration Integrations Integrate Atlan with collaboration tools like Microsoft Teams and Slack. Microsoft Teams Integrate Atlan with Microsoft Teams to enable collaboration and notifications. Slack Integrate Atlan with Slack to enable collaboration and notifications. What is included in the Slack integration? Learn about the features and capabilities of the Slack integration with Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/teams",
    "text": "One doc tagged with \"teams\" View all tags Collaboration Integrations Integrate Atlan with collaboration tools like Microsoft Teams and Slack."
  },
  {
    "url": "https://docs.atlan.com/tags/slack",
    "text": "5 docs tagged with \"slack\" View all tags Collaboration Integrations Integrate Atlan with collaboration tools like Microsoft Teams and Slack. Slack Integrate Atlan with Slack to enable collaboration and notifications. What does Atlan do with each Slack permission? Learn about what does atlan do with each slack permission?. What is included in the Slack integration? Learn about the features and capabilities of the Slack integration with Atlan. What is the difference between Copy Link and Share on Slack or Teams? Learn about what is the difference between copy link and share on slack or teams?."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements",
    "text": "Configure Atlan Integrations Communication SMTP and Announcements On this page SMTP and Announcements Integration Overview: Connect Atlan with SMTP to send system announcements and notifications, keeping your teams up to date with important information. Guides â How to configure SMTP : Step-by-step instructions to configure SMTP for sending notifications. How to create announcements : How to create and manage announcements for your data assets. How to manage system announcements : How to add and remove system-wide announcements in Atlan. Tags: integrations communication smtp announcements Previous Communication Integrations Next Configure SMTP Guides"
  },
  {
    "url": "https://docs.atlan.com/tags/communication",
    "text": "2 docs tagged with \"communication\" View all tags Communication Integrations Integrate Atlan with communication tools like SMTP and Announcements. SMTP and Announcements Integration Integrate Atlan with SMTP to send announcements and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/smtp",
    "text": "2 docs tagged with \"smtp\" View all tags Communication Integrations Integrate Atlan with communication tools like SMTP and Announcements. SMTP and Announcements Integration Integrate Atlan with SMTP to send announcements and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/announcements",
    "text": "2 docs tagged with \"announcements\" View all tags Communication Integrations Integrate Atlan with communication tools like SMTP and Announcements. SMTP and Announcements Integration Integrate Atlan with SMTP to send announcements and notifications."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim",
    "text": "Configure Atlan Integrations Identity Management SCIM On this page SCIM Integration Overview: Connect Atlan with SCIM to automate user provisioning and streamline identity management. Get started â How to configure SCIM provisioning : Learn how to set up SCIM provisioning in Atlan. Guides â How to enable Azure AD for SCIM provisioning : Step-by-step instructions to enable SCIM provisioning with Azure AD. How to enable Okta for SCIM provisioning : Step-by-step instructions to enable SCIM provisioning with Okta. Troubleshooting & FAQ â Troubleshooting SCIM provisioning : Answers to common questions and troubleshooting tips for SCIM provisioning. Tags: integrations identity management scim Previous Identity Management Integrations Next Configure SCIM provisioning Get started Guides Troubleshooting & FAQ"
  },
  {
    "url": "https://docs.atlan.com/tags/identity-management",
    "text": "3 docs tagged with \"identity management\" View all tags Identity Management Integrations Integrate Atlan with identity management tools like SCIM and SSO. SCIM Integration Integrate Atlan with SCIM to automate user provisioning. SSO Integration Integrate Atlan with SSO to enable secure authentication and access control."
  },
  {
    "url": "https://docs.atlan.com/tags/scim",
    "text": "2 docs tagged with \"scim\" View all tags Identity Management Integrations Integrate Atlan with identity management tools like SCIM and SSO. SCIM Integration Integrate Atlan with SCIM to automate user provisioning."
  },
  {
    "url": "https://docs.atlan.com/tags/sso",
    "text": "2 docs tagged with \"sso\" View all tags Identity Management Integrations Integrate Atlan with identity management tools like SCIM and SSO. SSO Integration Integrate Atlan with SSO to enable secure authentication and access control."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira",
    "text": "Configure Atlan Integrations Project Management Jira On this page Jira Overview: Connect Atlan with Jira to automate ticket creation, link your Jira account, and manage data-related tasks directly from your data catalog. Get started â Follow these steps to connect and integrate Jira with Atlan: Integrate Jira Cloud Integrate Jira Data Center Link your Jira account Guides â Integrate Jira Cloud : Step-by-step instructions to connect Atlan with Jira Cloud. Integrate Jira Data Center : Step-by-step instructions to connect Atlan with Jira Data Center. Link your Jira account : How to link your Jira account to Atlan for seamless ticket management. Troubleshooting â Troubleshooting Jira integration : Solutions for common issues encountered when integrating Atlan with Jira. FAQ â What is included in the Jira integration? Configure additional fields or auto-assign owners to Jira tickets created from Atlan Can site renaming affect the Jira integration? Tags: jira integration project management Previous Project Management Integrations Next How to integrate Jira Cloud Get started Guides Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow",
    "text": "Configure Atlan Integrations Project Management ServiceNow On this page ServiceNow Overview: Connect Atlan with ServiceNow to automate ticket creation, link your ServiceNow account, and manage data-related tasks directly from your data catalog. Get started â Follow these steps to connect and integrate ServiceNow with Atlan: Integrate ServiceNow Link your ServiceNow account Guides â Integrate ServiceNow : Step-by-step instructions to connect Atlan with ServiceNow. Link your ServiceNow account : How to link your ServiceNow account to Atlan for seamless ticket management. Troubleshooting â Troubleshooting ServiceNow integration : Solutions for common issues encountered when integrating Atlan with ServiceNow. Tags: servicenow integration project management Previous Can site renaming affect the Jira integration? Next How to integrate ServiceNow Get started Guides Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/tags/project-management",
    "text": "3 docs tagged with \"project management\" View all tags Jira Integrate Atlan with Jira to automate ticket creation and link your Jira account. Project Management Integrations Integrate Atlan with project management tools like Jira and ServiceNow. ServiceNow Integrate Atlan with ServiceNow to automate ticket creation and link your ServiceNow account."
  },
  {
    "url": "https://docs.atlan.com/tags/jira",
    "text": "2 docs tagged with \"jira\" View all tags Jira Integrate Atlan with Jira to automate ticket creation and link your Jira account. Project Management Integrations Integrate Atlan with project management tools like Jira and ServiceNow."
  },
  {
    "url": "https://docs.atlan.com/tags/servicenow",
    "text": "2 docs tagged with \"servicenow\" View all tags Project Management Integrations Integrate Atlan with project management tools like Jira and ServiceNow. ServiceNow Integrate Atlan with ServiceNow to automate ticket creation and link your ServiceNow account."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight",
    "text": "Connect data BI Tools Cloud-based BI Amazon QuickSight On this page Amazon QuickSight Overview: Catalog Amazon QuickSight dashboards, analyses, and datasets in Atlan. Gain visibility into lineage, usage, and governance for your AWS-based analytics assets. Get started â Follow these steps to connect and catalog Amazon QuickSight assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Amazon QuickSight? : Detailed list of QuickSight asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Amazon QuickSight : Technical checks and requirements needed for a successful QuickSight integration. Tags: amazon quicksight connector business intelligence connectivity Next Set up Amazon QuickSight Get started References"
  },
  {
    "url": "https://docs.atlan.com/tags/salesforce",
    "text": "10 docs tagged with \"salesforce\" View all tags Crawl Salesforce Once you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce. Does Atlan require an admin user in Salesforce? No. However, it is recommended that a Salesforce administrator establishes a [connection between Atlan and Salesforce](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce). To learn more, see [here](/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity). Preflight checks for Salesforce Before [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co. Set up client credentials flow Configure Salesforce for OAuth 2.0 client credentials authentication in Atlan. Set up JWT bearer flow Configure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan. Set up Salesforce Learn about setting up Salesforce authentication for Atlan. Set up username-password flow Configure Salesforce username-password flow for Atlan integration. What does Atlan crawl from Amazon QuickSight? Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:. What does Atlan crawl from Salesforce? Atlan only performs GET requests on these five endpoints:. Why does the description from Salesforce not show up in Atlan? Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce)."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt",
    "text": "Connect data ETL Tools dbt On this page dbt Overview: Catalog dbt models, sources, tests, and documentation in Atlan. Gain visibility into lineage, transformations, and governance for your data build tool assets. dbt core â Get started â Set up the connector Crawl dbt Core assets Guides â Manage dbt tags : Manage and sync tags from dbt Core. Add impact analysis in GitHub : Enable impact analysis for dbt Core projects in GitHub. Add impact analysis in GitLab : Enable impact analysis for dbt Core projects in GitLab. Migrate from dbt to Atlan Action : Migrate your dbt Core integration to Atlan Action. Enrich Atlan through dbt : Update and enrich Atlan metadata using dbt Core. References â What does Atlan crawl from dbt Core : Learn about the dbt Core assets and metadata that Atlan discovers and catalogs. Preflight checks for dbt : Verify prerequisites before setting up the dbt Core connector. Troubleshooting â Troubleshooting connectivity : Resolve common dbt Core connection issues and errors. dbt cloud â Get started â Set up the connector Crawl dbt Cloud assets Guides â Manage dbt tags : Manage and sync tags from dbt Cloud. Add impact analysis in GitHub : Enable impact analysis for dbt Cloud projects in GitHub. Add impact analysis in GitLab : Enable impact analysis for dbt Cloud projects in GitLab. Migrate from dbt to Atlan Action : Migrate your dbt Cloud integration to Atlan Action. Enrich Atlan through dbt : Update and enrich Atlan metadata using dbt Cloud. References â What does Atlan crawl from dbt Cloud : Learn about the dbt Cloud assets and metadata that Atlan discovers and catalogs. Preflight checks for dbt : Verify prerequisites before setting up the dbt Cloud connector. Troubleshooting â Troubleshooting connectivity : Resolve common dbt Cloud connection issues and errors. Tags: dbt connector etl data transformation connectivity Next Set up dbt Cloud dbt core dbt cloud"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/troubleshooting/troubleshooting-dbt-connectivity",
    "text": "Connect data ETL Tools dbt Troubleshooting Connection issues On this page Connection issues This guide helps you resolve common connection and authentication issues when setting up the dbt connector in Atlan. What are the known limitations of the dbt connector? â Following are the known limitations: For dbt Core , Atlan currently only processes the status of dbt tests   -  passed, failed, and warning   -  but not the job name and execution time. However, this information is available for dbt Cloud . You can hover over the test Status in the asset sidebar to view this information. Column-level lineage for dbt models is currently unavailable at source. However, Atlan applies a custom SQL-parsing algorithm to the materialized SQL tables to generate column-level lineage for dbt models. Note that due to the limitations of SQL parsing, Atlan doesn't guarantee generating lineage for all columns. Ephemeral dbt models don't materialize tables at source. Hence, these doesn't create lineage for the materialized layer or publish dbt model columns in Atlan. This is an expected behavior of ephemeral dbt models . A dbt source can include multiple SQL warehouse tables. Lineage between dbt sources and models isn't supported. Instead, Atlan builds lineage between dbt models and SQL tables when dbt models have an explicit dependency on specific SQL tables of a dbt source. Why are my dbt Cloud models not showing up in Atlan after setup? â Atlan only crawls dbt assets that are in the âappliedâ (built) state in dbt Cloud. Models must be part of a successful run to be picked up during crawling; models that are only defined in your project files but havenât been executed wonât be included. For more information about project state, see Project states in dbt Cloud. Does Atlan support data types for dbt model columns? â Yes, Atlan supports column data types for dbt models: dbt Cloud : Make sure there is at least one job run with the Generate docs on run option enabled for every environment where dbt models are executed. dbt Core -  upload the manifest.json and catalog.json files generated by the dbt docs generate command for every dbt Core project. Refer to dbt Core documentation to learn how to structure the bucket while uploading your files. Why are columns for dbt models missing? â If you've crawled your dbt models but columns are missing: Define the columns in the columns attribute of the model's .yml definition in the manifest.json file. Check if the dbt model is materializing a table or view asset, without which columns for a dbt model don't show up. Is dbt source metadata available for materialized columns? â If metadata enrichment is enabled for assets that dbt materializes, only table assets are updated with the materialized by dbt source or model metadata. Materialized columns are only updated with dbt model metadata   -  dbt source metadata is currently not supported for column assets. Can I map the Atlan GitHub action to multiple dbt projects? â Yes, you can configure the Atlan GitHub action for multiple dbt projects. Does Atlan support syncing terms from dbt? â Yes, you can update terms from dbt to Atlan. For more details, refer to the developer documentation . Why are some dbt tests missing? â Atlan doesn't support crawling dbt tests with an auto-generated unique_id that exceeds the character limit of 32,000 characters. If you want to catalog such dbt tests in Atlan, you need to define a custom name for your dbt tests within the character limit. Why is there a discrepancy in dbt test count between dbt and Atlan? â Atlan fetches dbt models, sources, and tests from the applied state of each dbt environment. This is the most recent project state for each environment. The asset count on Atlan may differ from whatâs present in the manifest files for individual job runs. Refer to dbt documentation to learn more about dbt project states. Why does Atlan link the dbt model description to my SQL source table? â If a materialized table is linked to multiple dbt assets, Atlan applies the description from the linked dbt model to the materialized asset. In case this is unavailable, Atlan then applies the description from the linked dbt source to the asset. Why does Atlan link the dbt seed description to my SQL source table? â If a materialized table is linked to multiple dbt assets, Atlan applies the description from the linked dbt seed to the materialized asset. In case this is unavailable, there is no description available. Previous What does Atlan crawl from dbt Core?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery",
    "text": "Connect data Data Warehouses Google BigQuery On this page Google BigQuery Overview: Catalog Google BigQuery projects, datasets, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your cloud data warehouse assets. Get started â Follow these steps to connect and catalog Google BigQuery assets in Atlan: Set up the connector Crawl Google BigQuery assets Guides â Mine Google BigQuery : Extract query history and build lineage for your BigQuery assets. Enable SSO for Google BigQuery : Set up SSO authentication for BigQuery connections. Manage Google BigQuery tags : Configure and manage tags and policy tags in BigQuery. References â What does Atlan crawl from Google BigQuery : Learn about the BigQuery assets and metadata that Atlan discovers and catalogs. Preflight checks for Google BigQuery : Verify prerequisites before setting up the Google BigQuery connector. Troubleshooting â Troubleshooting connectivity : Resolve common Google BigQuery connection issues and errors. Tags: google bigquery connector data warehouse connectivity Next Set up Google BigQuery Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics On this page IBM Cognos Analytics Overview: Catalog reports, dashboards, and data modules from IBM Cognos Analytics in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog IBM Cognos Analytics assets in Atlan: Set up the connector : Configure Atlan to connect to your IBM Cognos Analytics environment. Crawl assets : Extract and catalog IBM Cognos Analytics reports, dashboards, and data modules. Guides â Set up on-premises access : Configure Atlan to connect to IBM Cognos Analytics environments that are isolated from the public internet Crawl on-premises IBM Cognos Analytics : Step-by-step instructions for extracting metadata from on-premises IBM Cognos Analytics instances. References â What does Atlan crawl from IBM Cognos Analytics? : Detailed list of IBM Cognos Analytics asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to IBM Cognos Analytics, including permissions and network problems. Tags: ibm cognos connector business intelligence connectivity Next Set up IBM Cognos Analytics Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/troubleshooting/troubleshooting-ibm-cognos-analytics-connectivity",
    "text": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Troubleshooting Troubleshooting IBM Cognos Analytics connectivity On this page Troubleshooting IBM Cognos Analytics connectivity What are the known limitations of the IBM Cognos Analytics connector? â Atlan currently does not support the following: Field-level lineage for IBM Cognos Analytics assets. Why are some assets missing? â Due to issues with the IBM Cognos Analytics API response, it is possible that the connection may only catalog a partial number of assets. If you notice any inconsistencies among your cataloged assets in Atlan, raise a support ticket with IBM Cognos Analytics and contact Atlan support as well to triage the issue. Tags: lineage data-lineage impact-analysis api rest-api graphql catalog metadata discovery Previous What does Atlan crawl from IBM Cognos Analytics?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker",
    "text": "Connect data BI Tools Cloud-based BI Looker On this page Looker Overview: Catalog explores, dashboards, and models from Looker in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Looker assets in Atlan: Set up the connector Crawl Looker assets Guides â Set up on-premises access : Configure Atlan to connect to Looker environments that are isolated from the public internet. Crawl on-premises Looker : Extract metadata from on-premises Looker instances. References â What does Atlan crawl from Looker : Learn about the Looker assets and metadata that Atlan discovers and catalogs. Troubleshooting â Troubleshooting connectivity : Resolve common Looker connection issues and errors. Tags: looker connector business intelligence connectivity Next Set up Looker Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/troubleshooting/troubleshooting-looker-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Looker Troubleshooting Troubleshooting Looker connectivity On this page Troubleshooting Looker connectivity What are the known limitations of the Looker connector? â Following are the known limitations of the Looker connector: Persistent derived tables (PDTs) and Liquid parameterized tables are currently not supported for Looker views . What is the timeframe for calculating sourceViewCount for Looks and dashboards? â The Atlan property sourceViewCount is based on the source property view_count : view_count displays a total count of views for a Look or dashboard since creation of the asset. Atlan currently does not support a timestamp for views due to limitations of the Looker APIs. Why do I get a 403 forbidden error after clicking the test authentication button? â If you receive a 403 forbidden error when testing authentication, confirm the following: Ensure you have provided the API host URL for Looker. (This may be different from the Looker instance URL.) Ensure your Looker instance is running on port 433. If not, add the port number explicitly to the URL. Ensure your client ID and secret are correct. Ensure you have allowlisted the Atlan IP in your Looker instance. (To get details of your Atlan IP, please raise a support ticket .) Once you have confirmed or corrected these, retry the authentication. Why is lineage missing from an Explore to a Tile or Look? â If lineage is missing from an Explore to a Tile or Look, this is usually because you have not given content access to a folder . Once you have corrected the access in Looker, rerun the Looker crawler in Atlan. Why is lineage missing between data sources and Looker assets? â If lineage is missing from data sources to Looker assets, confirm the following: Ensure you have given the see_sql permission to the role for which you generated the API3 credentials . Ensure you have crawled the data sources' assets in Atlan, before crawling the Looker assets. Once you have confirmed or corrected these, rerun the Looker crawler in Atlan. Why is lineage missing from tables to models? â If lineage is missing from tables to models, this is usually because you have excluded a folder or project that contains these lineage details. Once you have corrected this in the crawler configuration, rerun the Looker crawler in Atlan. Why is relationship missing between models and queries, or Looks and models? â If a relationship is missing between some Looker assets, this is usually because you have excluded a project that contains these details. Once you have corrected this in the crawler configuration, rerun the Looker crawler in Atlan. Can I use deploy keys for setup? â Deploy keys only grant access to a single repository. For more complex projects where Looker connects to multiple repositories, Atlan does not support using deploy keys. Why am I seeing a failed to clone project error? â Exception: ('Failed to clone project %s. Git URL: %s', '<project>', '<git_url>') If you encounter the above error, please ensure that the SSH key youâre using has access to all the GitHub project files included for crawling. Refer to the Looker setup documentation to learn more. Tags: dashboards visualization analytics Previous Preflight checks for Looker Next Troubleshooting on-premises Looker connectivity"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode",
    "text": "Connect data BI Tools Cloud-based BI Mode On this page Mode Overview: Catalog reports, queries, and dashboards from Mode in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Mode assets in Atlan: Set up the connector : Configure Atlan to connect to your Mode environment. Crawl assets : Extract and catalog Mode reports, queries, and dashboards. References â What does Atlan crawl from Mode? : Detailed list of Mode asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Mode, including permissions and network problems. Tags: mode connector business intelligence connectivity Next Set up Mode Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud",
    "text": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud On this page Qlik Sense Cloud Overview: Catalog apps, sheets, and data sources from Qlik Sense Cloud in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Qlik Sense Cloud assets in Atlan: Set up the connector : Configure Atlan to connect to your Qlik Sense Cloud environment. Crawl assets : Extract and catalog Qlik Sense Cloud apps, sheets, and data sources. References â What does Atlan crawl from Qlik Sense Cloud? : Detailed list of Qlik Sense Cloud asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Qlik Sense Cloud, including permissions and network problems. Tags: qlik sense connector business intelligence connectivity Next Set up Qlik Sense Cloud Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud",
    "text": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud Get Started Set up Qlik Sense Cloud On this page Set up Qlik Sense Cloud Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps   -  you may not have access yourself. Invite a new user â To invite a new user for Atlan to use when integrating with Qlik Sense Cloud: Log in to your Qlik Sense Cloud instance. From the upper right corner of your Qlik Sense Cloud instance, click the menu icon and then click Management Console . In the left menu under Governance in the Management Console , click the Users tab. From the top right of the Users page, click Invite to invite a new user. In the Invite users dialog, for Email addresses , enter the email address of the new user that you want to invite and click Invite .Â You can also set up roles and groups for robust access management. Set permissions â Did you know? Atlan does not make any API requests or queries that will update the objects in your Qlik Sense Cloud instance. Once you've invited a new user, ensure that the new user has the following minimum permissions for crawling Qlik Sense Cloud : Assign developer role to the new user to allow the creation of API keys. Ensure that the new user has List , Read , and Open permissions to spaces, apps, data, sheets, charts, and connections in Qlik Sense Cloud. In order to ensure that the new user has the above permissions to Qlik Sense Cloud objects, the new user can either be the creator of the objects, have access to the shared or managed spaces where the objects are located, or the objects need to be made public .Â Enable API key creation â Once you have created a new user and ensured the necessary permissions, you will need to enable API key creation for that user. To enable API key creation for the new user: From the upper left corner of your Qlik Sense Cloud instance, click the menu icon and then click Management Console . In the left menu under Management Console , click the Settings tab. From Settings , scroll down to API keys and enter the following details: For Enable API keys , toggle the slider on to enable the creation of API keys. For Change maximum token expiration , enter a value for allowed maximum token age. For Change maximum of active API keys per user , enter a value for the maximum number of active API keys that a user may have in the tenant. Refresh the Management Console page and navigate to the left menu to view a new tab for API keys . Generate API key â Once API key creation has been enabled, the new user can generate API credentials for crawling Qlik Sense Cloud. To generate an API key for crawling Qlik Sense Cloud : Log in to your Qlik Sense Cloud instance as the new user. (If you just completed the steps above, you'll need to log out from the tenant admin account first.) From the top right of your Qlik Sense Cloud instance, click your profile avatar and then click Profile settings . From the left menu under Management , click API keys . From the upper right of the API keys page, click Generate new key .Â In the Generate a new API key dialog, enter an API key description and select an expiration time within the allowed maximum token age.Â Click Generate to generate an API key. From the resulting dialog, copy the generated API key value and save it in a temporary location. danger The API key is only displayed once. You need to copy and save the value since there is no way to see that specific key again. You will have to generate a new key if you do not copy it. Tags: api rest-api graphql Previous Qlik Sense Cloud Next Crawl Qlik Sense Cloud Invite a new user Set permissions Enable API key creation Generate API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/troubleshooting/troubleshooting-qlik-sense-cloud-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud Troubleshooting Troubleshooting Qlik Sense Cloud connectivity On this page Troubleshooting Qlik Sense Cloud connectivity What are the known limitations of the Qlik Sense Cloud connector? â Following are the known limitations of the Qlik Sense Cloud connector, related to lineage: Lineage can be missing in the following scenarios: Data joins or concatenations made in the Data manager within a Qlik App. Calculated fields created in the Data manager within a Qlik App. Joins within QVD files. Advanced usage Atlan currently does not support upstream lineage for Qlik applications that use: other apps as sources datasets loaded from the file system via SMTP or RESTful web services Due to limitations at source, Atlan does not get the requisite metadata to generate upstream lineage to data sources. To learn more about lineage errors in general, refer to Qlik documentation . Why is the count of charts in Atlan much higher than at source? â Atlan catalogs only charts that include fields. Charts without fields are not cataloged, which may result in a higher chart count in Atlan compared to the source. For more details, see What Atlan crawls from Qlik Sense Cloud . Tags: lineage data-lineage impact-analysis upstream-dependencies data-sources api rest-api graphql Previous Preflight checks for Qlik Sense Cloud"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash",
    "text": "Connect data BI Tools On-premises & Enterprise BI Redash On this page Redash Overview: Catalog queries, dashboards, and visualizations from Redash in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Redash assets in Atlan: Set up the connector : Configure Atlan to connect to your Redash environment. Crawl assets : Extract and catalog Redash queries, dashboards, and visualizations. References â What does Atlan crawl from Redash? : Detailed list of Redash asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Redash, including permissions and network problems. Tags: redash connector business intelligence connectivity Next Set up Redash Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce",
    "text": "Connect data CRM Salesforce On this page Salesforce Overview: Atlan's Salesforce connector enables you to discover, catalog, and govern your CRM data assets for improved data visibility and governance. Get started â Follow these steps to connect and catalog Salesforce assets in Atlan: Set up the connector : Configure authentication and connection settings for Salesforce Crawl Salesforce assets : Discover and catalog your Salesforce objects, fields, and metadata References â What does Atlan crawl from Salesforce : Understand the metadata and assets that Atlan discovers from your Salesforce instance Preflight checks for Salesforce : Verify your Salesforce configuration before connecting to Atlan Troubleshooting â Troubleshooting Salesforce connectivity : Resolve common connection and authentication issues FAQ â Admin user requirements for Salesforce : Understand the admin user requirements for connecting Salesforce to Atlan Salesforce description not showing : Troubleshoot issues with Salesforce descriptions not appearing in Atlan Tags: connector crm connectivity Next Set up Salesforce Get started References Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma",
    "text": "Connect data BI Tools Cloud-based BI Sigma On this page Sigma Overview: Catalog Sigma workbooks, pages, and elements in Atlan. Gain visibility into lineage, usage, and governance for your Sigma analytics assets. Get started â Follow these steps to connect and catalog Sigma assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Sigma? : Detailed list of Sigma asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Sigma : Technical checks and requirements needed for a successful Sigma integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Sigma, including permissions and network problems. Tags: sigma connector business intelligence hybrid bi connectivity Next Set up Sigma Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma",
    "text": "Connect data BI Tools Cloud-based BI Sigma Get Started Set up Sigma On this page Set up Sigma Who can do this? You will probably need your Sigma administrator to complete these steps   -  you may not have access yourself. Identify your organization's cloud â You will need your organization's cloud information to determine the endpoint while authenticating in Atlan. To identify your organization's cloud: Open your Sigma account. In the top right of the screen, click your profile avatar and then click Administration to open your Account page. On the Account page, under Site in General Settings , view the cloud information. Create an API token and client ID â To create an API token and client ID: Open your Sigma account. In the top right of the screen, click your profile avatar and then click Administration . From the left menu of the Administration page, click Developer Access . In the top right of the Developer Access page, click the Create New button. In the Create client credentials dialog, for Select privileges , click the Rest API checkbox, and then enter the following details: For Name , enter a meaningful name. For Owner , select the user you would like to associate with the token. Click Create to finish creating the API token. Once prompted, click Copy to copy and paste your API key secret in a secure location. danger The secret cannot be retrieved once this popover is closed. Your newly created API token will be listed in the Developer Access Â page. Hover over the tokenâs Client ID and click Copy . Both the API token and the client ID are required for authentication in Atlan. Verify necessary permissions â Ensure that the owner associated with the API token has the following permissions: Can View permission for all the Sigma workbooks and datasets you want to crawl. Can Use permission for all the Sigma connections used in the workbooks and datasets. Grant permissions â To grant permissions, follow the instructions in these links: Workbooks and datasets Connections Tags: api rest-api graphql Previous Sigma Next Crawl Sigma Identify your organization's cloud Create an API token and client ID Verify necessary permissions Grant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/troubleshooting/troubleshooting-sigma-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Sigma Troubleshooting Troubleshooting Sigma connectivity On this page Troubleshooting Sigma connectivity Why is the SQL query only visible for Sigma data elements and not Sigma datasets? â Atlan gets the SQL query only for Sigma data elements. SQL queries for Sigma datasets are currently unavailable from the Sigma APIs. Why am I not seeing columns for Sigma datasets? â Atlan currently does not support column metadata for Sigma datasets due to limitations of the Sigma APIs. Why am I not seeing Sigma datasets in lineage? â Atlan currently does not support showing lineage for Sigma datasets due to limitations of the Sigma APIs. Why is lineage missing between Sigma data elements? â Atlan currently does not support field-level lineage between Sigma data elements due to limitations of the Sigma Workbook APIs. However, to maintain field-level lineage, all Sigma data elements will have direct upstream lineage to SQL assets and field-level lineage to its column assets. Can users who do not have access to a workbook still see the preview? â Users can only see asset previews if the following conditions are met: They have the necessary permissions in both Sigma and Atlan. They are logged into Atlan and Sigma on the same browser. Therefore, if a user lacks the permission to view a workbook in Sigma, they will not be able to see the workbook preview in Atlan. Even if they do have the necessary permissions, they will need to be logged into Sigma on the same browser as their Atlan instance for asset previews to work. Why can I not see previews for my Sigma assets? â Your Sigma assets will be updated with previews during the next run of your Sigma workflow. If you have run the workflow and still do not see the previews, we suggest you rerun the workflow. Once you've rerun the workflow, the previews should be visible to all eligible users. Tags: lineage data-lineage impact-analysis api rest-api graphql Previous Preflight checks for Sigma"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity",
    "text": "Connect data BI Tools On-premises & Enterprise BI Tableau Troubleshooting Troubleshooting Tableau connectivity On this page Troubleshooting Tableau connectivity What are the known limitations of the Tableau connector? â Atlan currently does not support the following: Crawling Tableau flows when using the JWT bearer authentication method , due to limitations at source. Crawling tags from Tableau. Cataloging Tableau Pulse, stories, and views. Parsing custom SQL queries, where all tables referenced (whether in the main query or subqueries) are considered as upstream lineage for the asset. Why does Atlan require the Site Administrator Explorer role in Tableau? â Atlan requires the Site Administrator Explorer role in Tableau to extract data source fields and calculated fields. It is not possible to fetch data source fields and calculated fields with the Viewer role in the current version of the Tableau Metadata API. Atlan uses this data to generate granular column-level lineage across data sources and SQL assets. To extract lineage for assets in Tableau, the user must have the Site Administrator Explorer role. Is lineage available for Tableau custom SQL data sources? â Yes, Atlan can parse custom SQL queries in Tableau to generate lineage between the data source and tables. Lineage is available for tables from all SQL sources. However, column-level lineage is currently not supported. Why is upstream lineage missing for Tableau data sources? â If your Tableau data source is in a paused state , the Tableau Metadata API may fail to provide the requisite metadata on source databases and tables for Atlan to generate upstream lineage. Restart your Tableau data source and ensure that it remains active while crawling Tableau . This will allow Atlan to fetch the requisite metadata to generate upstream lineage for data sources. Why is there a discrepancy in asset count between Tableau and Atlan? â Dashboards   -  the Tableau UI does not display a unique count of dashboards. Dashboards in Tableau are represented in collections of one or more views. These may have same names as the views but are independent objects. Hence, the total count of these views in Tableau does not match the dashboard count in Atlan. Atlan sources the dashboard count from the Tableau API, which is the only reliable way to fetch the dashboard count. Data sources   -  embedded data sources are not reported on the Tableau UI. However, in Atlan, data sources can be filtered to show only published data sources, which should match the count of data sources on the Tableau UI. Can users who do not have access to a dashboard still see the preview? â Users can only see asset previews if the following conditions are met: They have the necessary permissions in both Tableau and Atlan. They are logged into Atlan and Tableau on the same browser. Therefore, if a user lacks the permission to view a dashboard in Tableau, they will not be able to view the dashboard preview in Atlan. Even if they do have the necessary permissions, they will need to be logged into Tableau on the same browser as their Atlan instance for asset previews to work. Why can I not see previews for my Tableau assets? â Your Tableau assets will be updated with previews during the next run of your Tableau workflow. If you have run the workflow and still do not see the previews, we suggest you rerun the workflow. Once you've rerun the workflow, the previews should be visible to all eligible users. If you're using Tableau Server with clickjack protection enabled and your Tableau instance URL is of a different origin than the Atlan instance URL, the asset previews will not load due to a same-origin error from the browser. You will need to disable clickjack protection to allow the Tableau asset previews to load. Is the certified status in Tableau mapped to the certificates field in Atlan? â Yes, the isCertified status for published data sources in Tableau is mapped to the certificates field in Atlan. Is the owner field in Tableau mapped to the owners field in Atlan? â No, the asset owner in Tableau is displayed as the source owner in the Overview section of the asset sidebar in Atlan. This is also only available for Tableau projects , flows , workbooks , and published data sources . Tableau has retired metrics methods in API 3.22 , hence source owner attribute for metrics is not supported in Atlan. Why am I getting a \"still creating the Metadata API store\" error? â Error message: Still creating the Metadata API Store. Results from the query might be incomplete at this time. BACKFILL-RUNNING If your Tableau workflow is failing with the above error message, this is because the Tableau Metadata API is being re-indexed after a quarterly release. The re-indexing of the Metadata API after quarterly releases can take up to a week, depending on the size of your instance. Since Atlan uses the Tableau Metadata API to fetch metadata, your Tableau workflows in Atlan may fail if the re-indexing has not been completed. You can check the backfill status of the Tableau Metadata API Store following this guide . Learn more about common errors in your Metadata API query. How to debug test authentication and preflight check errors? â Incorrect hostname Unable to connect to the specified host. Please verify that the host details are correct and retry. Ensure that you have entered the hostname for your Tableau Online or Tableau Server instance correctly. If you're using a domain name, verify that the DNS name correctly resolves to the corresponding IP address. Connection timed out Unable to connect to Tableau instance. Please verify server port or check if your server is up and running. Ensure that you're using the correct port number, especially if using a custom port for Tableau Server. Verify network connectivity and DNS resolution   -  you can also test from a different network or device. SSL error Unable to connect. Please check your SSL setting. Ensure that the server URL uses https if SSL is enabled . If the connection does not require an SSL, use http instead. The ssl details provided are incorrect. Please provide correct ssl certs. If your Tableau Server instance uses a self-signed or an internal CA SSL certificate, enter the SSL certificate correctly in the recommended format . Incorrect port number Unable to connect to Tableau instance. Please verify server port and retry. Ensure that you're using the correct port number, especially if using a custom port for Tableau Server. Invalid personal access token The personal access token you provided is invalid. Please check your PAT name and token value. Ensure that you have entered the token name correctly and it matches the token generated in Tableau: Token name is case-sensitive. Ensure that there are no extra spaces or characters. If the token you provided is invalid, you can create a new token .Â Incorrect site details The site details provided are incorrect. Please provide correct site details. Confirm that the site name in the URL matches the exact case and spelling of the site you are trying to access. Site names in Tableau are case-sensitive. Incorrect username Provided username is incorrect. Please check. Confirm that the username is present in Tableau. Otherwise, you can add a new user for basic authentication . Incorrect client ID The client id provided is incorrect or site is empty or connected app in tableau is deleted. Please check and try again. Ensure that you have specified the site name if using JWT bearer authentication. Ensure that the connected app is present in Tableau and verify the client ID. Incorrect secret ID or value The secret id provided is incorrect or the secret value is deleted. Please check and try again. or The secret value provided is incorrect. Please check your secret value and try again. Verify the secret ID of the connected app . Ensure that the secret value of the connected app has not been deleted. Tags: catalog metadata discovery Previous Preflight checks for Tableau"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot",
    "text": "Connect data BI Tools Cloud-based BI ThoughtSpot On this page ThoughtSpot Overview: Catalog ThoughtSpot liveboards, visualizations, and worksheets in Atlan. Gain visibility into lineage, usage, and governance for your ThoughtSpot analytics assets. Get started â Follow these steps to connect and catalog ThoughtSpot assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from ThoughtSpot? : Detailed list of ThoughtSpot asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to ThoughtSpot, including permissions and network problems. Tags: thoughtspot connector business intelligence hybrid bi connectivity Next Set up ThoughtSpot Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy",
    "text": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy On this page MicroStrategy Overview: Catalog MicroStrategy reports, documents, and dossiers in Atlan. Gain visibility into lineage, usage, and governance for your MicroStrategy analytics assets. Get started â Follow these steps to connect and catalog MicroStrategy assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from MicroStrategy? : Detailed list of MicroStrategy asset types and metadata fields that Atlan can extract and catalog. Preflight checks for MicroStrategy : Technical checks and requirements needed for a successful MicroStrategy integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to MicroStrategy, including permissions and network problems. Tags: microstrategy connector business intelligence hybrid bi connectivity Next Set up MicroStrategy Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/getting-started-with-the-apis",
    "text": "Get Started Quick Start Guides Developers Software development kits (SDKs) Software development kits (SDKs) You can integrate with Atlan in several ways. We recommend using one of our software development kits (SDKs) , if possible, which: encode best practices, are built for performance, and add validations and simplifications not found in the raw REST APIs themselves. Did you know? Our SDKs are nothing more than client libraries that wrap our underlying REST APIs. They prevent you from needing to reinvent the wheel by wrapping the REST APIs yourself. Tags: api rest-api graphql Previous Custom solutions Next Atlan's open API"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/delegate-administration",
    "text": "Configure Atlan Access control Manage users and groups Delegate administration On this page Delegate administration Atlan allows you to define granular access controls and delegate administrative functions with admin subroles. Atlan currently supports the following built-in admin subroles: Workflow admin Â   -  the workflow admin subrole allows Atlan admins to: Grant administrative access to users to manage connectors and connection workflows only. Restrict access to admin capabilities in the admin center and governance capabilities in the governance center. Governance admin -  the governance admin subrole allows Atlan admins to: Grant administrative access to users to manage governance capabilities only. Restrict access to admin capabilities in the admin center and connectors and connection workflows in the workflow center. Assign a subrole â Who can do this? You will need to be an admin user in Atlan to assign an admin subrole. To assign an admin subrole: From the left menu of any screen in Atlan, click Admin . Under Workspace , click Users . To assign an admin subrole, you can either: To assign the subrole to an existing user, navigate to any user and click the Role dropdown. In the Select Role dialog, click Workflow Admin or Governance Admin and then click Update . To assign the subrole to a new user, follow the steps in How to invite new users without SSO. Change the role of the user to Workflow Admin or Governance Admin and then click the Send Invite button. Workflow admin â The workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows. Permissions â A workflow admin has the following permissions and capabilities: Connections : Create a new connection for supported sources View all connections Manage all connections from the Connections tab in the Governance center Edit an existing connection   -  the user must also be a connection admin for that specific connection or have a policy granting them access to the connection. Workflows : Create and manage workflows from the Workflow center View all workflows and workflow runs Edit or delete any workflow credentials   - connection admin access not required Run any workflow Add, remove, or edit schedules for any workflow The following capabilities work exactly as that of a member user : Asset search and discovery -  can update metadata for assets in a connection that the workflow admin either created or was added to as a connection admin. Glossary -  can view all glossaries but will require edit access through glossary policies . If glossary restrictions are in place , then the workflow admin will only be able to view the glossaries as per their glossary policies. Insights -  requires data policies to query data and preview sample data. Reporting center -  if enabled by admins , can view the assets, glossary, Insights, and usage and cost dashboards. Data products -  requires domain policies to access domains and products. Restrictions â A workflow admin has the following explicit restrictions: Can only access the Connections tab in the Governance center . Cannot delete any existing connections using the Connection Delete workflow . Cannot access or perform any actions in the Admin center . Is excluded from the default All Admins group in any workflow configuration. (Optional) Restrict workflow visibility â By default, all workflow admins can see the existence of all workflows. However, you may want to limit specific teams from being able to see all workflows in Atlan. You can optionally turn off the default behavior to restrict workflow visibility. Once you have turned off the default behavior, in the Workflow center : The Monitor tab will no longer be visible to workflow admins. The Manage tab will display only the workflows created by workflow admins themselves. If there are no existing workflows, a workflow admin will only have access to the Marketplace tab to create a new one. To restrict workflow visibility: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn off Allow workflow admins to access all workflows . Your workflow admins will now only have access to the workflows they created by default. If you'd like to restore the default behavior, follow the steps above and then turn it on. Governance admin â The governance admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for managing the governance center. Permissions â A governance admin has the following permissions and capabilities: Personas : Create and manage personasÂ from the GovernanceÂ center View all personas Edit users and policies for existing personas   -  the user must either also be a connection admin or have a policy granting them access to the persona. Purposes : Create and manage purposes from the Governance center View all purposes Edit users and policies for existing purposes   -  the user must either also be a connection admin Â or have a policy granting them access to the purpose. Governance workflows -  create and manage governance workflows Playbooks -  create and run playbooks Policy center -  create and manage data governance policies README templates -  create and manage README templates Tags -  create and manage tags Domains -  only manage domains, cannot create them Custom metadata , badges , and options -  create and manage custom metadata and associated properties The following capabilities work exactly as that of a member user : Asset search and discovery -  can update metadata for assets in a connection that the governance admin was added to as a connection admin. Glossary -  can view all glossaries but will require edit access through glossary policies . If glossary restrictions are in place , then the governance admin will only be able to view the glossaries as per their glossary policies. Insights -  requires data policies to query data and preview sample data. Reporting center -  if enabled by admins , can view the assets, glossary, Insights, and usage and cost dashboards. Data products -  requires domain policies to access domains and products. Restrictions â A governance admin has the following explicit restrictions: Cannot access or perform any actions in the Admin center or Workflow center . Cannot access metadata and data policies if the user is neither a connection admin nor has a policy granting them access to a persona or purpose. Cannot access the Connections tab in the Governance center . Is excluded from the default All Admins group in any workflow configuration. Tags: workflow automation orchestration Previous Manage user authentication Next Automatically assign roles Assign a subrole Workflow admin Governance admin"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso",
    "text": "Configure Atlan Integrations Identity Management SSO Troubleshooting Troubleshooting SSO On this page Troubleshooting SSO Can I change the username of a provisioned user in Atlan? â No, once you have integrated SSO in Atlan , the usernames of provisioned users will be dependent on your SSO provider. For example, if a username has changed due to an automation at source or in the case of a migration from one provider to another, you will not be able to update usernames in Atlan. Usernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. Ensure that your username in the SSO provider matches that in Atlan. How does group mapping sync affect my current SSO setup? â If your groups in Atlan are mapped to their corresponding groups in the SSO provider, then any changes in group membership at source will be synced to Atlan: For any groups in the SSO provider not mapped to Atlan or vice versa, there will be no changes to your current SSO setup. If a user is part of a mapped group in Atlan but not in the SSO provider, the user will be removed from the mapped group in Atlan. If a user is part of a mapped group in the SSO provider but not in Atlan, the user will be added to the mapped group in Atlan. Tags: integration setup Previous SSO integration with PingFederate using SAML Next Troubleshooting connector-specific SSO authentication"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-associated-terms",
    "text": "Configure Atlan Administration Feature Management How to enable associated terms On this page Enable  associated terms Who can do this? You will need to be an admin user in Atlan to enable associated terms . To enable associated terms, follow these steps. Enable associated terms â To enable associated terms for your users: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Glossary Â heading of the Labs page, for Term attributes , click the No attributes applied dropdown.Â From the terms menu, select the associated terms you'd like to enable for your users.Â Your users will now be able to add associated terms to their glossaries! ð If you'd like to turn off any of the associated terms, follow the steps above and then deselect the corresponding checkboxes. Tags: glossary business-terms definitions Previous Disable user activity Next How to enable discovery of process assets Enable associated terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-governance-workflows",
    "text": "Build governance Stewardship Workflow Management Create governance workflows On this page Create governance workflows Who can do this? You must be an admin user in Atlan to enable , create, and manage governance workflows. Atlan provides no-code governance workflow templates with predefined steps. To create a governance workflow , complete the following steps. Create a governance workflow â To create a governance workflow: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Governance workflows . Click the + Workflow button to create a new governance workflow. Select a workflow template â To select a workflow template: From the Create new workflow menu, select the workflow template best suited to your use case: Change management New entity creation Access management Policy approval -  if policy center module is enabled In the upper right of the screen, click the Use template button to begin. In the New workflow dialog, enter the following details: For Name , enter a meaningful name for your workflow. (Optional) For Description , enter a brief description of your workflow. (Optional) Select an icon to represent your workflow. Click Create to create your workflow. Select the scope of workflow â Governance workflows must either be associated with assets, including data products, or certain actions in Atlan. If a user submits an access or update request, your workflow will be triggered to provision access or approve the update request, respectively.Â To select assets for the scope of your workflow: For When assets match rules , define the scope of your workflow to specific assets. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select a relevant option: Click Connection and then select an existing connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags.Â Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, click View Â for a preview. At the bottom of the form, click the Save & Continue button. (Optional) Conditional Branching â Conditional branching enables you to define dynamic paths within governance workflows based on specific rules. Each branch can have its own set of conditions, approvers, forms, and auto-approval settings. Conditional branching is currently supported only for the Change Management and New Entity Creation workflow templates. Change Management Template â Condition based on Metadata Attribute Type Apply approval logic only when specific metadata attributes are changed (e.g., tags, owners, certificates). Enable up to 10 conditional branches Supported attributes include: Name Alias Announcement Description Certificate Domain Owners Tags Terms Readme Custom metadata Each branch can include multiple conditions â if any condition is satisfied, the branch will trigger. Conditions cannot be reused across branches. Fallback Path (Change Management) Triggered when no other branch conditions match Allows exclusions â you can uncheck specific attributes that should not trigger approval Metadata changes grouped as: All asset metadata changes (common attributes) Glossary-specific changes AI asset-specific changes Product and domain-specific changes Only select applicable change types based on the asset universe. For example, if glossary assets aren't in scope, glossary-specific changes won't be triggered. New Entity Creation Template â Use this template to route requests based on where a new term, category, or product is being created. Define routing conditions based on the entity's destination. You can also set conditions at the category level. Each branch can include multiple conditions; if any condition is met, that branch is triggered. Attach different input forms to each branchâor reuse the same form, to collect context-specific information from requesters. This enables personalized context collection depending on the route taken. If no conditions are met, the fallback path is triggered automatically. (Optional) Collect information from requesters â Linking forms to your governance workflows allows you to collect more information from requesters as they raise requests, thus enabling approvers to make informed decisions.Â You can currently embed forms within the following governance workflow templates: Access management New entity creation To enable collecting information from requesters: For Collect information from requester , toggle on the Require requestor information slider. To link a form to your governance workflow, you can either: Click + Create new to create a new form . For Input form , click the dropdown to select an existing form. You can optionally preview or edit your selected form. At the bottom of the form, click the Save & Continue button. (Optional) Enable auto-approval â You can set up specific conditions for auto-approval of requests to reduce the need for human intervention. For example, you can enable auto-approval of: Data access requests from new users in your team to facilitate faster onboarding. Metadata update requests for assets with no restrictions. To enable auto-approval of requests: For If auto approval is enabled , toggle on the Auto approve request slider. Once you have turned on auto-approval, you can specify a subset of assets or a list of approved users that qualify for automated approval. You can either: For Filter assets for auto approval , create a subset of assets for auto-approval. Follow the steps in Select the scope of workflow to filter your selections. For Auto-approve eligible users requests , configure the following: Click + Add users/groups to select individual users or groups whose requests can be cleared for auto-approval. Click + Add owners to select asset owners whose requests can be cleared for auto-approval. At the bottom of the form, click the Save & Continue button. Set up manual approval â You can set up the approval process for requests and identify approvers. This ensures that each request is reviewed and authorized by designated approvers. Approvers can be individual users, user groups, or a combination of both. To set up the manual approval process: For Select approvers and process , determine the approval strategy from the following options: To enable any one approver from a list of approvers to approve requests, click Anyone approves . This means if any one approver approves or rejects a request, the workflow will be completed. To enable all selected approvers to approve requests in no particular order or simultaneously, click All approve - Parallel . This means that requests will go to all designated approvers and must be approved by all. To enable all selected approvers to approve requests in a predefined order, click All approve - Sequential . This means that the request will go through a particular order for approval and must be approved by all. For Who can approve requests , designate approvers: Click + Add users/groups Â to select individual users or groups as approvers. If you have selected the sequential option, you can drag and sort approvers to create a custom order for approval. Click + Add owners to select asset owners as approvers. If you have selected the sequential option, you can drag and sort approvers to create a custom order for approval. Click + Add users/groups to add at least one additional approver other than the selected asset owner(s). This is a mandatory step if you have designated asset owners as approvers. For Request expiry period , set the minimum or maximum number of days during which the approval window will be open. Non-approval will lead to automatic rejection of the request. At the bottom of the form, click the Save Continue button. Did you know? Atlan recommends that you limit your group selection for automated and manual approval to groups with fewer than 100 users. This will ensure that your governance workflows and approvals run smoothly. Review and publish workflow â If you'd like to continue working on your workflow, you can save it as a draft. If your workflow is ready, you can proceed to publishing it. Review all your selections, and then to publish your governance workflow: In the upper right of the screen, click the Publish button. Congratulations, your governance workflow is now active! ð Any requests on assets within the scope of your workflow will be immediately routed through the workflow you just created. Requesters will be notified about the outcome of their requests through the task inbox . Did you know? For governed assets, you can open the activity log to view whether it was updated using governance workflows, an approval timeline, who requested the change, and approvers that approved it. Tags: workflow automation orchestration Previous Automate data governance Next Manage governance workflows Create a governance workflow Select a workflow template Select the scope of workflow (Optional) Conditional Branching (Optional) Collect information from requesters (Optional) Enable auto-approval Set up manual approval Review and publish workflow"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/revoke-data-access",
    "text": "Build governance Stewardship Access Management Revoke data access Revoke data access Who can do this? You must be an admin user to revoke data access on governed assets in Atlan. As an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the scope of governance workflows . To revoke data access, complete the following steps. To revoke data access: From the left menu of any screen in Atlan, click Assets . Click on an asset to open the asset sidebar. In the right menu of the Overview sidebar, click the Access tab. In the Data access in Atlan tab, you can view the list of users that have data access to query data and preview sample data. Hover over the username to revoke data access and then click Revoke . In the Revoke data access dialog, you can: For Review and modify how (username) can access this asset in Atlan , you can revoke access provided through the following access control mechanisms: Personas Purposes Governance workflows Connection admin Click Raise Jira ticket to revoke data access on source to revoke data access through Jira. You will need to integrate with Jira Cloud and install or register a webhook to use this option. Click Raise ServiceNow request to revoke data access on source to revoke data access through ServiceNow. You will need to integrate with ServiceNow and link your ServiceNow account in Atlan to use this option. If a webhook has been configured for revoking data access in a source tool, you can optionally add a comment before proceeding with your revocation request. Click Revoke to confirm. (Optional) Click View to view your request(s) in the inbox . Tags: workflow automation orchestration Previous Manage policies Next Create forms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-governance-workflows",
    "text": "Build governance Stewardship Workflow Management Manage governance workflows On this page Manage governance workflows Who can do this? You must be an admin user in Atlan to enable , create , and manage governance workflows. Once you have created governance workflows, you can manage and modify your workflows and monitor requests from the Governance workflows dashboard. Manage governance workflows â To manage governance workflows: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Governance workflows . From the Overview tab, you can view the following: In the Activity section, monitor governance workflows by status and type. In the Requests section, monitor requests on assets within the scope of your workflows. The default date range for requests is set to 14 days. You can also view requests for the last 7, 30, or 45 days, or a custom date range of your choice. In the Workflows section, view the governance workflows you created or recently viewed. Change to the Definitions tab to modify your workflows: Filter your existing workflows by Published , Draft , or Disabled status. To edit a workflow, click the name of your workflow and edit it. To disable a workflow, hover over a workflow and then click the horizontal 3-dot icon. From the dropdown, click Disable workflow to disable it. Change to the Monitor tab to view all requests on governed assets: Filter requests on governed assets by Open , Approved , Rejected , or Archived status. Click the Request by filter to filter requests from specific users. Click any request to track the progress on that request and view the governance trail in detail. You can also view all other requests on a specific workflow. Tags: workflow automation orchestration Previous Create governance workflows Next Manage tasks Manage governance workflows"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-tasks",
    "text": "Build governance Stewardship Workflow Management Manage tasks On this page Manage tasks Who can do this? Anyone with access to Atlan   -  admin, member, or guest user   -  can use the inbox. The inbox in Atlan allows you to take action on and monitor requests waiting for your approval. Additionally, you can track any requests that you may have raised. For requesters, the task can help you: Track the progress of your requests as soon as you have raised them. Manage your alerts and requests all in one place. View requests listed in order of when the approval timeline expires. For approvers, the inbox can help you: Get notified immediately for requests that require your approval. Manage your alerts, approvals, and tasks all in one place. View tasks listed in order of when the approval timeline expires. Approve or reject requests, along with comments. Completed requests and tasks in your inbox are retained throughout the lifecycle of the Atlan instance for your organization. Manage requests â To manage requests: From the top right of any screen in Atlan, click the Inbox icon. In the inbox, switch to the Created by me tab to track your requests. (Optional) Filter requests by Open , Approved , Rejected , or Archived Â status. (Optional) To withdraw your request, in the top right of your request page, click the Withdraw button. Manage tasks â To manage tasks: From the top right of any screen in Atlan, click the Inbox icon. In the inbox, from the Assigned to me tab, select a task to review. (Optional) Filter tasks by status   - Open , Approved, Rejected , or Archived . To take action on a task, from the top right of the task page, you can either: Click Approve to approve the request. (Optional) In the Approve dialog, for Add comment , add a comment and then click Approve . Click Reject to reject the request and send back for revision. (Optional) In the Reject dialog, for Add comment , add a comment and then click Reject . Get notified on Slack â If your organization's Slack account is integrated with Atlan , you will receive Slack notifications for your approvals and requests. To receive Slack notifications on your approvals and requests: Integrate Slack and Atlan -  for Request notifications , toggle on the slider to receive Slack notifications when requests are raised in Atlan and approve or reject them directly from Slack. The email address used for Slack and Atlan should be the same, even if you haven't personally integrated the accounts. The Slack app should have been installed before August 12, 2022. If installed later, you'll need to update Slack. If different email addresses were used for Slack and Atlan, you'll first need to link your Slack account with Atlan . Tags: alerts monitoring notifications Previous Manage governance workflows Next Automate policy compliance Manage requests Manage tasks Get notified on Slack"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/set-up-glossaries",
    "text": "Build governance Glossary Get Started Set up glossaries On this page Set up glossaries The Atlan glossary allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy. Set up a glossary â To define the relevant terms and categories for your data assets, you will first need to set up a glossary. To create a glossary: From the left menu of any screen in Atlan, click Glossary and then click Get started . In the Create new glossary dialog, enter the following details: For Glossary name , enter a name for your glossary   -  for example, Finance . The character limit for a glossary name is 80 characters. In the top right, Draft is set as the default certificate. To change the certificate, click the dropdown arrow and select the certificate you'd like to apply to your glossary. Click the glossary icon to personalize the icon for your glossary. For Description , write a short or detailed description for your glossary   -  size limit for description values is 32766 bytes. For Add owners , add yourself or anyone else in your team as owners of the glossary. Click Create to add your glossary. (Optional) From the top right of the glossary profile: Click the user avatars to view a list of recently visited users, total views on your glossary, total number of unique visitors, and total views by user. Use the days filter to filter glossary views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your glossary . Click the clipboard icon to copy the link for your glossary. Click the pencil icon to edit the glossary name, description, and icon. Click the Slack or Teams icon to share directly on a Slack or Microsoft Teams channel. Click the bell icon to enable Slack or Microsoft Teams notifications for glossary updates in Atlan. Click the 3-dot icon and then: Click Add announcement to add an announcement to your glossary. Click Bulk upload terms to bulk upload terms to your glossary. Click Export to export nested categories and terms within a glossary to spreadsheets. Click Archive to archive the glossary. Your glossary is now ready for you to start adding terms and categories ! ð Add new glossary terms â Terms are the building blocks of your glossary. While defining a new glossary term, add as much information as possible for your term so that your team fully understands how to use it. To add a new glossary term: On the Glossary page, click the + icon next to All glossaries and then click Add term from the dropdown. In the Create new term dialog, enter the following details: For Select glossary , select a glossary for your term. In this example, we'll select the Finance glossary. For Term name , enter a name for your term   -  for example, Credit Score . The character limit for a term name is 80 characters. In the top right, Draft is set as the default certificate. To change the certificate, click the dropdown arrow and select the certificate you'd like to apply to your term. (Optional) For Alias , add an alias to your term. For Description , write a short or detailed description for your term   -  size limit for description values is 32766 bytes. For Add owners , add yourself or anyone else in your team as owners of the glossary. (Optional) Turn on Create multiple to create more terms from the same dialog. Click Create to add your term. (Optional) From the top right of the term profile: Click the user avatars to view a list of recently visited users, total views on your term, total number of unique visitors, and total views by user. Use the days filter to filter views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your term . Click the clipboard icon to copy the link for your term. Click the pencil icon to edit the term name and description or add an alias to your term. Click the Slack or Teams icon to share directly on a Slack or Microsoft Teams channel. Click the bell icon to enable Slack or Microsoft Teams notifications for glossary updates in Atlan. Click the 3-dot icon to add an announcement or archive the term. Did you know? Once you've added terms to your glossaries, you can also link them to your assets . Update your glossary terms â You can also add a term to your glossary without attaching a certificate or adding an owner at first. Once you have completed adding a term, navigate to the sidebar next to the term profile: Click + under Owners to assign owners for a term. Click Draft to update the certificate for a term. Choose from four certificate options   - Draft , Verified , Deprecated , and No certificate . Click + under Tags to classify the key characteristics of your term and configure tag propagation for linked assets . Click + under Categories to assign a term to a particular category. Click + under any of the associated term options to create relationships between terms. Did you know? Adding an owner to your term can help your teammates figure out who is an expert on a glossary term. This is the person they should reach out to if they have any questions about the term or would like to collaborate on updating it. Add new glossary categories â You can add categories to your glossary to better organize your terms and create a hierarchy of information. To add a category to your glossary: On the Glossary page, next to the name of your glossary in the left, click the three horizontal dots icon and then click Add category . In the Create new category dialog, enter the following details: For Category name , enter a name for your category   -  for example, Personal Finance . The character limit for a category name is 80 characters. In the top right, Draft is set as the default certificate. To change the certificate, click the dropdown arrow and select the certificate you'd like to apply to your category. For Description , write a short or detailed description for your category   -  size limit for description values is 32766 bytes. For Add owners , add yourself or anyone else in your team as owners of the glossary. (Optional) Turn on Create multiple to create more categories from the same dialog. Click Create to add your category. Next to the category name in the left menu, click the three horizontal dots icon and then add new terms or subcategories to your category. (Optional) From the top right of the category profile: Click the user avatars to view a list of recently visited users, total views on your category, total number of unique visitors, and total views by user. Use the days filter to filter views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your category . Click the clipboard icon to copy the link for your term. Click the pencil icon to edit the category name and description. Click the Slack or Teams icon to share directly on a Slack or Microsoft Teams channel. Click the bell icon to enable Slack or Microsoft Teams notifications for glossary updates in Atlan. Click the 3-dot icon and then: Click Add announcement to add an announcement to your category. Click Export to export nested terms within a category to spreadsheets. Click Archive to archive the category. Move terms and categories â You can move terms and categories within and across glossaries to better organize your business context. Move terms to a different category or create subcategories within the same glossary or across your glossaries in Atlan. You will need the following permissions: Moving a term or category from one glossary to another   -  read, update, and delete permissions on both glossaries. Moving a term or category within the same glossary   -  update permission on the glossary you want to reorganize. To move an existing term or category: From the left menu of any screen in Atlan, click Glossary . In the left menu of the Glossary page, you can either: Drag and drop a term or category into the relevant category within the same or a different glossary. In the popup, click Confirm to confirm the changes. To the right of the term or category name, click the three dots icon and then click Move to . In the Move to dialog, select a relevant category within the same or a different glossary and then click Move to confirm the changes. Search for glossary terms â There are two ways to search for glossary terms: In the left panel of the Glossary page, type the name of your term in the search bar and select your preferred option from the search results. Click the > icon preceding the name of a category to expand the full list of nested terms in that category. Add READMEs to your assets â For glossaries, terms, and categories, the asset profile provides a helpful summary. For example, the Linked assets section displays all the data assets that are linked to a particular term. This is also where you can add a README . The secret to making your glossary really useful is to provide as much information as possible. Adding a README will allow you to state your objectives for defining a glossary unit in greater detail. Inspect glossary terms and categories â The navigation bar to the right of the asset profile provides high-level information about the glossary item you are looking at. Here's what you can view: Overview shows key characteristics of a glossary term or category and helps you understand its relationship to other items in a glossary. Activity displays the changelog for your glossary items. For instance, you can find out who updated a term and when. Resources are links to internal or external URLs that can help your team better understand your glossary items. You can add links from GitHub, Google Docs, Google Sheets, or more as resources to your glossary item to provide additional context. Requests for a particular glossary item can be filtered by their status, such as Pending , Approved , and Rejected . Properties show the unique identification number of a glossary item and other properties. Integrations show Slack or Teams messages and Jira tickets pertaining to a particular glossary item. Add associated terms â Who can do this? You will need your Atlan administrator to enable associated terms -  except related terms. In order to inter-relate your terms , you will first need to set up a glossary and then add terms . To add relationships between terms: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under the glossary name, click the category in which your term is nested and then click the term you would like to enrich with an associated term. In the Overview tab of the term sidebar to the right, under Associated terms , click + to add relationships to your term. In the Associate terms dialog, configure the following: To select a term relationship: Click Related to to add a term that is related in some way . Click Recommended to add a standard form of use for the term . Click Synonyms to add a term that is similar in meaning . Click Antonyms to add a term that is opposite in meaning . Click Translates to to add a translated version of the term . Click Valid values for to add applicable values for the term . Click Classifies to add an umbrella category for the term . Click Classified By to add a term that falls under the purview of the term in use . For Select terms , select existing terms to associate. Click Associate terms to confirm your selections. The interrelated terms will reflect the relationships automatically in the term profile and sidebar. (Optional) Under Associated terms in the term profile, you can view a visual representation of your term relationships: Click any term attribute to focus on that specific term relationship.Â Click the minus or plus icons to zoom out or zoom in on the graph, respectively. Click the expand icon to enlarge the graph. Tags: glossary business-terms definitions Previous Glossary Next Bulk upload terms in the glossary Set up a glossary Add new glossary terms Update your glossary terms Add new glossary categories Move terms and categories Search for glossary terms Add READMEs to your assets Inspect glossary terms and categories Add associated terms"
  },
  {
    "url": "https://docs.atlan.com/tags/alerts",
    "text": "9 docs tagged with \"alerts\" View all tags Automate data governance You can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Configure alerts Set up real-time notifications for data quality rule failures via Slack or Microsoft Teams. Link your Microsoft Teams account To get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users. Manage tasks :::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can use the inbox. Security monitoring Learn about security monitoring. Send alerts for workflow events Learn how to configure alerts for workflow events in Atlan via email or Google Chat. Supported sources Learn about supported sources. Troubleshooting Fivetran connectivity Learn about troubleshooting fivetran connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity."
  },
  {
    "url": "https://docs.atlan.com/tags/notifications",
    "text": "9 docs tagged with \"notifications\" View all tags Automate data governance You can streamline your data governance requirements in Atlan with [governance workflows](/product/capabilities/governance/stewardship/how-tos/create-governance-workflows) and manage alerts, approvals, and tasks using the [inbox](/product/capabilities/governance/stewardship/how-tos/manage-tasks). Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Configure alerts Set up real-time notifications for data quality rule failures via Slack or Microsoft Teams. Create webhooks If your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or [submit a request](/support/submit-request). Link your Microsoft Teams account To get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users. Manage tasks :::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can use the inbox. Security monitoring Learn about security monitoring. Supported sources Learn about supported sources. Troubleshooting Fivetran connectivity Learn about troubleshooting fivetran connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/export-assets",
    "text": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to export assets On this page Export Assets Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to enable asset export . Atlan enables you to export all your assets or a filtered subset of assets to spreadsheets. Atlan currently supports exporting assets to: Google Sheets Microsoft Excel online Once your Atlan admin has integrated a supported tool, you will be able to export your assets and asset metadata to spreadsheets. Your existing permissions and access policies in Atlan will determine whether you can export assets, but at a minimum you'll require read permission on the assets you want to export. For example, you can: Export a list of assets that need enrichment and share the spreadsheet widely to crowdsource documentation. Export your business glossary and bring everyone up to speed on commonly used terminology across the organization. To export impacted assets, see How to download and export lineage . Did you know? Atlan currently limits the total number of assets you can export to 150,000 rows. Reach out to your customer success manager if you'd like to increase the limit for your organization. Supported assets for export â You can export assets from the following tabs: Assets tab â All assets Filtered subset of assets Child assets such as columns or fields from the parent asset profile Glossary tab â Glossary profile -  all categories and terms within a glossary Category profile -  all direct subcategories and terms within a category Term profile - linked assets for terms from the Linked Assets tab Reporting tab â Assets dashboard - total assets , archived assets, assets by certificates, SQL assets, assets without enrichment , assets with lineage , and assets with announcements Glossary dashboard - linked assets for terms Insights dashboard - query assets with certificate Usage & Cost dashboard - suggested assets for deprecation Enable asset export â Who can do this? You will need to be an admin user in Atlan to integrate a supported tool   -  Google Sheets and Microsoft Excel   -  and enable your users to export assets. For Atlan admins, you do not have to set the following permissions manually. You can simply sign in to your Google or Microsoft account while integrating the supported tool, which in turn will automatically set the permissions and grant appropriate access to your users. The export icon or button will only be visible if your Atlan admin has integrated a supported tool. If you cannot see the export icon or button, reach out to your Atlan admin to integrate Google Sheets or Microsoft Excel. Atlan uses the following permissions to integrate supported tools: Google Sheets: https://www.googleapis.com/auth/drive.file -  allows the app to see, edit, create, and delete only specific Google Drive files used with the Google Sheets app. This enables Atlan to create and update spreadsheets with exported assets. Refer to Google documentation . Microsoft Excel: Files.ReadWrite -  allows the app to read, create, update, and delete the signed-in user's OneDrive files. This enables Atlan to create and update spreadsheets with exported assets. Note that access is restricted to the signed-in user's files   -  Atlan does not access any shared files. Refer to Microsoft documentation . offline_access -  allows the app to access resources on behalf of the users, even when users are not currently using the app. Refer to Microsoft documentation . User.Read -  allows users to sign in to the app, and allows the app to read the profile of signed-in users. This enables Atlan to authenticate the user. Atlan only supports authenticating with an organizational account. Refer to Microsoft documentation . Did you know? For Microsoft users, if your global administrator has enabled the admin consent workflow , you will be prompted to request admin approval while attempting to integrate Microsoft Excel. Reach out to your admin to approve the admin consent request from the Microsoft Entra admin center. Additionally, if there is any expiry date set for such requests, ensure that the request is approved within that period of time, otherwise you will need to request approval again. To integrate a supported tool for exporting assets: From the left menu of any screen, click Admin . Under the Workspace heading, click Integrations . To connect Atlan to a supported tool for exporting assets: In the Google Sheets tile, click the Connect button. A sign-in dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click Allow to integrate Atlan with Google Sheets. In the Microsoft Excel title, click the Connect button. A sign-in dialog will appear and you will be redirected to sign in with your Microsoft account to integrate Atlan with Microsoft Excel. Your users can now export assets from Atlan! ð Export assets â Who can do this? Once an Atlan admin has integrated a supported tool, any admin, member, or guest user in Atlan with read permission on assets can export assets to spreadsheets. danger Atlan recommends that you avoid exporting assets during workflow runs. Exporting assets while you have workflows running in the background may lead to duplicate assets on the spreadsheet. Atlan allows you to export your assets to spreadsheets and view asset metadata in bulk. At a tenant level, Atlan supports running five exports concurrently for each supported tool. If both Google Sheets and Microsoft Excel have been integrated, a total of 10 concurrent exports is supported. At an individual user level, only one export is allowed for each supported tool at a time, the rest will be auto-queued for execution. To export your assets: From the left menu of any screen in Atlan, click Assets . To export assets, in the Assets page, you can either: Next to the search bar, click the 3-dot icon and then click Export to export all assets. Apply any filters , click the 3-dot icon, and then click Export to export a list of filtered assets. The Export dialog displays a total count of assets available for export: Click Google Sheets to export your assets to a Google Sheets spreadsheet. Click Microsoft Excel to export your assets to a Microsoft Excel spreadsheet. A sign-in dialog will appear and you will be redirected to sign in with your Google or Microsoft account. From the corresponding screen, click Allow to connect to Google Sheets or Microsoft Excel. To track the progress of the export, you can either: In the Export in progress popup, click Open Sheets or Open Excel to navigate to the spreadsheet. The Queued status will change to Success once assets have been exported. Click the 3-dot icon and then click Export to view asset export in progress, along with an estimated time of completion. (Optional) To cancel the export, hover over In queue and click the Stop button. On the spreadsheet, you will be able to view the following details and asset metadata: Status -  export status and total count of assets exported Title -  asset name and link Type -  asset type Connector -  name of supported source Business Name (Alias) -  business-oriented alias of assets, if any Created By and Created At -  username for user who created the asset and when it was created, only applicable to glossary exports Description - asset description , if any Owner Users and Owner Groups - asset owners , if any Certification Status and Certification Message - certification status of asset , if any Announcement Type , Announcement Title , and Announcement Message - announcements on assets , if any Tags and Propagated Tags - tags directly attached or propagated to an asset, if any Terms - linked assets Qualified Name -  fully qualified name of the asset GUID -  globally unique identifier of the asset Custom Metadata - customized metadata with organizational context, if any (Optional) To view your asset export history, click the 3-dot icon and then click Export . From the Export dialog, expand the History dropdown to view your last 10 exports. Note that only you can currently view your own export history. That's it, you've successfully exported your assets from Atlan! ð Update metadata in spreadsheets and sync to Atlan â Once you've exported your assets, you can use the Atlan extension for spreadsheets to update metadata for exported assets and sync your updates to Atlan. Atlan currently supports updating metadata and syncing updates for Google Sheets and Microsoft Excel. Google Sheets â Microsoft Excel â Update metadata â Who can do this? Any non-guest user with edit access to an asset's metadata can update metadata for exported assets and sync changes to Atlan. This only includes admin and member users. Atlan also recommends noting the following while updating metadata: If asset import from Atlan to a spreadsheet is in progress, avoid making any updates until the import is completed. You cannot make any changes to columns marked read-only or grayed out. Any changes to those columns will not be synced to Atlan and you will receive an error message. You can remove or change the order of assets in the spreadsheet, but you cannot add or replace it with an entirely new set of assets. You cannot add extra columns to the spreadsheet   -  sync to Atlan will fail. Avoid having any empty rows between assets. Although you can make changes in the spreadsheet while a sync is in progress, avoid starting a new sync until the current one has ended. You can easily collaborate with other users on the spreadsheet. However, sync to Atlan will only be recorded for the username that initiated the sync. Atlan uses hidden sheets to create a snapshot of the changes you've made. Refrain from making any changes in the hidden sheets. To update metadata for exported assets in a spreadsheet: Install the Atlan extension for: Google Sheets Microsoft Excel Connect your Atlan instance to: Google Sheets Microsoft Excel To update metadata for your exported assets: You can only make changes to the metadata in the following columns: Business Name (Alias) Description Owner Users and Owner Groups -  when adding an owner user or group, they must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple owners or groups as comma-separated values. Certification Status -  select valid values from the dropdown. Certification Message Announcement Type -  select valid values from the dropdown. Announcement Title Announcement Message Tags -  when adding a tag, it must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags as comma-separated values. Terms -  when adding a term, it must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple terms as comma-separated values. Custom Metadata -  enter custom metadata values for any properties you want to enrich based on the description and expected data type and format. Atlan uses two placeholder values, N/A indicates that the custom metadata property is not applicable on a specific asset and Not Permitted indicates that the user lacks permission to make metadata updates. Atlan recommends that you do not make any changes to these values. Neither of these values will be synced to Atlan. Additionally, Atlan currently only allows you to select a single value for Options data type when updating from spreadsheets. You cannot make the following changes: Edit headers for any of the columns. Edit the metadata in the following columns: Title Type Connector , Glossary and Categories Propagated Tags Qualified Name _GUID _ Created By and Created At Delete any columns or rows. danger Any of these changes will not be pushed to Atlan and you'll receive an error message. Sync to Atlan â danger If you do not have the permissions to update asset metadata in Atlan, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in the spreadsheet. Ensure that you have the requisite permissions to update an asset before syncing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. To sync your metadata updates to Atlan: To sync your metadata updates: For Google Sheets, in the menu bar of your Google Sheets spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Sync to Atlan . For Microsoft Excel, in the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Sync to Atlan . danger Atlan recommends that you do not change the name of your Microsoft Excel workbook if you want to sync metadata updates to Atlan, the sync will fail otherwise. Additionally, Microsoft Excel may take longer to autosave changes on the workbook. If you notice that zero changes were detected, retry syncing after a few seconds. Once the changes have synced, the Atlan sidebar will indicate completion of the sync. (Optional) At the bottom of the spreadsheet, switch to the Sync-Logs tab to view a record of your changes: View all changes synced to Atlan   -  the latest sync details will be displayed at the top of the spreadsheet. Changes are displayed on an asset level. For example, even if you have updated three attributes for an asset, it will be recorded as one change per asset. View the status of each update   - Synced or Failed . For Failed , Atlan will display a failure message: Invalid value -  for unsupported values. Unauthorized -  you do not have the permissions to update asset metadata. Not synced due to conflict -  if any attributes were updated in Atlan since the assets were last exported, Atlan will signal a conflict. In that case, you will need to export your assets once again with the latest changes to proceed. If the error message does not match any of the above options, reach out to Atlan support . Click any asset to verify the changes directly in Atlan. In Atlan, when viewing the activity log: An Updated via Google Sheets stamp will appear in the activity log for updated assets. Click the Google Sheets link to view the source spreadsheet from Atlan. An Updated via Microsoft Excel stamp will appear in the activity log for updated assets. Click the Excel link to view the source spreadsheet from Atlan. Tags: integrations spreadsheets assests Previous Download impacted assets in Microsoft Excel Next How to integrate Atlan with Google Sheets Supported assets for export Enable asset export Export assets Update metadata in spreadsheets and sync to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/bulk-upload-terms-in-the-glossary",
    "text": "Build governance Glossary Term Management Bulk upload terms in the glossary On this page Bulk upload terms in the glossary Who can do this? Currently, only member users with edit access on glossaries and admin users in Atlan can start a bulk glossary upload   -  you may not have access yourself. You can upload multiple terms and categories into a glossary using the bulk upload option: Download the template â A dynamic template can be generated per glossary. The template contains drop-down values for Type, Categories, Owner Users, Owner Groups and Tags. To download the template from within Atlan: Click on the desired glossary, choose Bulk Upload tab and proceed to download the sample template . Or to download the glossary bulk terms template from within Atlan: From the left menu, click Glossary . Terms and categories can only be added/updated to an existing glossary . Click All glossaries and then click the glossary you want to add terms to. To the right of the glossary's name in the glossary tree, click the Bulk Upload tab and proceed to download the sample template . Read the notes on each column before you start to edit the template. A file named {glossary_name}.xlsx downloads. You can edit this file with your existing terms and definitions in Microsoft Excel or Google Sheets. Populate the template â Populate the downloaded template following the template field structure. You can find the same details below in the comments embedded in the {glossary_name}.xlsx file. Column explanations â danger Except for the terms and category, these objects must already exist: users, groups, and tags. Term name â Enter the name of the term under the Name column. This is a mandatory field. Enter the term or category name. Avoid using @ in the name. If the name already exists, it updates the asset metadata based on the rest of the columns. Term names must be unique within a glossary, categories with same name can exist only at different levels in a glossary. Add all categories if term exists already. Avoid having empty rows in between. Did you know? You can use bulk upload to bulk edit term or category information. Type â Select the type of the entry in the Type column. This is a mandatory field. Specify whether the entry is a term or a category. Only Term or Category are valid options. Use drop-down to fill the value. Categories â Enter the categories to which to add the term in the Categories column. Multiple categories can be mentioned, as comma-separated values. You can manually add categories or select from drop-down options. Categories that don't exist get auto created. Even sub categories can get auto created. Use @ to define hierarchy (for example, Sales@USA), where sub-category USA is present/created under category Sales. Categories with same name can exist only at different levels. For example, one can add Term \"Customer\" at level 2 category with path Users@Users . Description â Enter the definition of the entry in the Description column. Business name â Enter the alternative name of the entry in the Business name column. User owners â Enter the usernames of owners in the User owners column. The users must already exist in Atlan. User owners are responsible for maintaining the data asset. You can manually add usernames or select from drop-down options. Enter valid Atlan usernames to assign ownership. Separate multiple owners with commas. Group owners â Enter the group names of owners in the Group owners column. The groups must already exist in Atlan. You can manually add group names or select from drop-down options. Enter valid group names responsible for the term. Separate multiple groups with commas. Certification â Enter the certification status in the Certification column. The value must be one of Verified , Draft , Deprecated , or leave empty. Keeping the cell empty removes the certificate status if it exists already. Certification message â Enter any additional information related to the certification in the Certification message column. Certification message is added only when Certification column contains non-empty value. Tags â Enter any tags to apply to the term in the Tags column. You can manually add tags or select from drop-down options. Tags must already exist in Atlan before being used. Comma-separated values can add multiple tags. Did you know? Tag propagation is disabled by default in Atlan. You can enable tag propagation to any assets linked to the term. Example â Following is a simple example of a filled in template, pivoted for readability: Column name Example value Name* Monthly Active Users Type* Term Description The number of users who are active over a month, often abbreviated MAU. Categories Metrics@Critical Metrics User Owners jsmith,jdoe Group Owners finance,ops Certification Verified Certification Message Agreed by governance council on May 10, 2021. Tags PII, Confidential Save as CSV or XLSX file â Before uploading the completed template to Atlan, save it as a CSV ( .csv ) or an XLSX ( .xlsx ) file. You may also need to check the following: Atlan requires the file to be comma-separated. If you are in a region where your preferred spreadsheet editor saves CSV or XLSX files with a separator other than a comma , you need to modify your spreadsheet's settings to use commas as separators. If your file contains special characters   -  for example, Ã± , Ã§ , Ãµ , and others   -  Atlan recommends uploading the completed template in an XLSX file to retain the special characters. If uploaded in a CSV file format, the bulk upload and decoding of special characters may fail. Important! For optimal results when converting XLSX files to CSV format, it's highly advised to use Google Spreadsheets, as this approach ensures better compatibility and accuracy during the conversion process. Upload the file â To upload your CSV or XLSX file to Atlan: From the left menu, click Glossary . Under All Glossaries , click the glossary you want to add terms to. (Terms can only be added to an existing glossary .) To the right of the glossary's name, click the Bulk Upload tab, and then use Click to upload or drag it here. . Click Select a CSV or XLSX file to upload * or drag and drop the file into the dialog. Review the summary of changes displayed and proceed . Monitor the upload â To track the status of your upload, from the glossary: View the history from the same Bulk Upload tab. If there are errors in your file, you see a message indicating Failed to upload . If you see the link Download file with errors , click it to download a copy of the file with notations on what the errors are. If you don't see this link, retry the process or reach out to Atlan support. Update your file, save your changes, and upload the edited file following the steps earlier. If your upload is successful, you see a message indicating Success . You can hover on the record to see View Summary . That's it, your terms and categories are now available in Atlan! ð Tags: glossary business-terms definitions Previous Set up glossaries Next Link terms to assets Download the template Populate the template Save as CSV or XLSX file Upload the file Monitor the upload"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/glossary-approval-issue",
    "text": "Build governance Glossary Troubleshooting Glossary update request approval issue On this page Glossary update request approval issue Why am I unable to approve a glossary update request? â If you encounter either of the following error messages while attempting to approve a glossary update request   - Request modification failed, try again or response status code does not match any response statuses defined for this endpoint in the swagger spec (status 409): {} , following may be the reasons why: The term or category to be created is already present in the glossary. The term or category to be updated has been archived. If neither of the above is true, reach out to Atlan support for help with debugging the issue. Tags: glossary business-terms definitions faq-governance Previous What is a glossary? Next Can I add duplicate glossary terms?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/add-duplicate-glossary-terms",
    "text": "Build governance Glossary FAQ Can I add duplicate glossary terms? Can I add duplicate glossary terms? Each term in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need. However, you can create terms with the same name and add them to separate glossaries. This can be helpful for terms that are common across different domains or teams   -  for example, the term Metrics can belong to the Finance , Marketing , and Product glossaries. In this case, when a user searches for the term Metrics , the glossary name will provide additional context, such as whether it belongs to the finance, marketing, or product team glossary. Tags: glossary business-terms definitions faq-governance Previous Glossary update request approval issue Next What is the default permission for a glossary?"
  },
  {
    "url": "https://docs.atlan.com/tags/glossary",
    "text": "18 docs tagged with \"glossary\" View all tags A Glossary A glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like `cost`, `P&L`, and `revenue` can be used to group and search all financial data assets. Bulk upload terms in the glossary Learn about bulk upload terms in the glossary. Can I add duplicate glossary terms? Each [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need. Enable  associated terms To enable associated terms, follow these steps. Glossary Learn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization. Glossary update request approval issue Learn about why am i unable to approve a glossary update request?. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. Link terms to assets Once you've [set up a glossary](/product/capabilities/governance/glossary/how-tos/set-up-glossaries), you can link terms from your glossary to your data assets in Atlan. Report on assets Learn about report on assets. Report on automations You can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions. Report on glossaries The [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further. Report on governance You can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup. Report on queries The Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs. Report on usage and cost The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. Restrict asset visibility Note that [glossary access works slightly differently](/product/administration/labs/how-tos/restrict-glossary-visibility). Restrict glossary visibility Restrict glossary visibility <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Set up glossaries The Atlan [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy. Summarize metadata The reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more."
  },
  {
    "url": "https://docs.atlan.com/tags/business-terms",
    "text": "17 docs tagged with \"business-terms\" View all tags A Glossary A glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like `cost`, `P&L`, and `revenue` can be used to group and search all financial data assets. Bulk upload terms in the glossary Learn about bulk upload terms in the glossary. Can I add duplicate glossary terms? Each [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need. Enable  associated terms To enable associated terms, follow these steps. Glossary update request approval issue Learn about why am i unable to approve a glossary update request?. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. Link terms to assets Once you've [set up a glossary](/product/capabilities/governance/glossary/how-tos/set-up-glossaries), you can link terms from your glossary to your data assets in Atlan. Report on assets Learn about report on assets. Report on automations You can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions. Report on glossaries The [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further. Report on governance You can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup. Report on queries The Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs. Report on usage and cost The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. Restrict asset visibility Note that [glossary access works slightly differently](/product/administration/labs/how-tos/restrict-glossary-visibility). Restrict glossary visibility Restrict glossary visibility <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Set up glossaries The Atlan [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy. Summarize metadata The reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more."
  },
  {
    "url": "https://docs.atlan.com/tags/definitions",
    "text": "18 docs tagged with \"definitions\" View all tags A Glossary A glossary is a list of terms that is organized in a specific way to help users understand their data assets. For example, terms like `cost`, `P&L`, and `revenue` can be used to group and search all financial data assets. Bulk upload terms in the glossary Learn about bulk upload terms in the glossary. Can I add duplicate glossary terms? Each [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need. Enable  associated terms To enable associated terms, follow these steps. Glossary Learn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization. Glossary update request approval issue Learn about why am i unable to approve a glossary update request?. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. Link terms to assets Once you've [set up a glossary](/product/capabilities/governance/glossary/how-tos/set-up-glossaries), you can link terms from your glossary to your data assets in Atlan. Report on assets Learn about report on assets. Report on automations You can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions. Report on glossaries The [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a [variety of filters](/product/capabilities/discovery/how-tos/use-the-filters-menu) to drill down further. Report on governance You can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup. Report on queries The Insights dashboard in the reporting center helps you track metrics for all your [queries](/product/capabilities/insights/how-tos/query-data) and query runs. Report on usage and cost The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. Restrict asset visibility Note that [glossary access works slightly differently](/product/administration/labs/how-tos/restrict-glossary-visibility). Restrict glossary visibility Restrict glossary visibility <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages private-preview\" /> Set up glossaries The Atlan [glossary](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy. Summarize metadata The reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/generate-lineage-between-assets",
    "text": "Use data Lineage Manage lineage Generate lineage between assets App On this page Generate lineage between assets App App You can use the Lineage Generator (no transformations) app to automatically create lineage between assets across two systems when their names match or follow a consistent pattern. This is useful for keeping track of how data flows between databases, warehouses, or storage systems without needing to configure every connection manually. Prerequisites â Before you begin, make sure you have: Access to the Lineage Generator (No Transformations) app. You can verify this by searching for Workflow Alerting in the Atlan marketplace. If you don't have access, contact Atlan support or your Atlan customer team to request it. At least one source connection and one target connection set up in Atlan with assets already crawled. Setup workflow â In your Atlan workspace, go to the homepage and click New workflow in the top navigation bar. Search for Lineage Generator (no transformations) , and then select Set up workflow . In the Workflow name field, enter a descriptive name such as: Staging to Reporting Lineage In the Source asset type property, select the type of asset from which lineage originates. For example, choose Table when working with a Snowflake staging schema. This limits scanning and matching to tables in the source prefix. For details on other supported asset types such as View, Materialized View, Column, or file-based objects, see the Source asset type . In the Source qualified name prefix property, enter the prefix that identifies the source assets. This narrows lineage generation to only those assets whose qualified names begin with the specified prefix. For example, if your Snowflake staging schema is stg , set the prefix to: default/snowflake/.../database_name/stg In the Target asset type property, select the type of asset to connect as the downstream target. For example, choose Table when linking Snowflake staging tables to reporting tables. This directs the workflow to search for target tables that match the source tables. For information on other supported asset types, including BI datasets and dashboards, see the Target asset type . In the Target qualified name prefix property, enter the prefix that identifies the target assets. This restricts lineage generation to assets whose qualified names begin with the specified prefix. For example, if your Snowflake reporting schema is rpt , set the prefix to: default/snowflake/.../database_name/rpt By default, the Case sensitive match property is set to Yes, while Ignore circular lineage and Match on schema are set to No. These defaults provide straightforward matching for a first run. For more information about how each option changes lineage generation, see the Lineage Generator reference . In the Output type property, select Preview lineage to generate a CSV preview of proposed matches without creating lineage in Atlan. Use this option to validate the results before making changes. For detailed explanations of each option, see the Output type . Run the workflow with Output type set to Preview lineage . This generates a CSV file showing the proposed matches between source and target assets. If the preview looks correct, update the Output type property to Generate lineage and run the workflow again. This applies the matches as lineage relationships in Atlan, making them visible in the lineage graph. You can configure advanced options such as regex transformations, schema-based matching, or child-asset lineage. These options help handle complex naming patterns, prevent false matches across schemas, and extend lineage to column-level detail. For more details, see the Lineage Generator reference . Need help? â If you have any issues related to configuring the app, contact Atlan support . See also â Lineage Generator (no transformations) : Detailed explanation of each configuration property, including valid values, examples, and behavior. Tags: lineage automation app data-lineage Previous Download and export lineage Next What is column-level lineage? Prerequisites Setup workflow Need help? See also"
  },
  {
    "url": "https://docs.atlan.com/product/connections/faq/snowflake-crawl-frequency",
    "text": "Connect data Connectivity Framework Connector Framework FAQ How often does Atlan crawl Snowflake? How often does Atlan crawl Snowflake? You can decide how often Atlan crawls Snowflake when setting up your Snowflake workflow in Atlan. If the Snowflake workflow is already set up and you'd like to know the current run schedule, you can click the Manage tab of the Workflow Run History for the last Snowflake run. The frequency will be on the top of the tab, next to the workflow name. Tags: workflow automation orchestration faq-connections Previous Can the Hive crawler connect to an independent Hive metastore? Next What column keys does Atlan crawl?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/troubleshooting/troubleshooting-anomalo-connectivity",
    "text": "Connect data Data Quality & Observability Anomalo Troubleshooting Troubleshooting Anomalo connectivity On this page Troubleshooting Anomalo connectivity What are the known limitations of the Anomalo connector? â Following are the known limitations of the Anomalo connector: If any of the checks that you've crawled in Atlan are deleted from your Anomalo instance, these will not be automatically archived in Atlan. You may have to manually archive your Anomalo checks from Atlan. If you exclude a previously included warehouse from an existing Anomalo workflow configuration, your Anomalo checks will not be automatically archived in Atlan. Atlan receives connector type and schema and table name metadata from Anomalo. If there are multiple assets from supported SQL sources in Atlan with the same schema or table name, your Anomalo checks on those assets will not be cataloged in Atlan. Tags: faq troubleshooting workflow automation orchestration Previous Preflight checks for Anomalo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx",
    "text": "Connect data ETL Tools Alteryx On this page Alteryx Private preview Overview: Connect your Alteryx environment with Atlan to automatically capture and catalog workflow metadata using OpenLineage. Discover Alteryx assets, visualize lineage, and drive data governance at scale. Get started â Follow these steps to configure the Alteryx integration and begin cataloging workflows in Atlan: Set up the Alteryx integration References â What does Atlan crawl from Alteryx :  Learn what metadata, workflows, and lineage Atlan ingests from Alteryx through OpenLineage. Troubleshooting â Troubleshooting Alteryx connectivity : Resolve common issues with connection setup, token validation, or missing assets. Tags: alteryx connector lineage workflows openlineage etl-tools Next Set up Alteryx Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/troubleshooting/troubleshooting-amazon-dynamodb-connectivity",
    "text": "Connect data Databases NoSQL Databases Amazon DynamoDB Troubleshooting Troubleshooting Amazon DynamoDB connectivity On this page Troubleshooting Amazon DynamoDB connectivity What are the known limitations of the Amazon DynamoDB connector? â Atlan currently does not support cataloging attributes for Amazon DynamoDB tables as native assets in Atlan. Hence, column-level lineage between Amazon DynamoDB as a source and supported data warehouses is currently not supported. Why are some tables and indexes missing? â Check that the IAM user defined for the crawler has been granted the dynamodb:DescribeTable permission on the missing tables. To grant permission on all DynamoDB tables in your selected AWS region, add the following to the IAM policy to Resource in the dynamodb:DescribeTable permission: arn:aws:dynamodb:<region>:<account_id>:table/* Check that the names of the missing tables do not match the table names in the Exclude tables regex filter , if you have specified tables to exclude. Why are some attributes missing from the table asset profile? â The Attributes section in the table asset profile of an Amazon DynamoDB table lists all the attributes that are either used as a partition key or a sort key in the table itself or its global or local secondary indexes. For example: The Orders table has a partition key OrderID and a sort key ProductID . This table also has a global secondary index with the partition key CustomerID . Keeping this in mind, the Attributes section in the table asset profile will display the following attributes only: OrderID ProductID CustomerID How to debug test authentication and preflight check errors? â Invalid region Invalid region. Please provide a valid region. Ensure that you have specified the correct AWS region in your configuration . The region must match the location of your Amazon DynamoDB tables. Invalid secret key Invalid secret key. Please provide a valid secret key. Ensure that the AWS access key ID matches the secret key . These must be paired correctly and belong to the same IAM user. Invalid access key Invalid access key. Please provide a valid access key. Ensure that the AWS access key ID matches the secret key . These must form a valid pair. Ensure that the IAM user associated with the access key is active and neither disabled nor deleted. No tables detected in the selected region No tables detected in the selected region. Please check the selected region. Ensure that you have specified the correct AWS region in your configuration . The region must match the location of your Amazon DynamoDB tables. Confirm that the IAM user or role you are using has the necessary permissions ( dynamodb:ListTables ) to view Amazon DynamoDB tables in the specified region. Access denied: 'ListTables' permission required Access denied: 'ListTables' permission required. Please reach out to your AWS administrator to request the permissions. Ensure that the IAM policy attached to your role includes the dynamodb:ListTables permission. Confirm that the role has the permissions to list tables. If you do not have the permission to update IAM policies, reach out to your AWS administrator to request the necessary permissions. Access denied: 'AssumeRole' permission required AccessÂ denied: 'AssumeRole' permission required. Please reach out to your AWS administrator to request the permissions. Ensure that the IAM policy attached to your user or role includes the sts:AssumeRole permission for the role you are trying to assume. Confirm that the IAM policy allows access to the specific role ARN you are trying to assume. Ensure that the role you are trying to assume has a trust policy allowing your user or role to assume it. Tags: lineage data-lineage impact-analysis catalog metadata discovery Previous What does Atlan crawl from Amazon DynamoDB?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/troubleshooting/why-is-my-databricks-lineage-api-not-working",
    "text": "Use data Lineage Troubleshooting Why is my Databricks lineage API not working? Why is my Databricks lineage API not working? Your Databricks workspace must be Unity Catalog-enabled to successfully extract lineage using APIs. The data lineage API is not supported if you're using Hive metastore. You will need to migrate your Hive tables and views to Unity Catalog. Atlan recommends extracting lineage for your Databricks assets using system tables . You must have a Unity Catalog-enabled workspace to use system tables. See How to set up Databricks . Tags: lineage data-lineage impact-analysis api rest-api graphql catalog metadata discovery Previous Troubleshooting lineage"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models",
    "text": "Configure Atlan Data Models On this page Data Models Overview: Data models in Atlan provide a framework for structuring and organizing your data assets. Use data models to create entity-relationship diagrams (ERDs), define relationships between assets, and maintain data consistency. Get started â How to view data models Guides â How to view data models : View and explore your data models in Atlan. Concepts â What are data models : Understand the fundamentals of data modeling in Atlan. Troubleshooting â Troubleshooting data models : Solutions for common data model problems. Tags: data-models erd data-modeling capabilities Next How to view data models Get started Guides Concepts Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting",
    "text": "Use data Reporting On this page Reporting Overview: Atlan's reporting capabilities help you generate insights about your data landscape. Create reports on governance, usage, queries, glossaries, and automations to track progress and make data-driven decisions. Get started â How to report on governance Guides â How to report on usage and cost : Generate reports on data usage and associated costs. How to report on queries : Track and analyze query patterns and performance. How to report on glossaries : Monitor glossary usage and coverage. How to report on automations : Track automation workflows and their impact. How to summarize metadata : Create concise summaries of your metadata. Tags: reporting analytics metrics capabilities Next Report on glossaries Get started Guides"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-asset-visibility",
    "text": "Configure Atlan Administration Feature Management Restrict asset visibility On this page Restrict asset visibility Who can do this? You will need to be an admin user in Atlan to restrict asset visibility. Note that glossary access works slightly differently . To build effective data governance, you need to control data access across your organization. Atlan handles this through access control policies . By default in Atlan, all users can see the existence of all assets. The All assets view includes all assets in your Atlan data estate. To change this default setting, see Disable all assets view . Generally, you may want to limit specific teams from being able to see all assets in Atlan. All you need to do is turn off the default behavior, so that your member and guest users will only have access to the assets curated through their personas and purposes . Admin users will still have full access to all assets, even when this default behavior is turned off. Summary â View all assets in Labs is OFF: To view any assets, a persona is required. Personas are necessary to grant access for viewing. View all assets in Labs is ON: All assets are visible by default, even without a persona. However, only the default metadata will be displayed and the rest will be locked. Personas are needed to restrict access to view specific assets. Example â Imagine a user who belongs to an Insurance Member Â purpose . Once you turn off the default behavior, this user will have access to: View details only about the curated assets in the Insurance Member purpose. Search and discover only the curated assets in the Insurance Member purpose. View details only about the related assets in the Insurance Member purpose. View only the linked assets in the Insurance Member purpose for glossary terms. View all assets in the lineage graph, but only view details in the sidebar forÂ assets in the Insurance Member purpose. If the user is a member of multiple purposes and personas, the assets they can access will be a superset of assets across all those purposes and personas. danger Turning off the default visibility of all assets currently does not apply to the Explorer section in Insights Â or the requests widget. Member and guest users will still be able to view all assets in Insights and the requests widget. Disable all assets view â The All assets view on the Assets page is enabled for all users by default. To turn it off for your member and guest users, complete the following steps. To disable all assets view: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn off View \"All assets\" in Assets Discovery . Your member and guest users will now only have access to the curated assets for their persona or purpose by default! ð If you'd like to enable the all assets view, follow the steps above and then turn it on. Did you know? If a user does not belong to any persona or purpose and the All assets view is disabled, then the user will be prompted to reach out to their Atlan administrator and request to be added to a persona or purpose. For more questions on restricting asset visibility, head over here . Tags: glossary business-terms definitions Previous How to enable scheduled queries Next Restrict glossary visibility Summary Example Disable all assets view"
  },
  {
    "url": "https://docs.atlan.com/tags/assets",
    "text": "3 docs tagged with \"assets\" View all tags Discovery FAQs Frequently asked questions about Atlan's Discovery capabilities. What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging. What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-discovery",
    "text": "One doc tagged with \"faq-discovery\" View all tags Discovery FAQs Frequently asked questions about Atlan's Discovery capabilities."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo",
    "text": "Connect data Data Quality & Observability Anomalo On this page Anomalo Overview: Catalog Anomalo checks, results, and metrics in Atlan. Gain visibility into data quality information, alerts, and governance for your data assets. Get started â Follow these steps to connect and catalog Anomalo assets in Atlan: Set up the connector Guides â Integrate Anomalo with Atlan : Configure the integration between Anomalo and Atlan for data quality monitoring. References â What does Atlan crawl from Anomalo : Learn about the Anomalo assets and metadata that Atlan discovers and catalogs. Preflight checks for Anomalo : Verify prerequisites before setting up the Anomalo connector. Troubleshooting â Troubleshooting connectivity : Resolve common Anomalo connection issues and errors. Tags: anomalo connector observability data quality connectivity Next Set up Anomalo Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/how-tos/configure-alerts",
    "text": "Build governance Data Quality Studio Advanced configuration Configure alerts On this page Configure alerts Set up real-time notifications for data quality rule failures in Atlan. Receive immediate alerts via Slack or Microsoft Teams when rules fail, enabling quick response to data quality issues. This guide shows how to configure organization-level alert destinations and set rule-specific alert priorities. Prerequisites â Before you begin, make sure you have: Administrative access to Atlan (for organization-level configuration) Access to a public Slack or Microsoft Teams channel Data quality rules already configured in your environment Configure organization-level alerts â Set up the alert destination for all data quality rules in your organization. This configuration applies to all rules and determines where alerts are sent. Only instance administrators can access this setting. IMPORTANT Only public channels are supported. Alerts can't be routed to private channels or Direct Messages at this time. Navigate to the Admin panel from the profile menu In the left-hand menu, select Integrations Choose your preferred messaging platform: Slack or Microsoft Teams Within the selected platform, scroll to the Data Quality section Enter the name of the public channel where rule failure alerts are delivered Click Update to activate the integration Once saved, the alerting configuration is in effect for all data quality rules based on their priority settings. Configure rule-level alert priority â Set alert priorities during rule creation or editing. This determines how frequently alerts are sent for each specific rule. Navigate to the data quality rule you want to configure Open the rule for editing or create a new rule In the rule creation workflow, scroll to the Additional Settings section All rules default to Normal priority unless explicitly changed by the user. Under Alerts , select the desired priority level: Normal (default): Alerts are sent up to three times per failure, then suppressed until the rule passes. Use this for most data quality rules. Urgent : Alerts are sent every time the rule fails. Use this for critical business rules. Low : No alerts are sent. Failures are silently logged. Use this for non-critical monitoring. Next steps â After completing these steps: Rule failures trigger alerts in the designated public Slack or Teams channel, based on priority Each alert includes full context - rule name, asset, severity, and relevant metadata - to aid quick triage and action Rule-level alert priority settings can be modified at any time by editing the rule Need help â If you have questions or need assistance with configuring alerts, reach out to Atlan Support by submitting a support request . See also â Set up Databricks - Configure Databricks for data quality monitoring Set up Snowflake - Configure Snowflake for data quality monitoring Tags: data-quality alerts notifications Next What's auto re-attachment Prerequisites Configure organization-level alerts Configure rule-level alert priority Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/enable-auto-re-attachment",
    "text": "Build governance Data Quality Studio Snowflake Data Quality Configure data quality Enable auto re-attachment of rules On this page Enable auto re-attachment of rules This guide explains how to configure your Snowflake environment to enable automatic reattachment of rules in Atlan. Prerequisites â Make sure the following conditions are met: The recreated asset has the same name and similar structure. The original rule is still present and not deleted. The feature is enabled for your tenant. Grant privileges â Below are the minimum privileges the atlan_dq_service_role role needs to reattach rules automatically. Replace <database-name> and similar placeholders with your own object names. Database and schema usage : Enables Atlan to discover and reference objects. GRANT USAGE ON DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; Table and view reference : Required for rules that reference columns in these objects. GRANT REFERENCES ON ALL TABLES IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; DMF schema and function usage : Enables execution of Snowflake Data Metric Functions created by Atlan. -- Grant usage on the helper database / schema holding DMF functions GRANT USAGE ON DATABASE ATLAN_DQ_DQ TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ_DQ . DMFS TO ROLE atlan_dq_service_role ; -- Grant usage on all existing and future functions within the DMF schema GRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ_DQ . DMFS TO ROLE atlan_dq_service_role ; GRANT USAGE ON FUTURE FUNCTIONS IN SCHEMA ATLAN_DQ_DQ . DMFS TO ROLE atlan_dq_service_role ; Future-proofing privileges : Ensures newly created objects are covered without manual grants. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; After these grants are applied, any table or view that's recreated with the same name automatically regains its attached rules, keeping your data quality checks continuous. Whatâs next â Once privileges are configured, rule reattachment happens automatically whenever matching assets are recreated in Snowflake. You can now continue monitoring your data quality workflows without needing to manually reapply rules. See also â Auto re-attachment rules : Understand how Atlan automatically reattaches rules to recreated Snowflake assets to maintain continuous data quality enforcement. Tags: snowflake data-quality auto-re-attachment Previous Enable data quality on connection Next Upgrade to Snowflake data quality studio Prerequisites Grant privileges Whatâs next See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/references/operations",
    "text": "Build governance Data Quality Studio Snowflake Data Quality References Operations On this page Operations Atlan crawls and manages the following data quality operations and results from Snowflake. Once you have enabled data quality on your connection , Atlan can perform data quality operations and retrieve results from your Snowflake environment. CUD operations for rules â When creating, updating, or deleting data quality rules: Component Details Procedure Atlan invokes the MANAGE_DMF stored procedure in Snowflake User Atlan DQ User Role atlan_dq_service_role Supported action types â Action Description ATTACH_DMF Attach a data metric function to a table DETACH_DMF Remove a data metric function from a table SUSPEND_DMF Pause execution of a data metric function RESUME_DMF Resume execution of a data metric function UPDATE_SCHEDULE Modify the execution schedule CREATE_DMF Create a new data metric function Procedure response â MANAGE_DMF provides only an acknowledgment response indicating successful execution Test SQL operations â For custom SQL rules, Atlan validates and tests SQL statements: Component Details Procedure Atlan triggers the MANAGE_DMF procedure for test SQL operations User Atlan DQ User Role atlan_dq_service_role Supported action types â Action Description EXECUTE_SQL Run custom SQL for testing VALIDATE_SQL_PERMISSIONS Verify SQL permissions Procedure responses â Action Response VALIDATE_SQL_PERMISSIONS Returns acknowledgment confirming permission validity EXECUTE_SQL Returns a scalar numeric result from executing the custom user-defined SQL statement, following rigorous validation Crawling â Atlan periodically crawls data from Snowflake to update rule results. Target tables â Table Purpose ATLAN_DQ.STORE.DQ_RULE Rule definitions ATLAN_DQ.STORE.DQ_RULE_RESULT Execution results Outcome â Crawled data populates Atlan's internal representation of rule execution results. Tags: snowflake data-quality operations reference Previous Upgrade to Snowflake data quality studio Next Roles and permissions CUD operations for rules Test SQL operations Crawling"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/references/data-quality-rules",
    "text": "Build governance Data Quality Studio What is Data Quality Studio References Rules and dimensions On this page Rules and dimensions This document lists the data quality rules and classification dimensions available in Snowflake Data Quality Studio. Predefined data quality rules â During the private preview, Atlan provides a set of predefined data quality rules, including: Blank & Null Checks Blank count Blank percentage Null count Null percentage Volume Checks Row count Freshness Metrics Data freshness tracking Statistical Insights Average value Minimum value Maximum value Standard deviation Uniqueness & Duplicates Duplicate count Unique count String Validations Regex String Length Valid Values Data quality dimensions â To provide better context and insights, Atlan classifies results into key data quality dimensions: â Accuracy : Verifying correctness and reliability â³ Timeliness : Validating data freshness and latency ð Validity : Checking data formats and constraints ð Completeness : Measuring missing or incomplete data ð Consistency : Maintaining data follows the same format and standards across datasets ð¢ Uniqueness : Verifying data records are distinct and free from duplicates ð Volume : Measuring data quantity and row counts See also â Set up Databricks - Configure Databricks for data quality monitoring Set up Snowflake - Configure Snowflake for data quality monitoring Tags: snowflake data-quality rules dimensions reference Previous Data quality permissions Predefined data quality rules Data quality dimensions See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/how-tos/set-up-databricks",
    "text": "Build governance Data Quality Studio Databricks Data Quality Get Started Set up Databricks On this page Set up Databricks Private Preview This guide walks through configuring Databricks to work with Atlan's data quality studio by creating the required service principal, setting up authentication, and granting the necessary privileges. Atlan recommends using serverless SQL warehouses for instant compute availability. System requirements â Before setting up the integration, make sure you meet the following requirements: Databricks Premium or Enterprise edition Serverless Compute for Jobs & Notebooks enabled Dedicated SQL warehouse for running DQ-related queries Outbound network access permitted from Serverless Compute (Enterprise tier only) Prerequisites â Before you begin, complete the following steps: Obtain Workspace admin and Metastore Admin or CREATE CATALOG privilege Identify your dedicated SQL warehouse for DQ operations Create an API token in Atlan that's stored in Databricks for authentication Review Data Quality permissions to understand required privileges Create service principal â Create the service principal that Atlan uses to perform Data Quality (DQ) operations within your Databricks workspace. Follow the appropriate guide based on your Databricks deployment environment: Creating a Service Principal in AWS based Databricks Accounts Creating a Service Principal in Azure based Databricks Accounts Store the following credentials securely: client_id client_secret tenant_id (Azure only) Service principal name Atlan recommends naming it: atlan-dq-service-principal Set up authentication: Choose one of the following authentication methods for your service principal: OAuth (Recommended) : Use the client_id , client_secret , and tenant_id (Azure only) from the service principal created in the previous step No additional configuration required Personal access token (pat) : Follow the Databricks Personal Access Token guide to generate a token for the service principal Store the token securely for use in the next steps Grant warehouse access: Grant the service principal access to a SQL warehouse that's used to run Data Quality queries. Go to your Databricks workspace UI Navigate to SQL > SQL Warehouses Click on the warehouse you want Atlan to use Click on the Permissions button Select the Service Principal ( atlan-dq-service-principal ) from the list Assign the Can Use permission Click Add Once access is granted, Atlan can use this warehouse to run SQL queries related to Data Quality operations. Set up Databricks objects â Create the required Databricks objects needed for the functioning of the Atlan Data Quality Studio. Create the atlan_dq catalog â The atlan_dq catalog is used by Atlan to store metadata, DQ rule execution results, and internal processing tables. Run the following SQL command in a Databricks notebook or SQL editor: CREATE CATALOG IF NOT EXISTS atlan_dq ; Set up secret scope and secret â Create a Databricks Secret Scope to securely store the Atlan API token. This token enables the service principal to authenticate and interact with Atlan's APIs. note Secret scopes and secret ACLs can only be managed using the Databricks CLI or REST API. These operations aren't supported through SQL. Create a new Secret Scope named atlan_dq : databricks secrets create-scope atlan_dq Save the Atlan API token in a secret named api_token in the scope: databricks secrets put-secret --json '{ \"scope\": \"atlan_dq\", \"key\": \"api_token\", \"string_value\": \"<ATLAN_API_TOKEN>\" }' Replace <ATLAN_API_TOKEN> with the API token value you created in Atlan. Grant privileges â Grant the following privileges to atlan-dq-service-principal so it can create internal objects, read the Atlan API token, and query data for quality checks. Replace placeholders with real values. Manage the atlan_dq catalog GRANT USE CATALOG ON CATALOG atlan_dq TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT CREATE SCHEMA ON CATALOG atlan_dq TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; Read the API token stored in the atlan_dq secret scope databricks secrets put-acl atlan_dq <SERVICE_PRINCIPAL_CLIENT_ID> READ Access data for quality checks (choose one scope) Catalog level GRANT USE CATALOG ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT USE SCHEMA ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT SELECT ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; Schema level GRANT USE CATALOG ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT USE SCHEMA ON SCHEMA < SCHEMA > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT SELECT ON SCHEMA < SCHEMA > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; Table level GRANT USE CATALOG ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT USE SCHEMA ON SCHEMA < SCHEMA > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT SELECT ON TABLE < TABLE > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; These grants let Atlan create its internal schemas, fetch the API token securely, and run SELECT queries needed for rule execution. Next steps â Enable data quality on connection - Configure your Databricks connection for data quality monitoring Need help â If you have questions or need assistance with setting up Databricks for data quality, reach out to Atlan Support by submitting a support request . See also â Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: databricks data-quality setup governance Previous Databricks Data Quality Studio Next Enable data quality on connection System requirements Prerequisites Create service principal Set up Databricks objects Create the atlan_dq catalog Set up secret scope and secret Grant privileges Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/set-up-snowflake",
    "text": "Build governance Data Quality Studio Snowflake Data Quality Get Started Set up Snowflake On this page Set up Snowflake Private Preview This guide walks through configuring Snowflake to work with Atlan's data quality studio by creating the required roles, setting up database objects, and granting the necessary privileges. System requirements â Before setting up the integration, make sure you meet the following requirements: Snowflake Enterprise or Business Critical edition Dedicated Snowflake warehouse for running DQ-related queries Prerequisites â Before you begin, complete the following steps: Obtain ACCOUNTADMIN role or equivalent administrative privileges in Snowflake Identify your dedicated warehouse name for DQ operations Have access to create a new Snowflake user for Atlan Review Data Quality permissions to understand required privileges Create roles â Create two roles for the integration: -- Create DQ Admin Role CREATE ROLE IF NOT EXISTS dq_admin ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE dq_admin ; -- Create Atlan Service Role CREATE ROLE IF NOT EXISTS atlan_dq_service_role ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_dq_service_role ; Create user â Create a dedicated Snowflake user for Atlan following your organization's standards, then grant the service role: GRANT ROLE atlan_dq_service_role TO USER < atlan_dq_user > ; Set up database objects â Create the database, schemas, and stored procedure required for Atlan data quality operations. Create the required database structure: -- Create database CREATE DATABASE ATLAN_DQ ; -- Create schemas CREATE SCHEMA ATLAN_DQ . SHARED ; CREATE SCHEMA IF NOT EXISTS ATLAN_DQ . DMFS ; The ATLAN_DQ database serves as a container for all objects related to Atlan Data Quality. The ATLAN_DQ.SHARED schema provides a separate namespace for shared procedures and functions, while the ATLAN_DQ.DMFS schema is required for custom Data Metric Functions (DMFs). Create stored procedure for DMF management: View procedure code /** * Manages Data Metric Functions (DMF) operations for Snowflake tabular entities. * This procedure handles various DMF operations including: * - Creating and managing DMFs (CREATE_DMF) * - Attaching/detaching DMFs to entities (ATTACH_DMF, DETACH_DMF) * - Managing DMF schedules (UPDATE_SCHEDULE) * - Executing SQL expressions (EXECUTE_SQL) * - Validating SQL permissions (VALIDATE_SQL_PERMISSIONS) * * The procedure runs with the privileges of the procedure owner and includes comprehensive * validation of all inputs and permissions before executing any operations. * * @param { string } ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE, CREATE_DMF, EXECUTE_SQL, VALIDATE_SQL_PERMISSIONS) * @param { string } ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param { string } ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param { string } [ DMF_NAME = null ] - Fully qualified name of the DMF (database.schema.name) * @param { string } [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param { string } [ SCHEDULE_TYPE = null ] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param { string } [ SCHEDULE_VALUE = null ] - Schedule value based on type * @param { string } [ DMF_DEFINITION = null ] - SQL expression defining the DMF * @param { string } [ ROLE_TO_CHECK = null ] - Role to check permissions for * @param { string } [ DATABASES_TO_CHECK = null ] - Comma-separated list of databases to validate permissions for * @param { string } [ SCHEMAS_TO_CHECK = null ] - Comma-separated list of schemas to validate permissions for * @param { string } [ TABLES_TO_CHECK = null ] - Comma-separated list of tables to validate permissions for * @returns { string } - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( ACTION STRING , ENTITY_TYPE STRING DEFAULT NULL , ENTITY_NAME STRING DEFAULT NULL , DMF_NAME STRING DEFAULT NULL , DMF_ARGUMENTS_JSON STRING DEFAULT '[]' , SCHEDULE_TYPE STRING DEFAULT NULL , SCHEDULE_VALUE STRING DEFAULT NULL , DMF_DEFINITION STRING DEFAULT NULL , ROLE_TO_CHECK STRING DEFAULT NULL , DATABASES_TO_CHECK STRING DEFAULT NULL , SCHEMAS_TO_CHECK STRING DEFAULT NULL , TABLES_TO_CHECK STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- /** * Executes a SQL query with parameters * @param { string } sqlText - SQL statement to execute * @param { Array } [binds=[]] - Array of bind parameters for the query * @param { boolean } [ returnFirstRow = false ] - Whether to return only the first row * @returns { Object } Object containing execution result or error information */ function executeQuery ( sqlText , binds = [ ] , returnFirstRow = false ) { try { if ( ! sqlText ) return { isErrored : true , message : \"SQL Text is required\" , result : null , } ; const statement = snowflake . createStatement ( { sqlText , binds } ) ; const result = statement . execute ( ) ; const response = { isErrored : false , message : \"\" , result : null , } ; if ( returnFirstRow ) { response . result = result . next ( ) ? result : null ; return response ; } response . result = result ; return response ; } catch ( err ) { return { isErrored : true , message : ` ${ err . code } - ${ err . message } - ${ sqlText } with binds: ${ binds . join ( \", \" ) } ` , result : null , } ; } } /** * Safely parses a JSON string * @param { string } jsonString - JSON string to parse * @returns { Object } Parsed JSON object or null if invalid */ function safelyParseJSON ( jsonString ) { try { return JSON . parse ( jsonString ) ; } catch ( err ) { return null ; } } /** * Validates a number within a range * @param { string } value - Number to validate * @param { number } min - Minimum value * @param { number } max - Maximum value * @returns { boolean } True if number is valid * @returns { boolean } False if number is invalid */ function isNumberValid ( value , min , max ) { const num = parseInt ( value , 10 ) ; return ! isNaN ( num ) && num >= min && num <= max ; } /** * Escapes and quotes a Snowflake identifier * @param { string } identifier - Raw identifier to normalize * @returns { string } Properly quoted identifier safe for SQL */ function normalizeIdentifier ( identifier ) { return ` \" ${ identifier . replace ( / \" / g , '\"\"' ) } \" ` ; } /** * Retrieves all columns for a given entity. Validates that the entityexists and procedure owner has access to it. * @param { string } entityName - Fully qualified entity name * @returns { Array } Array of column objects with name and dataType properties * @throws { Error } If entity doesn't exist or is inaccessible */ function getAllColumnsForEntity ( entityName ) { const sqlText = \"SHOW COLUMNS IN IDENTIFIER(?)\" ; const binds = [ entityName ] ; const { result , isErrored , message } = executeQuery ( sqlText , binds ) ; if ( isErrored ) { // Validates that the entity exists and procedure owner has access to it throw new Error ( message ) ; } const columns = [ ] ; while ( result . next ( ) ) { const column = { name : result . getColumnValue ( \"column_name\" ) , dataType : JSON . parse ( result . getColumnValue ( \"data_type\" ) ) . type , } ; if ( column . dataType === \"FIXED\" ) column . dataType = \"NUMBER\" ; columns . push ( column ) ; } return columns ; } /** * Validates that the DMF is valid and exists * @param { string } dmfName - Fully qualified name of the DMF * @param { string } dmfArguments - Arguments for the DMF * @returns { boolean } Whether the DMF is valid * @throws { Error } If DMF is invalid */ function isDMFValid ( dmfName , dmfArguments ) { const { isErrored , message } = executeQuery ( ` DESCRIBE FUNCTION IDENTIFIER(?)( ${ dmfArguments } ) ` , [ dmfName ] , true ) ; if ( isErrored ) throw new Error ( message ) ; return true ; } /** * Checks if a timezone is valid * @param { string } timezone - Timezone to validate * @returns { boolean } True if timezone is valid * @returns { boolean } False if timezone is invalid */ function isTimezoneValid ( timezone ) { const result = executeQuery ( ` SELECT CONVERT_TIMEZONE(?, CURRENT_TIMESTAMP()) ` , [ timezone ] , true ) ; return ! result . isErrored ; } /** * Generates a DMF type signature based on the arguments and entity columns * @param { Array } dmfArguments - Array of DMF arguments * @param { Object } entityColumnsMap - Map of entity names to column objects in the format { <ENTITY_NAME>: [ { name: <COLUMN_NAME> , dataType: <DATA_TYPE> } ] } * @param { string } baseEntityName - Name of the base entity * @returns { string } DMF type signature * @throws { Error } If entity not found in the cache */ function generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) { if ( ! dmfArguments || ! dmfArguments . length ) return \"\" ; const baseEntityColumns = entityColumnsMap [ baseEntityName ] ; if ( ! baseEntityColumns ) { throw new Error ( ` Entity ${ baseEntityName } not found in the cache ` ) ; } const baseEntityColumnArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => { const column = baseEntityColumns . find ( col => col . name === param . name ) ; return column ? column . dataType : null ; } ) . join ( \", \" ) ; const baseEntityArguments = ` TABLE( ${ baseEntityColumnArguments } ) ` ; const referencedEntityArguments = dmfArguments . filter ( param => param . type === \"TABLE\" ) . map ( entityParam => { const entityName = entityParam . name ; const entityColumns = entityColumnsMap [ entityName ] ; if ( ! entityColumns ) { throw new Error ( ` Entity ${ entityName } not found in the cache ` ) ; } const columnTypes = entityParam . nested . map ( nestedParam => { const column = entityColumns . find ( col => col . name === nestedParam . name ) ; return column ? column . dataType : null ; } ) . filter ( Boolean ) . join ( \", \" ) ; return ` TABLE( ${ columnTypes } ) ` ; } ) ; const arguments = [ baseEntityArguments , ... referencedEntityArguments ] . join ( \", \" ) ; return arguments ; } /** * Generates DMF arguments for SQL statements * @param { string } dmfArguments - Array of DMF arguments * @returns { string } Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"COLUMN\" ) { return normalizeIdentifier ( param . name ) ; } // Handle TABLE type with nested columns return ` TABLE( ${ normalizeIdentifier ( param . name ) } ( ${ param . nested . map ( nested => normalizeIdentifier ( nested . name ) ) . join ( \", \" ) } )) ` ; } ) . join ( \", \" ) ; } /** * Generates function parameters from DMF arguments * @param { Array } dmfArguments - Array of DMF arguments * @returns { string } Formatted function parameters */ function generateFunctionParameters ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"TABLE\" ) { const nestedParams = param . nested . map ( nested => ` ${ nested . name } ${ nested . dataType } ` ) . join ( \", \" ) ; return ` ${ param . name } TABLE( ${ nestedParams } ) ` ; } return ` ${ param . name } ${ param . dataType } ` ; } ) . join ( \", \" ) ; } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- /** * Validates that mandatory arguments are provided and valid * @throws { Error } If any mandatory argument is missing or invalid */ function validateMandatoryArguments ( ) { const VALID_ACTIONS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" , \"UPDATE_SCHEDULE\" , \"CREATE_DMF\" , \"EXECUTE_SQL\" , \"VALIDATE_SQL_PERMISSIONS\" ] ) ; const VALID_ENTITY_TYPES = new Set ( [ \"TABLE\" , \"VIEW\" , \"MATERIALIZED VIEW\" , \"EXTERNAL TABLE\" , \"ICEBERG TABLE\" ] ) ; const DMF_OPS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] ) ; const VALID_SCHEDULE_TYPES = new Set ( [ \"MINUTES\" , \"CRON\" , \"ON_DATA_CHANGE\" , \"NOT_SCHEDULED\" ] ) ; const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set ( [ \"MINUTES\" , \"CRON\" ] ) ; if ( ! VALID_ACTIONS . has ( ACTION ) ) throw new Error ( ` Invalid ACTION: \" ${ ACTION } \". Valid options are ${ Array . from ( VALID_ACTIONS ) . join ( \", \" ) } ` ) ; if ( ENTITY_TYPE && ! VALID_ENTITY_TYPES . has ( ENTITY_TYPE ) ) throw new Error ( ` Invalid ENTITY_TYPE: \" ${ ENTITY_TYPE } \". Valid options are ${ Array . from ( VALID_ENTITY_TYPES ) . join ( \", \" ) } ` ) ; if ( DMF_OPS . has ( ACTION ) && ! DMF_NAME ) throw new Error ( \"DMF_NAME is required for DMF related actions\" ) ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { if ( ! SCHEDULE_TYPE ) throw new Error ( \"SCHEDULE_TYPE is required for SCHEDULE action\" ) ; if ( ! VALID_SCHEDULE_TYPES . has ( SCHEDULE_TYPE ) ) throw new Error ( ` Invalid schedule type: \" ${ SCHEDULE_TYPE } \". Valid options are ${ Array . from ( VALID_SCHEDULE_TYPES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPES_THAT_REQUIRE_VALUE . has ( SCHEDULE_TYPE ) && ! SCHEDULE_VALUE ) throw new Error ( \"SCHEDULE_VALUE is required for SCHEDULE action\" ) ; } if ( ACTION === \"EXECUTE_SQL\" && ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to execute.\" ) ; } if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { if ( ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to validate permissions.\" ) ; } if ( ! ROLE_TO_CHECK ) { throw new Error ( \"Failed to fetch role to check permissions. Please ensure the role exists and is accessible.\" ) ; } if ( ! DATABASES_TO_CHECK && ! SCHEMAS_TO_CHECK && ! TABLES_TO_CHECK ) { throw new Error ( \"No databases, schemas, or tables provided. Please provide at least one database, schema, or table to validate permissions for.\" ) ; } } } /** * Parses a fully qualified name into its components * @param { string } fullyQualifiedName - Fully qualified name to parse * @returns { Object } Object with database, schema, and name properties * @throws { Error } If invalid fully qualified name */ function validateFullyQualifiedName ( fullyQualifiedName ) { const parts = fullyQualifiedName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) throw new Error ( ` Invalid fully qualified name: ${ fullyQualifiedName } . Expected format: database.schema.name ` ) ; } /** * Validates the structure of DMF arguments JSON * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid */ function validateDMFArgumentsStructure ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; if ( referencedEntities . length > 1 ) throw new Error ( \"Only one referenced entity is allowed\" ) ; const validationFunctions = { arrayItem : ( param ) => [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) && param . name , nestedItem : ( param ) => [ \"COLUMN\" ] . includes ( param . type ) && param . name , } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; if ( referencedEntities . length > 0 ) { for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } } /** * Validates that all specified columns exist in an entity * @param { Array } columnsToCheck - Array of column names to validate * @param { Array } entityColumns - Array of column metadata from the entity * @param { string } entityName - Name of the entity for error message * @throws { Error } If any column doesn't exist in the entity */ function validateColumnsExistInEntity ( entityName , allColumnsInEntity , columnsToCheck ) { for ( const column of columnsToCheck ) { if ( ! allColumnsInEntity . some ( col => col . name === column ) ) throw new Error ( ` Column ${ column } not found in entity ${ entityName } ` ) ; } } /** * Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param { string } entityName - Fully qualified name of the entity * @param { string } dmfName - Fully qualified name of the DMF * @param { Array } dmfArguments - Array of DMF arguments * @throws { Error } If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers ( entityName , dmfName = \"\" , dmfArguments = [ ] ) { if ( ! entityName ) { throw new Error ( \"Please provide a valid entity name. The entity name is required to validate identifiers.\" ) ; } validateFullyQualifiedName ( entityName ) ; // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName ; const baseEntityAllColumns = getAllColumnsForEntity ( entityName ) ; const entityColumnsMap = { [ baseEntityName ] : baseEntityAllColumns } ; const allReferencedEntities = dmfArguments . filter ( param => param . type === \"TABLE\" ) ; for ( const referencedEntity of allReferencedEntities ) { const columns = getAllColumnsForEntity ( referencedEntity . name ) ; entityColumnsMap [ referencedEntity . name ] = columns ; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => param . name ) ; validateColumnsExistInEntity ( baseEntityName , baseEntityAllColumns , allBaseEntityColumnsInArguments ) ; for ( const referencedEntity of allReferencedEntities ) { const columnsInArguments = referencedEntity . nested . map ( nestedParam => nestedParam . name ) ; validateColumnsExistInEntity ( referencedEntity . name , entityColumnsMap [ referencedEntity . name ] , columnsInArguments ) ; } if ( dmfName ) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) ; isDMFValid ( dmfName , generatedTypeSignature ) ; } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } /** * Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param { string } cronExpression - CRON expression to validate * @throws { Error } If CRON expression is invalid */ function validateCronExpression ( cronExpression ) { if ( cronExpression . length > 100 ) throw new Error ( \"Cron expression is too long\" ) ; const cronFields = cronExpression . trim ( ) . split ( / \\s + / ) ; if ( cronFields . length !== 6 ) throw new Error ( \"Invalid cron expression. Expected 6 fields\" ) ; const [ minute , hour , dayOfMonth , month , dayOfWeek , timezone ] = cronFields ; const isTimezoneValidResult = isTimezoneValid ( timezone ) ; if ( ! isTimezoneValidResult ) throw new Error ( \"Invalid timezone provided in the cron expression\" ) ; const regexPatterns = { minute : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , hour : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , dayOfMonth : / ^ ( \\* | L | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , month : / ^ ( \\* | \\d + | JAN | FEB | MAR | APR | MAY | JUN | JUL | AUG | SEP | OCT | NOV | DEC | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , dayOfWeek : / ^ ( \\* | \\d + | SUN | MON | TUE | WED | THU | FRI | SAT | \\d + L | [ A - Z ] {3} L | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , } ; if ( minute . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( minute , 0 , 59 ) ) throw new Error ( \"Invalid minute value\" ) ; if ( hour . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( hour , 0 , 23 ) ) throw new Error ( \"Invalid hour value\" ) ; if ( dayOfMonth . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfMonth , 1 , 31 ) ) throw new Error ( \"Invalid day of month value\" ) ; if ( month . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( month , 1 , 12 ) ) throw new Error ( \"Invalid month value\" ) ; if ( dayOfWeek . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfWeek , 0 , 6 ) ) throw new Error ( \"Invalid day of week value\" ) ; if ( ! regexPatterns . minute . test ( minute ) || ! regexPatterns . hour . test ( hour ) || ! regexPatterns . dayOfMonth . test ( dayOfMonth ) || ! regexPatterns . month . test ( month ) || ! regexPatterns . dayOfWeek . test ( dayOfWeek ) ) throw new Error ( \"Invalid cron expression\" ) ; } /** * Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws { Error } If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule ( ) { const VALID_MINUTES = new Set ( [ \"5\" , \"15\" , \"30\" , \"60\" , \"720\" , \"1440\" ] ) ; if ( SCHEDULE_TYPE === \"MINUTES\" && ! VALID_MINUTES . has ( SCHEDULE_VALUE ) ) throw new Error ( ` Invalid SCHEDULE_VALUE for MINUTES. Valid options are ${ Array . from ( VALID_MINUTES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPE === \"CRON\" ) validateCronExpression ( SCHEDULE_VALUE ) ; // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } /** * Validates DMF arguments with dataType checks * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid or dataType is missing */ function validateDMFArgumentsWithDataType ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const validationFunctions = { arrayItem : ( param ) => { if ( ! [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( param . type === \"COLUMN\" && ! param . dataType ) { throw new Error ( ` Missing dataType for COLUMN parameter: ${ param . name } ` ) ; } return true ; } , nestedItem : ( param ) => { if ( ! [ \"COLUMN\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( ! param . dataType ) { throw new Error ( ` Missing dataType for nested COLUMN parameter: ${ param . name } ` ) ; } return true ; } } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } /** * Validates DMF name format * @param { string } dmfName - Fully qualified name of the DMF * @throws { Error } If DMF name format is invalid */ function validateDmfName ( dmfName ) { const parts = dmfName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid DMF_NAME: ${ dmfName } . Expected format: database.schema.name ` ) ; } } /** * Validates that the provided SQL is read-only and doesn't contain dangerous operations * @param { string } sqlExpression - SQL to validate * @returns { boolean } Whether the SQL is safe * @throws { Error } If SQL contains potentially dangerous operations */ function validateSqlExpression ( sqlExpression ) { if ( ! sqlExpression ) { throw new Error ( \"Please provide a SQL query. The SQL expression cannot be empty.\" ) ; } // Step 1: Normalize Unicode characters to prevent encoding-based attacks const normalizedSql = sqlExpression . normalize ( 'NFKC' ) ; // Step 2: Check for multiple statements (handled by splitIntoSqlStatements) splitIntoSqlStatements ( normalizedSql ) ; // Step 3: Check whether it is a read-query or not if ( ! isReadQuery ( normalizedSql ) ) { throw new Error ( \"Your query must start with SELECT or WITH. Only read operations are allowed.\" ) ; } // Step 4: Check for suspicious patterns that might bypass filters checkForSuspiciousPatterns ( normalizedSql ) ; // Step 5: Check for dangerous operations const dangerousOperation = containsDangerousOperation ( normalizedSql ) ; if ( dangerousOperation ) { throw new Error ( \"For security reasons, this operation is not permitted. Please use only read operations in your query.\" ) ; } return true ; } /** * Enhanced detection of suspicious SQL patterns * @param { string } sql - SQL query to check * @throws { Error } If suspicious patterns are detected */ function checkForSuspiciousPatterns ( sql ) { // Create a copy where string literals are masked to prevent false positives const sqlWithoutStrings = sql . replace ( / ' [ ^ ' ] * ' / g , \"'STRING_LITERAL'\" ) . replace ( / \" [ ^ \" ] * \" / g , '\"STRING_LITERAL\"' ) ; const suspiciousPatterns = [ // Common SQL injection techniques { pattern : / \\b OR \\s + [ 0 - 9 ] + \\s * = \\s * [ 0 - 9 ] + \\b / i , message : \"Suspicious always-true condition detected\" } , // Alias abuse detection { pattern : / \\b AS \\s + [ '\"` ] ? . *? ( DELETE | INSERT | UPDATE | DROP | ALTER | EXEC ) \\b / i , message : \"Suspicious alias detected\" } , // Hex encoding and other obfuscation techniques { pattern : / 0x [ 0 - 9 a - f ] {10,} / i , message : \"Suspicious hex encoding detected\" } , { pattern : / CHAR \\s * \\( \\s * \\d + ( \\s * , \\s * \\d + ) + \\s * \\) / i , message : \"Character code conversion functions are not allowed\" } , ] ; // Check for suspicious patterns outside of string literals for ( const { pattern , message } of suspiciousPatterns ) { if ( pattern . test ( sqlWithoutStrings ) ) { throw new Error ( message ) ; } } } /** * Splits SQL into separate statements based on semicolons not in quotes * @param { string } sql - SQL query * @returns { string } - SQL query without semicolons */ function splitIntoSqlStatements ( sql ) { let inSingleQuote = false ; let inDoubleQuote = false ; for ( let i = 0 ; i < sql . length ; i ++ ) { const char = sql [ i ] ; // Handle quotes if ( char === \"'\" && sql [ i - 1 ] !== '\\\\' ) { inSingleQuote = ! inSingleQuote ; } else if ( char === '\"' && sql [ i - 1 ] !== '\\\\' ) { inDoubleQuote = ! inDoubleQuote ; } // If semicolon outside of quotes, throw error if ( char === ';' && ! inSingleQuote && ! inDoubleQuote ) { throw new Error ( \"Do not use semicolons to break or end your SQL statement. Submit your query without any semicolons.\" ) ; } } // If we get here, there were no semicolons outside quotes return sql . trim ( ) ; } /** * Checks if the SQL is a read-only query * @param { string } sql - SQL query without comments * @returns { boolean } - True if it's a read-only query */ function isReadQuery ( sql ) { const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) . trim ( ) ; if ( normalizedSql . startsWith ( 'SELECT ' ) ) { return true ; } if ( normalizedSql . startsWith ( 'WITH ' ) ) { return true ; } return false ; } /** * Checks if SQL contains any dangerous operations - using single keywords with word boundaries * @param { string } sql - SQL query without comments * @returns { string | null } - The dangerous operation found or null if safe */ function containsDangerousOperation ( sql ) { // Normalize whitespace and convert to uppercase for comparison const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) ; // Snowflake-specific dangerous commands - using single keywords with high precision const dangerousCommands = [ // Data Modification 'INSERT' , 'UPDATE' , 'DELETE' , 'MERGE' , 'TRUNCATE' , 'COPY' , // DDL statements 'CREATE' , 'DROP' , 'ALTER' , 'COMMENT' , 'GRANT' , 'REVOKE' , 'UNDROP' , // Transaction control 'BEGIN' , 'COMMIT' , 'ROLLBACK' , // System & session commands 'SET' , 'UNSET' , 'USE' , 'PUT' , 'GET' , 'REMOVE' , 'LIST' , // Information Schema & Metadata 'SHOW' , 'DESCRIBE' , // Procedures and functions 'CALL' , 'EXECUTE' , 'EXEC' , // Additional Snowflake operations 'EXPLAIN' ] ; // Dangerous functions specific to Snowflake const dangerousFunctions = [ 'SYSTEM' , 'CURRENT_USER' , 'CURRENT_ROLE' , 'CURRENT_ACCOUNT' , 'DATABASE' , 'VERSION' , 'SLEEP' , 'CALL_INTEGRATION' , 'PARSE_JSON' , 'RUN_JAVASCRIPT' , 'CALL_JAVASCRIPT' , 'TO_JAVASCRIPT' ] ; // Create a regex pattern with word boundaries for all dangerous commands const commandPattern = new RegExp ( ` \\\\b( ${ dangerousCommands . join ( '|' ) } )\\\\b ` , 'i' ) ; const functionPattern = new RegExp ( ` \\\\b( ${ dangerousFunctions . join ( '|' ) } )\\\\s*\\\\( ` , 'i' ) ; // Check for dangerous commands const commandMatch = normalizedSql . match ( commandPattern ) ; if ( commandMatch ) { return ` Dangerous operation detected: ${ commandMatch [ 0 ] } ` ; } // Check for dangerous functions const functionMatch = normalizedSql . match ( functionPattern ) ; if ( functionMatch ) { return ` Dangerous function call detected: ${ functionMatch [ 1 ] } ` ; } // Check for access to sensitive metadata if ( / \\b INFORMATION_SCHEMA \\b | \\b ACCOUNT_USAGE \\b / i . test ( normalizedSql ) ) { return 'Access to sensitive system metadata detected' ; } return null ; } /** * Executes SQL and returns a numeric result * @param { string } sql - SQL to execute * @returns { number } Numeric result * @throws { Error } If execution fails or result is not numeric */ function executeSqlAndReturnNumber ( sql ) { try { // Execute without returnFirstRow to get full result set const result = executeQuery ( sql , [ ] , false ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } // Check if the result set exists if ( ! result . result ) { throw new Error ( \"Your query didn't return any results. Please check your SQL and try again.\" ) ; } // Check number of columns const columnCount = result . result . getColumnCount ( ) ; if ( columnCount !== 1 ) { throw new Error ( \"Your query should return exactly one column. Please modify your query to return a single numeric value.\" ) ; } // Check if we have exactly one row if ( ! result . result . next ( ) ) { throw new Error ( \"Your query didn't return any rows. Please check your query and try again.\" ) ; } // Get the value const value = result . result . getColumnValue ( 1 ) ; // Check if it's a number if ( typeof value !== 'number' ) { throw new Error ( \"Your query must return a number. Please modify your query to calculate a numeric result.\" ) ; } // Check if there are more rows if ( result . result . next ( ) ) { throw new Error ( \"Your query returned multiple rows. Please modify your query to return a single result.\" ) ; } return value ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Validates all parameters for DMF creation * @throws { Error } If any validation fails */ function validateCreateDmf ( ) { validateDmfName ( DMF_NAME ) ; validateSqlExpression ( DMF_DEFINITION ) ; validateDMFArgumentsWithDataType ( DMF_ARGUMENTS_JSON ) ; } /** * Validates all provided arguments * Performs comprehensive validation on input parameters * @throws { Error } If any validation fails */ function validateAllArguments ( ) { validateMandatoryArguments ( ) ; // Validates all mandatory arguments are provided in the correct format if ( ACTION === \"CREATE_DMF\" ) { validateCreateDmf ( ) ; return ; } else if ( ACTION === \"EXECUTE_SQL\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"UPDATE_SCHEDULE\" ) { validateProvidedArgumentsForSchedule ( ) ; // Validates the provided schedule type and value } else { validateDMFArgumentsStructure ( DMF_ARGUMENTS_JSON ) ; } validateProvidedIdentifiers ( ENTITY_NAME , DMF_NAME , safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- /** * Extracts database, schema and table name from fully qualified entity name * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing database, schema and table name */ function parseEntityName ( entityName ) { const [ db , schema , table ] = entityName . split ( \".\" ) ; return { db , schema , table } ; } /** * Gets the owner of a table from information schema * @param { string } db - Database name * @param { string } schema - Schema name * @param { string } table - Table name * @returns { Object } Object containing success status and table owner */ function getTableOwner ( db , schema , table ) { const query = ` SELECT TABLE_OWNER FROM ${ db } .INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = ? AND TABLE_SCHEMA = ? AND TABLE_NAME = ? ` ; const result = executeQuery ( query , [ db , schema , table ] , true ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to get table owner: ${ result . message } ` , owner : null } ; } const owner = result . result ?. getColumnValue ( \"TABLE_OWNER\" ) ; if ( ! owner ) { return { isSuccessful : false , message : ` Could not find owner for table ${ db } . ${ schema } . ${ table } ` , owner : null } ; } return { isSuccessful : true , message : \"Successfully retrieved table owner\" , owner } ; } /** * Grants required permissions to a role * @param { string } role - Role to grant permissions to * @returns { Object } Object containing success status and message */ function grantPermissions ( role ) { const query = ` BEGIN GRANT USAGE ON SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE \" ${ role } \"; GRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; END; ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to grant permissions: ${ result . message } ` } ; } return { isSuccessful : true , message : ` Successfully granted permissions to role ${ role } ` } ; } /** * Handles permissions for DMF operations * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing success status and message */ function handleDMFPermissions ( entityName ) { try { // Parse entity name const { db , schema , table } = parseEntityName ( entityName ) ; // Get table owner const ownerResult = getTableOwner ( db , schema , table ) ; if ( ! ownerResult . isSuccessful ) { return ownerResult ; } // Grant permissions return grantPermissions ( ownerResult . owner ) ; } catch ( err ) { return { isSuccessful : false , message : ` Error handling permissions: ${ err . message } ` } ; } } /** * Parses comma-separated object lists into arrays * @param { string } databasesToCheck - Comma-separated list of databases * @param { string } schemasToCheck - Comma-separated list of schemas * @param { string } tablesToCheck - Comma-separated list of tables * @returns { Object } Object with parsed arrays */ function parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) { return { databases : databasesToCheck ? databasesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , schemas : schemasToCheck ? schemasToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , tables : tablesToCheck ? tablesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] } ; } /** * Checks database access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } databases - Array of databases to check * @returns { Array } Array of accessible databases */ function checkDatabaseAccess ( roleToCheck , databases ) { const accessibleDatabases = [ ] ; for ( const database of databases ) { try { const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'DATABASE' AND OBJECT_NAME = ' ${ database } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleDatabases . push ( database ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for database ' ${ database } '. Role ' ${ roleToCheck } ' may not have access or the database may not exist. ` ) ; } } return accessibleDatabases ; } /** * Checks schema access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } schemas - Array of schemas to check (format: database.schema) * @returns { Array } Array of accessible schemas */ function checkSchemaAccess ( roleToCheck , schemas ) { const accessibleSchemas = [ ] ; for ( const schema of schemas ) { try { const parts = schema . split ( '.' ) ; if ( parts . length !== 2 ) { throw new Error ( ` Invalid schema format: ' ${ schema } '. Expected format: database.schema ` ) ; } const [ database , schemaName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'SCHEMA' AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_NAME = ' ${ schemaName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleSchemas . push ( schema ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for schema ' ${ schema } '. Role ' ${ roleToCheck } ' may not have access or the schema may not exist. ` ) ; } } return accessibleSchemas ; } /** * Checks table access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } tables - Array of tables to check (format: database.schema.table) * @returns { Array } Array of accessible tables */ function checkTableAccess ( roleToCheck , tables ) { const accessibleTables = [ ] ; for ( const table of tables ) { try { const parts = table . split ( '.' ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid table format: ' ${ table } '. Expected format: database.schema.table ` ) ; } const [ database , schema , tableName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE IN ('TABLE', 'VIEW') AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_SCHEMA = ' ${ schema } ' AND OBJECT_NAME = ' ${ tableName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"SELECT\" || privilege === \"OWNERSHIP\" ) { accessibleTables . push ( table ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for table ' ${ table } '. Role ' ${ roleToCheck } ' may not have access or the table may not exist. ` ) ; } } return accessibleTables ; } /** * Validates SQL permissions for a given role and returns accessible objects * @param { string } sql - SQL to validate * @param { string } roleToCheck - Role to check permissions for * @param { string } databasesToCheck - Comma-separated list of databases to check access for * @param { string } schemasToCheck - Comma-separated list of schemas to check access for * @param { string } tablesToCheck - Comma-separated list of tables to check access for * @returns { Object } Object with validation result and accessible objects * @throws { Error } If SQL validation fails */ function validateSqlPermissions ( sql , roleToCheck , databasesToCheck , schemasToCheck , tablesToCheck ) { try { // Step 1: Run EXPLAIN command to validate SQL syntax and plan const explainSql = ` EXPLAIN ${ sql } ` ; const explainResult = executeQuery ( explainSql , [ ] ) ; if ( explainResult . isErrored ) { throw new Error ( \"We couldn't validate your query. Please check your SQL syntax and permissions and try again.\" ) ; } // Step 2: Parse objects to check const objectsToCheck = parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) ; // Step 3: Check access for each object type const accessibleDatabases = checkDatabaseAccess ( roleToCheck , objectsToCheck . databases ) ; const accessibleSchemas = checkSchemaAccess ( roleToCheck , objectsToCheck . schemas ) ; const accessibleTables = checkTableAccess ( roleToCheck , objectsToCheck . tables ) ; return { isSuccessful : true , message : \"SQL permissions validation successful\" , accessibleObjects : { databases : accessibleDatabases , schemas : accessibleSchemas , tables : accessibleTables } } ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns { string } JSON string with operation status and result message * @throws { Error } If any operation step fails */ function main ( ) { validateAllArguments ( ) ; // Handle permissions for DMF attachment/detachment operations if ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] . includes ( ACTION ) ) { const permissionResult = handleDMFPermissions ( ENTITY_NAME ) ; if ( ! permissionResult . isSuccessful ) { return JSON . stringify ( permissionResult ) ; } } // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments ( safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; const SQL_TEMPLATES = { ATTACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } ADD DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , DETACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } DROP DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , SUSPEND_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) SUSPEND ` , RESUME_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) RESUME ` , UPDATE_SCHEDULE : { MINUTES : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = ' ${ SCHEDULE_VALUE } MINUTE' ` , CRON : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'USING CRON ${ SCHEDULE_VALUE } ' ` , ON_DATA_CHANGE : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES' ` , NOT_SCHEDULED : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } UNSET DATA_METRIC_SCHEDULE ` , } , } ; let sqlText = \"\" ; let returnMessage = \"\" ; let binds = [ ] ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { sqlText = SQL_TEMPLATES [ ACTION ] [ SCHEDULE_TYPE ] ; returnMessage = ` Successfully updated schedule for ${ ENTITY_NAME } to ${ SCHEDULE_TYPE } ${ SCHEDULE_VALUE } ` ; } else if ( ACTION === \"CREATE_DMF\" ) { const DOLLAR = String . fromCharCode ( 36 ) ; // ASCII code for $ const dmfArguments = safelyParseJSON ( DMF_ARGUMENTS_JSON ) ; const functionParams = generateFunctionParameters ( dmfArguments ) ; sqlText = \"CREATE OR REPLACE DATA METRIC FUNCTION \" + DMF_NAME + \" (\" + functionParams + \" )\" + \"RETURNS NUMBER AS \" + DOLLAR + DOLLAR + \" \" + DMF_DEFINITION + \" \" + DOLLAR + DOLLAR ; returnMessage = ` DMF ${ DMF_NAME } registered successfully ` ; } else if ( ACTION === \"EXECUTE_SQL\" ) { // Execute SQL and get numeric result const result = executeSqlAndReturnNumber ( DMF_DEFINITION ) ; const response = { isSuccessful : true , message : \"SQL executed successfully\" , result : result } ; return JSON . stringify ( response ) ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { const validationResult = validateSqlPermissions ( DMF_DEFINITION , ROLE_TO_CHECK , DATABASES_TO_CHECK , SCHEMAS_TO_CHECK , TABLES_TO_CHECK ) ; return JSON . stringify ( validationResult ) ; } else { sqlText = SQL_TEMPLATES [ ACTION ] ; returnMessage = ` ACTION: ${ ACTION } performed successfully on ${ ENTITY_NAME } with DMF: ${ DMF_NAME } and DMF Arguments: ${ dmfArguments } ` ; } const result = executeQuery ( sqlText , binds ) ; return JSON . stringify ( { isSuccessful : ! result . isErrored , message : result . isErrored ? result . message : returnMessage , } ) ; } // Execute the main function and return the result try { return main ( ) ; } catch ( err ) { return JSON . stringify ( { isSuccessful : false , message : err . message , } ) ; } $$ ; Transfer ownership to dq_admin role: GRANT OWNERSHIP ON DATABASE ATLAN_DQ TO ROLE dq_admin ; GRANT OWNERSHIP ON SCHEMA ATLAN_DQ . SHARED TO ROLE dq_admin ; GRANT OWNERSHIP ON SCHEMA ATLAN_DQ . DMFS TO ROLE dq_admin ; GRANT OWNERSHIP ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE dq_admin ; Grant privileges â Grant the necessary permissions to enable data quality operations and maintain proper access control. System privileges: Grant Snowflake system-level permissions to enable data metric functions and monitoring capabilities. -- For DQ Admin Role GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE dq_admin ; -- For Atlan Service Role GRANT APPLICATION ROLE SNOWFLAKE . DATA_QUALITY_MONITORING_VIEWER TO ROLE atlan_dq_service_role ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE atlan_dq_service_role ; GRANT EXECUTE TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; Table owner privileges: For every role that owns tables in your environment (denoted by <table_owner> ), grant the following privileges: GRANT ROLE < table_owner > TO ROLE dq_admin ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE < table_owner > ; GRANT EXECUTE DATA METRIC FUNCTION ON ACCOUNT TO ROLE < table_owner > ; To identify table owner roles in your environment: -- Find table owners SELECT TABLE_CATALOG , TABLE_OWNER FROM SNOWFLAKE . ACCOUNT_USAGE . TABLES WHERE DELETED IS NULL AND TABLE_OWNER IS NOT NULL GROUP BY TABLE_CATALOG , TABLE_OWNER ; Database access: Grant Atlan's service role access to the created objects: GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . SHARED TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . DMFS TO ROLE atlan_dq_service_role ; GRANT USAGE ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE atlan_dq_service_role ; GRANT CREATE SCHEMA ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; Next steps â Enable data quality on connection - Configure your Snowflake connection for data quality monitoring Need help â If you have questions or need assistance with setting up Snowflake for data quality, reach out to Atlan Support by submitting a support request . See also â Data quality permissions - Understand the required permissions and roles for data quality operations Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: snowflake data-quality setup governance Previous Snowflake Data Quality Studio Next Enable data quality on connection System requirements Prerequisites Create roles Create user Set up database objects Grant privileges Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/faq/setup-and-configuration",
    "text": "Build governance Data Quality Studio Databricks Data Quality FAQ Setup and configuration On this page Setup and configuration This document answers common questions about prerequisites, permissions, and environment settings required to run Atlanâs Data Quality Studio on Databricks. What Databricks edition is required for data quality? â Atlan DQ support for Databricks is supported only on Premium and Enterprise tiers of Databricks. What administrative access is required? â The user performing the setup must be: A Workspace admin; and A Metastore Admin or have CREATE CATALOG privilege on the metastore linked to the workspace Is serverless compute required? â Yes, your workspace must have the following feature enabled: Serverless Compute for Jobs & Notebooks This is required to permit execution of Atlan's DQ jobs in your Databricks Workspace using Serverless compute. What SQL warehouse is recommended? â A dedicated SQL warehouse must be identified for running DQ-related queries. While Atlan supports any SQL Warehouse, Atlan recommends using a Serverless SQL Warehouse for faster startup times. Is network access configuration required? â Outbound Network Access Must Be Allowed from Serverless Compute: Databricks Serverless Compute uses network policies to control outbound traffic [only for Enterprise tier]. Verify that outbound connectivity to Atlan is permitted from the Serverless environment. What Atlan prerequisites are needed? â Before integrating with Databricks, you need to generate an API token in Atlan. This token is securely stored in Databricks in a secret and used to authenticate API requests from within Databricks. Can I enable data quality on multiple connections? â Currently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection, raise a support request . How long does the setup take? â After completing the setup steps, Atlan takes approximately 10 minutes to complete the setup in the background. Once finished, you'll see data quality options available on your Databricks assets. Can I use private channels for alerts? â Only public channels are supported for data quality alerts. Alerts can't be routed to private channels or Direct Messages at this time. Tags: databricks data-quality faq troubleshooting Previous Enable data quality on connection What Databricks edition is required for data quality? What administrative access is required? Is serverless compute required? What SQL warehouse is recommended? Is network access configuration required? What Atlan prerequisites are needed? Can I enable data quality on multiple connections? How long does the setup take? Can I use private channels for alerts?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda",
    "text": "Connect data Data Quality & Observability Soda On this page Soda Overview: Catalog Soda checks and test results in Atlan. Gain visibility into data quality metrics, alerts, and governance for your data assets. Get started â Follow these steps to connect and catalog Soda assets in Atlan: Set up the connector Crawl Soda assets References â What does Atlan crawl from Soda : Learn about the Soda assets and metadata that Atlan discovers and catalogs. Tags: soda connector observability data quality connectivity Next Set up Soda Get started References"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/migrate-snowflake",
    "text": "Build governance Data Quality Studio Snowflake Data Quality Upgrade setup Upgrade to Snowflake data quality studio On this page Upgrade to Snowflake data quality studio Private Preview Upgrade your existing Snowflake data quality setup to the latest version to access new features and improvements. This guide helps you migrate from an older version of the Snowflake data quality integration to the latest version. Prerequisites â Before you begin, make sure you have: Existing Snowflake data quality setup configured Permissions required â You need the following Snowflake roles: dq_admin role access atlan_dq_service_role role access Upgrade data quality setup â Follow these steps to upgrade your existing Snowflake data quality setup to the latest version. Switch to dq_admin role: USE ROLE dq_admin ; Drop the existing procedure: If you have already set up custom SQL previously, use the following command: DROP PROCEDURE IF EXISTS ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) ; If you haven't set up custom SQL , use this command: DROP PROCEDURE IF EXISTS ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING ) ; info If youâre not sure which command to use, go to the ATLAN_DQ.SHARED schema in your Snowflake environment and check the procedure signature for MANAGE_DMF to confirm which version youâre using. ::: Create the required schema: CREATE SCHEMA IF NOT EXISTS ATLAN_DQ . DMFS ; Create the updated procedure: View procedure code /** * Manages Data Metric Functions (DMF) operations for Snowflake tabular entities. * This procedure handles various DMF operations including: * - Creating and managing DMFs (CREATE_DMF) * - Attaching/detaching DMFs to entities (ATTACH_DMF, DETACH_DMF) * - Managing DMF schedules (UPDATE_SCHEDULE) * - Executing SQL expressions (EXECUTE_SQL) * - Validating SQL permissions (VALIDATE_SQL_PERMISSIONS) * * The procedure runs with the privileges of the procedure owner and includes comprehensive * validation of all inputs and permissions before executing any operations. * * @param { string } ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE, CREATE_DMF, EXECUTE_SQL, VALIDATE_SQL_PERMISSIONS) * @param { string } ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param { string } ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param { string } [ DMF_NAME = null ] - Fully qualified name of the DMF (database.schema.name) * @param { string } [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param { string } [ SCHEDULE_TYPE = null ] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param { string } [ SCHEDULE_VALUE = null ] - Schedule value based on type * @param { string } [ DMF_DEFINITION = null ] - SQL expression defining the DMF * @param { string } [ ROLE_TO_CHECK = null ] - Role to check permissions for * @param { string } [ DATABASES_TO_CHECK = null ] - Comma-separated list of databases to validate permissions for * @param { string } [ SCHEMAS_TO_CHECK = null ] - Comma-separated list of schemas to validate permissions for * @param { string } [ TABLES_TO_CHECK = null ] - Comma-separated list of tables to validate permissions for * @returns { string } - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( ACTION STRING , ENTITY_TYPE STRING DEFAULT NULL , ENTITY_NAME STRING DEFAULT NULL , DMF_NAME STRING DEFAULT NULL , DMF_ARGUMENTS_JSON STRING DEFAULT '[]' , SCHEDULE_TYPE STRING DEFAULT NULL , SCHEDULE_VALUE STRING DEFAULT NULL , DMF_DEFINITION STRING DEFAULT NULL , ROLE_TO_CHECK STRING DEFAULT NULL , DATABASES_TO_CHECK STRING DEFAULT NULL , SCHEMAS_TO_CHECK STRING DEFAULT NULL , TABLES_TO_CHECK STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- /** * Executes a SQL query with parameters * @param { string } sqlText - SQL statement to execute * @param { Array } [binds=[]] - Array of bind parameters for the query * @param { boolean } [ returnFirstRow = false ] - Whether to return only the first row * @returns { Object } Object containing execution result or error information */ function executeQuery ( sqlText , binds = [ ] , returnFirstRow = false ) { try { if ( ! sqlText ) return { isErrored : true , message : \"SQL Text is required\" , result : null , } ; const statement = snowflake . createStatement ( { sqlText , binds } ) ; const result = statement . execute ( ) ; const response = { isErrored : false , message : \"\" , result : null , } ; if ( returnFirstRow ) { response . result = result . next ( ) ? result : null ; return response ; } response . result = result ; return response ; } catch ( err ) { return { isErrored : true , message : ` ${ err . code } - ${ err . message } - ${ sqlText } with binds: ${ binds . join ( \", \" ) } ` , result : null , } ; } } /** * Safely parses a JSON string * @param { string } jsonString - JSON string to parse * @returns { Object } Parsed JSON object or null if invalid */ function safelyParseJSON ( jsonString ) { try { return JSON . parse ( jsonString ) ; } catch ( err ) { return null ; } } /** * Validates a number within a range * @param { string } value - Number to validate * @param { number } min - Minimum value * @param { number } max - Maximum value * @returns { boolean } True if number is valid * @returns { boolean } False if number is invalid */ function isNumberValid ( value , min , max ) { const num = parseInt ( value , 10 ) ; return ! isNaN ( num ) && num >= min && num <= max ; } /** * Escapes and quotes a Snowflake identifier * @param { string } identifier - Raw identifier to normalize * @returns { string } Properly quoted identifier safe for SQL */ function normalizeIdentifier ( identifier ) { return ` \" ${ identifier . replace ( / \" / g , '\"\"' ) } \" ` ; } /** * Retrieves all columns for a given entity. Validates that the entityexists and procedure owner has access to it. * @param { string } entityName - Fully qualified entity name * @returns { Array } Array of column objects with name and dataType properties * @throws { Error } If entity doesn't exist or is inaccessible */ function getAllColumnsForEntity ( entityName ) { const sqlText = \"SHOW COLUMNS IN IDENTIFIER(?)\" ; const binds = [ entityName ] ; const { result , isErrored , message } = executeQuery ( sqlText , binds ) ; if ( isErrored ) { // Validates that the entity exists and procedure owner has access to it throw new Error ( message ) ; } const columns = [ ] ; while ( result . next ( ) ) { const column = { name : result . getColumnValue ( \"column_name\" ) , dataType : JSON . parse ( result . getColumnValue ( \"data_type\" ) ) . type , } ; if ( column . dataType === \"FIXED\" ) column . dataType = \"NUMBER\" ; columns . push ( column ) ; } return columns ; } /** * Validates that the DMF is valid and exists * @param { string } dmfName - Fully qualified name of the DMF * @param { string } dmfArguments - Arguments for the DMF * @returns { boolean } Whether the DMF is valid * @throws { Error } If DMF is invalid */ function isDMFValid ( dmfName , dmfArguments ) { const { isErrored , message } = executeQuery ( ` DESCRIBE FUNCTION IDENTIFIER(?)( ${ dmfArguments } ) ` , [ dmfName ] , true ) ; if ( isErrored ) throw new Error ( message ) ; return true ; } /** * Checks if a timezone is valid * @param { string } timezone - Timezone to validate * @returns { boolean } True if timezone is valid * @returns { boolean } False if timezone is invalid */ function isTimezoneValid ( timezone ) { const result = executeQuery ( ` SELECT CONVERT_TIMEZONE(?, CURRENT_TIMESTAMP()) ` , [ timezone ] , true ) ; return ! result . isErrored ; } /** * Generates a DMF type signature based on the arguments and entity columns * @param { Array } dmfArguments - Array of DMF arguments * @param { Object } entityColumnsMap - Map of entity names to column objects in the format { <ENTITY_NAME>: [ { name: <COLUMN_NAME> , dataType: <DATA_TYPE> } ] } * @param { string } baseEntityName - Name of the base entity * @returns { string } DMF type signature * @throws { Error } If entity not found in the cache */ function generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) { if ( ! dmfArguments || ! dmfArguments . length ) return \"\" ; const baseEntityColumns = entityColumnsMap [ baseEntityName ] ; if ( ! baseEntityColumns ) { throw new Error ( ` Entity ${ baseEntityName } not found in the cache ` ) ; } const baseEntityColumnArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => { const column = baseEntityColumns . find ( col => col . name === param . name ) ; return column ? column . dataType : null ; } ) . join ( \", \" ) ; const baseEntityArguments = ` TABLE( ${ baseEntityColumnArguments } ) ` ; const referencedEntityArguments = dmfArguments . filter ( param => param . type === \"TABLE\" ) . map ( entityParam => { const entityName = entityParam . name ; const entityColumns = entityColumnsMap [ entityName ] ; if ( ! entityColumns ) { throw new Error ( ` Entity ${ entityName } not found in the cache ` ) ; } const columnTypes = entityParam . nested . map ( nestedParam => { const column = entityColumns . find ( col => col . name === nestedParam . name ) ; return column ? column . dataType : null ; } ) . filter ( Boolean ) . join ( \", \" ) ; return ` TABLE( ${ columnTypes } ) ` ; } ) ; const arguments = [ baseEntityArguments , ... referencedEntityArguments ] . join ( \", \" ) ; return arguments ; } /** * Generates DMF arguments for SQL statements * @param { string } dmfArguments - Array of DMF arguments * @returns { string } Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"COLUMN\" ) { return normalizeIdentifier ( param . name ) ; } // Handle TABLE type with nested columns return ` TABLE( ${ normalizeIdentifier ( param . name ) } ( ${ param . nested . map ( nested => normalizeIdentifier ( nested . name ) ) . join ( \", \" ) } )) ` ; } ) . join ( \", \" ) ; } /** * Generates function parameters from DMF arguments * @param { Array } dmfArguments - Array of DMF arguments * @returns { string } Formatted function parameters */ function generateFunctionParameters ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"TABLE\" ) { const nestedParams = param . nested . map ( nested => ` ${ nested . name } ${ nested . dataType } ` ) . join ( \", \" ) ; return ` ${ param . name } TABLE( ${ nestedParams } ) ` ; } return ` ${ param . name } ${ param . dataType } ` ; } ) . join ( \", \" ) ; } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- /** * Validates that mandatory arguments are provided and valid * @throws { Error } If any mandatory argument is missing or invalid */ function validateMandatoryArguments ( ) { const VALID_ACTIONS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" , \"UPDATE_SCHEDULE\" , \"CREATE_DMF\" , \"EXECUTE_SQL\" , \"VALIDATE_SQL_PERMISSIONS\" ] ) ; const VALID_ENTITY_TYPES = new Set ( [ \"TABLE\" , \"VIEW\" , \"MATERIALIZED VIEW\" , \"EXTERNAL TABLE\" , \"ICEBERG TABLE\" ] ) ; const DMF_OPS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] ) ; const VALID_SCHEDULE_TYPES = new Set ( [ \"MINUTES\" , \"CRON\" , \"ON_DATA_CHANGE\" , \"NOT_SCHEDULED\" ] ) ; const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set ( [ \"MINUTES\" , \"CRON\" ] ) ; if ( ! VALID_ACTIONS . has ( ACTION ) ) throw new Error ( ` Invalid ACTION: \" ${ ACTION } \". Valid options are ${ Array . from ( VALID_ACTIONS ) . join ( \", \" ) } ` ) ; if ( ENTITY_TYPE && ! VALID_ENTITY_TYPES . has ( ENTITY_TYPE ) ) throw new Error ( ` Invalid ENTITY_TYPE: \" ${ ENTITY_TYPE } \". Valid options are ${ Array . from ( VALID_ENTITY_TYPES ) . join ( \", \" ) } ` ) ; if ( DMF_OPS . has ( ACTION ) && ! DMF_NAME ) throw new Error ( \"DMF_NAME is required for DMF related actions\" ) ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { if ( ! SCHEDULE_TYPE ) throw new Error ( \"SCHEDULE_TYPE is required for SCHEDULE action\" ) ; if ( ! VALID_SCHEDULE_TYPES . has ( SCHEDULE_TYPE ) ) throw new Error ( ` Invalid schedule type: \" ${ SCHEDULE_TYPE } \". Valid options are ${ Array . from ( VALID_SCHEDULE_TYPES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPES_THAT_REQUIRE_VALUE . has ( SCHEDULE_TYPE ) && ! SCHEDULE_VALUE ) throw new Error ( \"SCHEDULE_VALUE is required for SCHEDULE action\" ) ; } if ( ACTION === \"EXECUTE_SQL\" && ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to execute.\" ) ; } if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { if ( ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to validate permissions.\" ) ; } if ( ! ROLE_TO_CHECK ) { throw new Error ( \"Failed to fetch role to check permissions. Please ensure the role exists and is accessible.\" ) ; } if ( ! DATABASES_TO_CHECK && ! SCHEMAS_TO_CHECK && ! TABLES_TO_CHECK ) { throw new Error ( \"No databases, schemas, or tables provided. Please provide at least one database, schema, or table to validate permissions for.\" ) ; } } } /** * Parses a fully qualified name into its components * @param { string } fullyQualifiedName - Fully qualified name to parse * @returns { Object } Object with database, schema, and name properties * @throws { Error } If invalid fully qualified name */ function validateFullyQualifiedName ( fullyQualifiedName ) { const parts = fullyQualifiedName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) throw new Error ( ` Invalid fully qualified name: ${ fullyQualifiedName } . Expected format: database.schema.name ` ) ; } /** * Validates the structure of DMF arguments JSON * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid */ function validateDMFArgumentsStructure ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; if ( referencedEntities . length > 1 ) throw new Error ( \"Only one referenced entity is allowed\" ) ; const validationFunctions = { arrayItem : ( param ) => [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) && param . name , nestedItem : ( param ) => [ \"COLUMN\" ] . includes ( param . type ) && param . name , } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; if ( referencedEntities . length > 0 ) { for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } } /** * Validates that all specified columns exist in an entity * @param { Array } columnsToCheck - Array of column names to validate * @param { Array } entityColumns - Array of column metadata from the entity * @param { string } entityName - Name of the entity for error message * @throws { Error } If any column doesn't exist in the entity */ function validateColumnsExistInEntity ( entityName , allColumnsInEntity , columnsToCheck ) { for ( const column of columnsToCheck ) { if ( ! allColumnsInEntity . some ( col => col . name === column ) ) throw new Error ( ` Column ${ column } not found in entity ${ entityName } ` ) ; } } /** * Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param { string } entityName - Fully qualified name of the entity * @param { string } dmfName - Fully qualified name of the DMF * @param { Array } dmfArguments - Array of DMF arguments * @throws { Error } If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers ( entityName , dmfName = \"\" , dmfArguments = [ ] ) { if ( ! entityName ) { throw new Error ( \"Please provide a valid entity name. The entity name is required to validate identifiers.\" ) ; } validateFullyQualifiedName ( entityName ) ; // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName ; const baseEntityAllColumns = getAllColumnsForEntity ( entityName ) ; const entityColumnsMap = { [ baseEntityName ] : baseEntityAllColumns } ; const allReferencedEntities = dmfArguments . filter ( param => param . type === \"TABLE\" ) ; for ( const referencedEntity of allReferencedEntities ) { const columns = getAllColumnsForEntity ( referencedEntity . name ) ; entityColumnsMap [ referencedEntity . name ] = columns ; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => param . name ) ; validateColumnsExistInEntity ( baseEntityName , baseEntityAllColumns , allBaseEntityColumnsInArguments ) ; for ( const referencedEntity of allReferencedEntities ) { const columnsInArguments = referencedEntity . nested . map ( nestedParam => nestedParam . name ) ; validateColumnsExistInEntity ( referencedEntity . name , entityColumnsMap [ referencedEntity . name ] , columnsInArguments ) ; } if ( dmfName ) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) ; isDMFValid ( dmfName , generatedTypeSignature ) ; } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } /** * Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param { string } cronExpression - CRON expression to validate * @throws { Error } If CRON expression is invalid */ function validateCronExpression ( cronExpression ) { if ( cronExpression . length > 100 ) throw new Error ( \"Cron expression is too long\" ) ; const cronFields = cronExpression . trim ( ) . split ( / \\s + / ) ; if ( cronFields . length !== 6 ) throw new Error ( \"Invalid cron expression. Expected 6 fields\" ) ; const [ minute , hour , dayOfMonth , month , dayOfWeek , timezone ] = cronFields ; const isTimezoneValidResult = isTimezoneValid ( timezone ) ; if ( ! isTimezoneValidResult ) throw new Error ( \"Invalid timezone provided in the cron expression\" ) ; const regexPatterns = { minute : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , hour : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , dayOfMonth : / ^ ( \\* | L | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , month : / ^ ( \\* | \\d + | JAN | FEB | MAR | APR | MAY | JUN | JUL | AUG | SEP | OCT | NOV | DEC | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , dayOfWeek : / ^ ( \\* | \\d + | SUN | MON | TUE | WED | THU | FRI | SAT | \\d + L | [ A - Z ] {3} L | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , } ; if ( minute . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( minute , 0 , 59 ) ) throw new Error ( \"Invalid minute value\" ) ; if ( hour . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( hour , 0 , 23 ) ) throw new Error ( \"Invalid hour value\" ) ; if ( dayOfMonth . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfMonth , 1 , 31 ) ) throw new Error ( \"Invalid day of month value\" ) ; if ( month . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( month , 1 , 12 ) ) throw new Error ( \"Invalid month value\" ) ; if ( dayOfWeek . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfWeek , 0 , 6 ) ) throw new Error ( \"Invalid day of week value\" ) ; if ( ! regexPatterns . minute . test ( minute ) || ! regexPatterns . hour . test ( hour ) || ! regexPatterns . dayOfMonth . test ( dayOfMonth ) || ! regexPatterns . month . test ( month ) || ! regexPatterns . dayOfWeek . test ( dayOfWeek ) ) throw new Error ( \"Invalid cron expression\" ) ; } /** * Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws { Error } If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule ( ) { const VALID_MINUTES = new Set ( [ \"5\" , \"15\" , \"30\" , \"60\" , \"720\" , \"1440\" ] ) ; if ( SCHEDULE_TYPE === \"MINUTES\" && ! VALID_MINUTES . has ( SCHEDULE_VALUE ) ) throw new Error ( ` Invalid SCHEDULE_VALUE for MINUTES. Valid options are ${ Array . from ( VALID_MINUTES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPE === \"CRON\" ) validateCronExpression ( SCHEDULE_VALUE ) ; // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } /** * Validates DMF arguments with dataType checks * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid or dataType is missing */ function validateDMFArgumentsWithDataType ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const validationFunctions = { arrayItem : ( param ) => { if ( ! [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( param . type === \"COLUMN\" && ! param . dataType ) { throw new Error ( ` Missing dataType for COLUMN parameter: ${ param . name } ` ) ; } return true ; } , nestedItem : ( param ) => { if ( ! [ \"COLUMN\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( ! param . dataType ) { throw new Error ( ` Missing dataType for nested COLUMN parameter: ${ param . name } ` ) ; } return true ; } } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } /** * Validates DMF name format * @param { string } dmfName - Fully qualified name of the DMF * @throws { Error } If DMF name format is invalid */ function validateDmfName ( dmfName ) { const parts = dmfName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid DMF_NAME: ${ dmfName } . Expected format: database.schema.name ` ) ; } } /** * Validates that the provided SQL is read-only and doesn't contain dangerous operations * @param { string } sqlExpression - SQL to validate * @returns { boolean } Whether the SQL is safe * @throws { Error } If SQL contains potentially dangerous operations */ function validateSqlExpression ( sqlExpression ) { if ( ! sqlExpression ) { throw new Error ( \"Please provide a SQL query. The SQL expression cannot be empty.\" ) ; } // Step 1: Normalize Unicode characters to prevent encoding-based attacks const normalizedSql = sqlExpression . normalize ( 'NFKC' ) ; // Step 2: Check for multiple statements (handled by splitIntoSqlStatements) splitIntoSqlStatements ( normalizedSql ) ; // Step 3: Check whether it is a read-query or not if ( ! isReadQuery ( normalizedSql ) ) { throw new Error ( \"Your query must start with SELECT or WITH. Only read operations are allowed.\" ) ; } // Step 4: Check for suspicious patterns that might bypass filters checkForSuspiciousPatterns ( normalizedSql ) ; // Step 5: Check for dangerous operations const dangerousOperation = containsDangerousOperation ( normalizedSql ) ; if ( dangerousOperation ) { throw new Error ( \"For security reasons, this operation is not permitted. Please use only read operations in your query.\" ) ; } return true ; } /** * Enhanced detection of suspicious SQL patterns * @param { string } sql - SQL query to check * @throws { Error } If suspicious patterns are detected */ function checkForSuspiciousPatterns ( sql ) { // Create a copy where string literals are masked to prevent false positives const sqlWithoutStrings = sql . replace ( / ' [ ^ ' ] * ' / g , \"'STRING_LITERAL'\" ) . replace ( / \" [ ^ \" ] * \" / g , '\"STRING_LITERAL\"' ) ; const suspiciousPatterns = [ // Common SQL injection techniques { pattern : / \\b OR \\s + [ 0 - 9 ] + \\s * = \\s * [ 0 - 9 ] + \\b / i , message : \"Suspicious always-true condition detected\" } , // Alias abuse detection { pattern : / \\b AS \\s + [ '\"` ] ? . *? ( DELETE | INSERT | UPDATE | DROP | ALTER | EXEC ) \\b / i , message : \"Suspicious alias detected\" } , // Hex encoding and other obfuscation techniques { pattern : / 0x [ 0 - 9 a - f ] {10,} / i , message : \"Suspicious hex encoding detected\" } , { pattern : / CHAR \\s * \\( \\s * \\d + ( \\s * , \\s * \\d + ) + \\s * \\) / i , message : \"Character code conversion functions are not allowed\" } , ] ; // Check for suspicious patterns outside of string literals for ( const { pattern , message } of suspiciousPatterns ) { if ( pattern . test ( sqlWithoutStrings ) ) { throw new Error ( message ) ; } } } /** * Splits SQL into separate statements based on semicolons not in quotes * @param { string } sql - SQL query * @returns { string } - SQL query without semicolons */ function splitIntoSqlStatements ( sql ) { let inSingleQuote = false ; let inDoubleQuote = false ; for ( let i = 0 ; i < sql . length ; i ++ ) { const char = sql [ i ] ; // Handle quotes if ( char === \"'\" && sql [ i - 1 ] !== '\\\\' ) { inSingleQuote = ! inSingleQuote ; } else if ( char === '\"' && sql [ i - 1 ] !== '\\\\' ) { inDoubleQuote = ! inDoubleQuote ; } // If semicolon outside of quotes, throw error if ( char === ';' && ! inSingleQuote && ! inDoubleQuote ) { throw new Error ( \"Do not use semicolons to break or end your SQL statement. Submit your query without any semicolons.\" ) ; } } // If we get here, there were no semicolons outside quotes return sql . trim ( ) ; } /** * Checks if the SQL is a read-only query * @param { string } sql - SQL query without comments * @returns { boolean } - True if it's a read-only query */ function isReadQuery ( sql ) { const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) . trim ( ) ; if ( normalizedSql . startsWith ( 'SELECT ' ) ) { return true ; } if ( normalizedSql . startsWith ( 'WITH ' ) ) { return true ; } return false ; } /** * Checks if SQL contains any dangerous operations - using single keywords with word boundaries * @param { string } sql - SQL query without comments * @returns { string | null } - The dangerous operation found or null if safe */ function containsDangerousOperation ( sql ) { // Normalize whitespace and convert to uppercase for comparison const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) ; // Snowflake-specific dangerous commands - using single keywords with high precision const dangerousCommands = [ // Data Modification 'INSERT' , 'UPDATE' , 'DELETE' , 'MERGE' , 'TRUNCATE' , 'COPY' , // DDL statements 'CREATE' , 'DROP' , 'ALTER' , 'COMMENT' , 'GRANT' , 'REVOKE' , 'UNDROP' , // Transaction control 'BEGIN' , 'COMMIT' , 'ROLLBACK' , // System & session commands 'SET' , 'UNSET' , 'USE' , 'PUT' , 'GET' , 'REMOVE' , 'LIST' , // Information Schema & Metadata 'SHOW' , 'DESCRIBE' , // Procedures and functions 'CALL' , 'EXECUTE' , 'EXEC' , // Additional Snowflake operations 'EXPLAIN' ] ; // Dangerous functions specific to Snowflake const dangerousFunctions = [ 'SYSTEM' , 'CURRENT_USER' , 'CURRENT_ROLE' , 'CURRENT_ACCOUNT' , 'DATABASE' , 'VERSION' , 'SLEEP' , 'CALL_INTEGRATION' , 'PARSE_JSON' , 'RUN_JAVASCRIPT' , 'CALL_JAVASCRIPT' , 'TO_JAVASCRIPT' ] ; // Create a regex pattern with word boundaries for all dangerous commands const commandPattern = new RegExp ( ` \\\\b( ${ dangerousCommands . join ( '|' ) } )\\\\b ` , 'i' ) ; const functionPattern = new RegExp ( ` \\\\b( ${ dangerousFunctions . join ( '|' ) } )\\\\s*\\\\( ` , 'i' ) ; // Check for dangerous commands const commandMatch = normalizedSql . match ( commandPattern ) ; if ( commandMatch ) { return ` Dangerous operation detected: ${ commandMatch [ 0 ] } ` ; } // Check for dangerous functions const functionMatch = normalizedSql . match ( functionPattern ) ; if ( functionMatch ) { return ` Dangerous function call detected: ${ functionMatch [ 1 ] } ` ; } // Check for access to sensitive metadata if ( / \\b INFORMATION_SCHEMA \\b | \\b ACCOUNT_USAGE \\b / i . test ( normalizedSql ) ) { return 'Access to sensitive system metadata detected' ; } return null ; } /** * Executes SQL and returns a numeric result * @param { string } sql - SQL to execute * @returns { number } Numeric result * @throws { Error } If execution fails or result is not numeric */ function executeSqlAndReturnNumber ( sql ) { try { // Execute without returnFirstRow to get full result set const result = executeQuery ( sql , [ ] , false ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } // Check if the result set exists if ( ! result . result ) { throw new Error ( \"Your query didn't return any results. Please check your SQL and try again.\" ) ; } // Check number of columns const columnCount = result . result . getColumnCount ( ) ; if ( columnCount !== 1 ) { throw new Error ( \"Your query should return exactly one column. Please modify your query to return a single numeric value.\" ) ; } // Check if we have exactly one row if ( ! result . result . next ( ) ) { throw new Error ( \"Your query didn't return any rows. Please check your query and try again.\" ) ; } // Get the value const value = result . result . getColumnValue ( 1 ) ; // Check if it's a number if ( typeof value !== 'number' ) { throw new Error ( \"Your query must return a number. Please modify your query to calculate a numeric result.\" ) ; } // Check if there are more rows if ( result . result . next ( ) ) { throw new Error ( \"Your query returned multiple rows. Please modify your query to return a single result.\" ) ; } return value ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Validates all parameters for DMF creation * @throws { Error } If any validation fails */ function validateCreateDmf ( ) { validateDmfName ( DMF_NAME ) ; validateSqlExpression ( DMF_DEFINITION ) ; validateDMFArgumentsWithDataType ( DMF_ARGUMENTS_JSON ) ; } /** * Validates all provided arguments * Performs comprehensive validation on input parameters * @throws { Error } If any validation fails */ function validateAllArguments ( ) { validateMandatoryArguments ( ) ; // Validates all mandatory arguments are provided in the correct format if ( ACTION === \"CREATE_DMF\" ) { validateCreateDmf ( ) ; return ; } else if ( ACTION === \"EXECUTE_SQL\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"UPDATE_SCHEDULE\" ) { validateProvidedArgumentsForSchedule ( ) ; // Validates the provided schedule type and value } else { validateDMFArgumentsStructure ( DMF_ARGUMENTS_JSON ) ; } validateProvidedIdentifiers ( ENTITY_NAME , DMF_NAME , safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- /** * Extracts database, schema and table name from fully qualified entity name * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing database, schema and table name */ function parseEntityName ( entityName ) { const [ db , schema , table ] = entityName . split ( \".\" ) ; return { db , schema , table } ; } /** * Gets the owner of a table from information schema * @param { string } db - Database name * @param { string } schema - Schema name * @param { string } table - Table name * @returns { Object } Object containing success status and table owner */ function getTableOwner ( db , schema , table ) { const query = ` SELECT TABLE_OWNER FROM ${ db } .INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = ? AND TABLE_SCHEMA = ? AND TABLE_NAME = ? ` ; const result = executeQuery ( query , [ db , schema , table ] , true ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to get table owner: ${ result . message } ` , owner : null } ; } const owner = result . result ?. getColumnValue ( \"TABLE_OWNER\" ) ; if ( ! owner ) { return { isSuccessful : false , message : ` Could not find owner for table ${ db } . ${ schema } . ${ table } ` , owner : null } ; } return { isSuccessful : true , message : \"Successfully retrieved table owner\" , owner } ; } /** * Grants required permissions to a role * @param { string } role - Role to grant permissions to * @returns { Object } Object containing success status and message */ function grantPermissions ( role ) { const query = ` BEGIN GRANT USAGE ON SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE \" ${ role } \"; GRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; END; ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to grant permissions: ${ result . message } ` } ; } return { isSuccessful : true , message : ` Successfully granted permissions to role ${ role } ` } ; } /** * Handles permissions for DMF operations * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing success status and message */ function handleDMFPermissions ( entityName ) { try { // Parse entity name const { db , schema , table } = parseEntityName ( entityName ) ; // Get table owner const ownerResult = getTableOwner ( db , schema , table ) ; if ( ! ownerResult . isSuccessful ) { return ownerResult ; } // Grant permissions return grantPermissions ( ownerResult . owner ) ; } catch ( err ) { return { isSuccessful : false , message : ` Error handling permissions: ${ err . message } ` } ; } } /** * Parses comma-separated object lists into arrays * @param { string } databasesToCheck - Comma-separated list of databases * @param { string } schemasToCheck - Comma-separated list of schemas * @param { string } tablesToCheck - Comma-separated list of tables * @returns { Object } Object with parsed arrays */ function parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) { return { databases : databasesToCheck ? databasesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , schemas : schemasToCheck ? schemasToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , tables : tablesToCheck ? tablesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] } ; } /** * Checks database access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } databases - Array of databases to check * @returns { Array } Array of accessible databases */ function checkDatabaseAccess ( roleToCheck , databases ) { const accessibleDatabases = [ ] ; for ( const database of databases ) { try { const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'DATABASE' AND OBJECT_NAME = ' ${ database } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleDatabases . push ( database ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for database ' ${ database } '. Role ' ${ roleToCheck } ' may not have access or the database may not exist. ` ) ; } } return accessibleDatabases ; } /** * Checks schema access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } schemas - Array of schemas to check (format: database.schema) * @returns { Array } Array of accessible schemas */ function checkSchemaAccess ( roleToCheck , schemas ) { const accessibleSchemas = [ ] ; for ( const schema of schemas ) { try { const parts = schema . split ( '.' ) ; if ( parts . length !== 2 ) { throw new Error ( ` Invalid schema format: ' ${ schema } '. Expected format: database.schema ` ) ; } const [ database , schemaName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'SCHEMA' AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_NAME = ' ${ schemaName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleSchemas . push ( schema ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for schema ' ${ schema } '. Role ' ${ roleToCheck } ' may not have access or the schema may not exist. ` ) ; } } return accessibleSchemas ; } /** * Checks table access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } tables - Array of tables to check (format: database.schema.table) * @returns { Array } Array of accessible tables */ function checkTableAccess ( roleToCheck , tables ) { const accessibleTables = [ ] ; for ( const table of tables ) { try { const parts = table . split ( '.' ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid table format: ' ${ table } '. Expected format: database.schema.table ` ) ; } const [ database , schema , tableName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE IN ('TABLE', 'VIEW') AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_SCHEMA = ' ${ schema } ' AND OBJECT_NAME = ' ${ tableName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"SELECT\" || privilege === \"OWNERSHIP\" ) { accessibleTables . push ( table ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for table ' ${ table } '. Role ' ${ roleToCheck } ' may not have access or the table may not exist. ` ) ; } } return accessibleTables ; } /** * Validates SQL permissions for a given role and returns accessible objects * @param { string } sql - SQL to validate * @param { string } roleToCheck - Role to check permissions for * @param { string } databasesToCheck - Comma-separated list of databases to check access for * @param { string } schemasToCheck - Comma-separated list of schemas to check access for * @param { string } tablesToCheck - Comma-separated list of tables to check access for * @returns { Object } Object with validation result and accessible objects * @throws { Error } If SQL validation fails */ function validateSqlPermissions ( sql , roleToCheck , databasesToCheck , schemasToCheck , tablesToCheck ) { try { // Step 1: Run EXPLAIN command to validate SQL syntax and plan const explainSql = ` EXPLAIN ${ sql } ` ; const explainResult = executeQuery ( explainSql , [ ] ) ; if ( explainResult . isErrored ) { throw new Error ( \"We couldn't validate your query. Please check your SQL syntax and permissions and try again.\" ) ; } // Step 2: Parse objects to check const objectsToCheck = parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) ; // Step 3: Check access for each object type const accessibleDatabases = checkDatabaseAccess ( roleToCheck , objectsToCheck . databases ) ; const accessibleSchemas = checkSchemaAccess ( roleToCheck , objectsToCheck . schemas ) ; const accessibleTables = checkTableAccess ( roleToCheck , objectsToCheck . tables ) ; return { isSuccessful : true , message : \"SQL permissions validation successful\" , accessibleObjects : { databases : accessibleDatabases , schemas : accessibleSchemas , tables : accessibleTables } } ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns { string } JSON string with operation status and result message * @throws { Error } If any operation step fails */ function main ( ) { validateAllArguments ( ) ; // Handle permissions for DMF attachment/detachment operations if ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] . includes ( ACTION ) ) { const permissionResult = handleDMFPermissions ( ENTITY_NAME ) ; if ( ! permissionResult . isSuccessful ) { return JSON . stringify ( permissionResult ) ; } } // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments ( safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; const SQL_TEMPLATES = { ATTACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } ADD DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , DETACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } DROP DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , SUSPEND_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) SUSPEND ` , RESUME_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) RESUME ` , UPDATE_SCHEDULE : { MINUTES : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = ' ${ SCHEDULE_VALUE } MINUTE' ` , CRON : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'USING CRON ${ SCHEDULE_VALUE } ' ` , ON_DATA_CHANGE : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES' ` , NOT_SCHEDULED : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } UNSET DATA_METRIC_SCHEDULE ` , } , } ; let sqlText = \"\" ; let returnMessage = \"\" ; let binds = [ ] ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { sqlText = SQL_TEMPLATES [ ACTION ] [ SCHEDULE_TYPE ] ; returnMessage = ` Successfully updated schedule for ${ ENTITY_NAME } to ${ SCHEDULE_TYPE } ${ SCHEDULE_VALUE } ` ; } else if ( ACTION === \"CREATE_DMF\" ) { const DOLLAR = String . fromCharCode ( 36 ) ; // ASCII code for $ const dmfArguments = safelyParseJSON ( DMF_ARGUMENTS_JSON ) ; const functionParams = generateFunctionParameters ( dmfArguments ) ; sqlText = \"CREATE OR REPLACE DATA METRIC FUNCTION \" + DMF_NAME + \" (\" + functionParams + \" )\" + \"RETURNS NUMBER AS \" + DOLLAR + DOLLAR + \" \" + DMF_DEFINITION + \" \" + DOLLAR + DOLLAR ; returnMessage = ` DMF ${ DMF_NAME } registered successfully ` ; } else if ( ACTION === \"EXECUTE_SQL\" ) { // Execute SQL and get numeric result const result = executeSqlAndReturnNumber ( DMF_DEFINITION ) ; const response = { isSuccessful : true , message : \"SQL executed successfully\" , result : result } ; return JSON . stringify ( response ) ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { const validationResult = validateSqlPermissions ( DMF_DEFINITION , ROLE_TO_CHECK , DATABASES_TO_CHECK , SCHEMAS_TO_CHECK , TABLES_TO_CHECK ) ; return JSON . stringify ( validationResult ) ; } else { sqlText = SQL_TEMPLATES [ ACTION ] ; returnMessage = ` ACTION: ${ ACTION } performed successfully on ${ ENTITY_NAME } with DMF: ${ DMF_NAME } and DMF Arguments: ${ dmfArguments } ` ; } const result = executeQuery ( sqlText , binds ) ; return JSON . stringify ( { isSuccessful : ! result . isErrored , message : result . isErrored ? result . message : returnMessage , } ) ; } // Execute the main function and return the result try { return main ( ) ; } catch ( err ) { return JSON . stringify ( { isSuccessful : false , message : err . message , } ) ; } $$ ; Grant usage to procedure and schema to the service role: GRANT USAGE ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . DMFS TO ROLE atlan_dq_service_role ; Need help â If you have questions or need assistance with migrating your Snowflake data quality setup, reach out to Atlan Support by submitting a support request . See also â Data quality permissions - Review the permissions required for the upgraded setup Tags: snowflake data-quality migration governance Previous Enable auto re-attachment of rules Next Operations Prerequisites Permissions required Upgrade data quality setup Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/concepts/auto-re-attachment-rules",
    "text": "Build governance Data Quality Studio Concepts What's auto re-attachment On this page What's auto re-attachment In modern data environments, pipelines (such as dbt) often drop and recreate tables or views as part of routine execution. This behavior can unintentionally detach existing data quality rules. To address this, Atlan supports automatic re-attachment of rules to assets, ensuring continuous enforcement of data quality. Auto re-attachment is a capability that enables Atlan to automatically re-link existing data quality rules to assets that have been dropped and recreated with the same name and structure. This prevents rules from being silently lost when underlying assets are refreshed by ELT pipelines. Important! Auto re-attachment is currently available for Snowflake only. Why it matters â When an asset is dropped and recreated, any associated data quality rules may be lost unless they're manually reattached. This can lead to gaps in data quality enforcement and loss of historical context. Without auto re-attachment: Rules become inactive after the asset is recreated. Users must manually reattach rules. Historical rule execution and incident data become disconnected. With auto re-attachment: Rules are automatically linked to the new version of the asset. No manual intervention is required. Historical context is preserved. How it works â Once enabled, Atlan continuously monitors for asset recreations. When a table or view is recreated: Atlan checks whether the asset matches a previously existing one based on name and structure. If a match is found, any undeleted rules associated with the old asset are automatically reattached. Rules are reapplied to the asset to resume enforcement. Execution history and incident tracking continue seamlessly. Tip: Reattachment typically occurs within a few minutes of asset recreation, provided the asset name and structure are unchanged and the original rule still exists. Next steps â To get started, explore: Enable auto re-attachment rules in Snowflake : Learn how to configure permissions and trigger automatic rule reattachment. Tags: data-quality auto-re-attachment Previous Configure alerts Why it matters How it works Next steps"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/concepts/data-quality-studio",
    "text": "Build governance Data Quality Studio What is Data Quality Studio On this page What's Data Quality Studio â Available via the Data Quality Studio package Data Quality Studio is Atlan's native data quality module that enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations directly within the Atlan platform. Why it exists â Data teams often rely on disconnected scripts or tools to define and run quality checks. These are typically siloed, difficult to maintain, and fail to deliver visibility to business users. This leads to: Blind spots in data pipelines Delayed issue detection Lack of trust across the organization Data Quality Studio bridges these gaps by embedding data quality into your warehouse and surfacing trust signals across Atlan, where your teams already work. What Data Quality Studio enables â With Data Quality Studio, you can: Define expectations about your data using familiar SQL logic Execute checks where your data lives, directly in the warehouse Surface trust across Atlan through warnings, trust scores, and notifications This helps build a proactive, transparent culture of data trust across your organization. Who is it for â Data Quality Studio is designed for: Analytics engineers who own data transformation pipelines Data stewards responsible for data quality and governance Business users who need visibility into data they can trust Each persona benefits from embedded checks, alerts, and transparency across the data lifecycle. Core mental model â To understand how Data Quality Studio works, here are some key terms: Rule : A SQL-based expectation about your data Rule set : A group of related rules, typically applied to a table or dataset Check run : Execution of rules in your warehouse Status : The result of a checkâpassed, failed, or warning Trust signals : Visual indicators and alerts shown in Atlan These concepts form the foundation of how data quality is evaluated and shared. How it works â Data Quality Studio uses a push-down execution model. Rules are defined through Atlanâs interface and executed natively in your data warehouse without needing additional infrastructure. The flow looks like this: Define rule sets using SQL logic that reflects your data expectations Push execution to your warehouse triggers native compute in your environment: Snowflake: Executes rules via Data Metric Functions (DMFs) Databricks: Leverages Delta Live Tables for rule execution Capture results as check runs with pass, fail, or warning statuses Surface signals through trust scores, visual indicators, and metadata in Atlan Notify users using alerts and integrations when checks fail or thresholds are breached This system ensures quality checks are versioned, repeatable, and integrated into your data stack. Key capabilities â These are the core capabilities that power the system: SQL-based rule authoring Versioned execution and history tracking Multi-rule validation per dataset Alerting and integrations with downstream tools Trust scoring and visual feedback in Atlan Query-based diagnostics for failed rules Centralized rule management These features combine to help teams operationalize trust across every dataset. Next steps â To get started, explore: Snowflake data quality setup guide : Learn how to define, execute, and manage rule sets natively in Snowflake Databricks data quality setup guide : Set up and run rule sets using Delta Live Tables in Databricks See also â Rules and dimensions reference : Explore all supported rule types, dimensions, and examples Advanced configuration : Set up notifications for failed rules, thresholds, and more Tags: data-quality governance monitoring rules Next Data quality permissions Why it exists What Data Quality Studio enables Who is it for Core mental model How it works Key capabilities Next steps See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/claude-remote-mcp",
    "text": "Configure Atlan Atlan AI Atlan MCP Remote MCP Claude with Remote MCP On this page Set up Claude with Remote MCP Private Preview You can connect Claude with the Atlan Remote MCP server to access Atlan metadata in your conversations or desktop environment. This enables you to search for assets, explore lineage, update metadata, create glossary terms, and more directly from Claude. Prerequisites â Before you begin, make sure you have: An Atlan tenant with Remote MCP enabled. If you don't have access, contact Atlan support or your Atlan customer team to request it. Claude Connector or Claude Desktop installed and updated to the latest version. Set up Claude connector â You can connect Claude Connector to Atlan Remote MCP using OAuth, which enables users to log in with their Atlan account and use their existing roles and permissions. API Key authentication isn't supported for Claude Connector. In Claude, navigate to Settings â Connectors â Organisation Connector â Add custom connector , and provide the MCP server URL . This step must be completed by an administrator. https://<your-tenant>.atlan.com/mcp Save the connector. In the Chat UI, go to Search and Tools â Add Custom Connector â Your Connector . Select the Atlan connector and complete the OAuth login flow. Once you complete authentication, the connector is ready to use. You can now use Atlan MCP tools directly in your Claude conversations. Set up Claude Desktop â Claude Desktop integrates with Remote MCP through the mcp-remote npm package. This provides a way to connect Claude Desktop with Atlan using either OAuth or API Key authentication. You can set up Claude Desktop with Remote MCP if you want to access Atlan metadata directly from your desktop environment, without switching to the web interface. This is useful for developers and analysts who prefer working locally while still leveraging Atlanâs metadata context. OAuth API Key Open Claude Desktop and go to Settings â Developer â Edit Config . Add the following configuration: { \"mcpServers\" : { \"atlan\" : { \"command\" : \"npx\" , \"args\" : [ \"mcp-remote\" , \"https://<your-tenant>.atlan.com/mcp\" ] } } } Restart Claude Desktop. A login prompt appears to complete authentication. After login, Claude Desktop connects to the Remote MCP server using your Atlan roles and permissions. You can now use Atlan MCP tools inside Claude Desktop. In Atlan, generate an API key from Admin Settings â API Keys / Tokens and copy it securely. Once the API key is generated, open Claude Desktop and go to Settings â Developer â Edit Config . Add the following configuration: { \"mcpServers\" : { \"atlan\" : { \"command\" : \"npx\" , \"args\" : [ \"mcp-remote\" , \"https://<your-tenant>.atlan.com/mcp/api-key\" , \"--header\" , \"Authorization: <your-api-key>\" ] } } } Restart Claude Desktop. The server is available with API Key authentication. You can now use Atlan MCP tools inside Claude Desktop. Troubleshooting â If you encounter InvalidGrantError: Code not valid , this typically indicates that OAuth was retried too quickly. Quit Claude Desktop, wait 5 minutes, and then reopen the application. If tools stop functioning after Claude Desktop has been open for an extended period, restart the application, the authentication token may have expired and not refreshed correctly. If you have any issues while configuring Claude Connector or Claude Desktop, contact Atlan Support for assistance. See also â Set up the Atlan MCP server locally Atlan MCP tools Tags: Atlan MCP remote Claude setup Previous Cursor with Remote MCP Next n8n with Remote MCP Prerequisites Set up Claude connector Set up Claude Desktop Troubleshooting See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/cursor-remote-mcp",
    "text": "Configure Atlan Atlan AI Atlan MCP Remote MCP Cursor with Remote MCP On this page Set up Cursor with Remote MCP Private Preview You can connect Cursor, an AI-powered editor, with the Atlan Remote MCP server. This lets you search for assets, explore lineage, update metadata, create glossary terms, and more directly from your development environment. Prerequisites â Before you begin, make sure you have: An Atlan tenant with Remote MCP enabled. If you don't have access, contact Atlan support or your Atlan customer team to request it. Cursor installed and updated to the latest version. Set up Remote MCP â You can connect Cursor to the Atlan Remote MCP server using either OAuth or an API key. Choose the method that best fits your use case. OAuth API Key You can set up OAuth if you want Cursor to connect using your Atlan login and permissions. In Cursor, go to Settings â Cursor Settings â Tools and Integration from the left-hand panel. Click New MCP Server and add the configuration: { \"mcpServers\" : { \"Atlan\" : { \"url\" : \"https://<your-tenant>.atlan.com/mcp\" } } } Save and return. Cursor prompts you to log in with your Atlan account. After selecting log in, you are redirected to the Atlan login page (SSO is supported). Once you complete the login, Cursor connects to the Remote MCP server using your roles and permissions. After setup, you can use MCP tools directly inside Cursor. For more information, see the list of Atlan MCP tools . You can set up API Key authentication if you want Cursor to connect using a static token for automation or service-to-service use cases. In Atlan, generate an API key by going to Admin Settings â API Keys / Tokens . Once the API key is generated, open Cursor and navigate to Settings â Cursor Settings â Tools and Integration from the left-hand panel. Click New MCP Server and add the configuration: { \"mcpServers\" : { \"Atlan MCP Remote API Key\" : { \"url\" : \"https://<your-tenant>.atlan.com/mcp/api-key\" , \"headers\" : { \"Authorization\" : \"<your-api-key>\" } } } } Save the configuration. The server is ready with API Key authentication, and Cursor connects to Atlan metadata through Remote MCP. After setup, you can use MCP tools directly inside Cursor. For more information, see the list of Atlan MCP tools . Need help? â If you have any issues while configuring the integration, contact Atlan Support for assistance. See also â Set up the Atlan MCP server locally Atlan MCP tools Tags: Atlan MCP remote Cursor setup Previous Remote MCP Next Claude with Remote MCP Prerequisites Set up Remote MCP Need help? See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/implement-the-atlan-mcp-server",
    "text": "Configure Atlan Atlan AI Atlan MCP Local MCP Set up Local MCP Server On this page Set up Local MCP Server The Model Context Protocol (MCP) is an open standard that enables AI agents to access contextual metadata from external systems. Atlan provides a reference implementation of MCP through the Atlan MCP server . This server acts as a bridge between Atlanâs metadata platform and AI tools such as Claude and Cursor. You can use the Atlan MCP server to support AI-driven use cases like searching for assets, understanding lineage, or updating metadata, all using real-time context from Atlan. Get started â The Atlan MCP server can be configured in multiple environments depending on your preferred development setup and integration target. Follow the instructions below to set up the server with your desired tool: Cursor Claude Local development Set up the Atlan MCP server in Cursor: Using uv : uv is a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker. Using Docker : Use Docker to run the MCP server in an isolated, containerized environment. Set up the Atlan MCP server in Claude Desktop: Using uv : uv is a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker. Using Docker : Use Docker to run the MCP server in an isolated, containerized environment. Build and run the Atlan MCP server locally: Local build guide : Walkthrough for cloning, building, and running the server locally. Need help? â For troubleshooting and feature requests, see the GitHub repo . Contact Atlan support for help with setup or integration. See also â Atlan MCP Server README on GitHub Tags: Atlan MCP setup model Previous Microsoft Copilot Studio with Remote MCP Next How to use Atlan AI for documentation Get started Need help? See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/copilot-remote-mcp",
    "text": "Configure Atlan Atlan AI Atlan MCP Remote MCP Microsoft Copilot Studio with Remote MCP On this page Set up Microsoft Copilot Studio with Remote MCP Private Preview You can connect Microsoft Copilot Studio with the Atlan Remote MCP server to enable metadata-powered experiences. Copilot Studio supports API Key authentication only. OAuth isn't available. Prerequisites â Before you begin, make sure you have: An Atlan tenant with Remote MCP enabled. If you don't have access, contact Atlan support or your Atlan customer team to request it. Microsoft Copilot Studio access with permissions to create connectors and agents. Create a custom connector â In Atlan, generate an API key by going to Admin Settings â API Keys / Tokens . In Microsoft Copilot Studio, go to Tools , and select Custom Connector . Choose Create From Blank and add a name for the connector. In the connector, go to the General Information tab and then open the Swagger Editor tab. Add the following Swagger YAML in the editor: swagger : '2.0' info : title : Atlan description : Atlan ModelContextProtocol server for copilot testing version : 1.0.0 host : <your - tenant > .atlan.com basePath : / schemes : - https paths : /mcp/api-key : post : summary : Atlan copilot mcp server x-ms-agentic-protocol : mcp - streamable - 1.0 consumes : - application/json operationId : InvokeMCP responses : '200' : description : Success parameters : - name : Accept in : header required : true type : string enum : - text/event - stream , application/json default : text/event - stream , application/json securityDefinitions : api_key : type : apiKey in : header name : Authorization security : - api_key : [ ] After adding the Swagger YAML, click Create Connector and wait for it to update. Create and configure an agent â After setting up the custom connector, you now need to create an agent in Copilot Studio and link it to the connector so it can access Atlanâs MCP tools. In Copilot Studio, go to Agents . Create a new agent by adding a name and description, then click Create . In the new agent, go to the Tools tab and click Add a Tool . Search for the custom connector you created earlier and add it. In Connection , choose Create New Connection and add the API key for your tenant. Click Add to Agent . Then click on the newly added tool name. Reload inputs and from the options dropdown choose: text/event-stream, application/json Go to Tools and reload. You'll now see all the tools available. Open the chatbot panel and run a query. If prompted to connect, use Connection Manager to create or select a connection and submit. Once connected, queries run successfully through Atlan MCP. For more information, see the list of Atlan MCP tools . Need help? â If you have any issues while configuring Microsoft Copilot Studio with Remote MCP, contact Atlan Support for assistance. Tags: Atlan MCP remote Microsoft Copilot Studio setup Previous Windsurf with Remote MCP Next Set up Local MCP Server Prerequisites Create a custom connector Create and configure an agent Need help?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/n8n-remote-mcp",
    "text": "Configure Atlan Atlan AI Atlan MCP Remote MCP n8n with Remote MCP On this page Set up n8n with Remote MCP Private Preview You can integrate n8n with the Atlan Remote MCP server to use Atlan metadata within your automated workflows. Authentication in n8n is supported only through API keys, OAuth isn't available. Prerequisites â Before you begin, make sure you have: An Atlan tenant with Remote MCP enabled. If you don't have access, contact Atlan support or your Atlan customer team to request it. n8n installed and updated to the latest version. n8n AI Agent node and LiteLLM n8n doesn't include a native LiteLLM node for the Agent chat model. Community LiteLLM nodes may fail or return unexpected responses. If you need LiteLLM models, use the OpenAI Chat Model node with the Atlan gateway as the base URL and your Atlan API key. Set up Remote MCP in n8n â You can configure n8n with the Atlan Remote MCP server using API Key authentication. In Atlan, generate an API key by going to Admin Settings â API Keys / Tokens . In n8n, create a new Workflow . Add an MCP Client node to the workflow. In the MCP Client node, choose the action List Tools . This enables you to test the connection and see available tools. Under Credentials , select Create New Credentials and fill the details: HTTP Stream URL : The endpoint for Remote MCP with API Key. https://<your-tenant>.atlan.com/mcp/api-key HTTP Connection Timeout : Time in milliseconds to wait for a response. 60000 Additional Headers : Add the API key for authentication. Authorization = <your-api-key> Save the credentials. When you execute the node with List Tools , you see all the MCP tools available in your tenant. To run a specific tool: In the MCP Client node, change Operation to Execute Tool . Select the tool you want to use (for example, search_assets ). Fill in the required parameters in the node configuration. For example, you can configure the node to search for ten tables in Snowflake by setting the query to tables , the limit to 10 , and selecting the Snowflake connection. Need help? â If you have any issues while configuring n8n with Remote MCP, contact Atlan Support for assistance. Tags: Atlan MCP remote n8n setup Previous Claude with Remote MCP Next Windsurf with Remote MCP Prerequisites Set up Remote MCP in n8n Need help?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/windsurf-remote-mcp",
    "text": "Configure Atlan Atlan AI Atlan MCP Remote MCP Windsurf with Remote MCP On this page Set up Windsurf with Remote MCP Private Preview You can connect Windsurf with the Atlan Remote MCP server to bring Atlan metadata into your AI-driven flows. Windsurf supports API Key authentication, OAuth isn't available. Prerequisites â Before you begin, make sure you have: An Atlan tenant with Remote MCP enabled. If you don't have access, contact Atlan support or your Atlan customer team to request it. Windsurf installed and updated to the latest version. Set up Remote MCP in Windsurf â You can set up Windsurf with the Remote MCP server using API Key authentication. In Atlan, generate an API key by going to Admin Settings â API Keys / Tokens , creating a new key, and copying it securely. For Remote MCP with API Key, use the following endpoint: https://<your-tenant>.atlan.com/mcp/api-key In Windsurf, open the MCP configuration file and add the following configuration: { \"mcpServers\" : { \"Atlan MCP Remote\" : { \"serverUrl\" : \"https://<your-tenant>.atlan.com/mcp/api-key\" , \"headers\" : { \"Authorization\" : \"<your-api-key>\" } } } } Save the configuration. Windsurf is now connected to Atlan Remote MCP with API Key authentication. After setup, you can use MCP tools directly inside Windsurf. For more information, see the list of Atlan MCP tools . Need help? â If you have any issues while configuring Windsurf with Remote MCP, contact Atlan Support for assistance. Tags: Atlan MCP remote Windsurf setup Previous n8n with Remote MCP Next Microsoft Copilot Studio with Remote MCP Prerequisites Set up Remote MCP in Windsurf Need help?"
  },
  {
    "url": "https://docs.atlan.com/faq",
    "text": "Configure Atlan Frequently Asked Questions On this page Frequently Asked Questions Overview: Get quick answers to frequently asked questions about Atlan's features, functionality, and common use cases. Browse by category to quickly locate solutions and learn best practices for getting the most out of your Atlan workspace. Core platform â Find answers efficiently by exploring these main FAQ categories: Administration and user management â Administration and configuration : Comprehensive guide to Atlan administration, from workspace configuration to system management and troubleshooting User management and access control : Managing users, groups, roles, permissions, and access control policies in Atlan Capabilities â Discovery : Filtering, search, and navigation tips in Atlan Insights : Query limits, scheduling, and Visual Query FAQs Reporting : Materialized views, dashboard metadata, and more Lineage : Supported lineage types, troubleshooting, and best practices Custom metadata : Managing custom metadata sets and attributes Glossary : Glossary setup, backups, permissions, and term management Data management â Data connections and integration : Everything about connecting and integrating various data sources with Atlan, including configuration and troubleshooting Tags and metadata management : Complete guide to managing tags, classifications, and metadata in your data catalog Workflows and data processing : Data processing workflows, automation, scheduling, and optimization techniques Integrations and connectors â Browser extension integration : Installing, troubleshooting, and using Atlan's browser extension Single sign-on (SSO) integration : Configuring and managing SSO providers Jira integration : Creating issues, field mappings, and common troubleshooting Salesforce connector : Prerequisites, metadata extraction, and common issues Microsoft Power BI connector : Data sources vs dataflows and other connector specifics Connector framework FAQs : General questions around connectivity capabilities and supported sources Microsoft Teams integration : Linking channels, notifications, and common issues Slack integration : Sharing assets, permissions, and troubleshooting Advanced features â AI and automation features : Guide to Atlan's AI capabilities and automation features for enhanced data operations Security and compliance : Security protocols, compliance standards, and data protection measures in Atlan Support â Need additional help beyond these FAQs? Contact support : Reach out to the support team through the Atlan platform or raise a ticket . Tags: faq help support troubleshooting guides Next Administration and Configuration Core platform Support"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/troubleshooting/troubleshooting-fivetran-connectivity",
    "text": "Connect data ETL Tools Fivetran Troubleshooting Troubleshooting Fivetran connectivity On this page Troubleshooting Fivetran connectivity What are the known limitations of the Fivetran connector? â Atlan currently doesn't support the following: There are two types of alerts in Fivetran -  errors and warnings. While errors are supported with the Fivetran Platform Connector package, warnings are currently not supported. Why is lineage missing? â If you notice any Fivetran lineage missing in Atlan after migrating to the Fivetran Platform Connector in both Fivetran and Atlan, following may be the possible causes: If any Fivetran Platform Connector tables or schema are dropped or renamed in the destination warehouse, then Fivetran recreates them with the original names in the next sync. However, it only adds any new incremental data. To access the historical data or fix any data integrity issues, you may need to trigger a re-sync. Refer to Fivetran documentation to learn more. Why are partial assets created instead of connecting lineage to existing assets or Why does lineage show up wrong across environments? â If you notice incorrect lineage such as a partial asset being created when a catalogued asset already exists in Atlan, or incorrect lineage across different environments, please contact Fivetran team to enable \"connection_details\" private preview feature. This feature gives Atlan the host and database information needed to accurately stitch together the end-to-end lineage. Why is click-through to Fivetran not working? â If you click View in Fivetran in Atlan and you are redirected to the Fivetran homepage, then you may not have the necessary permissions in Fivetran to view the connector. You must have at least a Connector Reviewer role or any other role hierarchically to view Fivetran connectors. Tags: lineage data-lineage impact-analysis alerts monitoring notifications Previous What does Atlan crawl from Fivetran?"
  },
  {
    "url": "https://docs.atlan.com/tags/dashboards",
    "text": "6 docs tagged with \"dashboards\" View all tags Allow members to view reports Permission to view the [governance](/product/capabilities/reporting/how-tos/report-on-governance) and [automations](/product/capabilities/reporting/how-tos/report-on-automations) dashboards in the reporting center is reserved for admin users only. Security monitoring Learn about security monitoring. Troubleshooting Looker connectivity Learn about troubleshooting looker connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity. Troubleshooting Salesforce connectivity Learn about troubleshooting salesforce connectivity. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase."
  },
  {
    "url": "https://docs.atlan.com/tags/visualization",
    "text": "6 docs tagged with \"visualization\" View all tags Allow members to view reports Permission to view the [governance](/product/capabilities/reporting/how-tos/report-on-governance) and [automations](/product/capabilities/reporting/how-tos/report-on-automations) dashboards in the reporting center is reserved for admin users only. Security monitoring Learn about security monitoring. Troubleshooting Looker connectivity Learn about troubleshooting looker connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity. Troubleshooting Salesforce connectivity Learn about troubleshooting salesforce connectivity. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase."
  },
  {
    "url": "https://docs.atlan.com/tags/analytics",
    "text": "8 docs tagged with \"analytics\" View all tags Allow members to view reports Permission to view the [governance](/product/capabilities/reporting/how-tos/report-on-governance) and [automations](/product/capabilities/reporting/how-tos/report-on-automations) dashboards in the reporting center is reserved for admin users only. Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan. Reporting Generate comprehensive reports on your data assets, usage, and governance. Security monitoring Learn about security monitoring. Troubleshooting Looker connectivity Learn about troubleshooting looker connectivity. Troubleshooting Redash connectivity Learn about troubleshooting redash connectivity. Troubleshooting Salesforce connectivity Learn about troubleshooting salesforce connectivity. What does Atlan crawl from Metabase? Atlan crawls and maps the following assets and properties from Metabase."
  },
  {
    "url": "https://docs.atlan.com/faq/administration-and-configuration",
    "text": "Configure Atlan Frequently Asked Questions Administration and Configuration On this page Administration and Configuration Complete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization. How do I change my Atlan URL? â To change the URL of your Atlan instance, contact Atlan support . Requests for instances hosted on Microsoft Azure and Google Cloud Platform may take longer to complete. If you use integrated SSO in Atlan, ask your SSO administrator to reconfigure the Atlan URL in your SSO provider to maintain connectivity. How can I change the workspace name? â You must be an admin user to be able to change the name of your Atlan workspace. To update the workspace name: From the left menu in Atlan, click Admin . In the Overview tab, under Workspace name , click the pencil icon to edit the workspace name. In the Workspace name dialog, enter a name for your Atlan workspace and then click Update . For example, you can use the name of your organization to customize your Atlan workspace. After you update the name, the change is visible to all your Atlan users. How can I add my own branding? â To add your own logo to Atlan, any admin user can go to the Admin Center and click the image in the Overview section to upload a new logo. Atlan resizes the logo automatically, maintaining its aspect ratio. Atlan recommends that you use the following size and format for your logo: A logo with a 1:1 aspect ratio In either PNG or JPEG format With a size of 150x150 pixels I am having issues editing a persona, purpose, or policy â If you have the Admin role (or another role that grants edit access) but still can't modify a persona, purpose, or policy, check the name of the item you are trying to change. Names can't include special characters such as \\\\ , * , and similar symbols. Dashes ( - ) are allowed. If the name already contains unsupported characters, or you continue to encounter errors after correcting the name, please raise a support ticket . How do I access audit logs? â Open any asset's activity log and audit all activity by using the auditSearch API. Configure the request with your preferred filtersâsuch as time range or action typeâto retrieve the audit records you need. Can restricting access to certain assets impact the lineage view? â Access control policies don't change which assets appear in the lineage graph . If an asset is restricted, its properties and attributes are simply hidden in the asset sidebar. The metadata policy assigned to each user determines which asset metadata they can view. Tags: integration api configuration faq-administration Previous Frequently Asked Questions Next AI and Automation Features"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/faq/auto-assign-owners",
    "text": "Configure Atlan Integrations Project Management Jira FAQ Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Can I configure additional fields or auto-assign owners to Jira tickets created from Atlan? Refer to our troubleshooting Jira documentation to learn more. Tags: integration faq faq-integrations Previous What is included in the Jira integration? Next Can site renaming affect the Jira integration?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/create-glossary-backups",
    "text": "Build governance Glossary FAQ Can I create backups of glossaries? Can I create backups of glossaries? Currently, deleting a glossary and its components will result in their permanent deletion. Therefore, when you attempt to archive a glossary, you will get a popup message asking you to confirm your intended action: Are you sure you want to archive [insert glossary name] and all its contents? Atlan also allows you to export your glossary assets to spreadsheets and keep a record of your contextual information. If you inadvertently archived your glossary, Atlan creates a daily backup and you could restore the backup snapshot from the previous day. This will override any other changes that were made in the instance since that time. Note that there is no way to selectively restore specific elements that have been deleted. Hence, Atlan advises caution before proceeding with any delete actions. Tags: integration faq faq-governance Previous Use personas to update a term in a glossary Next Fully delete glossary terms or archived items"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/send-messages-search-assets-slack",
    "text": "Configure Atlan Integrations Collaboration Slack FAQ How do I send messages or search assets from Slack? How do I send messages or search assets from Slack? Sending messages and searching assets from Slack are disabled. Refer to Troubleshooting Slack to learn more. Tags: integration faq faq-integrations Previous Troubleshooting Slack Next Slack permissions explained"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-glossary-visibility",
    "text": "Configure Atlan Administration Feature Management Restrict glossary visibility On this page Restrict glossary visibility Private Preview Who can do this? You will need to be an admin user in Atlan to restrict glossary visibility. Note that asset access works slightly differently . Once you have restricted glossary visibility: If a glossary policy has been configured for a selected persona, only the user belonging to that persona be able to view the glossaries curated through glossary policies. If a user isn't part of any persona, they're unable to view any glossaries in Atlan. If a user is part of a persona that doesn't have glossary policies specifically or any policies at all configured, the user is unable to view all glossaries. The restriction is applicable to the Assets page, glossary tree on the Glossary page, terms filter on the Assets page, terms in an asset sidebar, linked assets , and related terms . The All glossaries Â view on the Glossary Â page is enabled for all users by default. To turn it off for your member and guest users, complete the following steps. To disable all glossaries view: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access control heading on the Labs page, turn on Persona switcher in Glossary . Your member and guest users now only have access to the curated glossaries for their persona! ð If you'd like to restore the default all glossaries view, repeat steps 1 to 3 and then turn off the toggle. Summary â Persona switcher in Glossary in Labs is OFF: Users can see all glossaries. Persona switcher in Glossary in Labs is ON: Users can't see any glossaries until provided in a persona. Did you know? If a user doesn't belong to any persona and the All glossaries view is disabled, then the user is prompted to reach out to their Atlan administrator and request to be added to a persona. Tags: glossary business-terms definitions Previous Restrict asset visibility Next How to view event logs Summary"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-support",
    "text": "One doc tagged with \"faq-support\" View all tags Support and Technical Help Complete guide to getting support, understanding API limits, and accessing technical assistance for Atlan."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/references/what-does-atlan-crawl-from-sap-hana",
    "text": "Connect data Databases SQL Databases SAP HANA References What does Atlan crawl from SAP HANA? On this page What does Atlan crawl from SAP HANA? Atlan crawls and maps the following assets and properties from SAP HANA. Schemas â Atlan maps schemas from SAP HANA to its Schema asset type. Source property Atlan property SCHEMA_NAME name TABLE_COUNT tableCount VIEW_COUNT viewsCount Tables â Atlan maps tables from SAP HANA to its Table asset type. Source property Atlan property TABLE_NAME name COMMENTS description COLUMN_COUNT columnCount ROW_COUNT rowCount TABLE_SIZE sizeBytes IS_PARTITIONED isPartitioned IS_TEMPORARY isTemporary Views â Atlan maps views from SAP HANA to its View asset type. Source property Atlan property VIEW_NAME name COMMENTS description COLUMN_COUNT columnCount DEFINITION definition Calculation views â Atlan maps calculation views from SAP HANA to its CalculationView asset type. Atlan supports upstream lineage to source tables and calculation views. Column-level lineage is currently not supported. Source property Atlan property OBJECT_NAME name COLUMN_COUNT columnCount VERSION_ID calculationViewVersionId ACTIVATED_BY calculationViewActivatedBy ACTIVATED_AT calculationViewActivatedAt Columns â For tables and views â Atlan maps columns for tables and views from SAP HANA to its Column asset type. Source property Atlan property COLUMN_NAME name COMMENTS description POSITION order DATA_TYPE_NAME dataType IS_NULLABLE isNullable LENGTH maxLength CONSTRAINT (PRIMARY KEY) isPrimary REFERENCED_COLUMN_NAME isForeign DECIMAL_DIGITS precision SCALE numericScale DEFAULT_VALUE defaultValue For calculation views â Atlan maps columns for calculation views from SAP HANA to its Column asset type. Source property Atlan property COLUMN_NAME name LABEL description ORDER order COLUMN_SQL_TYPE dataType Â Â Â Â Â Â Â Â Â Â Â  - columnIsMeasure MEASURE_TYPE columnMeasureType The columnIsMeasure property is derived as a Boolean value based on the MEASURE_TYPE field. Stored procedures â Atlan maps stored procedures in SAP HANA to its Procedure asset type. Source property Atlan property PROCEDURE_NAME name PROCEDURE_TYPE subType DEFINITION definition Tags: schema schema-drift schema-monitoring Previous Crawl SAP HANA Next Preflight checks for SAP S/4HANA Schemas Tables Views Calculation views Columns Stored procedures"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/references/what-does-atlan-crawl-from-alteryx",
    "text": "Connect data ETL Tools Alteryx References What does Atlan crawl from Alteryx? On this page What does Atlan crawl from Alteryx? Private preview This document outlines the Alteryx metadata that Atlan crawls and maps as part of its cataloging process. Atlan supports two ingestion modes with varying property coverage. Once Alteryx is integrated with Atlan, connector-specific filters can be used for efficient asset discovery. Supported filters â Filter Type Description Status filter Filters workflows by their last run status. Duration filter Filters workflows by their last run duration. Asset mapping â Alteryx workflows are represented in Atlan using the AlteryxWorkflow asset type. The following table shows how Alteryx properties are mapped to Atlan properties: Alteryx Source Property Mapped Atlan Property eventTime alteryxRunStartTime eventTime alteryxRunEndTime eventType alteryxRunOpenLineageState runid alteryxRunID job.owners sourceOwner workflowID alteryxWorkflowID run.errorMessage alteryxWorkflowError Multiple eventTime fields are mapped to both Run Start Time and Run End Time , depending on context. Workflow run metadata includes operational details like error messages and lineage states, which are essential for observability and debugging. See also â Set up Alteryx Previous Set up Alteryx Next Connection issues Supported filters Asset mapping See also"
  },
  {
    "url": "https://docs.atlan.com/tags/insights",
    "text": "One doc tagged with \"insights\" View all tags Insights Query and analyze your data using Atlan's powerful query builder and SQL capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/query",
    "text": "One doc tagged with \"query\" View all tags Insights Query and analyze your data using Atlan's powerful query builder and SQL capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/analysis",
    "text": "One doc tagged with \"analysis\" View all tags Insights Query and analyze your data using Atlan's powerful query builder and SQL capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-insights",
    "text": "9 docs tagged with \"faq-insights\" View all tags Are there any limits on concurrent queries? Learn about are there any limits on concurrent queries?. Can I query any DW/DL? You can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature. Can I turn off sample data preview for the entire organization? Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data. Can we restrict who can query our data warehouse? Learn about can we restrict who can query our data warehouse?. How can I identify an Insights query in my database access log? Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs. How to use parameterized queries? Learn about how to use parameterized queries?. Monitor for runaway queries? Learn about monitor for runaway queries?. What controls the frequency of queries? Learn about what controls the frequency of queries?. Why do I only see tables from the same schema to join from in a visual query? When [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder."
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/slack-permissions",
    "text": "Configure Atlan Integrations Collaboration Slack FAQ Slack permissions explained On this page What does Atlan do with each Slack permission? As you set up the Slack integration , you will need to provide an Atlan Slack app permissions in Slack. Note the following: Atlan does not store any data for Slack messages. Slack messages are fetched on demand. Atlan connects to Slack via Slack APIs, and stores the following metadata to send messages on behalf of users   -  username, channel name, tenant name, refresh token, and access token. Atlan does not store any personally identifiable information of users. Here is what Atlan does with each of those permissions: What will Atlan be able to view? â Content and info about you â View profile details about people in your workspace ( users.profile:read ) View information about your identity Atlan requires these permissions to: Show rich Slack message previews within Atlan (for example, a user's profile picture) Content and info about channels & conversations â View messages and other content in your public channels ( channels:history ) View basic information about public channels in your workspace ( channels:read ) View Atlan URLs in messages ( links:read ) View messages and other content in public channels that Atlan has been added to ( channels:history ) Atlan requires these permissions to: Show rich Slack message previews within Atlan (the message itself, the channel where it was posted) Content and info about your workspace â View people in your workspace ( users:read ) View profile details about people in your workspace ( users.profile:read ) View email addresses of people in your workspace ( users:read.email ) Atlan requires these permissions to: Show rich Slack message previews within Atlan (for example, a user's profile picture) Send notifications about requests made in Atlan ( users:read.email ) What will Atlan be able to do? â Perform actions in channels and conversations â Show previews of Atlan URLs in messages ( links:write ) Send messages as @atlan ( chat:write ) Send messages to channels @atlan isn't a member of ( chat:write.public ) Add and edit emoji reactions View messages that directly mention @atlan in conversations that the app is in Upload, edit, and delete files as Atlan ( files.write ) Atlan requires these permissions to: Unfurl Atlan links (show previews for them) within Slack Allow the Slack share button on Atlan assets to work (sending messages to Slack as @atlan ) Search Atlan from any Slack channel and share search results to the channel, all from within Slack Share query output from within Atlan to Slack ( files.write ) Perform actions in your workspace â Add shortcuts and/or slash commands that people can use ( commands ) Atlan requires these permissions to: Search Atlan from within Slack (the /search-term and /search-query commands) Document a Slack conversation into Atlan (using the Slack context menu on any Slack conversation) Tags: slack faq faq-integrations Previous How do I send messages or search assets from Slack? Next What is included in the Slack integration? What will Atlan be able to view? What will Atlan be able to do?"
  },
  {
    "url": "https://docs.atlan.com/tags/monte-carlo",
    "text": "One doc tagged with \"monte carlo\" View all tags Monte Carlo Integrate, catalog, and govern Monte Carlo assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/observability",
    "text": "3 docs tagged with \"observability\" View all tags Anomalo Integrate, catalog, and govern Anomalo assets in Atlan. Monte Carlo Integrate, catalog, and govern Monte Carlo assets in Atlan. Soda Integrate, catalog, and govern Soda assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/faq/delete-glossary-terms",
    "text": "Build governance Glossary FAQ Fully delete glossary terms or archived items How do I fully delete glossary terms or archived items? To fully delete glossary terms or archived items: Archived assets are soft-deleted . This means that those assets will not be cluttering the UI but are still available to search for and even recover if needed. If absolutely essential, assets can be hard-deletedÂ via API . At that point, they are gone forever. Tags: api rest-api graphql glossary business-terms definitions faq-governance Previous Can I create backups of glossaries?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/snowflake-lookml-table-matching",
    "text": "Use data Lineage FAQ How does a Snowflake connection know that a table referenced in LookML is actually the same table? How does a Snowflake connection know that a table referenced in LookML is actually the same table? Each project in Looker can only be linked to a source that is individually obtained from the Looker APIs. In addition, the API provides the default database name while the LookML files contain the table name. Atlan links the two pieces of information to generate lineage. Tags: lineage data-lineage impact-analysis api rest-api graphql faq-lineage Previous How do you enable data lineage for different data sources? Next How does Atlan handle lineage from Spark jobs?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/tableau-upstream-lineage",
    "text": "Use data Lineage FAQ How to obtain upstream lineage if I connect to a Tableau data asset? How to obtain upstream lineage if I connect to a Tableau data asset? To understand how Atlan generates upstream lineage for supported sources, see What is lineage? . For Tableau specifically, Atlan also retrieves asset lineage data through the Tableau APIs . Tags: lineage data-lineage impact-analysis upstream-dependencies data-sources api rest-api graphql faq-lineage Previous How is the Atlan lineage graph depicted using Power BI measures? Next Is there a way to build lineage from NetSuite to Snowflake?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage/how-tos/integrate-google-cloud-composer-openlineage",
    "text": "Connect data Orchestration & Workflow Google Cloud OpenLineage Get Started How to integrate Google Cloud Composer/OpenLineage On this page Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to OpenLineage configuration and facets . Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . Create an API token in Atlan â Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan â Select the source in Atlan â To select Google Cloud Composer/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Google Cloud Composer Airflow Assets and then click Setup Workflow . Create the connection â danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Google Cloud Composer/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host , enter the URL of your Google Cloud Composer Airflow UI. This will allow Atlan to help you view your assets directly in Google Cloud Composer from the asset profile. (Optional) For Port , enter the port number for your Google Cloud Composer Airflow UI. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Google Cloud Composer â Did you know? You will need the Atlan API token and connection name to configure the integration in Google Cloud Composer. This will allow Google Cloud Composer to connect with the OpenLineage API and send events to Atlan. danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. To configure Google Cloud Composer to send OpenLineage events to Atlan: You will need to configure Google Cloud Composer for the integration: Open your Google Cloud console and navigate to the Environments page. From the list of environments, click the name of your environment. Configure the following: For Apache Airflow versions 2.7.0 onward, set override Airflow configuration options : In the Environment details page, click the Airflow configuration overrides tab and then click Edit . In the Airflow configuration overrides form, click the Add Airflow configuration override button to specify the first set of values: For Section 1 , enter openlineage . For Key 1 , enter namespace . For Value 1 , enter the connection name as exactly configured in Atlan. Click the Add Airflow configuration override button to specify the second set of values: For Section 2 , enter openlineage . For Key 2 , enter transport . For Value 2 , enter the following: { \"type\" : \"http\" , \"url\" : \"https://<instance>.atlan.com/events/openlineage/airflow-cloud-composer/\" , \"auth\" : { \"type\" : \"api_key\" , \"api_key\" : \"<API_KEY>\" } } For <API_key> , set the API token generated in Atlan. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0, set environment variables : In the Environment details page, click the Environment variables tab and then click Edit . Add the following environment variable names and corresponding values: OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow-cloud-composer/ . OPENLINEAGE_API_KEY : set the API token generated in Atlan. OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. Click Save to save your changes. You will also need to install the OpenLineage PyPI package in Google Cloud Composer. To install the OpenLineage PyPI package in your environment: In the Environment details page, click the PyPI packages tab and then click Edit . Click Add package to add a custom package. Under PyPI packages , for Package name , specify the package name. For Apache Airflow versions 2.7.0 onward: apache-airflow-providers-openlineage For Apache Airflow versions 2.5.0 onward: openlineage-airflow Click Save to save your configuration. Verify the Atlan connection in Google Cloud Composer â To verify connectivity to Google Cloud Composer: For Verify connection with Cloud Composer , click the clipboard icon to copy and run the preflight check DAG on your Google Cloud Composer instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! ð You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: lineage data-lineage impact-analysis api rest-api graphql Previous Google Cloud Composer OpenLineage Next What does Atlan crawl from Google Cloud Composer/OpenLineage? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Google Cloud Composer Verify the Atlan connection in Google Cloud Composer"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/set-up-qlik-sense-enterprise-on-windows",
    "text": "Connect data BI Tools On-premises & Enterprise BI Qlik Sense Enterprise on Windows Get Started Set up Qlik Sense Enterprise on Windows On this page Set up Qlik Sense Enterprise on Windows Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps   -  you may not have access yourself. Create user in Qlik Sense Enterprise on Windows â Did you know? By default, your identity provider for your Qlik Sense Enterprise on Windows version will be Microsoft Windows. So, your Microsoft Windows users will be your Qlik users. To add a new user in this case, you only need to create a new Windows local account . We recommend that you create a new user Qlik Sense Enterprise on Windows for integration with Atlan. To create a new user, follow the steps in the Microsoft Windows documentation and then add the new user: Log in to your Qlik Sense Enterprise on Windows instance. Navigate to the active directory or identity provider of your Qlik Sense Enterprise on Windows version and add the new user. Allocate user access â Once you've created a new user, you will need to allocate user access for integration with Atlan. To allocate user access to the new user: Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . To allocate a license to the new user, in the left menu,Â click License management . In the right panel for License management , click Professional access allocations . At the bottom of the Professional access allocations screen, click Allocate . From the Users dialog, select the new user you created and click Allocate to complete user allocation. You can also set up roles and groups for robust access management. Set permissions â Did you know? Atlan does not make any API requests or queries that will update the objects in your Qlik Sense Enterprise on Windows instance. Once you've added the new user, you will need to provide the new user with Read permission to your streams, apps, sheets, charts, and connections. To set the minimum permissions required to crawl Qlik Sense Enterprise on Windows :Â Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . In the left menuÂ under Manage Resources , click Security rules . At the bottom of the Security rules screen, click Create new .Â In the Edit security dialog, enter the following details: For Name , enter a meaningful name for your security rule.Â For Basic , under Actions , click Read to provide Read access. At the bottom of the dialog, click Apply to apply your security rule.Â danger If JWT authentication is already enabled for your Qlik Sense Enterprise on Windows instance, you can proceed to generating a JWT . If Windows authentication is already enabled for your Qlik Sense Enterprise on Windows instance, you can directly proceed to crawling Qlik Sense Enterprise on Windows . (Optional) Create a virtual proxy â Once you've set permissions for the new user, you can create a virtual proxy for authentication. Atlan supports the following authentication methods for Qlik Sense Enterprise on Windows: Windows authentication â Did you know? When Qlik Sense Enterprise on Windows is installed, it automatically creates a default virtual proxy called Central Â without a prefix that supports Windows authentication. If it is still available on your instance, you can skip creating a new one and simply edit it. To create a virtual proxy for Windows authentication: Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . In the left menu under Configure Systems , click Virtual proxies . At the bottom of the Virtual proxies screen, click Create new . In the Edit virtual proxy screen: For Identification , enter the following details: For Description , add a description for your virtual proxy. For Prefix , add a path name in the proxyâs URI   -  use only lowercase letters for the prefix. For Session cookie header name , add the name of the HTTP header used for the session cookie. For Authentication ,Â enter the following details: For Authentication method , select Ticket as the authentication method.Â For Windows authentication pattern , select Windows .Â Click Apply to save your authentication details. JWT authentication â To create a virtual proxy for JSON Web Token (JWT) authentication: Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . In the left menu under Configure Systems , click Virtual proxies . At the bottom of the Virtual proxies screen, click Create new . In the Edit virtual proxy screen: For Identification , enter the following details: For Description , add a description for your virtual proxy. For Prefix , add a path name in the proxyâs URI   -  use only lowercase letters for the prefix. For Session cookie header name , add the name of the HTTP header used for the session cookie. For Authentication ,Â enter the following details: For Authentication method , select JWT .Â For JWT certificate , you can either: To generate a key pair using openssl , open the public.key file in a text editor of your choice, copy the key, and paste it_._ To use the same certificate as your Qlik Sense Enterprise on Windows instance, you can find it in the path C:\\ProgramData\\Qlik\\Sense\\Repository\\Exported Certificates\\.Local Certificates . Open the server.pem file in a text editor of your choice, copy the content, and paste it. For JWT attribute for user ID , add the JWT attribute name for the attribute describing the user ID.Â For JWT attribute for user directory , add JWT attribute name for the attribute describing the user directory. (Optional) Under Advanced , for Host allow list , add the host IP addresses of your Qlik Sense Enterprise on Windows deployment. Click Apply to save your authentication details. (Optional) Generate a JWT â To generate a JSON Web Token (JWT) for crawling Qlik Sense Enterprise on Windows : Open the JWT website. At the top of the screen, click Debugger . For Algorithm , click the dropdown arrow and select RS256 . For Payload , add the user ID and directory for your virtual proxy. For Verify signature , paste the server_key.pem (private key) and server.pem (public key) pair from C:\\ProgramData\\Qlik\\Sense\\Repository\\Exported Certificates\\.Local Certificates in the appropriate fields. In the left Encoded field, copy the generated token and save it in a temporary location. danger To confirm that you've used the right key pair, navigate to the bottom of the screen and ensure that you can see the Signature Verified status. Tags: api rest-api graphql Previous Qlik Sense Enterprise (Windows) Next Crawl Qlik Sense Enterprise on Windows Create user in Qlik Sense Enterprise on Windows Allocate user access Set permissions (Optional) Create a virtual proxy (Optional) Generate a JWT"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/troubleshooting/troubleshooting-matillion-connectivity",
    "text": "Connect data ETL Tools Matillion Troubleshooting Troubleshooting Matillion connectivity On this page Troubleshooting Matillion connectivity What are the known limitations of the Matillion connector? â Atlan currently only supports the following: Lineage for asset transformations for Matillion version 1.68 LTS   -  end-to-end lineage is currently not supported. Column-level lineage is currently not supported due to limitations of the Matillion ETL Metadata API . Task history for Matillion components is calculated based on the last 7 days of metadata crawled from the Matillion APIs. Tags: lineage data-lineage impact-analysis api rest-api graphql Previous What lineage does Atlan extract from Matillion?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/troubleshooting/troubleshooting-trino-connectivity",
    "text": "Connect data Databases Query Engines Trino Troubleshooting Troubleshooting Trino connectivity On this page Troubleshooting Trino connectivity What are the known limitations of the Trino connector? â Materialized views from Trino may be displayed as tables in Atlan due to limitations at source. How to debug test authentication and preflight check errors? â TLS/SSL required for basic authentication TLS/SSL is required for authentication with username and password. Ensure thatÂ TLS/SSLÂ is correctly configured on the Trino server. This typically involves configuring the config.properties file for each Trino node. Once you have updated the configuration, restart the Trino services to apply the new settings. Ensure that the TLS/SSL certificates used by Trino are valid and unexpired. Disabled TLS/SSL Connection property SSLVerification requires TLS/SSL to be enabled. Check your connection settings to ensure that TLS/HTTPS is enabled in the crawler configuration . Incorrect credentials The username or the password provided to connect to the Trino account is incorrect. Log in to your Trino account for the specified host and verify that the username and password are correct. You can also create a new user , if required. Incorrect host address Your Trino hostname is not correct, please provide a correct hostname and retry. The host address provided by the user is not correct, please check the host address. Ensure that the hostname you have specified is correct   -  it should be a domain name. Server connection failure The host or port provided by the user is not correct, please check your host and port. Ensure that the hostname you have specified is correct   -  it should be a domain name. Verify the port number. Authentication failed: Unauthorized Authentication failed: Unauthorized Verify your credentials in Trino to confirm you have the necessary permissions to connect . SocketTimeoutException: connection timed out Connection timed out. Please check your host and port. Ensure that you have specified the hostname and port number correctly in your connection settings. Check if you can reach the Trino server from your machine. Use ping <host> or telnet <host> <port> to test connectivity. Tags: api rest-api graphql Previous Preflight checks for Trino"
  },
  {
    "url": "https://docs.atlan.com/tags/data-flow",
    "text": "One doc tagged with \"data-flow\" View all tags Lineage Track and visualize data lineage across your data landscape to understand data flow and dependencies."
  },
  {
    "url": "https://docs.atlan.com/tags/dependencies",
    "text": "9 docs tagged with \"dependencies\" View all tags Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Integrate Atlan with Google Sheets The Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan. Lineage Track and visualize data lineage across your data landscape to understand data flow and dependencies. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?."
  },
  {
    "url": "https://docs.atlan.com/tags/downstream-impact",
    "text": "8 docs tagged with \"downstream-impact\" View all tags Column Level Lineage Data lineage shows the upstream and downstream dependencies of an asset. For a more granular view of these dependencies, you can view column-level lineage. Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to [view]( view-impacted-assets), [download]( download-lineage), and [export]( export-impacted-assets) your impacted assets and share it with others in your organization. Download impacted assets in Google Sheets Once you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage impact-analysis). Integrate Atlan with Google Sheets The Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan. Troubleshooting Amazon MSK connectivity Learn about troubleshooting amazon msk connectivity. Troubleshooting MongoDB connectivity Learn about troubleshooting mongodb connectivity. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/summarize-metadata",
    "text": "Use data Reporting Metadata Summarize metadata Summarize metadata Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more. Use the following dashboards in the reporting center to: Assets -  monitor your assets Glossary -  track metrics for your glossaries, categories, and terms Governance -  review your governance setup Insights -  track metrics for your queries Automations -  monitor asset enrichment through automation features Usage and cost -  track asset usage and associated costs Tags: glossary business-terms definitions Previous Report on governance Next Report on assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/references/lineage-generator",
    "text": "Use data Lineage References Lineage Generator (no transformations) On this page Lineage generator (no transformations) App The Lineage Generator (No Transformations) app automatically detects matching or similar assets between two connections in Atlan and generates lineage links between them. This enables teams to establish upstream and downstream data flows without requiring transformation logic, making lineage creation faster and more consistent across your data estate. The app provides three capabilities: Preview lineage: Validate matches before creating them. Generate lineage: Bulk-create relationships between matched assets. Delete lineage: Clean up or roll back lineage previously created by the app. It works across multiple asset types, including tables, views, files, BI assets, and more. You can apply custom matching rules using case sensitivity, schema matching, and regex-based name replacements to make sure accurate lineage generation. This reference provides complete configuration details to help you set up and customize lineage generation. Access â The Lineage Generator (No Transformations) app isn't enabled by default. To use this app, contact Atlan support and request it be added to your tenant. Configurations â This section explains the properties you can configure in the Lineage Generator (no transformations) app. Workflow name â Specifies the display name for the workflow in Atlan. This name is used to identify the workflow in the UI and logs. Choose a name that clearly reflects the purpose or scope of the lineage generation run. Example: Sales Data Lineage Generation Source asset type â Specifies the type of the input assets from which lineage must originate. This determines which assets in Atlan are scanned and matched with the Target asset type to generate lineage. Supported types include: Relational : Table, View, Materialized View, Calculation View, Column Other sources : MongoDB Collection, Salesforce Object, Salesforce Field, S3 / ADLS / GCS Object, Power BI Table, Power BI Column, Kafka Topic, Looker Field, Looker View For detailed parsing rules, and examples for each supported type, see the Source asset type reference . When Match on schema is set to Yes , schema names are included in the matching logic. This means that both schema and asset names must align for lineage to be created. Schema matching applies only when both the source and target asset types are relational (Table, View, Materialized View, Calculation View, Column) or a MongoDB Collection. If either type is outside these categories, schema matching is ignored. Example: With Match on schema = No , sales.orders and marketing.orders both map to a downstream target named orders . With Match on schema = Yes , only sales.orders connects to sales.orders , while marketing.orders is treated as separate. Source qualified name prefix â Specifies the qualified name prefix for the source assets. The qualified name is the unique identifier Atlan assigns to every asset, and the prefix is the first part of this identifier that indicates the connection, environment, and often the schema or directory where the asset resides. When this property is set, the app only considers source assets whose qualified names begin with the specified prefix. This helps narrow the search scope to a specific connection or subset of assets, improving both accuracy and performance. You can find this prefix in the qualifiedName field of an asset in Atlan. Open the asset's details, locate its qualifiedName , and copy the portion that represents the schema, folder, or relevant path to use as the prefix. Make sure the prefix matches the exact format of the asset's qualified name in Atlan. Incorrect or incomplete prefixes can result in no matches. Example: Snowflake schema: Use a schema-level prefix when you want to generate or validate lineage only for assets inside a specific Snowflake schema. default / snowflake / 1678901234 / warehouse_name / database_name / schema_name S3 bucket folder: Use a folder-level prefix when you want to restrict lineage operations to objects within a particular folder of an S3 bucket. default/s3/1678904567/bucket-name/folder-name/.../object-name Target asset type â Specifies the type of assets in the destination connection for which lineage is created. The system searches for assets of this type in the target connection and matches them with source assets according to the configured rules. The target asset type usually represents the downstream asset in the lineage relationship. For example, when generating lineage between a staging schema in a data warehouse and a BI tool, the target asset type might be Table for the warehouse layer or Dashboard for the BI layer. If either the Source asset type or Target asset type isn't a relational type (Table, View, Materialized View, Calculation View, or Column) or a MongoDB Collection, the Match on schema option is ignored. Example: Same technology lineage Table as the Source asset type and Table as the Target asset type in the same Snowflake environment, mapping tables from a staging schema to a reporting schema. Example: Cross-technology lineage Table as the Source asset type in Snowflake and Dataset as the Target asset type in Looker, mapping data warehouse tables to the Looker datasets that use them. Target qualified name prefix â Specifies the qualified name prefix used to identify target assets for lineage generation. Only assets whose qualified names begin with this prefix are considered when matching them to source assets. You can find the qualified name prefix in the qualifiedName field of an asset in Atlan. Open the assetâs details, copy its qualifiedName , and use the portion that represents the schema, folder, or relevant path as the prefix. Example: If the target assets are in a Snowflake schema named analytics, the qualified name prefix might look like: default / snowflake / 1234567890 / warehouse_name / database_name / analytics If the target assets are files in an S3 bucket, the prefix might be: default / s3 / 1234567890 / bucket - name / reports / .../ object - name Case sensitive match â Determines whether asset name matching between the source and target is performed with case sensitivity. Yes: Only matches assets when the names have exactly the same letter case. For example, Orders matches only Orders and not orders . No (default) : Matches assets regardless of letter case. For example, Orders matches both Orders and orders . This setting is useful when working with systems or naming conventions where case differences indicate different assets, or when aligning with case-sensitive data source rules. Ignore circular lineage â Specifies whether lineage generation must skip relationships that might create a circular reference between assets. Yes: Prevents the creation of lineage where the source and target assets are connected in a loop, avoiding redundant or misleading paths in lineage graphs. No (default) : Enables lineage to be created even if it results in a circular path. Example: If you generate lineage between staging and reporting tables in Snowflake, you normally expect data to flow one way (staging â reporting). If reporting tables also reference staging tables, a circular path can appear. When enabled : The lineage is created only in the forward direction, avoiding the circular loop: Table A (staging) â Table B (reporting) When not enabled : The lineage includes both directions, creating a loop: Table A (staging) â Table B (reporting) â Table A (staging) info This option blocks self-referencing lineage paths when the same asset is included in both source and target scopes. To establish valid bidirectional lineage between different assets, such as staging and reporting layers, configure and run the workflow separately for each direction. Match on schema â Controls whether lineage matching compares both the schema name and the asset name , or only the asset name, when connecting source and target assets. Schema matching applies only when both the Source asset type and Target asset type are relational objects (Table, View, Materialized View, Calculation View, or Column) or MongoDB Collections. If either asset type falls outside these categories, schema names aren't considered in the matching process. Yes : The schema name is included in the matching logic. A source and target asset are considered a match only if both the schema and the asset name align. Useful when the same asset names exist across multiple schemas. Prevents incorrect matches across different domains or functional areas. Example: If you're working in a multi-schema warehouse where sales.orders and marketing.orders both exist, enabling this option makes sure that only sales.orders maps to sales.orders . No (default) : Only the asset name is compared between source and target. Schemas are ignored during matching. Useful when assets with the same name are always intended to be connected, regardless of their schema. Example: If you're consolidating data and want both sales.orders and marketing.orders to connect to a single downstream target orders_combined , select No so that schema differences don't prevent the match. Output type â Defines the type of action the app performs after matching source and target assets. You can select one of the following actions: Preview Lineage (default) : Generates a CSV preview of the potential lineage mappings without applying changes in Atlan. Use this option when validating the configuration before making permanent updates. Example: If you configure the app to match orders tables across sales and marketing schemas, the preview generates a file showing which assets might be connected, but no lineage is created in Atlan. Generate Lineage : Creates lineage relationships directly in Atlan between the matched source and target assets. Example: If the preview showed sales.orders â warehouse.orders_combined and the configuration is approved, running with Generate Lineage adds this lineage in Atlan so it appears in the lineage graph. Use this option when confident the mappings are correct and ready to be reflected in the workspace. Delete Lineage : Removes lineage previously created by this app from Atlan. Only relationships generated by the Lineage Generator (no transformations) app are deleted. Example: If sales.orders was earlier mapped to warehouse.orders_combined but the connection is no longer valid, selecting Delete Lineage removes this relationship from Atlan. Generate lineage on child assets â Specifies whether lineage is also created between the child assets of the selected source and target asset types. This option is relevant only when both the Source asset type and Target asset type include child assets, such as tables with columns or Power BI tables with columns. For asset types that don't have a hierarchical structure, the option is ignored. Yes: Lineage is created at both the parent level (for example, table-to-table) and the child level (for example, column-to-column). Example: If sales.orders (table) is mapped to warehouse.orders_combined (table), enabling this option also generates lineage for the columns under those tables, such as orders.customer_id â orders_combined.customer_id . Useful when column-level lineage is required to understand transformations or dependencies in greater detail. No (default): Lineage is generated only at the parent level (for example, table-to-table), and no column-level (or child-level) mappings are created. Example: With the same configuration, lineage connects only the sales.orders table to the warehouse.orders_combined table, without adding relationships between individual columns. Suitable when high-level lineage is sufficient or when child asset mappings may add unnecessary noise. Name transformation (regex) â If you want to replace specific characters or patterns in asset names so they align across systems, configure the regex fields together. When Match on schema = Yes , use the schema-specific regex fields to handle schema-level differences. The name-only regex fields apply only to object names, such as tables or columns, and aren't used for schema comparisons. Regex to match characters to replace : Defines the regex patterns you want to match in the source asset names. Regex with replacement characters : Defines what those matched patterns must be replaced with. Example: If you want to change all _id suffixes into _identifier to match your target system: In Regex to match characters to replace , enter: _id$ In Regex with replacement characters , enter: _identifier Example: To replace the temporary prefix tmp_ with a standard prefix stg_ : In Regex to match characters to replace , enter: ^tmp_ In Regex with replacement characters , enter: stg_ Schema name transformation (regex) â Use these properties when schema names differ between environments or systems and need to be standardized for lineage generation. Regex to match characters to replace on the schema : Defines the pattern in the schema name to identify the part that must be replaced. Regex with replacement characters on the schema : Specifies the replacement text to use for each pattern defined earlier. Both fields must be configured together: each regex pattern requires a corresponding replacement. If one is left blank, the transformation doesn't occur. Example: If you want to make schema names lowercase across environments: In Regex to match characters to replace on the schema , enter: [A-Z] In Regex with replacement characters on the schema , enter: \\L$0 This makes sure schemas like Finance , HR_DATA , and SALES all become lowercase ( finance , hr_data , sales ) for consistent matching. Example: If you want to ignore version numbers in schema names: In Regex to match characters to replace on the schema , enter: _[0-9]+$ In Regex with replacement characters on the schema , leave the field empty to remove the matched text. This maps schemas like analytics_2023 and analytics_01 into a single logical schema: analytics . Using name + schema regex properties â If you want to standardize both schema and object names together, configure the following properties: Regex to match characters to replace on the name + schema : Defines the pattern across the full schema.object identifier to identify the part that must be replaced. Regex with replacement characters on the name + schema : Specifies the replacement text to use for each pattern defined earlier. These properties apply only when Match on schema = Yes , because the system uses the combined schema.object key instead of handling the schema and object name separately. When configured, the name + schema regex pair takes precedence over name-only or schema-only regex pairs. Example: If you want to align schema and table names together, you can configure both the schema and object name in a single regex pair. This is useful when naming conventions include environment-specific prefixes or temporary suffixes that need to be normalized across systems. In Regex to match characters to replace on the name + schema , enter: ^(raw_|stg_|prod_)|(_tmp$) In Regex with replacement characters on the name + schema , enter: \"\" With this configuration, common environment prefixes ( raw_ , stg_ , prod_ ) and temporary suffixes ( _tmp ) are removed: raw_sales.orders_tmp â sales.orders prod_sales.orders â sales.orders Match prefix â Adds a fixed string to the beginning of each source asset name before comparing it to target assets. This property is useful when target assets follow a consistent naming convention that uses a prefix (such as stg_ , prod_ , or team_ ), but source assets don't. For example, if the source asset is orders and the target asset is stg_orders , setting the match prefix to stg_ enables them to match. If both Match prefix and Match suffix are set, the prefix is applied first, followed by the suffix. If Match on schema = Yes , the prefix applies only to the object name, not the schema. If regex transformations on name + schema are configured, those transformations take precedence, and the prefix is ignored. If regex transformations on name only are configured, the regex runs first, and then the prefix is added. If the differences between source and target names go beyond a simple prefix, regex transformations are the preferred option. Example: To match a source asset named orders with a target asset named stg_orders , set Match prefix to stg_ . This adds the prefix to the source name so both align. Match suffix â Appends a fixed string to the end of each source asset name before comparing it to target assets. This property is useful when target systems consistently add a suffix (such as _prod , _stg , or _2024 ) to asset names, but source systems don't. For example, if the source asset is orders and the target asset is orders_prod , adding the suffix _prod enables them to match. If both Match prefix and Match suffix are set, the prefix is applied first, followed by the suffix. If Match on schema = Yes , the suffix applies only to the object name, not the schema. If regex transformations on name + schema are configured, those transformations take precedence, and the suffix is ignored. If regex transformations on name only are configured, the regex runs first, then the suffix is added. If the naming differences are more complex than a consistent suffix, regex transformations are the preferred option. Example : To match a source table named orders with a target table named orders_prod , set Match suffix to _prod . This transforms the source name to orders_prod before comparison, creating a correct match. File path segmentation (file-based assets only) â For file-based assets (such as S3, ADLS, or GCS objects), use these properties to extract the meaningful parts of a file path for lineage matching. File advanced separator : Defines the character used to split the path into segments (for example, / in an S3 path or \\ in a Windows-style path). File advanced position : Specifies how many segments to keep from the end of the split path. The count is right-to-left (for example, 3 keeps the last three segments). These properties are designed to work together : The separator defines where to cut the path. The position defines which slice of the path to use for matching. If only a separator is set, the path is split but the full string is still compared. If only a position is set, it has no effect because no split occurs. For best results, configure both properties together. Other transformations (such as regex, prefix, or suffix) are applied after separator and position logic. If Match on schema = Yes , the folder path segments kept by this configuration are treated like a schema. This makes it possible to align files across environments in the same way schemas align tables in databases. Example: If your S3 folder paths include the environment (such as staging or prod ), you can use the file segmentation properties to focus only on the meaningful parts of the path. In File advanced separator , enter: / In File advanced position , enter: 3 This keeps the last three segments of the path for matching. For example: arn:aws:s3:::mybucket/staging/customers/data.csv becomes: customers/data.csv so it can correctly align with: arn:aws:s3:::mybucket/prod/customers/data.csv Process connection â The Process connection property defines which connection the generated lineage processes belong to. If left blank, processes are automatically assigned to the same connection as the source assets. This property is particularly useful when the lineage logically represents a transformation or movement between two systems and must live in a neutral or dedicated connection, rather than being tied only to the source. Process connection works independently from regex , prefix/suffix , and file path segmentation properties. Those affect how matching is computed, while Process connection affects only process placement. Example: If you want to centralize lineage processes in a dedicated connection called lineage_sandbox , configure: lineage_sandbox See also â Source asset type Generate lineage between assets App Tags: lineage data-lineage impact-analysis upstream-dependencies app Previous How can Atlan generate upstream lineage from the data warehouse layer? Next Source asset type Access Configurations See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/field-level-lineage-bi-tools",
    "text": "Use data Lineage FAQ Does Atlan support field-level lineage for BI tools? Does Atlan support field-level lineage for BI tools? Atlan supports field-level lineage across most BI sources that have the concept of fields. See supported BI sources to drill down further. Tags: lineage data-lineage impact-analysis faq-lineage Previous Can I be notified if there is a change in downstream dashboards or a schema drift? Next Does lineage only cover calculated fields for Tableau dashboards?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/enable-lineage-for-sources",
    "text": "Use data Lineage FAQ How do you enable data lineage for different data sources? How do you enable data lineage for different data sources? This will vary by connector. For more information, see Connectors and capabilities . Tags: lineage data-lineage impact-analysis faq-lineage Previous Does lineage only cover calculated fields for Tableau dashboards? Next How does a Snowflake connection know that a table referenced in LookML is actually the same table?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/power-bi-measures-lineage-graph",
    "text": "Use data Lineage FAQ How is the Atlan lineage graph depicted using Power BI measures? How is the Atlan lineage graph depicted using Power BI measures? Atlan currently does not support lineage for Microsoft Power BI measures . To learn more about asset types for which lineage is available, see What does Atlan crawl from Microsoft Power BI? . Tags: lineage data-lineage impact-analysis faq-lineage Previous How does Atlan handle lineage from Spark jobs? Next How to obtain upstream lineage if I connect to a Tableau data asset?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/lineage-view-number-meaning",
    "text": "Use data Lineage FAQ What do the numbers in lineage view mean? What do the numbers in lineage view mean? The numbers represent the total number of lineage processes , which are the upstream and downstream transformations of an asset.Â Atlan also displays connector branding for the procedures to help you determine where the transformation originates from, such as Snowflake or dbt. Tags: lineage data-lineage impact-analysis downstream-impact dependencies upstream-dependencies data-sources faq-lineage Previous What are Power BI processes on the lineage graph? Next What lineage do you support?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/supported-lineage-types",
    "text": "Use data Lineage FAQ What lineage do you support? What lineage do you support? Following are some examples of built-in lineage generation : Inside a data warehouse (table-to-table, view-to-table   -  this goes down to a column level) Data warehouse   -  Databricks, Snowflake, Amazon Redshift, Google BigQuery, and more The miner packages in the Atlan marketplace bring in lineage Snowflake , Google BigQuery , and Amazon Redshift lineage are built by parsing queries in query history Unity Catalog APIs and system tables provide Databricks lineage (only if the Databricks workspace is Unity Catalog-enabled   -  which is still in preview from Databricks) Â Lineage from data warehouse to BI tool BI tool   -  Microsoft Power BI, Tableau, Looker, and more Data warehouse   -  all the aforementioned sources Table- and column-level lineage from dbt To view supported capabilities for Atlan's current integrations, see Connectors and capabilities . If you see a connector you'd like to manually build lineage for that is not automated, you can do it by utilizing our APIs . Tags: lineage data-lineage impact-analysis faq-lineage Previous What do the numbers in lineage view mean? Next Why is lineage available for table level but not column level?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/faq/sql-query-visibility-snowflake",
    "text": "Use data Lineage FAQ Why is the SQL query visible only in Snowflake process and not in dbt process nodes? Why is the SQL query visible only in Snowflake process and not in dbt process nodes? Atlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage. Tags: model faq faq-lineage Previous Why is lineage available for table level but not column level? Next Troubleshooting lineage"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-lineage",
    "text": "16 docs tagged with \"faq-lineage\" View all tags Can Atlan integrate with Airflow to generate lineage? Atlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage). Can Atlan read a dump of SQL statements to create lineage? Atlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/. Can I be notified if there is a change in downstream dashboards or a schema drift? You can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events. Does Atlan support field-level lineage for BI tools? Atlan supports field-level lineage across most BI sources that have the concept of fields. See [supported BI sources](/product/connections/references/supported-. Does lineage only cover calculated fields for Tableau dashboards? Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f. How do you enable data lineage for different data sources? Learn about how do you enable data lineage for different data sources?. How does a Snowflake connection know that a table referenced in LookML is actually the same table? Learn about how does a snowflake connection know that a table referenced in lookml is actually the same table?. How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. How is the Atlan lineage graph depicted using Power BI measures? Learn about how is the atlan lineage graph depicted using power bi measures?. How to obtain upstream lineage if I connect to a Tableau data asset? Learn about how to obtain upstream lineage if i connect to a tableau data asset?. Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. What are Power BI processes on the lineage graph? Note that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets. What do the numbers in lineage view mean? Learn about what do the numbers in lineage view mean?. What lineage do you support? Learn about what lineage do you support?. Why is lineage available for table level but not column level? The home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset. Why is the SQL query visible only in Snowflake process and not in dbt process nodes? Atlan displays SQL queries for dbt process nodes between SQL tables and also on dbt models. However, Atlan does not display dbt process nodes between two dbt assets   -  for example, between two models. This is because Atlan does not use any SQL query to generate this type of lineage."
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/references/source-asset-type",
    "text": "Use data Lineage References Lineage Generator (no transformations) Source asset type On this page Source asset type App The Source asset type property is part of the Lineage Generator (no transformations) app. It defines the type of asset from which lineage originates in Atlan. This reference documentation explains what each supported source asset type represents, how the app parses its qualified names, how parsing behavior changes when related properties such as Match on schema , Case sensitive match , File segmentation , or Regex transformations are enabled or disabled. The same parsing rules apply to both Source asset type and target asset type. While this page focuses on source assets, the information also applies when configuring the Target asset type . ADLS object â ADLS objects represent files stored in Azure Data Lake Storage. Qualified names include the connection, container, folder hierarchy, and file name. File path segmentation can be applied using File advanced separator and File advanced position to retain folder context. Match on schema isn't applied to file-based assets. Regex transformations are applied after segmentation to remove suffixes or standardize naming. Example: If you want to extract only the file name without any folder context, use the default configuration. This is useful when you need to match files across different folder structures: default/adls/.../container/folder/data.csv â data.csv Example: If you want to retain folder context for better organization, enable file segmentation. This helps when you need to distinguish between files in different environments or stages: default/adls/.../container/env/staging/customers.csv â staging/customers.csv Calculation view â Calculation views are relational assets used in systems like SAP HANA. These views combine multiple tables or other views using business logic, calculations, and aggregations to create a unified data model for reporting and analytics. Default parsing strips connection, database, and schema, leaving only the view name. If Match on schema = Yes , schema is included in the key for more precise lineage tracking. Case sensitivity applies depending on system configuration and can affect matching across different environments. Regex transformations can be used to standardize naming conventions or remove system-specific prefixes. Example: If you want to match calculation views across different schemas with the same name, use the default configuration. This is useful when you have standardized view names across multiple schemas: default/sap/.../DATABASE/SCHEMA/VIEW â VIEW Example: If you need to distinguish between calculation views with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when views have identical names but different contexts: default/sap/.../DATABASE/SCHEMA/VIEW â SCHEMA/VIEW Example: If you want to clean up SAP-specific naming conventions, apply regex transformations to standardize view names across your lineage: SAP_CALC_VIEW_CUSTOMERS â CUSTOMERS Column â Columns represent individual fields inside tables or views. They're the fundamental building blocks of data structures and contain the actual data values. Column-level lineage is crucial for understanding how individual data elements flow through your data pipeline, from source systems to final analytics. Default parsing includes table and column names, providing context about which table contains the column. If Match on schema = Yes , schema is included for more granular lineage tracking across different database schemas. Regex transformations can be used for column name cleanup, standardization, or alignment with business terminology. Case sensitivity is important for systems where column names may have different casing conventions. Example: If you want to match columns across different schemas with the same table and column names, use the default configuration. This is useful for cross-schema column lineage: default/snowflake/.../DB/SCHEMA/TABLE/COLUMN â TABLE/COLUMN Example: If you need to distinguish between columns with the same name in different schemas, enable schema matching. This ensures accurate lineage when columns exist in multiple schemas: default/snowflake/.../DB/SCHEMA/TABLE/COLUMN â SCHEMA/TABLE/COLUMN Example: If you want to standardize column naming conventions across different systems, apply regex transformations to align warehouse columns with source system fields: CUSTOMER_ID_RAW â CUSTOMER_ID DynamoDB table â DynamoDB tables are schemaless key-value store assets. Parsing extracts the table name only. Case sensitivity determines whether differently cased names are matched separately. Example: If you want to extract just the table name for lineage matching, use the default configuration. This simplifies matching across different DynamoDB instances: default/dynamodb/.../CUSTOMERS â CUSTOMERS GCS object â GCS objects represent files stored in Google Cloud Storage. Qualified names include the connection, bucket, folder hierarchy, and object. Default parsing extracts only the object (file) name. File path segmentation enables keeping multiple trailing parts of the path. Regex transformations apply after segmentation. Example: If you want to extract only the file name without folder context, use the default configuration. This is useful for matching files across different bucket structures: default/gcs/.../bucket/folder/object.json â object.json Example: If you need to retain folder context for better file organization, enable file segmentation. This helps when you want to distinguish between files in different environments or project folders: default/gcs/.../bucket/env/prod/sales/data.json â prod/sales/data.json Kafka topic â Kafka topics represent message streams. Qualified names include the connection and topic name. Parsing extracts only the topic name. Case sensitivity determines if topics with the same name but different case are considered matches. Example: If you want to extract just the topic name for lineage matching, use the default configuration. This simplifies matching across different Kafka clusters: default/kafka/.../topic/orders_events â orders_events Looker field â Looker fields represent individual dimensions or measures in LookML models. Parsing extracts only the field name portion. Regex transformations can align field names with warehouse columns. Example: If you want to extract only the field name for lineage matching, use the default configuration. This helps align Looker fields with their corresponding warehouse columns: default/looker/.../distribution_centers.location â location Looker view â Looker views represent groupings of fields in LookML models. Parsing extracts only the view name. Regex transformations can clean prefixes or suffixes in view names. Example: If you want to extract only the view name for lineage matching, use the default configuration. This simplifies matching Looker views with their underlying data sources: default/looker/.../distribution_centers â distribution_centers Materialized view â Materialized views are persisted relational query results. Default parsing strips connection, database, and schema. If Match on schema = Yes , schema is included. Case sensitivity applies depending on system rules. Example: If you want to match materialized views across different schemas with the same name, use the default configuration. This is useful when you have standardized view names across multiple schemas: default/redshift/.../DB/SCHEMA/MV_NAME â MV_NAME Example: If you need to distinguish between materialized views with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when views exist in multiple schemas: default/redshift/.../DB/SCHEMA/MV_NAME â SCHEMA/MV_NAME MongoDB collection â MongoDB collections represent groups of documents within databases. Default parsing extracts only the collection name. If Match on schema = Yes , database and collection are included. Case sensitivity can affect results when collection names overlap. Example: If you want to match collections across different databases with the same name, use the default configuration. This is useful when you have standardized collection names across multiple databases: default/mongodb/.../DB/COLLECTION â COLLECTION Example: If you need to distinguish between collections with the same name in different databases, enable database matching. This ensures accurate lineage when collections exist in multiple databases: default/mongodb/.../DB/COLLECTION â DB/COLLECTION Power BI column â Power BI columns represent fields within BI tables. Default parsing extracts both table and column. Regex transformations can rename BI fields to match warehouse columns. Example: If you want to extract both table and column names for lineage matching, use the default configuration. This helps align Power BI fields with their corresponding warehouse columns: default/powerbi/.../TABLE/COLUMN â TABLE/COLUMN Power BI table â Power BI tables represent datasets inside BI models. Default parsing extracts only the table portion. Regex transformations can clean or rename tables to align with upstream assets. Example: If you want to extract only the table name for lineage matching, use the default configuration. This simplifies matching Power BI tables with their underlying data sources: default/powerbi/.../TABLE â TABLE S3 object â S3 objects represent files stored in Amazon S3 buckets. Qualified names include the connection, bucket, folder hierarchy, and object. Default parsing extracts only the file name. File segmentation properties can keep multiple trailing segments. Regex transformations are applied after segmentation to remove environment labels or extensions. Match on schema isn't applied to file-based assets. Example: If you want to extract only the file name without folder context, use the default configuration. This is useful for matching files across different bucket structures: default/s3/.../mybucket/folder/data.csv â data.csv Example: If you need to retain folder context for better file organization, enable file segmentation. This helps when you want to distinguish between files in different environments or project folders: default/s3/.../mybucket/env/staging/customers/data.csv â staging/customers/data.csv Example: If you want to clean up environment-specific suffixes after segmentation, apply regex transformations. This helps standardize file names while maintaining folder context: staging/customers/data.csv â customers/data.csv Salesforce field â Salesforce fields represent attributes within Salesforce objects. Parsing removes connection and org identifiers, leaving only object and field. Regex transformations can normalize Salesforce field names to align with warehouse fields. Example: If you want to extract object and field names for lineage matching, use the default configuration. This helps align Salesforce fields with their corresponding warehouse columns: default/salesforce/.../ORG/ACCOUNT/ID â ACCOUNT/ID Salesforce object â Salesforce objects represent entities such as Accounts or Contacts. Parsing removes connection and org identifiers, leaving only the object. Regex transformations can handle additional suffixes or prefixes across systems. Example: If you want to extract only the object name for lineage matching, use the default configuration. This simplifies matching Salesforce objects with their corresponding warehouse tables: default/salesforce/.../ORG/ACCOUNT â ACCOUNT Table â Tables are standard relational assets in systems like Snowflake, BigQuery, and Redshift. Default parsing removes connection, database, and schema. If Match on schema = Yes , schema is included in the parsed key. Case sensitivity determines whether same-named tables with different casing are treated separately. Regex transformations can clean or align names. Example: If you want to match tables across different schemas with the same name, use the default configuration. This is useful when you have standardized table names across multiple schemas: default/snowflake/.../DB/SCHEMA/TABLE â TABLE Example: If you need to distinguish between tables with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when tables exist in multiple schemas: default/snowflake/.../DB/SCHEMA/TABLE â SCHEMA/TABLE View â Views are relational query-based objects. Default parsing strips connection, database, and schema. If Match on schema = Yes , schema is included. Case sensitivity determines whether views with different casing match. Example: If you want to match views across different schemas with the same name, use the default configuration. This is useful when you have standardized view names across multiple schemas: default/bigquery/.../PROJECT/DB/SCHEMA/VIEW â VIEW Example: If you need to distinguish between views with the same name in different schemas, enable schema matching. This ensures proper lineage tracking when views exist in multiple schemas: default/bigquery/.../PROJECT/DB/SCHEMA/VIEW â SCHEMA/VIEW Tags: lineage reference app data-lineage parsing Previous Lineage Generator (no transformations) Next Can Atlan integrate with Airflow to generate lineage? ADLS object Calculation view Column DynamoDB table GCS object Kafka topic Looker field Looker view Materialized view MongoDB collection Power BI column Power BI table S3 object Salesforce field Salesforce object Table View"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-metadata",
    "text": "One doc tagged with \"faq-metadata\" View all tags Tags and Metadata Management Complete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift",
    "text": "Connect data Data Warehouses Amazon Redshift On this page Amazon Redshift Overview: Catalog Amazon Redshift clusters, databases, schemas, and tables in Atlan. Gain visibility into lineage, usage, and governance for your AWS data warehouse assets. Get started â Follow these steps to connect and catalog Amazon Redshift assets in Atlan: Set up the connector Crawl Amazon Redshift assets Guides â Mine Amazon Redshift : Extract query history and build lineage for your Redshift assets. Enable SSO for Amazon Redshift : Set up Okta SSO authentication for Redshift connections. Set up a private network link to Amazon Redshift : Establish a secure, private network connection to Redshift for metadata extraction. References â What does Atlan crawl from Amazon Redshift : Learn about the Redshift assets and metadata that Atlan discovers and catalogs. Preflight checks for Amazon Redshift : Verify prerequisites before setting up the Amazon Redshift connector. Troubleshooting â Troubleshooting connectivity : Resolve common Amazon Redshift connection issues and errors. Tags: amazon redshift connector data warehouse connectivity Next Set up Amazon Redshift Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/troubleshooting/troubleshooting-amazon-redshift-connectivity",
    "text": "Connect data Data Warehouses Amazon Redshift Troubleshooting Troubleshooting Amazon Redshift connectivity On this page Troubleshooting Amazon Redshift connectivity What are the known limitations of the Amazon Redshift connector? â If you run the miner for an Amazon Redshift connection with Serverless deployment type, lineage will be missing for queries with comments included. This is a limitation of AWS   -  queries with comments do not include line breaks when fetched from query history. Why do I get an error when querying an external schema in Amazon Redshift? â Atlan currently does not support search paths for external schemas and tables in Amazon Redshift. If you would like to query an external schema in Amazon Redshift from Atlan, you will need to write the query in Insights with the fully qualified name and keep the editor context empty   -  for example, select * db.schema.table and as shown here: Does Atlan support connecting to Amazon Redshift via SSL? â Yes, Atlan supports connecting to Amazon Redshift over the Secure Sockets Layer (SSL) protocol. If your Amazon Redshift cluster is configured to require an SSL connection, with the require_SSL parameter set to true , Atlan will be able to connect to your cluster. Tags: lineage data-lineage impact-analysis Previous Preflight checks for Amazon Redshift"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage",
    "text": "Connect data Orchestration & Workflow Amazon MWAA OpenLineage On this page Amazon MWAA OpenLineage Overview: Capture lineage from Amazon Managed Workflows for Apache Airflow in Atlan. Gain visibility into data transformations, dependencies, and governance for your AWS-managed Airflow DAGs. Get started â Follow these steps to connect and capture Amazon MWAA lineage in Atlan: Configure OpenLineage integration References â What does Atlan capture from Amazon MWAA? : Detailed list of MWAA lineage metadata that Atlan can extract and visualize. Tags: amazon mwaa openlineage connector lineage connectivity Next How to integrate Amazon MWAA/OpenLineage Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage On this page Apache Airflow OpenLineage Overview: Capture lineage from Apache Airflow workflows in Atlan. Gain visibility into data transformations, dependencies, and governance for your Airflow DAGs. Get started â Follow these steps to connect and capture Apache Airflow lineage in Atlan: Set up the connector Integrate OpenLineage integration References â What does Atlan crawl from Apache Airflow OpenLineage : Learn about the Airflow lineage metadata that Atlan captures and visualizes. Preflight checks for Apache Airflow OpenLineage : Verify prerequisites before setting up the Apache Airflow OpenLineage connector. Troubleshooting â Troubleshooting connectivity : Resolve common Apache Airflow OpenLineage connection issues and errors. Tags: apache airflow openlineage connector lineage connectivity Next How to integrate Apache Airflow/OpenLineage Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage",
    "text": "Connect data Orchestration & Workflow Apache Spark OpenLineage On this page Apache Spark OpenLineage Overview: Capture lineage from Apache Spark jobs in Atlan. Gain visibility into data transformations, dependencies, and governance for your Spark applications. Get started â Follow these steps to connect and capture Apache Spark lineage in Atlan: Integrate the connector References â What does Atlan crawl from Apache Spark? : Detailed list of Spark lineage metadata that Atlan can extract and visualize. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Apache Spark, including permissions and network problems. Tags: apache spark openlineage connector lineage connectivity Next How to integrate Apache Spark/OpenLineage Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage",
    "text": "Connect data Orchestration & Workflow Astronomer OpenLineage On this page Astronomer OpenLineage Overview: Capture lineage from Astronomer-managed Apache Airflow in Atlan. Gain visibility into data transformations, dependencies, and governance for your Astronomer workflows. Get started â Follow these steps to connect and capture Astronomer lineage in Atlan: Integrate the connector References â What does Atlan crawl from Astronomer? : Detailed list of Astronomer lineage metadata that Atlan can extract and visualize. Tags: astronomer openlineage connector lineage connectivity Next How to integrate Astronomer/OpenLineage Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster",
    "text": "Connect data Orchestration & Workflow Dagster On this page Dagster Private Preview Overview: Capture lineage from Dagster workflows in Atlan. Gain visibility into data transformations, dependencies, and governance for your Dagster assets. Get started â Follow these steps to connect and capture Dagster lineage in Atlan: Set up the connector : Configure Dagster integration with Atlan to enable lineage capture Crawl Dagster assets : Create a crawler workflow in Atlan to capture lineage from Dagster References â What does Atlan crawl from Dagster : Learn about the Dagster metadata that Atlan captures and visualizes. FAQ â Dagster FAQ : Frequently asked questions about Dagster integration with Atlan. Tags: dagster connector lineage connectivity Next Set up Dagster Get started References FAQ"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise",
    "text": "Connect data Databases NoSQL Databases Datastax Enterprise On this page DataStax Enterprise Overview: Catalog DataStax Enterprise keyspaces, tables, materialised views, indexes, and columns in Atlan. Build asset- and column-level lineage for your distributed data. Get started â Follow these steps to connect and catalog DataStax Enterprise assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from DataStax Enterprise? : Detailed list of DataStax Enterprise asset types and metadata fields. Preflight checks for DataStax Enterprise : Technical checks and requirements. Tags: datastax cassandra connector database lineage connectivity Next Set up DataStax Enterprise Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage",
    "text": "Connect data Orchestration & Workflow Google Cloud OpenLineage On this page Google Cloud Composer OpenLineage Overview: Capture lineage from Google Cloud Composer in Atlan. Gain visibility into data transformations, dependencies, and governance for your GCP-managed Airflow workflows. Get started â Follow these steps to connect and capture Google Cloud Composer lineage in Atlan: Integrate the connector References â What does Atlan crawl from Google Cloud Composer? : Detailed list of Cloud Composer lineage metadata that Atlan can extract and visualize. Tags: google cloud composer openlineage connector lineage connectivity Next How to integrate Google Cloud Composer/OpenLineage Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/references/preflight-checks-for-apache-airflow",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage References Preflight checks for Apache Airflow On this page Preflight checks for Apache Airflow Before running your DAGs in Apache Airflow , Amazon MWAA , Astronomer , or Google Cloud Composer , you can run a preflight check DAG in your Apache Airflow instance to perform the necessary technical validations. The preflight check DAG: Neither collects nor transmits any sensitive data during the validation process, ensuring the security of your integration. In case of any errors, it will provide detailed feedback for troubleshooting, including error codes and next steps. Includes a retry mechanism for the API call to handle temporary network issues or server unavailability. Preflight checks â The preflight check DAG performs the following steps to validate your Atlan and OpenLineage setup: Collects environment variables -  to verify OpenLineage-related environment variables set during the configuration of your Apache Airflow , Amazon MWAA , Astronomer , or Google Cloud Composer instance. These variables can vary depending on your Apache Airflow version. Validates the OpenLineage library installation -  to check whether the openlineage-python library has been installed and identify its version. This ensures that the necessary library for sending OpenLineage events to Atlan is in place. Sends API call for validation -  with the information collected in the previous steps, the preflight check DAG makes a POST request to Atlan's preflight check endpoint. This is to confirm that there are no network issues or configuration errors obstructing the communication. For example, the payload sent for validation looks like this: { \"connector_type\" : \"airflow-mwaa\" , \"version\" : \"2.5.0\" , // Airflow version \"ol_namespace\" : \"staging-mwaa\" , // Environment variable \"ol_endpoint\" : \"https://<host>/events/openlineage/airflow-mwaa/\" , // Environment variable \"ol_version\" : \"1.8\" // Installed OpenLineage library version } Note that Atlan will conduct some additional validations on the Atlan server using the provided data. This is to ensure that the integration is successful. If successful, the DAG will succeed. Check Apache Airflow DAG logs â To check Apache Airflow DAG logs: Open your Apache Airflow homepage. From the homepage, navigate to the AtlanOpenLineageConnectionTest Â DAG. Under the Runs column, click the failed run, circled in red. On the List Dag Run page, under the Run Id column, click the latest or top most failed run ID. From the corresponding screen, click the run_ol_preflight_check task. From the tabs along the top of the Task Instance popup, click Log Â to view logs. On the Log page, scroll down to the Exception section to view the error code and message. Refer to the troubleshooting guide below to make the necessary changes. For other distributions, refer to Amazon MWAA , Astronomer , or Google Cloud Composer documentation for more details. Troubleshoot errors â Missing environment variables â Environment variable {var} is missing. Please set it before running the DAG. Ensure that the required environment variables are set in your environment. These variables can vary depending on your Apache Airflow version: For Apache Airflow versions 2.7.0 onward: AIRFLOW__OPENLINEAGE__NAMESPACE and AIRFLOW__OPENLINEAGE__TRANSPORT . For Apache Airflow versions 2.5.0 onward and prior to 2.7.0: OPENLINEAGE_URL , OPENLINEAGE_API_KEY , and OPENLINEAGE_NAMESPACE . Invalid transport configuration â danger For all errors related to the environment variable configuration for AIRFLOW__OPENLINEAGE__TRANSPORT , Atlan recommends paying attention to the JSON structure and adhering to the expected schema. You will need to ensure that the string values are correctly formatted, and the dictionaries contain the correct types and necessary keys. The following errors are related to the AIRFLOW__OPENLINEAGE__TRANSPORT configuration for Apache Airflow versions 2.7.0 onward: Missing keys in 'transport_info' â \"'{key}' is missing, update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\" Ensure that the specified key is present in the AIRFLOW__OPENLINEAGE__TRANSPORT JSON structure. Incorrect type for keys in 'transport_info' â \"'{key}' must be of type {expected_type.name}, update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\" Update the type of specified key in the AIRFLOW__OPENLINEAGE__TRANSPORT JSON structure to match the expected type. White space in impermissible field â \"'{key}' cannot contain whitespace, update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\" Remove any white space from the specified key value in the AIRFLOW__OPENLINEAGE__TRANSPORT JSON structure. Empty or white space in fields â \"'{key}' cannot be empty or contain whitespaces. update variable - AIRFLOW__OPENLINEAGE__TRANSPORT.\" Ensure that the specified key in the AIRFLOW__OPENLINEAGE__TRANSPORT JSON structure is neither empty nor does it contain any white space. Network permission error â ERROR - Failed to emit OpenLineage eventHTTPSConnectionPool(host='<instance>.atlan.com', port=443):Max retries exceeded with url: /events/openlineage/airflow-cloud-composer/api/v1/lineage This error may result from the firewalls or VPC on which your Apache Airflow instance is hosted. Contact your network team to update the network permissions. This will allow your Airflow instance to make API calls to the URL mentioned in the error message. Unsupported Apache Airflow version â { \"status\" : \"fail\" , \"error_code\" : \"unsupported_airflow_version\" , \"error_message\" : \"Minimum supported version is 2.5.0, you are using 2.4\" } Atlan does not support integrating with Apache Airflow versions older than 2.5.0. Upgrade your Apache Airflow version to 2.5.0 or above. This will allow OpenLineage to push metadata to Atlan. Connection not found â { \"status\" : \"fail\" , \"error_code\" : \"connection_not_found\" , \"error_message\" : \"<connection_name> is not present on Atlan.\" } The connection name set in your environment variables does not match the connection name created in Atlan. Create a new connection or use an existing connection name. Invalid OpenLineage endpoint â { \"status\" : \"fail\" , \"error_code\" : \"invalid_openlineage_endpoint\" , \"error_message\" : \"Your OL endpoint should be: https://<instance>.atlan.com/events/openlineage/airflow/ || received: https://<instance>.atlan.com/events/openlineage/airflow/haha please update this.\" } Update the environment variable containing the OpenLineage URL to the expected URL in the error message. Unsupported OpenLineage version â { \"status\" : \"fail\" , \"error_code\" : \"unsupported_ol_version\" , \"error_message\" : \"Minimum supported version is 1.2.0, you are using 1.1\" } Install the latest version of the openlineage-airflow library   -  1.8.0 and above. The preflight check DAG will fail if you're using any OpenLineage version older than 1.2.0. Connection fetching failed â { \"status\" : \"fail\" , \"error_code\" : \"connection_fetching_failed\" , \"error_message\" : \"Atlas Error: Failed to fetch connections - Post \\\"http://localhost:21000/api/atlas/v2/search/indexsearch\\\\\" : dial tcp [ : : 1 ] : 21000 : connect : connection refused } Contact Atlan support to help you debug this error. Did you know? If you continue to encounter any issues, Atlan recommends enabling debug logging for OpenLineage using the OPENLINEAGE_CLIENT_LOGGING=DEBUG environment variable. Run your DAGs again and then share the debug log with Atlan support for troubleshooting. Tags: lineage data-lineage impact-analysis Previous What does Atlan crawl from Apache Airflow/OpenLineage? Next Provider package versions for OpenLineage Preflight checks Check Apache Airflow DAG logs Troubleshoot errors"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-glossaries",
    "text": "Use data Reporting Get Started Report on glossaries On this page Report on glossaries Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The glossary dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a variety of filters to drill down further. Did you know? You can also view and take action on all your requests for updating terms and categories, right from the glossary dashboard. Filter glossaries â You can use the filters in the glossary dashboard to customize your view of glossary metrics. To filter glossaries: From the left menu in Atlan, click Reporting and then click Glossary . Under Glossary , for the All glossaries filter, select a glossary   -  for this example, we'll select the Concepts glossary. For the All Categories filter, select a category   -  for this example, we'll select the Marketing category. (Optional) To further refine your search , click More filters . You will now be able to view metrics for your filtered glossary! ð View terms by linked assets â You can view metrics for terms with linked assets before making any changes to your terms. This can help you understand the downstream impact of your modifications in advance.Â To view linked assets for your terms: From the left menu in Atlan, click Reporting and then click Glossary . From the Glossary dashboard, scroll down to Terms by linked assets . Click a term to view a list of all the linked assets in the sidebar. (Optional) In the sidebar, next to Search all assets , click the export icon to export linked assets for a term to a spreadsheet. Update new terms and categories â You can also get to work on recently created terms and categories from the glossary dashboard   -  for example, attach a tag to a recently added term. To update a recently created term: From the left menu in Atlan, click Reporting and then click Glossary . From the Glossary dashboard, scroll down to Recently Created Terms & Categories . Click any term to view the term details in the sidebar   -  for this example, we'll select Economic Census .Â In the term sidebar, navigate to Tags and click the + sign to attach a tag to your term   -  for example, Public . Next to your selected tag in the popup, click the downward arrow to configure tag propagation . Click Save to save your changes. Tags: lineage data-lineage impact-analysis glossary business-terms definitions Previous Reporting Next Report on automations Filter glossaries View terms by linked assets Update new terms and categories"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-queries",
    "text": "Use data Reporting Report Types Report on queries On this page Report on queries Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The Insights dashboard in the reporting center helps you track metrics for all your queries and query runs. You can also use a variety of filters to customize your view of query metrics   -  including query collections , folders, saved queries , and more. Did you know? The default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable. Filter by query type â Atlan allows you to write your own SQL queries or use the Visual Query Builder . This means you can also filter query metrics by query type. To filter queries by type of query: From the left menu in Atlan, click Reporting and then click Insights . From the Insights dashboard, in the upper right, click Query Type . From the Query Type dropdown, click SQL Query to view metrics for your SQL queries or click Visual Query to view metrics for your visual queries. (Optional) Under Queries by certificate , click any data point to view query assets in a sidebar. In the top right of the sidebar, click the Export button to export filtered assets to a spreadsheet. View scheduled queries by user â The Insights dashboard also offers you a breakdown of scheduled queries by individual users. You can keep track of your scheduled queries as well as view top users. To view scheduled queries by user: From the left menu in Atlan, click Reporting and then click Insights . From the Insights dashboard, scroll down to Scheduled Queries UnderÂ Scheduled Queries , click Created by all users . From the Created by all users dropdown, select the user by whom you want to filter scheduled queries. Tags: lineage data-lineage impact-analysis glossary business-terms definitions Previous Report on automations Next Report on usage and cost Filter by query type View scheduled queries by user"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-usage-and-cost",
    "text": "Use data Reporting Report Types Report on usage and cost On this page Report on usage and cost Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. The reporting center in Atlan currently supports usage and cost metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Snowflake -  tables, views, and columns Usage and cost metrics for Microsoft Power BI will be added in the future. Filter asset usage by users â You can track total assets queried and storage consumption by users in your organization. To filter asset usage by a user: From the left menu in Atlan, click Reporting and then click Usage & Cost . From the Usage & Cost dashboard, click the Queried by filter and select the user by whom you want to filter usage metrics. (Optional) To further refine your search , click More filters . You will now be able to view usage and cost metrics for a specific user! ð View least used assets â You can find suggestions for deprecating assets based on the following factors: Tables and views without any lineage Asset size less than 100 GB Assets not queried at source in the last 30 days Less popular assets not updated at source in the last 3 or 6 months To view least used assets: From the left menu in Atlan, click Reporting and then click Usage & Cost . From the Usage & Cost dashboard, scroll down to Review your tables & views . (Optional) To further refine your search , click More filters . Click View assets for any card to view a list of suggested assets for deprecation in the sidebar. (Optional) In the top right of the sidebar, click the Export button to export filtered assets to a spreadsheet. Filter assets by context â Discover the top 25 most popular, most expensive, and most queried tables and views from the usage and cost dashboard. This can provide you with additional context regarding asset usage across the organization. Did you know? The compute cost for an asset is split between read and write queries, allowing you to better understand the cost breakdown for individual assets. To filter assets by context: From the Usage & Cost dashboard, scroll down to Top tables & views . In the Top tables & views section, click the filters menu, and depending on the type of asset metrics you'd like to view: Click Most Popular Â to view the top 25 most popular assets.Â Click Most Expensive to view the top 25 most expensive assets. Click Most Queried to view the top 25 most queried assets.Â Did you know? If you have any questions about usage and popularity metrics, head over here . Tags: lineage data-lineage impact-analysis glossary business-terms definitions Previous Report on queries Next Report on governance Filter asset usage by users View least used assets Filter assets by context"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata",
    "text": "Connect data Databases SQL Databases Teradata On this page Teradata Overview: Catalog Teradata schemas, tables, views, and columns in Atlan. Optionally mine query history to build lineage for your Teradata assets. Get started â Follow these steps to connect and catalog Teradata assets in Atlan: Set up the connector Crawl Teradata assets Guides â Mine Teradata query history : Extract query history and build lineage for your Teradata assets. Set up on-premises Teradata miner access : Configure access for secure/on-premises Teradata environments. References â What does Atlan crawl from Teradata : Learn about the Teradata assets and metadata that Atlan discovers and catalogs. Preflight checks for Teradata : Verify prerequisites before setting up the Teradata connector. Tags: teradata connector database lineage query history connectivity Next Set up Teradata Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/troubleshooting/troubleshooting-amazon-msk-connectivity",
    "text": "Connect data Event/Messaging Amazon MSK Troubleshooting Troubleshooting Amazon MSK connectivity On this page Troubleshooting Amazon MSK connectivity What are the known limitations of the Amazon MSK connector? â Following are the known limitations of the Amazon MSK connector: Atlan currently does not support upstream or downstream lineage for Amazon MSK assets. Atlan currently only supports asset-level lineage between topics and consumer groups . Column-level lineage is currently not supported. SASL/SCRAM and TLS authentication methods are currently not supported. Atlan currently only supports IAM role authentication . Serverless deployment for Amazon MSK is currently not supported. Atlan currently only supports Apache Kafka version 2.7.1 or higher for Amazon MSK. How to debug test authentication and preflight check errors? â Invalid region Invalid region. Please provide a valid region. Ensure that you have specified the correct AWS region in your configuration . The region must match where your Amazon MSK cluster is located. Invalid role ARN Invalid role ARN. Please provide a valid role ARN. Ensure that you have specified the correct role ARN in your configuration . Ensure that the role has the required permissions to perform operations in your Amazon MSK cluster. Invalid bootstrap server Invalid bootstrap server details. Please provide a valid bootstrap server. Ensure that you have specified the correct bootstrap server in your configuration . Ensure that the IAM role has the required permissions to connect to Amazon MSK. If you're also using private network link, ensure that private network connectivity between Atlan and Amazon MSK is properly configured. Internal server error Internal server error. Please contact Atlan admin. There may be a connectivity issue between Atlan and your Amazon MSK cluster. Reach out to Atlan support . Timeout exception Caused by: org.apache.kafka.common.errors.TimeoutException: Error listing groups on <broker-address>:<broker-port> : Timed out waiting to send the call. Ensure that the port is authorized in the inbound rule of the security group for your Amazon MSK cluster. If you're also using private network link, ensure that private network connectivity between Atlan and Amazon MSK is properly configured. Ensure that both your Amazon MSK cluster and Atlan tenant are hosted in the same AWS region. Tags: lineage data-lineage impact-analysis downstream-impact dependencies upstream-dependencies data-sources Previous Preflight checks for Amazon MSK"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/troubleshooting/troubleshooting-apache-airflow-openlineage-connectivity",
    "text": "Connect data Orchestration & Workflow Apache Airflow OpenLineage Troubleshooting Troubleshooting Apache Airflow/OpenLineage connectivity On this page Troubleshooting Apache Airflow/OpenLineage connectivity Does Atlan support Spark SQL? â Yes, Atlan supports Spark SQL through Spark jobs . How does Atlan handle multiple DAG owner email addresses? â Atlan captures up to the first 10 valid owner email addresses for a DAG. If a DAG has more than 10 owner emails, only the first 10 are stored as âSource Ownersâ in Atlan. To capture DAG owners, make sure to set the owner field when defining the DAG in Apache Airflow. Ensure that the owner emails are comma-separated and valid to be recognized correctly. Can I connect multiple Apache Airflow instances to a single Atlan instance? â Yes. However, you will need to create a separate connection for each Apache Airflow instance you want to connect to Atlan. Why are some Apache Airflow assets or lineage missing even after the workflow ran successfully? â If OpenLineage has not been configured properly, it may have been unable to send any events while the DAG ran. You can view event logs in Atlan to track and debug events received from OpenLineage. However, if you have verified that your OpenLineage connection was configured correctly and events are still missing, please reach out to your customer success manager at Atlan or raise a support ticket . Does Atlan support Column Level Lineage (CLL) for object storage? â Atlan currently does not support Column Level Lineage (CLL) for object storage. This is because object storage systems do not have structured schema, unlike relational data sources. Object storage systems store unstructured data, unlike relational data sources where columns and relationships are clearly defined. As a result, object storage systems cannot support column level lineage. For example, unstructured data can include a collection of image files stored in an S3 bucket, which doesn't support column-level lineage. To enable CLL for object storage, users must register S3 objects as tables using AWS Glue, Hive, or similar cataloging tools. Column level lineage support is also not available for the following Apache Airflow distributions: Amazon MWAA Astronomer Google Cloud Composer Apache Spark Tags: lineage data-lineage impact-analysis Previous Provider package versions for OpenLineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/troubleshooting/troubleshooting-apache-spark-openlineage-connectivity",
    "text": "Connect data Orchestration & Workflow Apache Spark OpenLineage Troubleshooting Troubleshooting Apache Spark/OpenLineage connectivity On this page Troubleshooting Apache Spark/OpenLineage connectivity Does Atlan support Column Level Lineage (CLL) for object storage? â Atlan currently does not support Column Level Lineage (CLL) for object storage. This is because object storage systems do not have structured schema, unlike relational data sources. Object storage systems store unstructured data, unlike relational data sources where columns and relationships are clearly defined. As a result, object storage systems cannot support column level lineage. For example, unstructured data can include a collection of image files stored in an S3 bucket, which doesn't support column-level lineage. To enable CLL for object storage, users must register S3 objects as tables using AWS Glue, Hive, or similar cataloging tools. Column level lineage support is also not available for the following Apache Airflow distributions: Amazon MWAA Astronomer Google Cloud Composer Apache Airflow Tags: lineage data-lineage impact-analysis schema schema-drift schema-monitoring Previous What does Atlan crawl from Apache Spark/OpenLineage?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/troubleshooting/troubleshooting-domo-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Domo Troubleshooting Troubleshooting Domo connectivity On this page Troubleshooting Domo connectivity What are the known limitations of the Domo connector? â Following are the known limitations of the Domo connector: Column-level lineage is currently not supported. Admin privileges are required for the user creating the client credentials to crawl both datasets and dataset columns. Otherwise, only datasets will be crawled. All three DomoStats dataset IDs are required to run the Domo crawler. An access token is required to generate upstream lineage for datasets , currently only supported with Google BigQuery and Snowflake as data sources. Tags: lineage data-lineage impact-analysis Previous Preflight checks for Domo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/troubleshooting/troubleshooting-microsoft-azure-cosmos-db-connectivity",
    "text": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB Troubleshooting Troubleshooting Microsoft Azure Cosmos DB connectivity On this page Troubleshooting Microsoft Azure Cosmos DB connectivity What are the known limitations of the Microsoft Azure Cosmos DB connector? â Following are the known limitations: Atlan currently does not support asset or column-level lineage for Microsoft Azure Cosmos DB. Microsoft Azure Cosmos DB displays MongoDB views as collections and does not provide metadata for view definition. Microsoft Azure Cosmos DB for NoSQL, Apache Cassandra, Apache Gremlin, Azure Table Storage, and PostgreSQL are currently not supported. Microsoft Azure Cosmos DB for MongoDB deployment currently does not support service principal authentication with role-based access control (RBAC). Microsoft Azure Cosmos DB for MongoDB deployment does not support service principal authentication for vCore cluster-based deployments. Tags: lineage data-lineage impact-analysis Previous What does Atlan crawl from Microsoft Azure Cosmos DB?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/troubleshooting/troubleshooting-microsoft-azure-data-factory-connectivity",
    "text": "Connect data ETL Tools Microsoft Azure Data Factory Troubleshooting Troubleshooting Microsoft Azure Data Factory connectivity On this page Troubleshooting Microsoft Azure Data Factory connectivity What are the known limitations of the Microsoft Azure Data Factory connector? â Following are the known limitations of the Microsoft Azure Data Factory connector: Lineage is not supported for data loaded through data flow activity . The ability to include or exclude certain pipelines or activities is currently notÂ supported. Datasets defined in Microsoft Azure Data Factory are currently not mapped to actual data assets in Atlan. Atlan currently does not display pipelines as DAGs on the lineage graph. Only the activities that are part of a pipeline are displayed as related assets. Lineage is not supported for activities that do not have a defined source and sink attribute in their configuration. Why is lineage missing? â Remember that the Microsoft Azure Data Factory Â package only enriches data assets that already exist in Atlan. Check that the data assets corresponding to Microsoft Azure Data Factory's sources and sinks already exist in Atlan. For example, that the Microsoft Azure Cosmos DB assets have been crawled from Microsoft Azure Cosmos DB , Snowflake assets have been crawled from Snowflake , and so on. The fact that a crawler has run (such as Microsoft Azure Cosmos DB or Snowflake) does not mean all assets from that source have been crawled. The filtering configuration of the crawler could have omitted assets. Or the credentials configured in the crawler may not have access to all of the assets. Tags: lineage data-lineage impact-analysis Previous What lineage does Atlan extract from Microsoft Azure Data Factory?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/troubleshooting/troubleshooting-mongodb-connectivity",
    "text": "Connect data Databases NoSQL Databases MongoDB Troubleshooting Troubleshooting MongoDB connectivity On this page Troubleshooting MongoDB connectivity Does Atlan support lineage for MongoDB? â Atlan currently does not provide built-in lineage support for MongoDB assets. You can generate and ingest lineage using a custom package. Once lineage is added to an asset, whether upstream or downstream, it appears in Atlanâs lineage view. Tags: lineage data-lineage impact-analysis downstream-impact dependencies upstream-dependencies data-sources Previous What does Atlan crawl from MongoDB?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/troubleshooting/troubleshooting-on-premises-looker-connectivity",
    "text": "Connect data BI Tools Cloud-based BI Looker Troubleshooting Troubleshooting on-premises Looker connectivity On this page Troubleshooting on-premises Looker connectivity How do I disable cloning Looker projects from git? â The looker-extractor tool clones your Looker git project to parse field-level lineage. By default, the extractor needs the looker_git_private_key and looker_git_private_key_passphrase secrets. To disable this behavior, you need to set the USE_FIELD_LEVEL_LINEAGE parameter to \"false\" . For example: services:   Example Looker connection looker-example: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project-1,project-2\" USE_FIELD_LEVEL_LINEAGE: \"false\" volumes: - ./output/looker-example:/output/process secrets: - looker_config secrets: looker_config: file: ./looker.ini Note that we refer to only the looker-config secret in this looker-example service. This is possible because the USE_FIELD_LEVEL_LINEAGE environment variable is set to \"false\" . How do I use different credentials for different connections? â You may define as many Looker connections under the services section as you want. As mentioned before, by default each connection requires the following secrets: looker_config looker_git_private_key looker_git_private_key_passphrase You may want to define many connections with different credentials. For example, you may want to extract field-level lineage for some projects and not for others. To do this, you need to define and refer to those secrets separately for each connection. For example: services:   The first-connection uses secrets with default names,   so no need to specify its own `secrets` section first-connection: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project-1\" USE_FIELD_LEVEL_LINEAGE: \"true\" volumes: - ./output/first-connection:/output/process   The second-connection uses alternative secrets,   so it defines its own `secrets` section to refer to them second-connection: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project-2\" USE_FIELD_LEVEL_LINEAGE: \"true\" volumes: - ./output/first-connection:/output/process secrets: - target: looker_config source: alternative_config - target: looker_git_private_key source: alternative_private_key - target: looker_git_private_key_passphrase source: alternative_key_passphrase secrets:   Credentials for the first-connection (default names) looker_config: file: ./looker.ini looker_git_private_key: file: ./id_rsa looker_git_private_key_passphrase: file: ./passphrase.txt   Credentials for the second-connection alternative_config: file: ./path/to/alternative/looker.ini alternative_private_key: file: ./path/to/alternative/id_rsa alternative_key_passphrase: file: ./path/to/alternative/passphrase.txt In the second-connection secrets list, note that: the target attribute gives the default name of the secret the extractor expects the source attribute gives the alternative secret name Tags: lineage data-lineage impact-analysis Previous Troubleshooting Looker connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/faq/copy-link-share-slack-teams",
    "text": "What is the difference between Copy Link and Share on Slack or Teams? When using the Copy link or clipboard button: You must open Slack or Teams and change tabs to paste the URL. You can share the link in any channel you want (including DMs or private chats). The message does not produce a rich embed in Slack or Teams. When using the Share on Slack or Share on Teams option: You can handle the full communication from directly within Atlan, without having to switch to Slack or Teams . (You can choose the channel and add a message in Atlan.) You are only able to share on public channels that your Atlan admins have preconfigured. You can also optionally have Atlan add a link to the conversation in Slack or Teams. The message produces a rich embed in your messaging app of choice. Tags: slack faq faq-integrations"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/remote-mcp-overview",
    "text": "Configure Atlan Atlan AI Atlan MCP Remote MCP On this page Remote MCP Private Preview The Atlan Remote MCP server is a hosted, per-tenant implementation of the Model Context Protocol (MCP) . It provides a secure bridge between Atlanâs metadata platform and AI tools, so users can query, search, and update metadata directly from their preferred environments. Unlike the Local MCP server , which requires you to run a service on your own machine, the Remote MCP server is fully managed and hosted by Atlan. This means: No installation or infrastructure setup is required Authentication and authorization are handled through the same policies and permissions you already use in Atlan Integrations can be enabled quickly across your organization without developer-level setup The Remote MCP server is designed to support both interactive AI experiences (for example, searching metadata through Claude or Cursor) and automation workflows (for example, using n8n, Windsurf, or Microsoft Copilot Studio). info An Atlan tenant with Remote MCP enabled is required. If you don't have access, contact Atlan Support or your Atlan customer team to request enablement. Authentication methods â You can authenticate with the Remote MCP server using two methods. Each method is suited for different tools and use cases. OAuth (SSO) API Key OAuth uses your existing Atlan login, including SSO, to authenticate with the MCP server. This method is recommended for interactive tools where users log in with their own identity, since it ensures that roles, policies, and permissions in Atlan are consistently enforced. The OAuth endpoint is available at: https://<your-tenant>.atlan.com/mcp Available for Cursor, Claude Connector, and Claude Desktop (via mcp-remote , intermittent support). API Key authentication uses a static token generated in Atlan to authenticate with the MCP server. This method is recommended for automation workflows, service-to-service integrations, and other non-interactive environments where OAuth login isn't feasible. The API Key endpoint is available at: https://<your-tenant>.atlan.com/mcp/api-key You can generate an API key in Atlan by going to Admin Settings â API Keys / Tokens and creating a new key. Available for Cursor, n8n, Windsurf, Microsoft Copilot Studio, and Claude Desktop (via mcp-remote , intermittent support). Supported connectors â Remote MCP can be integrated with a range of AI assistants and automation frameworks. Each connector has its own setup guide linked below. Interactive AI tools â Cursor : Integrate Atlan metadata into the Cursor editor for search, lineage, and updates. Claude Connector : Use Atlan metadata directly in Claude conversations in the browser. Claude Desktop : Configure through the mcp-remote package (currently intermittent). Automation and workflow tools â n8n : Automate workflows and service-to-service integrations using Atlan metadata. Windsurf : Bring Atlan metadata into automation and AI-driven flows. Microsoft Copilot Studio : Extend Copilot flows with Atlan metadata context. Tags: Atlan MCP remote hosted AI agents Previous Atlan MCP Next Cursor with Remote MCP Authentication methods Supported connectors"
  },
  {
    "url": "https://docs.atlan.com/tags/ai",
    "text": "2 docs tagged with \"AI\" View all tags Atlan AI Integrate and leverage Atlan AI capabilities for enhanced data documentation, and lineage analysis. Atlan MCP Overview Learn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup."
  },
  {
    "url": "https://docs.atlan.com/tags/atlan-mcp",
    "text": "8 docs tagged with \"Atlan MCP\" View all tags Atlan MCP Overview Learn what the Atlan MCP server is, what it enables, and how to connect using Remote or Local setup. Remote MCP Learn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup. Set up Claude with Remote MCP Learn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access. Set up Cursor with Remote MCP Learn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access. Set up Local MCP Server The Atlan Model Context Protocol (MCP) server enables seamless interaction between external tools and Atlan services. It defines the data structures and conventions used to prepare and transmit context for model execution, making it easier to integrate Atlan into your existing workflows. Set up Microsoft Copilot Studio with Remote MCP Learn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication. Set up n8n with Remote MCP Learn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows. Set up Windsurf with Remote MCP Learn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/tags/tags",
    "text": "One doc tagged with \"tags\" View all tags Tags Learn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/classification",
    "text": "One doc tagged with \"classification\" View all tags Tags Learn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/categorization",
    "text": "One doc tagged with \"categorization\" View all tags Tags Learn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/organization",
    "text": "2 docs tagged with \"organization\" View all tags Domains Learn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way. Tags Learn how to use tags in Atlan to categorize and organize your data assets for improved discoverability and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/custom-metadata",
    "text": "One doc tagged with \"custom metadata\" View all tags Custom Metadata Learn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information."
  },
  {
    "url": "https://docs.atlan.com/tags/attributes",
    "text": "One doc tagged with \"attributes\" View all tags Custom Metadata Learn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information."
  },
  {
    "url": "https://docs.atlan.com/tags/data-catalog",
    "text": "6 docs tagged with \"data-catalog\" View all tags Crawl GCS assets Configure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan. Crawl S3 assets Configure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan. Custom Metadata Learn how to create and manage custom metadata attributes in Atlan to extend your data catalog with organization-specific information. Google Cloud Storage Integrate, catalog, and govern Google Cloud Storage assets in Atlan. What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging. What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/data-products",
    "text": "One doc tagged with \"data-products\" View all tags Data Products Create and manage data products to organize and govern your data assets by domain."
  },
  {
    "url": "https://docs.atlan.com/tags/data-domains",
    "text": "One doc tagged with \"data-domains\" View all tags Data Products Create and manage data products to organize and govern your data assets by domain."
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-members-to-view-reports",
    "text": "Configure Atlan Administration Get Started Allow members to view reports On this page Allow members to view reports Who can do this? You will need to be an admin user in Atlan to allow member users to view the reporting center . Admin users can control access to the reporting center for member users in their organization. If enabled, member users will be able to view the following dashboards: Assets dashboard to monitor assets Glossary dashboard to track metrics for glossaries, categories, and terms Insights dashboard to track metrics for queries Usage and cost dashboard to track asset usage and associated costs danger Permission to view the governance and automations dashboards in the reporting center is reserved for admin users only. To allow member users to view the reporting center , follow these steps. Enable member users to view reports â To enable member users to view the reporting center: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access control Â heading of the Labs page, turn on Allow member users to access Reporting Center . Your member users will now be able to view the assets , glossary , Insights , and usage and cost dashboards in the reporting center! ð If you would like to revoke access, follow the steps above to turn it off. Tags: dashboards visualization analytics Previous Allow guests to request updates Next Disable user activity Enable member users to view reports"
  },
  {
    "url": "https://docs.atlan.com/tags/administration",
    "text": "One doc tagged with \"administration\" View all tags Users and groups Learn how to manage users and groups in Atlan to control access and organize your data team."
  },
  {
    "url": "https://docs.atlan.com/tags/ai-agents",
    "text": "One doc tagged with \"AI agents\" View all tags Remote MCP Learn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup."
  },
  {
    "url": "https://docs.atlan.com/tags/air-gapped",
    "text": "One doc tagged with \"air-gapped\" View all tags On-Premises Databases Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/airflow",
    "text": "One doc tagged with \"airflow\" View all tags Apache Airflow OpenLineage Integrate, catalog, and visualize Apache Airflow lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/aiven",
    "text": "One doc tagged with \"aiven\" View all tags Aiven Kafka Integrate, catalog, and govern Aiven Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/alteryx",
    "text": "2 docs tagged with \"alteryx\" View all tags Alteryx Integrate, catalog, and govern Alteryx assets in Atlan using OpenLineage. Set up Alteryx Set up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run."
  },
  {
    "url": "https://docs.atlan.com/tags/amazon",
    "text": "5 docs tagged with \"amazon\" View all tags Amazon DynamoDB Integrate, catalog, and govern Amazon DynamoDB assets in Atlan. Amazon MSK Integrate, catalog, and govern Amazon MSK assets in Atlan. Amazon MWAA OpenLineage Integrate, catalog, and visualize Amazon MWAA lineage in Atlan. Amazon QuickSight Integrate, catalog, and govern Amazon QuickSight assets in Atlan. Amazon Redshift Integrate, catalog, and govern Amazon Redshift assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/amazon-athena",
    "text": "One doc tagged with \"amazon-athena\" View all tags Amazon Athena Integrate, catalog, and govern Amazon Athena assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/amazon-s-3",
    "text": "4 docs tagged with \"amazon-s3\" View all tags Crawl S3 assets Configure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan. Set up Amazon S3 Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects. Set up Inventory reports Create Inventory report for Amazon S3 in case of inventory based ingestion through the crawler. What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/anomalo",
    "text": "One doc tagged with \"anomalo\" View all tags Anomalo Integrate, catalog, and govern Anomalo assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/apache",
    "text": "3 docs tagged with \"apache\" View all tags Apache Airflow OpenLineage Integrate, catalog, and visualize Apache Airflow lineage in Atlan. Apache Kafka Integrate, catalog, and govern Apache Kafka assets in Atlan. Apache Spark OpenLineage Integrate, catalog, and visualize Apache Spark lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/app",
    "text": "6 docs tagged with \"app\" View all tags Automatically assign roles based on group names Learn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app. Generate lineage between assets App Learn how to generate lineage between assets across two connections in Atlan by matching table names using the Lineage Generator (no transformations) app. Lineage Generator (no transformations) Learn how Atlan can generate upstream lineage from the data warehouse layer, including configuration options and behavior. Send alerts for workflow events Learn how to configure alerts for workflow events in Atlan via email or Google Chat. Source asset type Detailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app. User Role Sync Complete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/architecture",
    "text": "One doc tagged with \"architecture\" View all tags Architecture Architecture overview and core components of Secure Agent 2.0"
  },
  {
    "url": "https://docs.atlan.com/tags/assests",
    "text": "One doc tagged with \"assests\" View all tags Export Assets :::warning Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to [enable asset export](enable-asset-export)."
  },
  {
    "url": "https://docs.atlan.com/tags/astronomer",
    "text": "One doc tagged with \"astronomer\" View all tags Astronomer OpenLineage Integrate, catalog, and visualize Astronomer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/atlas",
    "text": "One doc tagged with \"atlas\" View all tags MongoDB Catalog and govern MongoDB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/auto-re-attachment",
    "text": "2 docs tagged with \"auto-re-attachment\" View all tags Enable auto re-attachment of rules Learn how to enable automatic re-attachment of data quality rules to Snowflake tables and views. What's auto re-attachment Understand automatic re-attachment of data quality rules to assets that are dropped and recreated."
  },
  {
    "url": "https://docs.atlan.com/tags/aws",
    "text": "4 docs tagged with \"aws\" View all tags Amazon S3 Integrate, catalog, and govern Amazon S3 assets in Atlan. AWS Glue Integrate, catalog, and govern AWS Glue assets in Atlan. Set up Amazon S3 Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects. Set up Inventory reports Create Inventory report for Amazon S3 in case of inventory based ingestion through the crawler."
  },
  {
    "url": "https://docs.atlan.com/tags/azure",
    "text": "4 docs tagged with \"azure\" View all tags Microsoft Azure Cosmos DB Catalog and govern Cosmos DB assets in Atlan for discovery and governance. Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan. Microsoft Azure Event Hubs Integrate, catalog, and govern Azure Event Hubs assets in Atlan. Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/bigid",
    "text": "4 docs tagged with \"bigid\" View all tags BigID Integrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata. Crawl BigID Configure and run the Atlan BigID workflow to crawl metadata from BigID. Set up BigID Create a BigID system user and API token for Atlan integration. What does Atlan crawl from BigID? Reference guide for BigID metadata crawled by Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/bigquery",
    "text": "One doc tagged with \"bigquery\" View all tags Google BigQuery Integrate, catalog, and govern Google BigQuery assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/calculation-view",
    "text": "One doc tagged with \"calculation-view\" View all tags SAP HANA Catalog and govern SAP HANA assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/cassandra",
    "text": "One doc tagged with \"cassandra\" View all tags DataStax Enterprise Catalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data."
  },
  {
    "url": "https://docs.atlan.com/tags/catalog",
    "text": "7 docs tagged with \"catalog\" View all tags How does Atlan handle lineage from Spark jobs? Learn about how does atlan handle lineage from spark jobs?. Is there a way to build lineage from NetSuite to Snowflake? Learn about is there a way to build lineage from netsuite to snowflake?. OpenLineage configuration and facets Learn about openlineage configuration and facets. Troubleshooting Amazon DynamoDB connectivity Learn about troubleshooting amazon dynamodb connectivity. Troubleshooting IBM Cognos Analytics connectivity Learn about troubleshooting ibm cognos analytics connectivity. Troubleshooting Tableau connectivity Learn about troubleshooting tableau connectivity. Why is my Databricks lineage API not working? Learn about why is my databricks lineage api not working?."
  },
  {
    "url": "https://docs.atlan.com/tags/cdi",
    "text": "4 docs tagged with \"cdi\" View all tags Crawl Informatica CDI assets Configure and run the crawler to discover and catalog your Informatica CDI assets Set up Informatica CDI Configure authentication and user permissions for Informatica Cloud Data Integration connector Task and crawl issues Troubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance. Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector"
  },
  {
    "url": "https://docs.atlan.com/tags/claude",
    "text": "One doc tagged with \"Claude\" View all tags Set up Claude with Remote MCP Learn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access."
  },
  {
    "url": "https://docs.atlan.com/tags/cloud",
    "text": "One doc tagged with \"cloud\" View all tags Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/cloudera",
    "text": "2 docs tagged with \"cloudera\" View all tags Cloudera Impala Integrate, catalog, and govern Cloudera Impala assets in Atlan. Crawl Cloudera Impala Learn how to crawl metadata from Cloudera Impala into Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/composer",
    "text": "One doc tagged with \"composer\" View all tags Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/concepts",
    "text": "One doc tagged with \"concepts\" View all tags Transformations Understand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/confluent",
    "text": "2 docs tagged with \"confluent\" View all tags Confluent Kafka Integrate, catalog, and govern Confluent Kafka assets in Atlan. Confluent Schema Registry Integrate, catalog, and govern Confluent Schema Registry assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/connect",
    "text": "2 docs tagged with \"connect\" View all tags Connectors Learn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement."
  },
  {
    "url": "https://docs.atlan.com/tags/container-images",
    "text": "One doc tagged with \"container-images\" View all tags Verify container images Verify the authenticity and integrity of Secure Agent container images with Cosign"
  },
  {
    "url": "https://docs.atlan.com/tags/cosmosdb",
    "text": "One doc tagged with \"cosmosdb\" View all tags Microsoft Azure Cosmos DB Catalog and govern Cosmos DB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/cratedb",
    "text": "6 docs tagged with \"cratedb\" View all tags Connection issues Resolve common connection and authentication issues when setting up CrateDB connector CrateDB Integrate, catalog, and govern CrateDB assets in Atlan. Permissions and limitations Frequently asked questions about CrateDB connector setup, permissions, and limitations Preflight checks for CrateDB Technical validations performed before running the CrateDB crawler to verify connectivity and permissions Set up CrateDB Configure authentication and connection settings for CrateDB connector What does Atlan crawl from CrateDB? Complete list of CrateDB assets and metadata properties extracted by Atlan during crawling"
  },
  {
    "url": "https://docs.atlan.com/tags/crawling",
    "text": "One doc tagged with \"crawling\" View all tags Crawl Cloudera Impala Learn how to crawl metadata from Cloudera Impala into Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/credentials",
    "text": "One doc tagged with \"credentials\" View all tags Secret management Understand how Secure Agent 2.0 handles secrets and why credentials never leave your environment."
  },
  {
    "url": "https://docs.atlan.com/tags/crm",
    "text": "One doc tagged with \"crm\" View all tags Salesforce Integrate, catalog, and govern Salesforce assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/cursor",
    "text": "One doc tagged with \"Cursor\" View all tags Set up Cursor with Remote MCP Learn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access."
  },
  {
    "url": "https://docs.atlan.com/tags/dagster",
    "text": "4 docs tagged with \"dagster\" View all tags Crawl Dagster assets Create a crawler workflow in Atlan to capture lineage from Dagster assets Dagster Integrate, catalog, and visualize Dagster lineage in Atlan. Set up Dagster Configure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assets What does Atlan crawl from Dagster Learn about the Dagster metadata that Atlan captures and visualizes"
  },
  {
    "url": "https://docs.atlan.com/tags/dapr",
    "text": "One doc tagged with \"dapr\" View all tags Secret management Understand how Secure Agent 2.0 handles secrets and why credentials never leave your environment."
  },
  {
    "url": "https://docs.atlan.com/tags/data-assets",
    "text": "One doc tagged with \"data assets\" View all tags Domains Learn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way."
  },
  {
    "url": "https://docs.atlan.com/tags/data-factory",
    "text": "One doc tagged with \"data factory\" View all tags Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-integration",
    "text": "6 docs tagged with \"data integration\" View all tags AWS Glue Integrate, catalog, and govern AWS Glue assets in Atlan. Connectors Learn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. Fivetran Integrate, catalog, and govern Fivetran assets in Atlan. Matillion Integrate, catalog, and govern Matillion assets in Atlan. Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-lake",
    "text": "One doc tagged with \"data lake\" View all tags Hive Catalog and govern Hive assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/data-transformation",
    "text": "One doc tagged with \"data transformation\" View all tags dbt Integrate, catalog, and govern dbt assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/data-modeling",
    "text": "One doc tagged with \"data-modeling\" View all tags Data Models Create and manage data models to structure and organize your data assets."
  },
  {
    "url": "https://docs.atlan.com/tags/data-models",
    "text": "One doc tagged with \"data-models\" View all tags Data Models Create and manage data models to structure and organize your data assets."
  },
  {
    "url": "https://docs.atlan.com/tags/data-transfer",
    "text": "One doc tagged with \"data-transfer\" View all tags Data transfer and observability Understand how metadata moves from your sources to Atlan and what visibility you get into operations."
  },
  {
    "url": "https://docs.atlan.com/tags/database",
    "text": "21 docs tagged with \"database\" View all tags Amazon Athena Integrate, catalog, and govern Amazon Athena assets in Atlan. Cloudera Impala Integrate, catalog, and govern Cloudera Impala assets in Atlan. Connection issues Resolve common connection and authentication issues when setting up CrateDB connector CrateDB Integrate, catalog, and govern CrateDB assets in Atlan. DataStax Enterprise Catalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data. Hive Catalog and govern Hive assets in Atlan for discovery and governance. Microsoft Azure Cosmos DB Catalog and govern Cosmos DB assets in Atlan for discovery and governance. Microsoft SQL Server Integrate, catalog, and govern Microsoft SQL Server assets in Atlan. MongoDB Catalog and govern MongoDB assets in Atlan for discovery and governance. MySQL Integrate, catalog, and govern MySQL assets in Atlan. On-Premises Databases Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required. Oracle Integrate, catalog, and govern Oracle assets in Atlan. Permissions and limitations Frequently asked questions about CrateDB connector setup, permissions, and limitations PostgreSQL Integrate, catalog, and govern PostgreSQL assets in Atlan. Preflight checks for CrateDB Technical validations performed before running the CrateDB crawler to verify connectivity and permissions PrestoSQL Integrate, catalog, and govern PrestoSQL assets in Atlan. SAP HANA Catalog and govern SAP HANA assets in Atlan for discovery and governance. Set up CrateDB Configure authentication and connection settings for CrateDB connector Teradata Catalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage. Trino Integrate, catalog, and govern Trino assets in Atlan. What does Atlan crawl from CrateDB? Complete list of CrateDB assets and metadata properties extracted by Atlan during crawling"
  },
  {
    "url": "https://docs.atlan.com/tags/datastax",
    "text": "One doc tagged with \"datastax\" View all tags DataStax Enterprise Catalog and govern DataStax Enterprise assets in Atlan. Build asset- and column-level lineage for your distributed data."
  },
  {
    "url": "https://docs.atlan.com/tags/dbt",
    "text": "One doc tagged with \"dbt\" View all tags dbt Integrate, catalog, and govern dbt assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/deployment",
    "text": "4 docs tagged with \"deployment\" View all tags Architecture Architecture overview and core components of Secure Agent 2.0 Customer environment security Customer environment security best practices for deploying and operating Secure Agent 2.0 Deployment and security Frequently asked questions about Secure Agent 2.0 deployment and security Deployment options Understand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/dimensions",
    "text": "One doc tagged with \"dimensions\" View all tags Rules and dimensions Reference for available data quality rules and classification dimensions in Snowflake data quality."
  },
  {
    "url": "https://docs.atlan.com/tags/docker",
    "text": "One doc tagged with \"docker\" View all tags Deployment options Understand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/domains",
    "text": "One doc tagged with \"domains\" View all tags Domains Learn how to organize and manage domains in Atlan to structure your data assets in a logical and business-aligned way."
  },
  {
    "url": "https://docs.atlan.com/tags/domo",
    "text": "One doc tagged with \"domo\" View all tags Domo Integrate, catalog, and govern Domo assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/dynamodb",
    "text": "One doc tagged with \"dynamodb\" View all tags Amazon DynamoDB Integrate, catalog, and govern Amazon DynamoDB assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/ecc",
    "text": "One doc tagged with \"ecc\" View all tags SAP ECC Integrate, catalog, and govern SAP ECC assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/enrichment",
    "text": "One doc tagged with \"enrichment\" View all tags Enrich Atlan through dbt Beyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property."
  },
  {
    "url": "https://docs.atlan.com/tags/erd",
    "text": "One doc tagged with \"erd\" View all tags Data Models Create and manage data models to structure and organize your data assets."
  },
  {
    "url": "https://docs.atlan.com/tags/erp",
    "text": "6 docs tagged with \"erp\" View all tags Crawl SAP ECC To crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. Crawl SAP S/4HANA To crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps. SAP ECC Integrate, catalog, and govern SAP ECC assets in Atlan. SAP S/4HANA Integrate, catalog, and govern SAP S/4HANA assets in Atlan. Set up SAP ECC Set up user accounts and permissions required for SAP ECC metadata extraction in Atlan. Set up SAP S/4HANA Set up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/etl",
    "text": "7 docs tagged with \"etl\" View all tags AWS Glue Integrate, catalog, and govern AWS Glue assets in Atlan. Data Pipelines Learn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement. dbt Integrate, catalog, and govern dbt assets in Atlan. ETL tools connectors Overview and entry point for all ETL tools connectors in Atlan. Fivetran Integrate, catalog, and govern Fivetran assets in Atlan. Matillion Integrate, catalog, and govern Matillion assets in Atlan. Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/etl-tools",
    "text": "10 docs tagged with \"etl-tools\" View all tags Alteryx Integrate, catalog, and govern Alteryx assets in Atlan using OpenLineage. Crawl Informatica CDI assets Configure and run the crawler to discover and catalog your Informatica CDI assets Informatica CDI Integrate, catalog, and govern Informatica Cloud Data Integration assets in Atlan. Set up Alteryx Set up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run. Set up Informatica CDI Configure authentication and user permissions for Informatica Cloud Data Integration connector Set up Matillion Configure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance. Task and crawl issues Troubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance. Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector Transformations Understand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan What does Atlan crawl from Informatica CDI Understand the metadata and assets discovered during crawling from Informatica Cloud Data Integration"
  },
  {
    "url": "https://docs.atlan.com/tags/event-hubs",
    "text": "One doc tagged with \"event hubs\" View all tags Microsoft Azure Event Hubs Integrate, catalog, and govern Azure Event Hubs assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-administration",
    "text": "2 docs tagged with \"faq-administration\" View all tags Administration and Configuration Complete guide to administering and configuring your Atlan workspace, from basic settings to advanced customization. User Management and Access Control Complete guide to managing users, configuring access controls, and understanding permissions in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-automation",
    "text": "2 docs tagged with \"faq-automation\" View all tags AI and Automation Features Guide to Atlan's AI capabilities and automation features for enhanced data governance and productivity. Workflows and Data Processing Everything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-connections",
    "text": "6 docs tagged with \"faq-connections\" View all tags Can I connect to any source with an ODBC/JDBC driver? A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case. Can the Hive crawler connect to an independent Hive metastore? Learn about can the hive crawler connect to an independent hive metastore?. Data Connections and Integration Complete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues. How often does Atlan crawl Snowflake? Learn about how often does atlan crawl snowflake?. What column keys does Atlan crawl? Learn about what column keys does atlan crawl?. What's the difference between connecting to Athena and Glue? Learn about what's the difference between connecting to athena and glue?."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-connectors",
    "text": "One doc tagged with \"faq-connectors\" View all tags Permissions and limitations Frequently asked questions about CrateDB connector setup, permissions, and limitations"
  },
  {
    "url": "https://docs.atlan.com/tags/faq-governance",
    "text": "6 docs tagged with \"faq-governance\" View all tags Can I add duplicate glossary terms? Each [term](/product/capabilities/governance/glossary/concepts/what-is-a-glossary term) in a glossary should be unique. Duplicate terms in the same glossary may cause confusion rather than provide the context that users need. Can I create backups of glossaries? Atlan also allows you to [export your glossary assets](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) to spreadsheets and keep a record of your contextual information. Glossary update request approval issue Learn about why am i unable to approve a glossary update request?. How can I use personas to update a term in a glossary? By default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section. How do I fully delete glossary terms or archived items? Learn about how do i fully delete glossary terms or archived items?. What is the default permission for a glossary? By default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access."
  },
  {
    "url": "https://docs.atlan.com/tags/faq-security",
    "text": "One doc tagged with \"faq-security\" View all tags Security and Compliance Complete guide to Atlan's security features, compliance certifications, and data protection capabilities."
  },
  {
    "url": "https://docs.atlan.com/tags/firewall",
    "text": "One doc tagged with \"firewall\" View all tags Configure network security Configure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services"
  },
  {
    "url": "https://docs.atlan.com/tags/fivetran",
    "text": "One doc tagged with \"fivetran\" View all tags Fivetran Integrate, catalog, and govern Fivetran assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/gcp",
    "text": "2 docs tagged with \"gcp\" View all tags Google Cloud Storage Integrate, catalog, and govern Google Cloud Storage assets in Atlan. Set up Google Cloud Storage Configure Google Cloud Storage for secure metadata ingestion with Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/gcs",
    "text": "3 docs tagged with \"gcs\" View all tags Crawl GCS assets Configure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan. Google Cloud Storage Integrate, catalog, and govern Google Cloud Storage assets in Atlan. What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/glue",
    "text": "One doc tagged with \"glue\" View all tags AWS Glue Integrate, catalog, and govern AWS Glue assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/google",
    "text": "2 docs tagged with \"google\" View all tags Google BigQuery Integrate, catalog, and govern Google BigQuery assets in Atlan. Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/google-gcs",
    "text": "3 docs tagged with \"google-gcs\" View all tags Crawl GCS assets Configure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan. Set up Google Cloud Storage Configure Google Cloud Storage for secure metadata ingestion with Atlan. What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/groups",
    "text": "One doc tagged with \"groups\" View all tags Users and groups Learn how to manage users and groups in Atlan to control access and organize your data team."
  },
  {
    "url": "https://docs.atlan.com/tags/guides",
    "text": "One doc tagged with \"guides\" View all tags Frequently Asked Questions Find answers to common questions about using Atlan, organized by topic area for quick resolution."
  },
  {
    "url": "https://docs.atlan.com/tags/helm",
    "text": "One doc tagged with \"helm\" View all tags Deployment options Understand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/help",
    "text": "One doc tagged with \"help\" View all tags Frequently Asked Questions Find answers to common questions about using Atlan, organized by topic area for quick resolution."
  },
  {
    "url": "https://docs.atlan.com/tags/hive",
    "text": "One doc tagged with \"hive\" View all tags Hive Catalog and govern Hive assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/hosted",
    "text": "One doc tagged with \"hosted\" View all tags Remote MCP Learn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup."
  },
  {
    "url": "https://docs.atlan.com/tags/hybrid-bi",
    "text": "5 docs tagged with \"hybrid bi\" View all tags Domo Integrate, catalog, and govern Domo assets in Atlan. Metabase Integrate, catalog, and govern Metabase assets in Atlan. MicroStrategy Integrate, catalog, and govern MicroStrategy assets in Atlan. Sigma Integrate, catalog, and govern Sigma assets in Atlan. ThoughtSpot Integrate, catalog, and govern ThoughtSpot assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/ibm-cognos",
    "text": "One doc tagged with \"ibm cognos\" View all tags IBM Cognos Analytics Integrate, catalog, and govern IBM Cognos Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/impala",
    "text": "2 docs tagged with \"impala\" View all tags Cloudera Impala Integrate, catalog, and govern Cloudera Impala assets in Atlan. Crawl Cloudera Impala Learn how to crawl metadata from Cloudera Impala into Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/informatica",
    "text": "6 docs tagged with \"informatica\" View all tags Crawl Informatica CDI assets Configure and run the crawler to discover and catalog your Informatica CDI assets Set up Informatica CDI Configure authentication and user permissions for Informatica Cloud Data Integration connector Task and crawl issues Troubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance. Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector Transformations Understand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan What does Atlan crawl from Informatica CDI Understand the metadata and assets discovered during crawling from Informatica Cloud Data Integration"
  },
  {
    "url": "https://docs.atlan.com/tags/inventory-reports",
    "text": "One doc tagged with \"inventory-reports\" View all tags S3 Inventory Report Structure Expected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion."
  },
  {
    "url": "https://docs.atlan.com/tags/kafka",
    "text": "5 docs tagged with \"kafka\" View all tags Aiven Kafka Integrate, catalog, and govern Aiven Kafka assets in Atlan. Amazon MSK Integrate, catalog, and govern Amazon MSK assets in Atlan. Apache Kafka Integrate, catalog, and govern Apache Kafka assets in Atlan. Confluent Kafka Integrate, catalog, and govern Confluent Kafka assets in Atlan. Redpanda Kafka Integrate, catalog, and govern Redpanda Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/kubernetes",
    "text": "One doc tagged with \"kubernetes\" View all tags Deployment options Understand how Secure Agent 2.0 deploys across different container environments."
  },
  {
    "url": "https://docs.atlan.com/tags/logic",
    "text": "One doc tagged with \"logic\" View all tags Transformations Understand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/looker",
    "text": "One doc tagged with \"looker\" View all tags Looker Integrate, catalog, and govern Looker assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/matillion",
    "text": "2 docs tagged with \"matillion\" View all tags Matillion Integrate, catalog, and govern Matillion assets in Atlan. Set up Matillion Configure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance."
  },
  {
    "url": "https://docs.atlan.com/tags/messaging",
    "text": "6 docs tagged with \"messaging\" View all tags Aiven Kafka Integrate, catalog, and govern Aiven Kafka assets in Atlan. Amazon MSK Integrate, catalog, and govern Amazon MSK assets in Atlan. Apache Kafka Integrate, catalog, and govern Apache Kafka assets in Atlan. Confluent Kafka Integrate, catalog, and govern Confluent Kafka assets in Atlan. Microsoft Azure Event Hubs Integrate, catalog, and govern Azure Event Hubs assets in Atlan. Redpanda Kafka Integrate, catalog, and govern Redpanda Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/metabase",
    "text": "One doc tagged with \"metabase\" View all tags Metabase Integrate, catalog, and govern Metabase assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/metadata-extractor",
    "text": "One doc tagged with \"metadata-extractor\" View all tags On-Premises Databases Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/metrics",
    "text": "2 docs tagged with \"metrics\" View all tags Reporting Generate comprehensive reports on your data assets, usage, and governance. Usage and Popularity Track and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/microsoft",
    "text": "4 docs tagged with \"microsoft\" View all tags Microsoft Azure Data Factory Integrate, catalog, and govern Microsoft Azure Data Factory assets in Atlan. Microsoft Azure Event Hubs Integrate, catalog, and govern Azure Event Hubs assets in Atlan. Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan. Microsoft SQL Server Integrate, catalog, and govern Microsoft SQL Server assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/microsoft-copilot-studio",
    "text": "One doc tagged with \"Microsoft Copilot Studio\" View all tags Set up Microsoft Copilot Studio with Remote MCP Learn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/tags/microsoft-teams",
    "text": "One doc tagged with \"microsoft teams\" View all tags Microsoft Teams Integrate Atlan with Microsoft Teams to enable collaboration and notifications."
  },
  {
    "url": "https://docs.atlan.com/tags/microstrategy",
    "text": "One doc tagged with \"microstrategy\" View all tags MicroStrategy Integrate, catalog, and govern MicroStrategy assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/migration",
    "text": "One doc tagged with \"migration\" View all tags Upgrade to Snowflake data quality studio Update existing Snowflake data quality integration to the latest version"
  },
  {
    "url": "https://docs.atlan.com/tags/mode",
    "text": "One doc tagged with \"mode\" View all tags Mode Integrate, catalog, and govern Mode assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/mongodb",
    "text": "One doc tagged with \"mongodb\" View all tags MongoDB Catalog and govern MongoDB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/msk",
    "text": "One doc tagged with \"msk\" View all tags Amazon MSK Integrate, catalog, and govern Amazon MSK assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/mwaa",
    "text": "One doc tagged with \"mwaa\" View all tags Amazon MWAA OpenLineage Integrate, catalog, and visualize Amazon MWAA lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/mysql",
    "text": "One doc tagged with \"mysql\" View all tags MySQL Integrate, catalog, and govern MySQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/n-8-n",
    "text": "One doc tagged with \"n8n\" View all tags Set up n8n with Remote MCP Learn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows."
  },
  {
    "url": "https://docs.atlan.com/tags/network",
    "text": "One doc tagged with \"network\" View all tags Configure network security Configure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services"
  },
  {
    "url": "https://docs.atlan.com/tags/nosql",
    "text": "3 docs tagged with \"nosql\" View all tags Amazon DynamoDB Integrate, catalog, and govern Amazon DynamoDB assets in Atlan. Microsoft Azure Cosmos DB Catalog and govern Cosmos DB assets in Atlan for discovery and governance. MongoDB Catalog and govern MongoDB assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/oauth",
    "text": "One doc tagged with \"oauth\" View all tags Authentication Understand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure."
  },
  {
    "url": "https://docs.atlan.com/tags/offline",
    "text": "One doc tagged with \"offline\" View all tags On-Premises Databases Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/on-premises",
    "text": "One doc tagged with \"on-premises\" View all tags On-Premises Databases Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required."
  },
  {
    "url": "https://docs.atlan.com/tags/openlineage",
    "text": "6 docs tagged with \"openlineage\" View all tags Alteryx Integrate, catalog, and govern Alteryx assets in Atlan using OpenLineage. Amazon MWAA OpenLineage Integrate, catalog, and visualize Amazon MWAA lineage in Atlan. Apache Airflow OpenLineage Integrate, catalog, and visualize Apache Airflow lineage in Atlan. Apache Spark OpenLineage Integrate, catalog, and visualize Apache Spark lineage in Atlan. Astronomer OpenLineage Integrate, catalog, and visualize Astronomer lineage in Atlan. Google Cloud Composer OpenLineage Integrate, catalog, and visualize Google Cloud Composer lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/operations",
    "text": "One doc tagged with \"operations\" View all tags Operations Atlan crawls and manages the following data quality operations and results from Snowflake."
  },
  {
    "url": "https://docs.atlan.com/tags/oracle",
    "text": "One doc tagged with \"oracle\" View all tags Oracle Integrate, catalog, and govern Oracle assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/parsing",
    "text": "One doc tagged with \"parsing\" View all tags Source asset type Detailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app."
  },
  {
    "url": "https://docs.atlan.com/tags/policies",
    "text": "One doc tagged with \"policies\" View all tags Stewardship Learn how to implement data stewardship in Atlan through automated workflows, policies, and task management."
  },
  {
    "url": "https://docs.atlan.com/tags/popularity",
    "text": "One doc tagged with \"popularity\" View all tags Usage and Popularity Track and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/postgresql",
    "text": "One doc tagged with \"postgresql\" View all tags PostgreSQL Integrate, catalog, and govern PostgreSQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/preflight-checks",
    "text": "One doc tagged with \"preflight-checks\" View all tags Preflight checks for CrateDB Technical validations performed before running the CrateDB crawler to verify connectivity and permissions"
  },
  {
    "url": "https://docs.atlan.com/tags/prestosql",
    "text": "One doc tagged with \"prestosql\" View all tags PrestoSQL Integrate, catalog, and govern PrestoSQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/privacy",
    "text": "4 docs tagged with \"privacy\" View all tags BigID Integrate with BigID and enrich assets in Atlan with BigID-discovered privacy metadata. Crawl BigID Configure and run the Atlan BigID workflow to crawl metadata from BigID. Set up BigID Create a BigID system user and API token for Atlan integration. What does Atlan crawl from BigID? Reference guide for BigID metadata crawled by Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/properties",
    "text": "2 docs tagged with \"properties\" View all tags What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging. What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/qlik-sense",
    "text": "2 docs tagged with \"qlik sense\" View all tags Qlik Sense Cloud Integrate, catalog, and govern Qlik Sense Cloud assets in Atlan. Qlik Sense Enterprise (Windows) Integrate, catalog, and govern Qlik Sense Enterprise on Windows assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/query-history",
    "text": "One doc tagged with \"query history\" View all tags Teradata Catalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/quicksight",
    "text": "One doc tagged with \"quicksight\" View all tags Amazon QuickSight Integrate, catalog, and govern Amazon QuickSight assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/redash",
    "text": "One doc tagged with \"redash\" View all tags Redash Integrate, catalog, and govern Redash assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/redpanda",
    "text": "One doc tagged with \"redpanda\" View all tags Redpanda Kafka Integrate, catalog, and govern Redpanda Kafka assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/redshift",
    "text": "One doc tagged with \"redshift\" View all tags Amazon Redshift Integrate, catalog, and govern Amazon Redshift assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/reference",
    "text": "8 docs tagged with \"reference\" View all tags Data quality permissions Reference for data quality permission scopes and configuration in Atlan. Operations Atlan crawls and manages the following data quality operations and results from Snowflake. Rules and dimensions Reference for available data quality rules and classification dimensions in Snowflake data quality. Source asset type Detailed parsing rules and configuration details for all supported source asset types in the Lineage Generator (no transformations) app. User Role Sync Complete configuration reference for the User Role Sync app properties and settings. What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging. What does Atlan crawl from Dagster Learn about the Dagster metadata that Atlan captures and visualizes What does Atlan crawl from Google GCS Complete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/relational",
    "text": "4 docs tagged with \"relational\" View all tags Microsoft SQL Server Integrate, catalog, and govern Microsoft SQL Server assets in Atlan. MySQL Integrate, catalog, and govern MySQL assets in Atlan. Oracle Integrate, catalog, and govern Oracle assets in Atlan. PostgreSQL Integrate, catalog, and govern PostgreSQL assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/remote",
    "text": "6 docs tagged with \"remote\" View all tags Remote MCP Learn about Atlan's Remote MCP server, a hosted solution that enables AI agents to access Atlan metadata without local infrastructure setup. Set up Claude with Remote MCP Learn how to configure Claude (Connector and Desktop) to use Atlan's Remote MCP server for seamless AI-powered metadata access. Set up Cursor with Remote MCP Learn how to configure Cursor to use Atlan's Remote MCP server for seamless AI-powered metadata access. Set up Microsoft Copilot Studio with Remote MCP Learn how to configure Microsoft Copilot Studio to use Atlan's Remote MCP server with API Key authentication. Set up n8n with Remote MCP Learn how to configure n8n to use Atlan's Remote MCP server with API Key authentication for automated workflows. Set up Windsurf with Remote MCP Learn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/tags/reporting",
    "text": "One doc tagged with \"reporting\" View all tags Reporting Generate comprehensive reports on your data assets, usage, and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/requests",
    "text": "One doc tagged with \"requests\" View all tags Requests Request and manage changes to assets that you don't have direct edit access to."
  },
  {
    "url": "https://docs.atlan.com/tags/roles",
    "text": "2 docs tagged with \"roles\" View all tags Automatically assign roles based on group names Learn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app. User Role Sync Complete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/rules",
    "text": "2 docs tagged with \"rules\" View all tags Rules and dimensions Reference for available data quality rules and classification dimensions in Snowflake data quality. What's Data Quality Studio Understand Atlan's Data Quality Studio and how it enables business and data teams to collaborate on defining, monitoring, and enforcing data quality expectations"
  },
  {
    "url": "https://docs.atlan.com/tags/s-3",
    "text": "4 docs tagged with \"s3\" View all tags Amazon S3 Integrate, catalog, and govern Amazon S3 assets in Atlan. Crawl S3 assets Configure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan. S3 Inventory Report Structure Expected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion. What does Atlan crawl from Amazon S3 Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging."
  },
  {
    "url": "https://docs.atlan.com/tags/s-4-hana",
    "text": "One doc tagged with \"s4hana\" View all tags SAP S/4HANA Integrate, catalog, and govern SAP S/4HANA assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sap",
    "text": "2 docs tagged with \"sap\" View all tags SAP ECC Integrate, catalog, and govern SAP ECC assets in Atlan. SAP S/4HANA Integrate, catalog, and govern SAP S/4HANA assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sap-ecc",
    "text": "One doc tagged with \"sap-ecc\" View all tags Set up SAP ECC Set up user accounts and permissions required for SAP ECC metadata extraction in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sap-hana",
    "text": "One doc tagged with \"sap-hana\" View all tags SAP HANA Catalog and govern SAP HANA assets in Atlan for discovery and governance."
  },
  {
    "url": "https://docs.atlan.com/tags/sap-s-4-hana",
    "text": "One doc tagged with \"sap-s4hana\" View all tags Set up SAP S/4HANA Set up user accounts and permissions required for SAP S/4HANA metadata extraction in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/schema",
    "text": "6 docs tagged with \"schema\" View all tags Confluent Schema Registry Integrate, catalog, and govern Confluent Schema Registry assets in Atlan. Preflight Checks for Cloudera Impala Learn about preflight checks for cloudera impala. Troubleshooting Apache Spark/OpenLineage connectivity Learn about troubleshooting apache spark/openlineage connectivity. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Confluent Schema Registry? Atlan crawls and maps the following assets and properties from Confluent Schema Registry. What does Atlan crawl from SAP HANA? Atlan crawls and maps the following assets and properties from SAP HANA."
  },
  {
    "url": "https://docs.atlan.com/tags/schema-registry",
    "text": "One doc tagged with \"schema registry\" View all tags Confluent Schema Registry Integrate, catalog, and govern Confluent Schema Registry assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/schema-drift",
    "text": "5 docs tagged with \"schema-drift\" View all tags Preflight Checks for Cloudera Impala Learn about preflight checks for cloudera impala. Troubleshooting Apache Spark/OpenLineage connectivity Learn about troubleshooting apache spark/openlineage connectivity. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Confluent Schema Registry? Atlan crawls and maps the following assets and properties from Confluent Schema Registry. What does Atlan crawl from SAP HANA? Atlan crawls and maps the following assets and properties from SAP HANA."
  },
  {
    "url": "https://docs.atlan.com/tags/schema-monitoring",
    "text": "5 docs tagged with \"schema-monitoring\" View all tags Preflight Checks for Cloudera Impala Learn about preflight checks for cloudera impala. Troubleshooting Apache Spark/OpenLineage connectivity Learn about troubleshooting apache spark/openlineage connectivity. What does Atlan crawl from Cloudera Impala? Learn about what does atlan crawl from cloudera impala?. What does Atlan crawl from Confluent Schema Registry? Atlan crawls and maps the following assets and properties from Confluent Schema Registry. What does Atlan crawl from SAP HANA? Atlan crawls and maps the following assets and properties from SAP HANA."
  },
  {
    "url": "https://docs.atlan.com/tags/scopes",
    "text": "One doc tagged with \"scopes\" View all tags Data quality permissions Reference for data quality permission scopes and configuration in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/secrets",
    "text": "One doc tagged with \"secrets\" View all tags Secret management Understand how Secure Agent 2.0 handles secrets and why credentials never leave your environment."
  },
  {
    "url": "https://docs.atlan.com/tags/secure-agent",
    "text": "10 docs tagged with \"secure-agent\" View all tags Architecture Architecture overview and core components of Secure Agent 2.0 Authentication Understand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure. Configure network security Configure firewall rules and network policies to secure communication between Secure Agent 2.0 and Atlan services Customer environment security Customer environment security best practices for deploying and operating Secure Agent 2.0 Data transfer and observability Understand how metadata moves from your sources to Atlan and what visibility you get into operations. Deployment and security Frequently asked questions about Secure Agent 2.0 deployment and security Deployment options Understand how Secure Agent 2.0 deploys across different container environments. Secret management Understand how Secure Agent 2.0 handles secrets and why credentials never leave your environment. Security Security overview and controls for Secure Agent 2.0 Verify container images Verify the authenticity and integrity of Secure Agent container images with Cosign"
  },
  {
    "url": "https://docs.atlan.com/tags/sigma",
    "text": "One doc tagged with \"sigma\" View all tags Sigma Integrate, catalog, and govern Sigma assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sisense",
    "text": "One doc tagged with \"sisense\" View all tags Sisense Integrate, catalog, and govern Sisense assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/soda",
    "text": "One doc tagged with \"soda\" View all tags Soda Integrate, catalog, and govern Soda assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/spark",
    "text": "One doc tagged with \"spark\" View all tags Apache Spark OpenLineage Integrate, catalog, and visualize Apache Spark lineage in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/spreadsheets",
    "text": "One doc tagged with \"spreadsheets\" View all tags Export Assets :::warning Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to [enable asset export](enable-asset-export)."
  },
  {
    "url": "https://docs.atlan.com/tags/sql",
    "text": "4 docs tagged with \"sql\" View all tags Cloudera Impala Integrate, catalog, and govern Cloudera Impala assets in Atlan. CrateDB Integrate, catalog, and govern CrateDB assets in Atlan. PrestoSQL Integrate, catalog, and govern PrestoSQL assets in Atlan. Trino Integrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/sql-server",
    "text": "One doc tagged with \"sql server\" View all tags Microsoft SQL Server Integrate, catalog, and govern Microsoft SQL Server assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/stewardship",
    "text": "One doc tagged with \"stewardship\" View all tags Stewardship Learn how to implement data stewardship in Atlan through automated workflows, policies, and task management."
  },
  {
    "url": "https://docs.atlan.com/tags/storage",
    "text": "8 docs tagged with \"storage\" View all tags Amazon DynamoDB Integrate, catalog, and govern Amazon DynamoDB assets in Atlan. Amazon S3 Integrate, catalog, and govern Amazon S3 assets in Atlan. Crawl GCS assets Configure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan. Crawl S3 assets Configure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan. Google Cloud Storage Integrate, catalog, and govern Google Cloud Storage assets in Atlan. Set up Amazon S3 Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects. Set up Google Cloud Storage Configure Google Cloud Storage for secure metadata ingestion with Atlan. Set up Inventory reports Create Inventory report for Amazon S3 in case of inventory based ingestion through the crawler."
  },
  {
    "url": "https://docs.atlan.com/tags/synapse",
    "text": "One doc tagged with \"synapse\" View all tags Microsoft Azure Synapse Analytics Integrate, catalog, and govern Microsoft Azure Synapse Analytics assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/tasks",
    "text": "One doc tagged with \"tasks\" View all tags Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector"
  },
  {
    "url": "https://docs.atlan.com/tags/teradata",
    "text": "One doc tagged with \"teradata\" View all tags Teradata Catalog and govern Teradata assets in Atlan. Optionally mine query history to build lineage."
  },
  {
    "url": "https://docs.atlan.com/tags/terminology",
    "text": "One doc tagged with \"terminology\" View all tags Glossary Learn how to create and maintain a centralized business glossary in Atlan to standardize terminology and definitions across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/thoughtspot",
    "text": "One doc tagged with \"thoughtspot\" View all tags ThoughtSpot Integrate, catalog, and govern ThoughtSpot assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/tokens",
    "text": "One doc tagged with \"tokens\" View all tags Authentication Understand how Secure Agent 2.0 authentication works and why it protects your enterprise infrastructure."
  },
  {
    "url": "https://docs.atlan.com/tags/transformations",
    "text": "2 docs tagged with \"transformations\" View all tags Tasks, transformations, and lineage Learn about supported tasks, transformations, and lineage generation in the Informatica CDI connector Transformations Understand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in Atlan"
  },
  {
    "url": "https://docs.atlan.com/tags/trino",
    "text": "One doc tagged with \"trino\" View all tags Trino Integrate, catalog, and govern Trino assets in Atlan."
  },
  {
    "url": "https://docs.atlan.com/tags/usage",
    "text": "One doc tagged with \"usage\" View all tags Usage and Popularity Track and analyze how your data assets are being used across your organization."
  },
  {
    "url": "https://docs.atlan.com/tags/user-groups",
    "text": "2 docs tagged with \"user groups\" View all tags Automatically assign roles based on group names Learn how to automatically assign roles and sub-roles to users in Atlan based on their group memberships using the group-role sync app. User Role Sync Complete configuration reference for the User Role Sync app properties and settings."
  },
  {
    "url": "https://docs.atlan.com/tags/users",
    "text": "One doc tagged with \"users\" View all tags Users and groups Learn how to manage users and groups in Atlan to control access and organize your data team."
  },
  {
    "url": "https://docs.atlan.com/tags/windsurf",
    "text": "One doc tagged with \"Windsurf\" View all tags Set up Windsurf with Remote MCP Learn how to configure Windsurf to use Atlan's Remote MCP server with API Key authentication."
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk",
    "text": "Connect data Event/Messaging Amazon MSK On this page Amazon MSK Overview: Catalog Amazon MSK clusters, topics, and schemas in Atlan. Gain visibility into event streams, lineage, and governance for your AWS-based messaging platform. Get started â Follow these steps to connect and catalog Amazon MSK assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Amazon MSK? : Detailed list of Amazon MSK asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Amazon MSK : Technical checks and requirements needed for a successful Amazon MSK integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Amazon MSK, including permissions and network problems. Tags: amazon msk kafka connector messaging connectivity Next Set up Amazon MSK Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server",
    "text": "Connect data Databases SQL Databases Microsoft SQL Server On this page Microsoft SQL Server Overview: Catalog Microsoft SQL Server databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your SQL Server data assets. Get started â Follow these steps to connect and catalog Microsoft SQL Server assets in Atlan: Set up the connector Crawl Microsoft SQL Server assets Guides â Set up a private network link to Microsoft SQL Server on Amazon EC2 : Configure Atlan to connect to SQL Server in secure, private AWS EC2 environments. Set up a private network link to Microsoft SQL Server on Amazon RDS : Configure Atlan to connect to SQL Server in secure, private AWS RDS environments. References â What does Atlan crawl from Microsoft SQL Server : Learn about the Microsoft SQL Server assets and metadata that Atlan discovers and catalogs. Preflight checks for Microsoft SQL Server : Verify prerequisites before setting up the Microsoft SQL Server connector. Tags: microsoft sql server connector database relational connectivity Next Set up Microsoft SQL Server Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql",
    "text": "Connect data Databases SQL Databases MySQL On this page MySQL Overview: Catalog MySQL databases, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your MySQL data assets. Get started â Follow these steps to connect and catalog MySQL assets in Atlan: Set up the connector Crawl MySQL assets Guides â Set up a private network link to MySQL : Configure a secure private connection to your MySQL database. References â What does Atlan crawl from MySQL : Learn about the MySQL assets and metadata that Atlan discovers and catalogs. Preflight checks for MySQL : Verify prerequisites before setting up the MySQL connector. Troubleshooting â Troubleshooting connectivity : Resolve common MySQL connection issues and errors. Tags: mysql connector database relational connectivity Next Set up MySQL Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena",
    "text": "Connect data Databases Query Engines Amazon Athena On this page Amazon Athena Overview: Catalog Amazon Athena databases, schemas, tables, views, and columns in Atlan. Organize and govern your Athena data lake assets with rich metadata for discovery. Get started â Follow these steps to connect and catalog Amazon Athena assets in Atlan: Set up the connector Crawl Amazon Athena assets Guides â Set up a private network link to Amazon Athena : Configure Atlan to connect to Athena in secure, private AWS environments. References â What does Atlan crawl from Amazon Athena : Learn about the Amazon Athena assets and metadata that Atlan discovers and catalogs. Tags: amazon-athena connector database connectivity Next Set up Amazon Athena Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka",
    "text": "Connect data Event/Messaging Apache Kafka On this page Apache Kafka Overview: Catalog Apache Kafka topics, schemas, and consumer groups in Atlan. Gain visibility into event streams, lineage, and governance for your messaging platform. Get started â Follow these steps to connect and catalog Apache Kafka assets in Atlan: Set up the connector Crawl Apache Kafka assets Guides â Set up on-premises Kafka access : Configure Atlan to connect to Kafka environments that are isolated from the public internet. Crawl on-premises Kafka : Extract metadata from on-premises Kafka instances. References â What does Atlan crawl from Apache Kafka : Learn about the Apache Kafka assets and metadata that Atlan discovers and catalogs. Preflight checks for Apache Kafka : Verify prerequisites before setting up the Apache Kafka connector. Tags: apache kafka connector messaging connectivity Next Set up Apache Kafka Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala",
    "text": "Connect data Databases Query Engines Cloudera Impala On this page Cloudera Impala Overview: Catalog Cloudera Impala databases, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your Cloudera Impala data assets. Get started â Follow these steps to connect and catalog Cloudera Impala assets in Atlan: Set up the connector Crawl Cloudera Impala assets References â What does Atlan crawl from Cloudera Impala : Detailed list of metadata objects extracted from Cloudera Impala. Preflight checks for Cloudera Impala : Verify your environment before running crawlers. Tags: connector database cloudera impala sql connectivity Next Set up Cloudera Impala Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/how-tos/crawl-cloudera-impala",
    "text": "Connect data Databases Query Engines Cloudera Impala Crawl Cloudera Impala Assets Crawl Cloudera Impala On this page Crawl Cloudera Impala Once you have configured the Cloudera Impala user permissions , you can establish a connection between Atlan and Cloudera Impala. To crawl metadata from Cloudera Impala, review the order of operations and then complete the following steps. Select the source â To select Cloudera Impala as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Cloudera Impala Assets . In the right panel, click Setup Workflow . Provide your credentials â To enter your Cloudera Impala credentials: For Extraction method , Direct is the default selection. For Hostname , enter the host name of your Cloudera Impala coordinator or load balancer. For Authentication , select LDAP as the authentication method. For Username , enter the LDAP username that has access to Cloudera Impala. For Password , enter the password associated with the LDAP username. For SSL , keep Enabled to connect via a Secure Sockets Layer (SSL) channel or click Disabled . Click the Test Authentication button to confirm connectivity to Cloudera Impala. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection â To complete the Cloudera Impala connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . Careful If you do not specify any user or group, no one will be able to manage the connection â not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler â Before running the Cloudera Impala crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: To include specific assets in crawling, click Include Metadata , and select the assets you want. If you don't select any, all assets will be included by default. To exclude specific assets from crawling, click Exclude Metadata , and choose the assets you want to omit. If you don't select any, no assets will be excluded. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler â To run the Cloudera Impala crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! ð Tags: cloudera impala connectivity crawling Previous Set up Cloudera Impala Next What does Atlan crawl from Cloudera Impala? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase",
    "text": "Connect data BI Tools Cloud-based BI Metabase On this page Metabase Overview: Catalog Metabase questions, dashboards, and collections in Atlan. Gain visibility into lineage, usage, and governance for your Metabase analytics assets. Get started â Follow these steps to connect and catalog Metabase assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Metabase? : Detailed list of Metabase asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Metabase : Technical checks and requirements needed for a successful Metabase integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Metabase, including permissions and network problems. Tags: metabase connector business intelligence hybrid bi connectivity Next Set up Metabase Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases",
    "text": "Connect data Databases On-premises On-premises Databases On this page On-Premises Databases Overview: Extract metadata from on-premises databases using Atlan's metadata-extractor tool. Securely upload metadata to S3 for ingestion into Atlanâno direct connection required. Get started â Follow these steps to extract and ingest metadata from on-premises databases: Set up on-premises database access Crawl on-premises databases Guides â Connect on-premises databases to Kubernetes : For advanced/enterprise environments. References â Supported connections for on-premises databases : Full list of supported databases and configuration options. Troubleshooting â Troubleshooting connectivity : Solutions for common issues and offline extraction scenarios. Tags: on-premises metadata-extractor offline database air-gapped connectivity Next Set up on-premises database access Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql",
    "text": "Connect data Databases SQL Databases PostgreSQL On this page PostgreSQL Overview: Catalog PostgreSQL databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your PostgreSQL data assets. Get started â Follow these steps to connect and catalog PostgreSQL assets in Atlan: Set up the connector Crawl PostgreSQL assets Guides â Set up a private network link to PostgreSQL : Configure a secure private connection to your PostgreSQL database. References â What does Atlan crawl from PostgreSQL : Learn about the PostgreSQL assets and metadata that Atlan discovers and catalogs. Preflight checks for PostgreSQL : Verify prerequisites before setting up the PostgreSQL connector. Troubleshooting â Troubleshooting connectivity : Resolve common PostgreSQL connection issues and errors. Tags: postgresql connector database relational connectivity Next Set up PostgreSQL Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka",
    "text": "Connect data Event/Messaging Aiven Kafka On this page Aiven Kafka Overview: Catalog Aiven Kafka topics, schemas, and consumer groups in Atlan. Gain visibility into event streams, lineage, and governance for your Aiven-hosted messaging platform. Get started â Follow these steps to connect and catalog Aiven Kafka assets in Atlan: Set up the connector Crawl assets Guides â Set up on-premises Kafka access Crawl on-premises Kafka References â What does Atlan crawl from Aiven Kafka? : Detailed list of Aiven Kafka asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Aiven Kafka : Technical checks and requirements needed for a successful Aiven Kafka integration. Tags: aiven kafka connector messaging connectivity Next Set up Aiven Kafka Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb",
    "text": "Connect data Databases NoSQL Databases Amazon DynamoDB On this page Amazon DynamoDB Overview: Catalog Amazon DynamoDB tables, items, and attributes in Atlan. Gain visibility into NoSQL data structures, access patterns, and governance for your AWS-based storage. Get started â Follow these steps to connect and catalog Amazon DynamoDB assets in Atlan: Set up the connector Crawl Amazon DynamoDB assets References â What does Atlan crawl from Amazon DynamoDB : Learn about the DynamoDB assets and metadata that Atlan discovers and catalogs. Troubleshooting â Troubleshooting connectivity : Resolve common Amazon DynamoDB connection issues and errors. Tags: amazon dynamodb connector storage nosql connectivity Next Set up Amazon DynamoDB Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3",
    "text": "Connect data Storage Amazon S3 On this page Amazon S3 Overview: Catalog Amazon S3 buckets and objects in Atlan. Gain visibility into your S3 data assets and their organization. Get started â Follow these steps to connect and catalog Amazon S3 assets in Atlan: Set up Amazon S3 : Configure AWS permissions and credentials Crawl S3 assets : Run the crawler to catalog your S3 assets Guides â Set up inventory reports for S3 : Configure inventory reports for efficient large-scale ingestion References â Inventory report structure for Amazon S3 : Required folder structure and format for S3 inventory reports What does Atlan crawl from S3 : Complete reference for S3 assets and properties that Atlan crawls Tags: s3 storage aws connectivity Next Set up Amazon S3 Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue",
    "text": "Connect data ETL Tools AWS Glue On this page AWS Glue Overview: Catalog AWS Glue jobs, crawlers, and databases in Atlan. Gain visibility into lineage, transformations, and governance for your AWS ETL assets. Get started â Follow these steps to connect and catalog AWS Glue assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from AWS Glue? : Detailed list of AWS Glue asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to AWS Glue, including permissions and network problems. Tags: aws glue connector etl data integration connectivity Next Set up AWS Glue Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka",
    "text": "Connect data Event/Messaging Confluent Kafka On this page Confluent Kafka Overview: Catalog Confluent Kafka topics, schemas, and connectors in Atlan. Gain visibility into event streams, lineage, and governance for your Confluent platform. Get started â Follow these steps to connect and catalog Confluent Kafka assets in Atlan: Set up the connector Crawl Confluent Kafka assets Guides â Set up on-premises Kafka access : Configure Atlan to connect to Kafka environments that are isolated from the public internet. Crawl on-premises Kafka : Extract metadata from on-premises Kafka instances. References â What does Atlan crawl from Confluent Kafka : Learn about the Confluent Kafka assets and metadata that Atlan discovers and catalogs. References â What does Atlan crawl from Confluent Kafka : Learn about the Confluent Kafka assets and metadata that Atlan discovers and catalogs. Tags: confluent kafka connector messaging connectivity Next Set up Confluent Kafka Get started Guides References References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry",
    "text": "Connect data Event/Messaging Confluent Schema Registry On this page Confluent Schema Registry Overview: Catalog Confluent Schema Registry subjects and schemas in Atlan. Gain visibility into schema evolution, compatibility, and governance for your event data structures. Get started â Follow these steps to connect and catalog Confluent Schema Registry assets in Atlan: Set up the connector Crawl Confluent Schema Registry assets References â What does Atlan crawl from Confluent Schema Registry : Learn about the Schema Registry assets and metadata that Atlan discovers and catalogs. Preflight checks for Confluent Schema Registry : Verify prerequisites before setting up the Confluent Schema Registry connector. Tags: confluent schema registry connector schema connectivity Next Set up Confluent Schema Registry Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo",
    "text": "Connect data BI Tools Cloud-based BI Domo On this page Domo Overview: Catalog Domo dashboards, cards, and datasets in Atlan. Gain visibility into lineage, usage, and governance for your Domo analytics assets. Get started â Follow these steps to connect and catalog Domo assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Domo? : Detailed list of Domo asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Domo : Technical checks and requirements needed for a successful Domo integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Domo, including permissions and network problems. Tags: domo connector business intelligence hybrid bi connectivity Next Set up Domo Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran",
    "text": "Connect data ETL Tools Fivetran On this page Fivetran Overview: Catalog Fivetran connectors, destinations, and transformations in Atlan. Gain visibility into lineage, data movement, and governance for your Fivetran ETL assets. Get started â Follow these steps to connect and catalog Fivetran assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Fivetran? : Detailed list of Fivetran asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Fivetran : Technical checks and requirements needed for a successful Fivetran integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Fivetran, including permissions and network problems. Tags: fivetran connector etl data integration connectivity Next Set up Fivetran Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive",
    "text": "Connect data Databases Query Engines Hive On this page Hive Overview: Catalog Hive databases, schemas, tables, views, materialized views, and columns in Atlan for discovery and governance. Get started â Follow these steps to connect and catalog Hive assets in Atlan: Set up the connector Crawl Hive assets Guides â Set up a private network link to Hive : Configure a secure private connection to your Hive metastore. References â What does Atlan crawl from Hive : Learn about the Hive assets and metadata that Atlan discovers and catalogs. Preflight checks for Hive : Verify prerequisites before setting up the Hive connector. Troubleshooting â Troubleshooting connectivity : Resolve common Hive connection issues and metastore limitations. Tags: hive connector database data lake connectivity Next Set up Hive Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi",
    "text": "Connect data ETL Tools Informatica CDI On this page Informatica CDI Overview: Connect to Informatica Cloud Data Integration to automatically discover, catalog, and track lineage for your data integration workflows and assets. Get started â Follow these steps to connect and catalog Informatica CDI assets in Atlan: Set up the connector : Configure user authentication and gather required parameter files Crawl Informatica CDI assets : Create crawler workflow and discover your assets Concepts â Transformation logic : Understand how transformation logic and business rules are discovered and cataloged References â What does Atlan crawl from Informatica CDI : Understand the metadata and assets discovered during crawling Troubleshooting â Task and crawl issues : Fix connection problems and resolve crawling issues FAQ â Tasks, transformations, and lineage : Get answers about tasks, transformations, API calls, parameter files, lineage, and limitations Tags: connector etl-tools connectivity Next Set up Informatica CDI Get started Concepts References Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion",
    "text": "Connect data ETL Tools Matillion On this page Matillion Overview: Catalog Matillion jobs, components, and transformations in Atlan. Gain visibility into lineage, data movement, and governance for your Matillion ETL assets. Get started â Follow these steps to connect and catalog Matillion assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Matillion? : Detailed list of Matillion asset types and metadata fields that Atlan can extract and catalog. What lineage does Atlan extract from Matillion? : Learn about supported lineage extraction for Matillion assets. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Matillion, including permissions and network problems. Tags: matillion connector etl data integration connectivity Next Set up Matillion Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db",
    "text": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB On this page Microsoft Azure Cosmos DB Overview: Catalog Microsoft Azure Cosmos DB accounts, databases, collections, and columns in Atlan for discovery and governance. Get started â Follow these steps to connect and catalog Cosmos DB assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Microsoft Azure Cosmos DB? : Detailed list of Cosmos DB asset types and metadata fields. Troubleshooting â Troubleshooting connectivity : Solutions for common issues. Tags: cosmosdb azure connector database nosql connectivity Next Set up Microsoft Azure Cosmos DB Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory",
    "text": "Connect data ETL Tools Microsoft Azure Data Factory On this page Microsoft Azure Data Factory Overview: Catalog Microsoft Azure Data Factory pipelines, datasets, and activities in Atlan. Gain visibility into lineage, data movement, and governance for your Azure ETL assets. Get started â Follow these steps to connect and catalog Microsoft Azure Data Factory assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Microsoft Azure Data Factory? : Detailed list of Azure Data Factory asset types and metadata fields that Atlan can extract and catalog. What lineage does Atlan extract from Microsoft Azure Data Factory? : Learn about supported lineage extraction for Azure Data Factory assets. Preflight checks for Microsoft Azure Data Factory : Technical checks and requirements needed for a successful Azure Data Factory integration. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Microsoft Azure Data Factory, including permissions and network problems. Tags: microsoft azure data factory connector etl data integration connectivity Next Set up Microsoft Azure Data Factory Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs",
    "text": "Connect data Event/Messaging Microsoft Azure Event Hubs On this page Microsoft Azure Event Hubs Overview: Catalog Azure Event Hubs namespaces and event hubs in Atlan. Gain visibility into event streams, lineage, and governance for your Azure-based messaging platform. Get started â Follow these steps to connect and catalog Azure Event Hubs assets in Atlan: Set up the connector Crawl assets References â What does Atlan crawl from Azure Event Hubs? : Detailed list of Azure Event Hubs asset types and metadata fields that Atlan can extract and catalog. Tags: microsoft azure event hubs connector messaging connectivity Next Set up Microsoft Azure Event Hubs Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics",
    "text": "Connect data Data Warehouses Microsoft Azure Synapse Analytics On this page Microsoft Azure Synapse Analytics Overview: Catalog Microsoft Azure Synapse Analytics workspaces, pools, databases, and tables in Atlan. Gain visibility into lineage, usage, and governance for your Azure data warehouse assets. Get started â Follow these steps to connect and catalog Microsoft Azure Synapse Analytics assets in Atlan: Set up the connector Crawl Microsoft Azure Synapse Analytics assets Guides â Mine Microsoft Azure Synapse Analytics : Extract query history and build lineage for your Synapse Analytics assets. Set up on-premises Microsoft Azure Synapse Analytics miner access : Configure Atlan to mine query history from on-premises Synapse Analytics environments. References â What does Atlan crawl from Microsoft Azure Synapse Analytics : Learn about the Synapse Analytics assets and metadata that Atlan discovers and catalogs. What lineage does Atlan extract from Microsoft Azure Synapse Analytics : Learn about supported lineage extraction for Synapse Analytics assets. Preflight checks for Microsoft Azure Synapse Analytics : Verify prerequisites before setting up the Microsoft Azure Synapse Analytics connector. Tags: microsoft azure synapse analytics connector data warehouse connectivity Next Set up Microsoft Azure Synapse Analytics Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb",
    "text": "Connect data Databases NoSQL Databases MongoDB On this page MongoDB Overview: Catalog MongoDB databases, collections, and columns in Atlan for discovery and governance. Get started â Follow these steps to connect and catalog MongoDB assets in Atlan: Set up the connector Crawl MongoDB assets References â What does Atlan crawl from MongoDB : Learn about the MongoDB assets and metadata that Atlan discovers and catalogs. Troubleshooting â Troubleshooting connectivity : Resolve common MongoDB connection issues and errors. Tags: mongodb atlas connector database nosql connectivity Next Set up MongoDB Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle",
    "text": "Connect data Databases SQL Databases Oracle On this page Oracle Overview: Catalog Oracle databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your Oracle data assets. Get started â Follow these steps to connect and catalog Oracle assets in Atlan: Set up the connector Crawl Oracle assets References â What does Atlan crawl from Oracle : Learn about the Oracle assets and metadata that Atlan discovers and catalogs. Preflight checks for Oracle : Verify prerequisites before setting up the Oracle connector. Tags: oracle connector database relational connectivity Next Set up Oracle Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql",
    "text": "Connect data Databases Query Engines PrestoSQL On this page PrestoSQL Overview: Catalog PrestoSQL databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your PrestoSQL data assets. Atlan currently only supports PrestoSQL up to version 349. PrestoDB is not supported. Get started â Follow these steps to connect and catalog PrestoSQL assets in Atlan: Set up the connector Crawl PrestoSQL assets References â What does Atlan crawl from PrestoSQL : Learn about the PrestoSQL assets and metadata that Atlan discovers and catalogs. Preflight checks for PrestoSQL : Verify prerequisites before setting up the PrestoSQL connector. Tags: prestosql connector database sql connectivity Next Set up PrestoSQL Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows",
    "text": "Connect data BI Tools On-premises & Enterprise BI Qlik Sense Enterprise on Windows On this page Qlik Sense Enterprise (Windows) Overview: Catalog apps, sheets, and data sources from Qlik Sense Enterprise on Windows in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Qlik Sense Enterprise on Windows assets in Atlan: Set up the connector : Configure Atlan to connect to your Qlik Sense Enterprise on Windows environment. Crawl assets : Extract and catalog Qlik Sense Enterprise on Windows apps, sheets, and data sources. References â What does Atlan crawl from Qlik Sense Enterprise on Windows? : Detailed list of Qlik Sense Enterprise on Windows asset types and metadata fields that Atlan can extract and catalog. Tags: qlik sense connector business intelligence connectivity Next Set up Qlik Sense Enterprise on Windows Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka",
    "text": "Connect data Event/Messaging Redpanda Kafka On this page Redpanda Kafka Overview: Catalog Redpanda topics, schemas, and consumer groups in Atlan. Gain visibility into event streams, lineage, and governance for your Redpanda messaging platform. Get started â Follow these steps to connect and catalog Redpanda Kafka assets in Atlan: Set up the connector Crawl assets Guides â Set up on-premises Kafka access Crawl on-premises Kafka References â What does Atlan crawl from Redpanda Kafka? : Detailed list of Redpanda Kafka asset types and metadata fields that Atlan can extract and catalog. Preflight checks for Redpanda Kafka : Technical checks and requirements needed for a successful Redpanda Kafka integration. Tags: redpanda kafka connector messaging connectivity Next Set up Redpanda Kafka Get started Guides References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc",
    "text": "Connect data ERP SAP ECC On this page SAP ECC Overview: Catalog SAP ECC modules, tables, and fields in Atlan. Gain visibility into lineage, usage, and governance for your enterprise resource planning data. Get started â Follow these steps to connect and catalog SAP ECC assets in Atlan: Set up SAP ECC : Configure user accounts and permissions for metadata extraction Crawl SAP ECC assets : Extract and catalog your SAP ECC metadata References â What does Atlan crawl from SAP ECC : Learn about the SAP ECC assets and metadata that Atlan discovers and catalogs. Tags: sap ecc connector erp connectivity Next Set up SAP ECC Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana",
    "text": "Connect data Databases SQL Databases SAP HANA On this page SAP HANA Overview: Catalog SAP HANA schemas, tables, views, calculation views, columns, and stored procedures in Atlan for discovery and governance. Get started â Follow these steps to connect and catalog SAP HANA assets in Atlan: Set up the connector Crawl SAP HANA assets References â What does Atlan crawl from SAP HANA : Learn about the SAP HANA assets and metadata that Atlan discovers and catalogs. Preflight checks for SAP HANA : Verify prerequisites before setting up the SAP HANA connector. Tags: sap-hana connector database calculation-view connectivity Next Set up SAP HANA Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana",
    "text": "Connect data ERP SAP S/4HANA On this page SAP S/4HANA Overview: Catalog SAP S/4HANA components, tables, views, CDS views, and more in Atlan. Gain visibility into your enterprise resource planning data and its lineage. Get started â Follow these steps to connect and catalog SAP S/4HANA assets in Atlan: Set up SAP S/4HANA : Configure user accounts and permissions for metadata extraction Crawl SAP S/4HANA assets : Extract and catalog your SAP S/4HANA metadata References â What does Atlan crawl from SAP S/4HANA : Learn about the SAP S/4HANA assets and metadata that Atlan discovers and catalogs. Tags: sap s4hana connector erp connectivity Next Set up SAP S/4HANA Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense",
    "text": "Connect data BI Tools Cloud-based BI Sisense On this page Sisense Overview: Catalog dashboards, widgets, and data models from Sisense in Atlan to enable discovery, lineage, and governance for your analytics assets. Get started â Follow these steps to connect and catalog Sisense assets in Atlan: Set up the connector : Configure Atlan to connect to your Sisense environment. Crawl assets : Extract and catalog Sisense dashboards, widgets, and data models. References â What does Atlan crawl from Sisense? : Detailed list of Sisense asset types and metadata fields that Atlan can extract and catalog. Troubleshooting â Troubleshooting connectivity : Solutions for common issues encountered when connecting Atlan to Sisense, including permissions and network problems. Tags: sisense connector business intelligence connectivity Next Set up Sisense Get started References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino",
    "text": "Connect data Databases Query Engines Trino On this page Trino Overview: Catalog Trino databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your Trino data assets. Atlan supports Trino with basic authentication and private network options. Get started â Follow these steps to connect and catalog Trino assets in Atlan: Set up the connector Crawl Trino assets Guides â Set up a private network link to Trino : Configure Atlan to connect to Trino in secure, private environments. References â What does Atlan crawl from Trino : Learn about the Trino assets and metadata that Atlan discovers and catalogs. Preflight checks for Trino : Verify prerequisites before setting up the Trino connector. Troubleshooting â Troubleshooting connectivity : Resolve common Trino connection issues and errors. Tags: trino connector database sql connectivity Next Set up Trino Get started Guides References Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb",
    "text": "Connect data Databases SQL Databases CrateDB On this page CrateDB Overview: Catalog CrateDB databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your CrateDB data assets. Get started â Follow these steps to connect and catalog CrateDB assets in Atlan: Set up the connector Crawl CrateDB assets References â What does Atlan crawl from CrateDB : Learn about the CrateDB assets and metadata that Atlan discovers and catalogs. Preflight checks for CrateDB : Verify prerequisites before setting up the CrateDB connector. Troubleshooting â Connection issues : Resolve common CrateDB connection issues and errors. FAQ â Permissions and limitations : Find answers to frequently asked questions about CrateDB setup and limitations. Tags: cratedb connector database sql Next Set up CrateDB Get started References Troubleshooting FAQ"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-governance",
    "text": "Use data Reporting Report Types Report on governance On this page Report on governance Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The governance dashboard in the reporting center helps you review and report on metrics related to all your personas , purposes , tags , and requests .Â Track query access â You can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup. To view query access for a persona: From the left menu in Atlan, click Reporting and then click Governance . From the Governance dashboard, under Personas , navigate to Query Access .Â Under Query Access ,Â click Personas with query access to view more details in the governance center.Â View assets tagged by propagation â Apart from viewing the total count of assets tagged by propagation , you can also view the propagated assets right from the dashboard for lineage analysis.Â To view assets tagged by propagation:Â From the left menu in Atlan, click Reporting and then click Governance . From the Governance dashboard, scroll down to the Tag by Propagation section. Click any tag to view a list of propagated assets in the sidebar.Â Track requests â You can view and take action on all your requests Â from the governance dashboard. To track metadata update requests: From the left menu in Atlan, click Reporting and then click Governance . From the Governance dashboard, scroll down to the Requests section. (Optional) Under Requests , click the date selector dropdown to filter requests by a predefined or custom date range. (Optional) Click the All asset types dropdown to filter requests by a specific asset type. Under Requests overview , view all requests grouped by request status. Click any request to take action in the Governance center . Tags: glossary business-terms definitions Previous Report on usage and cost Next Summarize metadata Track query access View assets tagged by propagation Track requests"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/troubleshooting/troubleshooting-alteryx-connectivity",
    "text": "Connect data ETL Tools Alteryx Troubleshooting Connection issues On this page Connection issues Private preview This guide helps you resolve common issues when setting up the Alteryx connector in Atlan. Unexpected workflow input-output mapping in technical lineage â When you are setting up lineage for Alteryx workflows in Atlan, especially during private preview, you may encounter errors related to unexpected input-output mapping. These errors typically surface when Atlan doesnât receive enough information from Alteryx to generate accurate technical lineage. Error messages Unexpected Workflow Input-Output Mapping in Technical Lineage Cause Atlan currently receives limited metadata from Alteryx as part of the private preview integration. This incomplete data results in an inability to correctly determine how input datasets relate to output datasets within workflows. As a result, the generated lineage may be partial, incorrect, or missing entirely. How to fix While this limitation is inherent to the current private preview, here are some steps you can take: Verify all workflow inputs and outputs are explicitly defined with clear and consistent names. Avoid using tools like Dynamic Input, Download, or Run Command that make data paths harder to trace. Use consistent naming conventions for datasets and tools across all workflows. Reorganize complex workflows into simpler sections to make lineage easier to infer. Re-ingest the workflow in Atlan after making changes to check if the technical lineage appears correctly. Contact Atlan support if the issue continues. Previous What does Atlan crawl from Alteryx?"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs",
    "text": "Connect data Storage Google GCS On this page Google Cloud Storage Overview: Catalog Google Cloud Storage buckets and objects in Atlan. Gain visibility into your GCS data assets and their organization. Get started â Follow these steps to connect and catalog Google Cloud Storage assets in Atlan: Set up the connector Crawl GCS assets References â What does Atlan crawl from GCS : Learn about the GCS metadata that Atlan discovers and catalogs. Tags: connector data-catalog gcs storage gcp Next Set up Google Cloud Storage Get started References"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/references/preflight-checks-for-cloudera-impala",
    "text": "Connect data Databases Query Engines Cloudera Impala References Preflight Checks for Cloudera Impala Preflight Checks for Cloudera Impala Before running the Cloudera Impala you can run preflight checks to perform the necessary technical validations. The following preflight checks will be completed: Assets Schema Permission â Check successful â Check failed. Failed to connect to the Impala cluster. Tags: schema schema-drift schema-monitoring Previous What does Atlan crawl from Cloudera Impala?"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/references/architecture",
    "text": "Secure Agent 2.0 Architecture & Security Architecture On this page Architecture Secure Agent 2.0 enables secure metadata extraction from enterprise data sources and transfers this metadata to Atlan SaaS for processing and asset management. The agent operates entirely within your infrastructure while maintaining secure communication with Atlan's cloud services. Core components â Secure Agent 2.0 consists of four core components that work together to provide metadata extraction and transfer capabilities: Temporal worker : Orchestrates extraction workflows through persistent connections Dapr : Makes it easier to connect to back-end services (such as object storage and secret stores) using standard, secure APIs FastAPI server : Provides operational APIs for monitoring and control Application code : Implements source-specific extraction logic Temporal worker â Purpose: Orchestrates metadata extraction workflows through persistent gRPC connections with Atlan's Temporal service. Temporal workers are long-running processes that orchestrate the entire metadata extraction workflow. These workers establish persistent gRPC connections with the Temporal service hosted in Atlan's environment and continuously listen on designated task queues. Key responsibilities: Poll the task queue for available extraction tasks Execute the extraction logic for each task Send results back to the Temporal service hosted by Atlan Technical details: Connection type: Persistent gRPC Authentication: OAuth 2.0 client credentials flow with unique Client ID and Client Secret per application Communication pattern: Worker-initiated (no inbound connections required) Security model: All connections originate from your infrastructure to Atlan services, ensuring no inbound network access is required For security details, see Security . Dapr â Purpose: Provides abstraction layer for secure interaction with back-end services including secret stores, object storage, and Atlan SaaS. Dapr (Distributed Application Runtime) serves as the abstraction layer between the application and various back-end services. It provides a consistent interface for common distributed application needs, simplifying integration with different infrastructure components. Core capabilities: Retrieve secrets from secret stores Write data to object stores Send events and extracted metadata to Atlan SaaS over HTTPS Supported storage targets: AWS S3 Azure Blob Storage Google Cloud Storage (GCS) Local file systems Dapr handles all the complexity of different storage APIs and authentication mechanisms, providing a unified interface for the application code. Learn more: Dapr FastAPI server â Purpose: Provides REST APIs for operational control, monitoring, and observability of the Secure Agent. Each Secure Agent application hosts its own FastAPI server that exposes REST APIs for operational management. Available endpoints: Endpoint Purpose Returns Health check System health monitoring Platform, hostname, IP address, MAC address, processor, RAM Readiness Service availability Confirmation if system is ready to handle requests Observability Operational insights Metrics, logs, and runtime status These APIs enable integration with existing enterprise monitoring and alerting systems. Application code for extraction â Purpose: Implements source-specific application logic (for example metadata extraction and transformation). The application code implements the actual business logic for metadata extraction from various source systems. Process flow: Metadata Extraction Workflow trigerred by user or on schedule Connect to source system based on configured secret store Fetch metadata based on parameters Transform metadata into standardized formats (JSON or Parquet) Write transformed data to an object store Transfer metadata to Atlan SaaS for further processing Data flow â The Secure Agent implements a controlled pipeline for metadata extraction and transfer: Source Systems â Extraction â Local Storage â Object Store â Atlan SaaS â             â              â             â Application    Temporary      Persistent      Cloud Code          Files         Storage       Processing How it works â The complete workflow follows these steps: Deploy and register : Applications are deployed on your enterprise infrastructure and registered with your Atlan tenant. This establishes the secure connection between your environment and Atlan services using OAuth 2.0 client credentials. Configure workflow : A user configures a workflow from the Atlan UI, defining source systems to connect to, extraction schedules or triggers, transformation requirements, and target storage locations. Execute extraction : The application retrieves relevant job details from Atlan and performs defined actions to extract metadata from the source. This includes: Connecting to specified source systems using appropriate protocols Executing extraction logic based on configured parameters Transforming metadata into standardized formats (JSON or Parquet) Transfer metadata : Extracted metadata is first written to configured storage (S3, Azure Blob, GCS, or others), and then securely transferred to the Atlan SaaS tenant using HTTPS. Process and publish : Atlan workflows processes the transferred metadata files and publishes them as searchable, governed assets in your Atlan workspace. Monitor execution : Throughout the execution, logs are collected and sent to Atlan for monitoring and auditing. Status updates are shown in the Atlan UI, and health metrics are exposed via FastAPI endpoints for integration with enterprise monitoring systems. See also â Authentication : How OAuth 2.0 authentication works and protects your infrastructure. Data transfer and observability : How metadata moves from sources to Atlan and monitoring capabilities. Deployment options : How containerization enables flexible deployment across environments. Security : Security architecture, authentication, encryption, and compliance controls for Secure Agent 2.0. Tags: secure-agent architecture deployment Previous Secure Agent 2.0 Next Security Core components Data flow How it works See also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/data-transfer-and-observability",
    "text": "Secure Agent 2.0 Concepts Data transfer and observability On this page Data transfer and observability Data transfer in Secure Agent 2.0 moves extracted metadata from your source systems to Atlan through a secure, multi-stage process. Understanding this flow helps you see how your data stays protected and where you get visibility into operations throughout the metadata extraction journey. How metadata moves to Atlan â After metadata is extracted from source systems, it follows a secure transfer process to Atlan SaaS for further processing: Agent App writes output to container local volume : The application initially stores extracted metadata in the container's local storage, which uses EBS volumes or node disks. Output moves to persistent storage : The metadata is then periodically transferred to your configured persistent storage systems, such as local volume mounts or cloud object storage like AWS S3, Azure Blob Storage, or Google Cloud Storage. Data replicates to Atlan SaaS tenant storage : Using OAuth-based client credentials for secure authentication, the metadata is replicated from your storage to your specific Atlan SaaS tenant's storage infrastructure. Transfer happens via Dapr abstraction : The actual transfer occurs through Dapr, which handles the complexity of connecting to Atlan's data storage service within your tenant and routing data to Atlan's tenant-configured object storage. Processing and persistence in Atlan : Once the metadata reaches your Atlan SaaS tenant, it gets processed by Atlan workflows and persisted in the Atlan metastore where it becomes searchable and governable. How data stays encrypted â Data transfer to Atlan : All communication with Atlan services is encrypted in transit using TLS 1.2+ by default, protecting metadata as it moves from your infrastructure to Atlan. Your cloud storage : Any data written to your cloud storage systems like S3, Azure buckets, or GCS is encrypted at rest using your configured bucket encryption settings, typically AES-256. Atlan storage : When data reaches Atlan's infrastructure, Atlan encrypts your metadata using AES-256 encryption in its cloud storage buckets. How you can monitor operations â The Secure Agent provides visibility into all its operations so you can track what's happening, troubleshoot issues, and monitor performance. You can monitor using logs, traces, and metrics that capture detailed information about extraction job executions, interactions with Atlan services, and connections to your data sources and secret stores. You can store these logs in your own infrastructure (for example, using S3 buckets). The logs are already in OpenTelemetry Protocol (OTLP) format, which makes them compatible with popular monitoring systems and security information and event management (SIEM) tools. See also â Architecture : System components and overall data flow. Security - Logging and monitoring : Detailed security monitoring and observability features. Tags: secure-agent data-transfer Previous Authentication Next Deployment options How metadata moves to Atlan How data stays encrypted How you can monitor operations See also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/deployment-options",
    "text": "Secure Agent 2.0 Concepts Deployment options On this page Deployment options Deployment in Secure Agent 2.0 leverages containerization to run across any OCI-compliant runtime environment. Since each connector or application is containerized, you can deploy them on your existing container infrastructure without changing your operational practices. Understanding the deployment model helps you choose the right approach for your infrastructure and security requirements. Deployment artifacts â Atlan provides deployment artifacts for popular runtimes: Docker Runtime: Docker Compose files for direct container deployment Kubernetes: Helm charts compatible with both managed and self-hosted clusters The deployment chart includes sections where you can modify infrastructure-specific attributes like security context, resource constraints, labels and annotations, and environment variables. Container image â Each container image includes four key components that work in collaboration: Dapr sidecar process for service interaction Temporal Python SDK for connecting to Atlan's orchestrator FastAPI server for handling requests Core application logic implemented as Temporal workers For more details about how these components interact, see the Architecture - Core components reference. Once built, images are distributed through Atlan's Harbor open-source registry with vulnerability scanning, image signing for integrity verification, and secure distribution capabilities. Access to the Harbor registry requires authentication so only authorized personnel can pull or push images. See also â Verify container images : Step-by-step guide to verify image signatures and integrity. Security : Security considerations for deployment configurations. Tags: secure-agent deployment docker kubernetes helm Previous Data transfer and observability Next Secret management Deployment artifacts Container image See also"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/2.0/concepts/secret-management",
    "text": "Secure Agent 2.0 Concepts Secret management On this page Secret management Secret management in Secure Agent 2.0 ensures that credentials for source systems never leave your environment. By integrating with your existing secret vaults and fetching credentials just-in-time, the agent can authenticate with databases and applications without exposing sensitive credentials to external systems, including Atlan SaaS. How credentials are protected â Secure Agent deployed applications integrate with enterprise secret vaults for successful source system authentication. The key principle is that secrets are never stored locally or sent to Atlan and remain entirely within your organization's security perimeter. Supported secret stores include: AWS Secrets Manager Azure Key Vault GCP Secret Manager HashiCorp Vault Others supported secret stores How it works â Secrets (like database credentials) are fetched just-in-time via Dapr when the agent needs to connect to source systems. This dynamic access means credentials are only in memory during the specific authentication operation, then immediately discarded. The secret management makes sure that credentials remain entirely within your organization's security perimeter through: No local storage or transmission : Secrets are never stored locally or sent to Atlan Network isolation : Network isolation for secret store access Access controls : Access controls with principle of least privilege Audit logging : Audit logging for all secret access and modifications Regular reviews : Regular access reviews and backup/disaster recovery for secret stores See also â Authentication : How OAuth 2.0 credentials work with Atlan services. Security : Overall security architecture and controls. Tags: secure-agent secrets credentials dapr Previous Deployment options Next Deployment and security How credentials are protected How it works See also"
  }
]